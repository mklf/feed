<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-13T01:30:00Z">10-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition. (arXiv:2110.05571v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05571">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture has been well adopted as a dominant architecture
in most sequence transduction tasks including automatic speech recognition
(ASR), since its attention mechanism excels in capturing long-range
dependencies. While models built solely upon attention can be better
parallelized than regular RNN, a novel network architecture, SRU++, was
recently proposed. By combining the fast recurrence and attention mechanism,
SRU++ exhibits strong capability in sequence modeling and achieves
near-state-of-the-art results in various language modeling and machine
translation tasks with improved compute efficiency. In this work, we present
the advantages of applying SRU++ in ASR tasks by comparing with Conformer
across multiple ASR benchmarks and study how the benefits can be generalized to
long-form speech inputs. On the popular LibriSpeech benchmark, our SRU++ model
achieves 2.0% / 4.7% WER on test-clean / test-other, showing competitive
performances compared with the state-of-the-art Conformer encoder under the
same set-up. Specifically, SRU++ can surpass Conformer on long-form speech
input with a large margin, based on our analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Data Mining of Public Transport Incidents reported in Social Media. (arXiv:2110.05573v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05573">
<div class="article-summary-box-inner">
<span><p>Public transport agencies use social media as an essential tool for
communicating mobility incidents to passengers. However, while the short term,
day-to-day information about transport phenomena is usually posted in social
media with low latency, its availability is short term as the content is rarely
made an aggregated form. Social media communication of transport phenomena
usually lacks GIS annotations as most social media platforms do not allow
attaching non-POI GPS coordinates to posts. As a result, the analysis of
transport phenomena information is minimal. We collected three years of social
media posts of a polish public transport company with user comments. Through
exploration, we infer a six-class transport information typology. We
successfully build an information type classifier for social media posts,
detect stop names in posts, and relate them to GPS coordinates, obtaining a
spatial understanding of long-term aggregated phenomena. We show that our
approach enables citizen science and use it to analyze the impact of three
years of infrastructure incidents on passenger mobility, and the sentiment and
reaction scale towards each of the events. All these results are achieved for
Polish, an under-resourced language when it comes to spatial language
understanding, especially in social media contexts. To improve the situation,
we released two of our annotated data sets: social media posts with incident
type labels and matched stop names and social media comments with the annotated
sentiment. We also opensource the experimental codebase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing to New Domains by Mapping Natural Language to Lifted LTL. (arXiv:2110.05603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05603">
<div class="article-summary-box-inner">
<span><p>Recent work on using natural language to specify commands to robots has
grounded that language to LTL. However, mapping natural language task
specifications to LTL task specifications using language models require
probability distributions over finite vocabulary. Existing state-of-the-art
methods have extended this finite vocabulary to include unseen terms from the
input sequence to improve output generalization. However, novel
out-of-vocabulary atomic propositions cannot be generated using these methods.
To overcome this, we introduce an intermediate contextual query representation
which can be learned from single positive task specification examples,
associating a contextual query with an LTL template. We demonstrate that this
intermediate representation allows for generalization over unseen object
references, assuming accurate groundings are available. We compare our method
of mapping natural language task specifications to intermediate contextual
queries against state-of-the-art CopyNet models capable of translating natural
language to LTL, by evaluating whether correct LTL for manipulation and
navigation task specifications can be output, and show that our method
outperforms the CopyNet model on unseen object references. We demonstrate that
the grounded LTL our method outputs can be used for planning in a simulated
OO-MDP environment. Finally, we discuss some common failure modes encountered
when translating natural language task specifications to grounded LTL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TCube: Domain-Agnostic Neural Time-series Narration. (arXiv:2110.05633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05633">
<div class="article-summary-box-inner">
<span><p>The task of generating rich and fluent narratives that aptly describe the
characteristics, trends, and anomalies of time-series data is invaluable to the
sciences (geology, meteorology, epidemiology) or finance (trades, stocks, or
sales and inventory). The efforts for time-series narration hitherto are
domain-specific and use predefined templates that offer consistency but lead to
mechanical narratives. We present TCube (Time-series-to-text), a
domain-agnostic neural framework for time-series narration, that couples the
representation of essential time-series elements in the form of a dense
knowledge graph and the translation of said knowledge graph into rich and
fluent narratives through the transfer-learning capabilities of PLMs
(Pre-trained Language Models). TCube's design primarily addresses the challenge
that lies in building a neural framework in the complete paucity of annotated
training data for time-series. The design incorporates knowledge graphs as an
intermediary for the representation of essential time-series elements which can
be linearized for textual translation. To the best of our knowledge, TCube is
the first investigation of the use of neural strategies for time-series
narration. Through extensive evaluations, we show that TCube can improve the
lexical diversity of the generated narratives by up to 65.38% while still
maintaining grammatical integrity. The practicality and deployability of TCube
is further validated through an expert review (n=21) where 76.2% of
participating experts wary of auto-generated narratives favored TCube as a
deployable system for time-series narration due to its richer narratives. Our
code-base, models, and datasets, with detailed instructions for reproducibility
is publicly hosted at https://github.com/Mandar-Sharma/TCube.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Construction Grammars Converge Across Registers Given Increased Exposure. (arXiv:2110.05663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05663">
<div class="article-summary-box-inner">
<span><p>This paper measures the impact of increased exposure on whether learned
construction grammars converge onto shared representations when trained on data
from different registers. Register influences the frequency of constructions,
with some structures common in formal but not informal usage. We expect that a
grammar induction algorithm exposed to different registers will acquire
different constructions. To what degree does increased exposure lead to the
convergence of register-specific grammars? The experiments in this paper
simulate language learning in 12 languages (half Germanic and half Romance)
with corpora representing three registers (Twitter, Wikipedia, Web). These
simulations are repeated with increasing amounts of exposure, from 100k to 2
million words, to measure the impact of exposure on the convergence of
grammars. The results show that increased exposure does lead to converging
grammars across all languages. In addition, a shared core of register-universal
constructions remains constant across increasing amounts of exposure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are you doing what I say? On modalities alignment in ALFRED. (arXiv:2110.05665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05665">
<div class="article-summary-box-inner">
<span><p>ALFRED is a recently proposed benchmark that requires a model to complete
tasks in simulated house environments specified by instructions in natural
language. We hypothesize that key to success is accurately aligning the text
modality with visual inputs. Motivated by this, we inspect how well existing
models can align these modalities using our proposed intrinsic metric, boundary
adherence score (BAS). The results show the previous models are indeed failing
to perform proper alignment. To address this issue, we introduce approaches
aimed at improving model alignment and demonstrate how improved alignment,
improves end task performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05679">
<div class="article-summary-box-inner">
<span><p>Differentially Private (DP) learning has seen limited success for building
large deep learning models of text, and attempts at straightforwardly applying
Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have
resulted in large performance drops and high computational overhead. We show
that this performance drop can be mitigated with (1) the use of large
pretrained models; (2) hyperparameters that suit DP optimization; and (3)
fine-tuning objectives aligned with the pretraining procedure. With these
factors set right, we obtain private NLP models that outperform
state-of-the-art private training approaches and strong non-private baselines
-- by directly fine-tuning pretrained models with DP optimization on
moderately-sized corpora. To address the computational challenge of running
DP-SGD with large Transformers, we propose a memory saving technique that
allows clipping in DP-SGD to run without instantiating per-example gradients
for any layer in the model. The technique enables privately training
Transformers with almost the same memory cost as non-private training at a
modest run-time overhead. Contrary to conventional wisdom that DP optimization
fails at learning high-dimensional models (due to noise that scales with
dimension) empirical results reveal that private learning with pretrained
models tends to not suffer from dimension-dependent performance degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doubly-Trained Adversarial Data Augmentation for Neural Machine Translation. (arXiv:2110.05691v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05691">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation (NMT) models are known to suffer from noisy
inputs. To make models robust, we generate adversarial augmentation samples
that attack the model and preserve the source-side semantic meaning at the same
time. To generate such samples, we propose a doubly-trained architecture that
pairs two NMT models of opposite translation directions with a joint loss
function, which combines the target-side attack and the source-side semantic
similarity constraint. The results from our experiments across three different
language pairs and two evaluation metrics show that these adversarial samples
improve the model robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Releasing Annotator-Level Labels and Information in Datasets. (arXiv:2110.05699v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05699">
<div class="article-summary-box-inner">
<span><p>A common practice in building NLP datasets, especially using crowd-sourced
annotations, involves obtaining multiple annotator judgements on the same data
instances, which are then flattened to produce a single "ground truth" label or
score, through majority voting, averaging, or adjudication. While these
approaches may be appropriate in certain annotation tasks, such aggregations
overlook the socially constructed nature of human perceptions that annotations
for relatively more subjective tasks are meant to capture. In particular,
systematic disagreements between annotators owing to their socio-cultural
backgrounds and/or lived experiences are often obfuscated through such
aggregations. In this paper, we empirically demonstrate that label aggregation
may introduce representational biases of individual and group perspectives.
Based on this finding, we propose a set of recommendations for increased
utility and transparency of datasets for downstream use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05717">
<div class="article-summary-box-inner">
<span><p>Temporal language grounding in videos aims to localize the temporal span
relevant to the given query sentence. Previous methods treat it either as a
boundary regression task or a span extraction task. This paper will formulate
temporal language grounding into video reading comprehension and propose a
Relation-aware Network (RaNet) to address it. This framework aims to select a
video moment choice from the predefined answer set with the aid of
coarse-and-fine choice-query interaction and choice-choice relation
construction. A choice-query interactor is proposed to match the visual and
textual information simultaneously in sentence-moment and token-moment levels,
leading to a coarse-and-fine cross-modal interaction. Moreover, a novel
multi-choice relation constructor is introduced by leveraging graph convolution
to capture the dependencies among video moment choices for the best choice
selection. Extensive experiments on ActivityNet-Captions, TACoS, and
Charades-STA demonstrate the effectiveness of our solution. Codes will be
released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations. (arXiv:2110.05719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05719">
<div class="article-summary-box-inner">
<span><p>Majority voting and averaging are common approaches employed to resolve
annotator disagreements and derive single ground truth labels from multiple
annotations. However, annotators may systematically disagree with one another,
often reflecting their individual biases and values, especially in the case of
subjective tasks such as detecting affect, aggression, and hate speech.
Annotator disagreements may capture important nuances in such tasks that are
often ignored while aggregating annotations to a single ground truth. In order
to address this, we investigate the efficacy of multi-annotator models. In
particular, our multi-task based approach treats predicting each annotators'
judgements as separate subtasks, while sharing a common learned representation
of the task. We show that this approach yields same or better performance than
aggregating labels in the data prior to training across seven different binary
classification tasks. Our approach also provides a way to estimate uncertainty
in predictions, which we demonstrate better correlate with annotation
disagreements than traditional methods. Being able to model uncertainty is
especially useful in deployment scenarios where knowing when not to make a
prediction is important.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LightSeq: Accelerated Training for Transformer-based Models on GPUs. (arXiv:2110.05722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05722">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have proven to be powerful in many natural language,
computer vision, and speech recognition applications. It is expensive to train
these types of models due to unfixed input length, complex computation, and
large numbers of parameters. Existing systems either only focus on efficient
inference or optimize only BERT-like encoder models. In this paper, we present
LightSeq, a system for efficient training of Transformer-based models on GPUs.
We propose a series of GPU optimization techniques tailored to computation flow
and memory access patterns of neural layers in Transformers. LightSeq supports
a variety of network architectures, including BERT (encoder-only), GPT
(decoder-only), and Transformer (encoder-decoder). Our experiments on GPUs with
varying models and datasets show that LightSeq is 1.4-3.5x faster than previous
systems. In particular, it gains 308% training speedup compared with existing
systems on a large public machine translation benchmark (WMT14 English-German).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of Political Leanings of Chinese Speaking Twitter Users. (arXiv:2110.05723v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05723">
<div class="article-summary-box-inner">
<span><p>This work presents a supervised method for generating a classifier model of
the stances held by Chinese-speaking politicians and other Twitter users. Many
previous works of political tweets prediction exist on English tweets, but to
the best of our knowledge, this is the first work that builds prediction model
on Chinese political tweets. It firstly collects data by scraping tweets of
famous political figure and their related users. It secondly defines the
political spectrum in two groups: the group that shows approvals to the Chinese
Communist Party and the group that does not. Since there are not space between
words in Chinese to identify the independent words, it then completes
segmentation and vectorization by Jieba, a Chinese segmentation tool. Finally,
it trains the data collected from political tweets and produce a classification
model with high accuracy for understanding users' political stances from their
tweets on Twitter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anatomy of OntoGUM--Adapting GUM to the OntoNotes Scheme to Evaluate Robustness of SOTA Coreference Algorithms. (arXiv:2110.05727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05727">
<div class="article-summary-box-inner">
<span><p>SOTA coreference resolution produces increasingly impressive scores on the
OntoNotes benchmark. However lack of comparable data following the same scheme
for more genres makes it difficult to evaluate generalizability to open domain
data. Zhu et al. (2021) introduced the creation of the OntoGUM corpus for
evaluating geralizability of the latest neural LM-based end-to-end systems.
This paper covers details of the mapping process which is a set of
deterministic rules applied to the rich syntactic and discourse annotations
manually annotated in the GUM corpus. Out-of-domain evaluation across 12 genres
shows nearly 15-20% degradation for both deterministic and deep learning
systems, indicating a lack of generalizability or covert overfitting in
existing coreference resolution models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VarArray: Array-Geometry-Agnostic Continuous Speech Separation. (arXiv:2110.05745v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05745">
<div class="article-summary-box-inner">
<span><p>Continuous speech separation using a microphone array was shown to be
promising in dealing with the speech overlap problem in natural conversation
transcription. This paper proposes VarArray, an array-geometry-agnostic speech
separation neural network model. The proposed model is applicable to any number
of microphones without retraining while leveraging the nonlinear correlation
between the input channels. The proposed method adapts different elements that
were proposed before separately, including transform-average-concatenate,
conformer speech separation, and inter-channel phase differences, and combines
them in an efficient and cohesive way. Large-scale evaluation was performed
with two real meeting transcription tasks by using a fully developed
transcription system requiring no prior knowledge such as reference
segmentations, which allowed us to measure the impact that the continuous
speech separation system could have in realistic settings. The proposed model
outperformed a previous approach to array-geometry-agnostic modeling for all of
the geometry configurations considered, achieving asclite-based
speaker-agnostic word error rates of 17.5% and 20.4% for the AMI development
and evaluation sets, respectively, in the end-to-end setting using no
ground-truth segmentations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEPP: Similarity Estimation of Predicted Probabilities for Defending and Detecting Adversarial Text. (arXiv:2110.05748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05748">
<div class="article-summary-box-inner">
<span><p>There are two cases describing how a classifier processes input text, namely,
misclassification and correct classification. In terms of misclassified texts,
a classifier handles the texts with both incorrect predictions and adversarial
texts, which are generated to fool the classifier, which is called a victim.
Both types are misunderstood by the victim, but they can still be recognized by
other classifiers. This induces large gaps in predicted probabilities between
the victim and the other classifiers. In contrast, text correctly classified by
the victim is often successfully predicted by the others and induces small
gaps. In this paper, we propose an ensemble model based on similarity
estimation of predicted probabilities (SEPP) to exploit the large gaps in the
misclassified predictions in contrast to small gaps in the correct
classification. SEPP then corrects the incorrect predictions of the
misclassified texts. We demonstrate the resilience of SEPP in defending and
detecting adversarial texts through different types of victim classifiers,
classification tasks, and adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary. (arXiv:2110.05750v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05750">
<div class="article-summary-box-inner">
<span><p>Sports game summarization aims to generate news articles from live text
commentaries. A recent state-of-the-art work, SportsSum, not only constructs a
large benchmark dataset, but also proposes a two-step framework. Despite its
great contributions, the work has three main drawbacks: 1) the noise existed in
SportsSum dataset degrades the summarization performance; 2) the neglect of
lexical overlap between news and commentaries results in low-quality
pseudo-labeling algorithm; 3) the usage of directly concatenating rewritten
sentences to form news limits its practicability. In this paper, we publish a
new benchmark dataset SportsSum2.0, together with a modified summarization
framework. In particular, to obtain a clean dataset, we employ crowd workers to
manually clean the original dataset. Moreover, the degree of lexical overlap is
incorporated into the generation of pseudo labels. Further, we introduce a
reranker-enhanced summarizer to take into account the fluency and
expressiveness of the summarized news. Extensive experiments show that our
model outperforms the state-of-the-art baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware Pre-Training. (arXiv:2110.05752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05752">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) is a long-standing goal for speech processing,
since it utilizes large-scale unlabeled data and avoids extensive human
labeling. Recent years witness great successes in applying self-supervised
learning in speech recognition, while limited exploration was attempted in
applying SSL for modeling speaker characteristics. In this paper, we aim to
improve the existing SSL framework for speaker representation learning. Two
methods are introduced for enhancing the unsupervised speaker information
extraction. First, we apply the multi-task learning to the current SSL
framework, where we integrate the utterance-wise contrastive loss with the SSL
objective function. Second, for better speaker discrimination, we propose an
utterance mixing strategy for data augmentation, where additional overlapped
utterances are created unsupervisely and incorporate during training. We
integrate the proposed methods into the HuBERT framework. Experiment results on
SUPERB benchmark show that the proposed system achieves state-of-the-art
performance in universal representation learning, especially for speaker
identification oriented tasks. An ablation study is performed verifying the
efficacy of each proposed method. Finally, we scale up training dataset to 94
thousand hours public audio data and achieve further performance improvement in
all SUPERB tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Cognitive Factors in Lexical Decline. (arXiv:2110.05775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05775">
<div class="article-summary-box-inner">
<span><p>We adopt an evolutionary view on language change in which cognitive factors
(in addition to social ones) affect the fitness of words and their success in
the linguistic ecosystem. Specifically, we propose a variety of
psycholinguistic factors -- semantic, distributional, and phonological -- that
we hypothesize are predictive of lexical decline, in which words greatly
decrease in frequency over time. Using historical data across three languages
(English, French, and German), we find that most of our proposed factors show a
significant difference in the expected direction between each curated set of
declining words and their matched stable words. Moreover, logistic regression
analyses show that semantic and distributional factors are significant in
predicting declining words. Further diachronic analysis reveals that declining
words tend to decrease in the diversity of their lexical contexts over time,
gradually narrowing their 'ecological niches'.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">We've had this conversation before: A Novel Approach to Measuring Dialog Similarity. (arXiv:2110.05780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05780">
<div class="article-summary-box-inner">
<span><p>Dialog is a core building block of human natural language interactions. It
contains multi-party utterances used to convey information from one party to
another in a dynamic and evolving manner. The ability to compare dialogs is
beneficial in many real world use cases, such as conversation analytics for
contact center calls and virtual agent design.
</p>
<p>We propose a novel adaptation of the edit distance metric to the scenario of
dialog similarity. Our approach takes into account various conversation aspects
such as utterance semantics, conversation flow, and the participants. We
evaluate this new approach and compare it to existing document similarity
measures on two publicly available datasets. The results demonstrate that our
method outperforms the other approaches in capturing dialog flow, and is better
aligned with the human perception of conversation similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTraffic: A Robust BERT-Based Approach for Speaker Change Detection and Role Identification of Air-Traffic Communications. (arXiv:2110.05781v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05781">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) is gaining special interest in Air Traffic
Control (ATC). ASR allows transcribing the communications between air traffic
controllers (ATCOs) and pilots. These transcriptions are used to extract ATC
command types and named entities such as aircraft callsigns. One common problem
is when the Speech Activity Detection (SAD) or diarization system fails and
then two or more single speaker segments are in the same recording,
jeopardizing the overall system's performance. We developed a system that
combines the segmentation of a SAD module with a BERT-based model that performs
Speaker Change Detection (SCD) and Speaker Role Identification (SRI) based on
ASR transcripts (i.e., diarization + SRI). This research demonstrates on a
real-life ATC test set that performing diarization directly on textual data
surpass acoustic level diarization. The proposed model reaches up to
~0.90/~0.95 F1-score on ATCO/pilot for SRI on several test sets. The text-based
diarization system brings a 27% relative improvement on Diarization Error Rate
(DER) compared to standard acoustic-based diarization. These results were on
ASR transcripts of a challenging ATC test set with an estimated ~13% word error
rate, validating the approach's robustness even on noisy ASR transcripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting TTS models For New Speakers using Transfer Learning. (arXiv:2110.05798v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05798">
<div class="article-summary-box-inner">
<span><p>Training neural text-to-speech (TTS) models for a new speaker typically
requires several hours of high quality speech data. Prior works on voice
cloning attempt to address this challenge by adapting pre-trained multi-speaker
TTS models for a new voice, using a few minutes of speech data of the new
speaker. However, publicly available large multi-speaker datasets are often
noisy, thereby resulting in TTS models that are not suitable for use in
products. We address this challenge by proposing transfer-learning guidelines
for adapting high quality single-speaker TTS models for a new speaker, using
only a few minutes of speech data. We conduct an extensive study using
different amounts of data for a new speaker and evaluate the synthesized speech
in terms of naturalness and voice/style similarity to the target speaker. We
find that fine-tuning a single-speaker TTS model on just 30 minutes of data,
can yield comparable performance to a model trained from scratch on more than
27 hours of data for both male and female target speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Average and Worst-case Accuracy in Multitask Learning. (arXiv:2110.05838v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05838">
<div class="article-summary-box-inner">
<span><p>When training and evaluating machine learning models on a large number of
tasks, it is important to not only look at average task accuracy -- which may
be biased by easy or redundant tasks -- but also worst-case accuracy (i.e. the
performance on the task with the lowest accuracy). In this work, we show how to
use techniques from the distributionally robust optimization (DRO) literature
to improve worst-case performance in multitask learning. We highlight several
failure cases of DRO when applied off-the-shelf and present an improved method,
Lookahead-DRO (L-DRO), which mitigates these issues. The core idea of L-DRO is
to anticipate the interaction between tasks during training in order to choose
a dynamic re-weighting of the various task losses, which will (i) lead to
minimal worst-case loss and (ii) train on as many tasks as possible. After
demonstrating the efficacy of L-DRO on a small controlled synthetic setting, we
evaluate it on two realistic benchmarks: a multitask version of the CIFAR-100
image classification dataset and a large-scale multilingual language modeling
experiment. Our empirical results show that L-DRO achieves a better trade-off
between average and worst-case accuracy with little computational overhead
compared to several strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Abstractive Summarisation Models with Machine Translation in Deliberative Processes. (arXiv:2110.05847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05847">
<div class="article-summary-box-inner">
<span><p>We present work on summarising deliberative processes for non-English
languages. Unlike commonly studied datasets, such as news articles, this
deliberation dataset reflects difficulties of combining multiple narratives,
mostly of poor grammatical quality, in a single text. We report an extensive
evaluation of a wide range of abstractive summarisation models in combination
with an off-the-shelf machine translation model. Texts are translated into
English, summarised, and translated back to the original language. We obtain
promising results regarding the fluency, consistency and relevance of the
summaries produced. Our approach is easy to implement for many languages for
production purposes by simply changing the translation model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">text2sdg: An open-source solution to monitoring sustainable development goals from text. (arXiv:2110.05856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05856">
<div class="article-summary-box-inner">
<span><p>Monitoring progress on the United Nations Sustainable Development Goals
(SDGs) is important for both academic and non-academic organizations. Existing
approaches to monitoring SDGs have focused on specific data types, namely,
publications listed in proprietary research databases. We present the text2sdg
R package, a user-friendly, open-source package that detects SDGs in any kind
of text data using several different query systems from any text source. The
text2sdg package thereby facilitates the monitoring of SDGs for a wide array of
text sources and provides a much-needed basis for validating and improving
extant methods to detect SDGs from text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetricGAN-U: Unsupervised speech enhancement/ dereverberation based only on noisy/ reverberated speech. (arXiv:2110.05866v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05866">
<div class="article-summary-box-inner">
<span><p>Most of the deep learning-based speech enhancement models are learned in a
supervised manner, which implies that pairs of noisy and clean speech are
required during training. Consequently, several noisy speeches recorded in
daily life cannot be used to train the model. Although certain unsupervised
learning frameworks have also been proposed to solve the pair constraint, they
still require clean speech or noise for training. Therefore, in this paper, we
propose MetricGAN-U, which stands for MetricGAN-unsupervised, to further
release the constraint from conventional unsupervised learning. In MetricGAN-U,
only noisy speech is required to train the model by optimizing non-intrusive
speech quality metrics. The experimental results verified that MetricGAN-U
outperforms baselines in both objective and subjective metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages. (arXiv:2110.05877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05877">
<div class="article-summary-box-inner">
<span><p>AI technologies for Natural Languages have made tremendous progress recently.
However, commensurate progress has not been made on Sign Languages, in
particular, in recognizing signs as individual words or as complete sentences.
We introduce OpenHands, a library where we take four key ideas from the NLP
community for low-resource languages and apply them to sign languages for
word-level recognition. First, we propose using pose extracted through
pretrained models as the standard modality of data to reduce training time and
enable efficient inference, and we release standardized pose datasets for 6
different sign languages - American, Argentinian, Chinese, Greek, Indian, and
Turkish. Second, we train and release checkpoints of 4 pose-based isolated sign
language recognition models across all 6 languages, providing baselines and
ready checkpoints for deployment. Third, to address the lack of labelled data,
we propose self-supervised pretraining on unlabelled data. We curate and
release the largest pose-based pretraining dataset on Indian Sign Language
(Indian-SL). Fourth, we compare different pretraining strategies and for the
first time establish that pretraining is effective for sign language
recognition by demonstrating (a) improved fine-tuning performance especially in
low-resource settings, and (b) high crosslingual transfer from Indian-SL to few
other sign languages. We open-source all models and datasets in OpenHands with
a hope that it makes research in sign languages more accessible, available here
at https://github.com/AI4Bharat/OpenHands .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigation on Data Adaptation Techniques for Neural Named Entity Recognition. (arXiv:2110.05892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05892">
<div class="article-summary-box-inner">
<span><p>Data processing is an important step in various natural language processing
tasks. As the commonly used datasets in named entity recognition contain only a
limited number of samples, it is important to obtain additional labeled data in
an efficient and reliable manner. A common practice is to utilize large
monolingual unlabeled corpora. Another popular technique is to create synthetic
data from the original labeled data (data augmentation). In this work, we
investigate the impact of these two methods on the performance of three
different named entity recognition tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05896">
<div class="article-summary-box-inner">
<span><p>Trained on the large corpus, pre-trained language models (PLMs) can capture
different levels of concepts in context and hence generate universal language
representations. They can benefit multiple downstream natural language
processing (NLP) tasks. Although PTMs have been widely used in most NLP
applications, especially for high-resource languages such as English, it is
under-represented in Lao NLP research. Previous work on Lao has been hampered
by the lack of annotated datasets and the sparsity of language resources. In
this work, we construct a text classification dataset to alleviate the
resource-scare situation of the Lao language. We additionally present the first
transformer-based PTMs for Lao with four versions: BERT-small, BERT-base,
ELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:
part-of-speech tagging and text classification. Experiments demonstrate the
effectiveness of our Lao models. We will release our models and datasets to the
community, hoping to facilitate the future development of Lao NLP applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Order Does Not Matter For Speech Recognition. (arXiv:2110.05994v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05994">
<div class="article-summary-box-inner">
<span><p>In this paper, we study training of automatic speech recognition system in a
weakly supervised setting where the order of words in transcript labels of the
audio training data is not known. We train a word-level acoustic model which
aggregates the distribution of all output frames using LogSumExp operation and
uses a cross-entropy loss to match with the ground-truth words distribution.
Using the pseudo-labels generated from this model on the training set, we then
train a letter-based acoustic model using Connectionist Temporal Classification
loss. Our system achieves 2.4%/5.3% on test-clean/test-other subsets of
LibriSpeech, which is competitive with the supervised baseline's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer. (arXiv:2110.05999v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05999">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances in applying pre-trained language models to
generate high-quality texts, generating long passages that maintain long-range
coherence is yet challenging for these models. In this paper, we propose
DiscoDVT, a discourse-aware discrete variational Transformer to tackle the
incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes
the global structure of the text and then applies it to guide the generation
process at each decoding step. To further embed discourse-aware information
into the discrete latent representations, we introduce an auxiliary objective
to model the discourse relations within the text. We conduct extensive
experiments on two open story generation datasets and demonstrate that the
latent codes learn meaningful correspondence to the discourse structures that
guide the model to generate long texts with better long-range coherence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Model Supervised by Understanding Map. (arXiv:2110.06043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06043">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects. (arXiv:2110.06078v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06078">
<div class="article-summary-box-inner">
<span><p>A popular approach to decompose the neural bases of language consists in
correlating, across individuals, the brain responses to different stimuli (e.g.
regular speech versus scrambled words, sentences, or paragraphs). Although
successful, this `model-free' approach necessitates the acquisition of a large
and costly set of neuroimaging data. Here, we show that a model-based approach
can reach equivalent results within subjects exposed to natural stimuli. We
capitalize on the recently-discovered similarities between deep language models
and the human brain to compute the mapping between i) the brain responses to
regular speech and ii) the activations of deep language models elicited by
modified stimuli (e.g. scrambled words, sentences, or paragraphs). Our
model-based approach successfully replicates the seminal study of Lerner et al.
(2011), which revealed the hierarchy of language areas by comparing the
functional-magnetic resonance imaging (fMRI) of seven subjects listening to
7min of both regular and scrambled narratives. We further extend and precise
these results to the brain signals of 305 individuals listening to 4.1 hours of
narrated stories. Overall, this study paves the way for efficient and flexible
analyses of the brain bases of language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A large scale lexical and semantic analysis of Spanish language variations in Twitter. (arXiv:2110.06128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06128">
<div class="article-summary-box-inner">
<span><p>Dialectometry is a discipline devoted to studying the variations of a
language around a geographical region. One of their goals is the creation of
linguistic atlases capturing the similarities and differences of the language
under study around the area in question. For instance, Spanish is one of the
most spoken languages across the world, but not necessarily Spanish is written
and spoken in the same way in different countries. This manuscript presents a
broad analysis describing lexical and semantic relationships among 26
Spanish-speaking countries around the globe. For this study, we analyze
four-year of the Twitter geotagged public stream to provide an extensive survey
of the Spanish language vocabularies of different countries, its distributions,
semantic usage of terms, and emojis. We also offer open regional word-embedding
resources for Spanish Twitter to help other researchers and practitioners take
advantage of regionalized models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Feelings of People Regarding COVID-19 by Social Network Mining. (arXiv:2110.06151v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06151">
<div class="article-summary-box-inner">
<span><p>In 2020, COVID-19 became the chief concern of the world and is still
reflected widely in all social networks. Each day, users post millions of
tweets and comments on this subject, which contain significant implicit
information about the public opinion. In this regard, a dataset of
COVID-related tweets in English language is collected, which consists of more
than two million tweets from March 23 to June 23 of 2020 to extract the
feelings of the people in various countries in the early stages of this
outbreak. To this end, first, we use a lexicon-based approach in conjunction
with the GeoNames geographic database to label the tweets with their locations.
Next, a method based on the recently introduced and widely cited RoBERTa model
is proposed to analyze their sentimental content. After that, the trend graphs
of the frequency of tweets as well as sentiments are produced for the world and
the nations that were more engaged with COVID-19. Graph analysis shows that the
frequency graphs of the tweets for the majority of nations are significantly
correlated with the official statistics of the daily afflicted in them.
Moreover, several implicit knowledge is extracted and discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mention Memory: incorporating textual knowledge into Transformers through entity mention attention. (arXiv:2110.06176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06176">
<div class="article-summary-box-inner">
<span><p>Natural language understanding tasks such as open-domain question answering
often require retrieving and assimilating factual information from multiple
sources. We propose to address this problem by integrating a semi-parametric
representation of a large text corpus into a Transformer model as a source of
factual knowledge. Specifically, our method represents knowledge with `mention
memory', a table of dense vector representations of every entity mention in a
corpus. The proposed model - TOME - is a Transformer that accesses the
information through internal memory layers in which each entity mention in the
input passage attends to the mention memory. This approach enables synthesis of
and reasoning over many disparate sources of information within a single
Transformer model. In experiments using a memory of 150 million Wikipedia
mentions, TOME achieves strong performance on several open-domain
knowledge-intensive tasks, including the claim verification benchmarks HoVer
and FEVER and several entity-based QA benchmarks. We also show that the model
learns to attend to informative mentions without any direct supervision.
Finally we demonstrate that the model can generalize to new unseen entities by
updating the memory without retraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12804">
<div class="article-summary-box-inner">
<span><p>This work demonstrates that using the objective with independence assumption
for modelling the span probability $P(a_s,a_e) = P(a_s)P(a_e)$ of span starting
at position $a_s$ and ending at position $a_e$ has adverse effects. Therefore
we propose multiple approaches to modelling joint probability $P(a_s,a_e)$
directly. Among those, we propose a compound objective, composed from the joint
probability while still keeping the objective with independence assumption as
an auxiliary objective. We find that the compound objective is consistently
superior or equal to other assumptions in exact match. Additionally, we
identified common errors caused by the assumption of independence and manually
checked the counterpart predictions, demonstrating the impact of the compound
objective on the real examples. Our findings are supported via experiments with
three extractive QA models (BIDAF, BERT, ALBERT) over six datasets and our
code, individual results and manual analysis are available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimal Supervision for Morphological Inflection. (arXiv:2104.08512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08512">
<div class="article-summary-box-inner">
<span><p>Neural models for the various flavours of morphological inflection tasks have
proven to be extremely accurate given ample labeled data -- data that may be
slow and costly to obtain. In this work we aim to overcome this annotation
bottleneck by bootstrapping labeled data from a seed as little as {\em five}
labeled paradigms, accompanied by a large bulk of unlabeled text. Our approach
exploits different kinds of regularities in morphological systems in a
two-phased setup, where word tagging based on {\em analogies} is followed by
word pairing based on {\em distances}. We experiment with the Paradigm Cell
Filling Problem over eight typologically different languages, and find that, in
languages with relatively simple morphology, orthographic regularities on their
own allow inflection models to achieve respectable accuracy. Combined
orthographic and semantic regularities alleviate difficulties with particularly
complex morpho-phonological systems. Our results suggest that hand-crafting
many tagged examples might be an unnecessary effort. However, more work is
needed in order to address rarely used forms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding. (arXiv:2104.12763v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12763">
<div class="article-summary-box-inner">
<span><p>Multi-modal reasoning systems rely on a pre-trained object detector to
extract regions of interest from the image. However, this crucial module is
typically used as a black box, trained independently of the downstream task and
on a fixed vocabulary of objects and attributes. This makes it challenging for
such systems to capture the long tail of visual concepts expressed in free form
text. In this paper we propose MDETR, an end-to-end modulated detector that
detects objects in an image conditioned on a raw text query, like a caption or
a question. We use a transformer-based architecture to reason jointly over text
and image by fusing the two modalities at an early stage of the model. We
pre-train the network on 1.3M text-image pairs, mined from pre-existing
multi-modal datasets having explicit alignment between phrases in text and
objects in the image. We then fine-tune on several downstream tasks such as
phrase grounding, referring expression comprehension and segmentation,
achieving state-of-the-art results on popular benchmarks. We also investigate
the utility of our model as an object detector on a given label set when
fine-tuned in a few-shot setting. We show that our pre-training approach
provides a way to handle the long tail of object categories which have very few
labelled instances. Our approach can be easily extended for visual question
answering, achieving competitive performance on GQA and CLEVR. The code and
models are available at https://github.com/ashkamath/mdetr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prosodic segmentation for parsing spoken dialogue. (arXiv:2105.12667v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12667">
<div class="article-summary-box-inner">
<span><p>Parsing spoken dialogue poses unique difficulties, including disfluencies and
unmarked boundaries between sentence-like units. Previous work has shown that
prosody can help with parsing disfluent speech (Tran et al. 2018), but has
assumed that the input to the parser is already segmented into sentence-like
units (SUs), which isn't true in existing speech applications. We investigate
how prosody affects a parser that receives an entire dialogue turn as input (a
turn-based model), instead of gold standard pre-segmented SUs (an SU-based
model). In experiments on the English Switchboard corpus, we find that when
using transcripts alone, the turn-based model has trouble segmenting SUs,
leading to worse parse performance than the SU-based model. However, prosody
can effectively replace gold standard SU boundaries: with prosody, the
turn-based model performs as well as the SU-based model (90.79 vs. 90.65 F1
score, respectively), despite performing two tasks (SU segmentation and
parsing) rather than one (parsing alone). Analysis shows that pitch and
intensity features are the most important for this corpus, since they allow the
model to correctly distinguish an SU boundary from a speech disfluency -- a
distinction that the model otherwise struggles to make.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. (arXiv:2106.05707v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05707">
<div class="article-summary-box-inner">
<span><p>Fact verification has attracted a lot of attention in the machine learning
and natural language processing communities, as it is one of the key methods
for detecting misinformation. Existing large-scale benchmarks for this task
have focused mostly on textual sources, i.e. unstructured information, and thus
ignored the wealth of information available in structured formats, such as
tables. In this paper we introduce a novel dataset and benchmark, Fact
Extraction and VERification Over Unstructured and Structured information
(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated
with evidence in the form of sentences and/or cells from tables in Wikipedia,
as well as a label indicating whether this evidence supports, refutes, or does
not provide enough information to reach a verdict. Furthermore, we detail our
efforts to track and minimize the biases present in the dataset and could be
exploited by models, e.g. being able to predict the label without using
evidence. Finally, we develop a baseline for verifying claims against text and
tables which predicts both the correct evidence and verdict for 18% of the
claims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding. (arXiv:2106.07250v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07250">
<div class="article-summary-box-inner">
<span><p>In knowledge graph embedding, the theoretical relationship between the
softmax cross-entropy and negative sampling loss functions has not been
investigated. This makes it difficult to fairly compare the results of the two
different loss functions. We attempted to solve this problem by using the
Bregman divergence to provide a unified interpretation of the softmax
cross-entropy and negative sampling loss functions. Under this interpretation,
we can derive theoretical findings for fair comparison. Experimental results on
the FB15k-237 and WN18RR datasets show that the theoretical findings are valid
in practical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07306">
<div class="article-summary-box-inner">
<span><p>A major challenge in structured prediction is to represent the
interdependencies within output structures. When outputs are structured as
sequences, linear-chain conditional random fields (CRFs) are a widely used
model class which can learn \textit{local} dependencies in the output. However,
the CRF's Markov assumption makes it impossible for CRFs to represent
distributions with \textit{nonlocal} dependencies, and standard CRFs are unable
to respect nonlocal constraints of the data (such as global arity constraints
on output labels). We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding. We prove that
constrained training is never worse than constrained decoding, and show
empirically that it can be substantially better in practice. Additionally, we
demonstrate a practical benefit on downstream tasks by incorporating a RegCCRF
into a deep neural model for semantic role labeling, exceeding state-of-the-art
results on a standard dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01951">
<div class="article-summary-box-inner">
<span><p>The task of learning from only a few examples (called a few-shot setting) is
of key importance and relevance to a real-world setting. For question answering
(QA), the current state-of-the-art pre-trained models typically need
fine-tuning on tens of thousands of examples to obtain good results. Their
performance degrades significantly in a few-shot setting (&lt; 100 examples). To
address this, we propose a simple fine-tuning framework that leverages
pre-trained text-to-text models and is directly aligned with their pre-training
framework. Specifically, we construct the input as a concatenation of the
question, a mask token representing the answer span and a context. Given this
input, the model is fine-tuned using the same objective as that of its
pre-training objective. Through experimental studies on various few-shot
configurations, we show that this formulation leads to significant gains on
multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when
there are only 16 training examples). The gains extend further when used with
larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)
and translate well to a multilingual setting . On the multilingual TydiQA
benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of
upto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64
training examples). We conduct detailed ablation studies to analyze factors
contributing to these gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Voice Activated Framework using Self-supervised Learning. (arXiv:2110.01077v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01077">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning methods such as wav2vec 2.0 have shown promising
results in learning speech representations from unlabelled and untranscribed
speech data that are useful for speech recognition. Since these representations
are learned without any task-specific supervision, they can also be useful for
other voice-activated tasks like speaker verification, keyword spotting,
emotion classification etc. In our work, we propose a general purpose framework
for adapting a pre-trained wav2vec 2.0 model for different voice-activated
tasks. We develop downstream network architectures that operate on the
contextualized speech representations of wav2vec 2.0 to adapt the
representations for solving a given task. Finally, we extend our framework to
perform multi-task learning by jointly optimizing the network parameters on
multiple voice activated tasks using a shared transformer backbone. Both of our
single and multi-task frameworks achieve state-of-the-art results in speaker
verification and keyword spotting benchmarks. Our best performing models
achieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on VoxCeleb2 and
VoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0
keyword spotting dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Lexical Normalization with Multilingual Transformers. (arXiv:2110.02869v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02869">
<div class="article-summary-box-inner">
<span><p>Current benchmark tasks for natural language processing contain text that is
qualitatively different from the text used in informal day to day digital
communication. This discrepancy has led to severe performance degradation of
state-of-the-art NLP models when fine-tuned on real-world data. One way to
resolve this issue is through lexical normalization, which is the process of
transforming non-standard text, usually from social media, into a more
standardized form. In this work, we propose a sentence-level
sequence-to-sequence model based on mBART, which frames the problem as a
machine translation problem. As the noisy text is a pervasive problem across
languages, not just English, we leverage the multi-lingual pre-training of
mBART to fine-tune it to our data. While current approaches mainly operate at
the word or subword level, we argue that this approach is straightforward from
a technical standpoint and builds upon existing pre-trained transformer
networks. Our results show that while word-level, intrinsic, performance
evaluation is behind other methods, our model improves performance on
extrinsic, downstream tasks through normalization compared to models operating
on raw, unprocessed, social media text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03370">
<div class="article-summary-box-inner">
<span><p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus
consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly
labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in
total. We collect the data from YouTube and Podcast, which covers a variety of
speaking styles, scenarios, domains, topics, and noisy conditions. An optical
character recognition (OCR) based method is introduced to generate the
audio/text segmentation candidates for the YouTube data on its corresponding
video captions, while a high-quality ASR transcription system is used to
generate audio/text pair candidates for the Podcast data. Then we propose a
novel end-to-end label error detection approach to further validate and filter
the candidates. We also provide three manually labelled high-quality test sets
along with WenetSpeech for evaluation -- Dev for cross-validation purpose in
training, Test_Net, collected from Internet for matched test, and
Test\_Meeting, recorded from real meetings for more challenging mismatched
test. Baseline systems trained with WenetSpeech are provided for three popular
speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition
results on the three test sets are also provided as benchmarks. To the best of
our knowledge, WenetSpeech is the current largest open-sourced Mandarin speech
corpus with transcriptions, which benefits research on production-level speech
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taming Sparsely Activated Transformer with Stochastic Experts. (arXiv:2110.04260v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04260">
<div class="article-summary-box-inner">
<span><p>Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can
easily scale to have outrageously large amounts of parameters without
significant increase in computational cost. However, SAMs are reported to be
parameter inefficient such that larger models do not always lead to better
performance. While most on-going research focuses on improving SAMs models by
exploring methods of routing inputs to experts, our analysis reveals that such
research might not lead to the solution we expect, i.e., the commonly-used
routing methods based on gating mechanisms do not work better than randomly
routing inputs to experts. In this paper, we propose a new expert-based model,
THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,
such as the Switch Transformer, experts in THOR are randomly activated for each
input during training and inference. THOR models are trained using a
consistency regularized loss, where experts learn not only from training data
but also from other experts as teachers, such that all the experts make
consistent predictions. We validate the effectiveness of THOR on machine
translation tasks. Results show that THOR models are more parameter efficient
in that they significantly outperform the Transformer and MoE models across
various settings. For example, in multilingual translation, THOR outperforms
the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as
that of a state-of-the-art MoE model that is 18 times larger. Our code is
publicly available at:
https://github.com/microsoft/Stochastic-Mixture-of-Experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning. (arXiv:2110.04725v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04725">
<div class="article-summary-box-inner">
<span><p>Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot
and Few-Shot learning on many natural language processing (NLP) tasks by
scaling up model size, dataset size and the amount of computation. However,
training a model like GPT-3 requires huge amount of computational resources
which makes it challengeable to researchers. In this work, we propose a method
that incorporates large-scale distributed training performance into model
architecture design. With this method, Yuan 1.0, the current largest singleton
language model with 245B parameters, achieves excellent performance on
thousands GPUs during training, and the state-of-the-art results on NLP tasks.
A data processing method is designed to efficiently filter massive amount of
raw data. The current largest high-quality Chinese corpus with 5TB high quality
texts is built based on this method. In addition, a calibration and label
expansion method is proposed to improve the Zero-Shot and Few-Shot performance,
and steady improvement is observed on the accuracy of various tasks. Yuan 1.0
presents strong capacity of natural language generation, and the generated
articles are difficult to distinguish from the human-written ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advances in Multi-turn Dialogue Comprehension: A Survey. (arXiv:2110.04984v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04984">
<div class="article-summary-box-inner">
<span><p>Training machines to understand natural language and interact with humans is
an elusive and essential task of artificial intelligence. A diversity of
dialogue systems has been designed with the rapid development of deep learning
techniques, especially the recent pre-trained language models (PrLMs). Among
these studies, the fundamental yet challenging type of task is dialogue
comprehension whose role is to teach the machines to read and comprehend the
dialogue context before responding. In this paper, we review the previous
methods from the technical perspective of dialogue modeling for the dialogue
comprehension task. We summarize the characteristics and challenges of dialogue
comprehension in contrast to plain-text reading comprehension. Then, we discuss
three typical patterns of dialogue modeling. In addition, we categorize
dialogue-related pre-training techniques which are employed to enhance PrLMs in
dialogue scenarios. Finally, we highlight the technical advances in recent
years and point out the lessons from the empirical analysis and the prospects
towards a new frontier of researches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05006">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have been the de facto paradigm for most
natural language processing (NLP) tasks. This also benefits biomedical domain:
researchers from informatics, medicine, and computer science (CS) communities
propose various PLMs trained on biomedical datasets, e.g., biomedical text,
electronic health records, protein, and DNA sequences for various biomedical
tasks. However, the cross-discipline characteristics of biomedical PLMs hinder
their spreading among communities; some existing works are isolated from each
other without comprehensive comparison and discussions. It expects a survey
that not only systematically reviews recent advances of biomedical PLMs and
their applications but also standardizes terminology and benchmarks. In this
paper, we summarize the recent progress of pre-trained language models in the
biomedical domain and their applications in biomedical downstream tasks.
Particularly, we discuss the motivations and propose a taxonomy of existing
biomedical PLMs. Their applications in biomedical downstream tasks are
exhaustively discussed. At last, we illustrate various limitations and future
trends, which we hope can provide inspiration for the future research of the
research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViSeRet: A simple yet effective approach to moment retrieval via fine-grained video segmentation. (arXiv:2110.05146v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05146">
<div class="article-summary-box-inner">
<span><p>Video-text retrieval has many real-world applications such as media
analytics, surveillance, and robotics. This paper presents the 1st place
solution to the video retrieval track of the ICCV VALUE Challenge 2021. We
present a simple yet effective approach to jointly tackle two video-text
retrieval tasks (video retrieval and video corpus moment retrieval) by
leveraging the model trained only on the video retrieval task. In addition, we
create an ensemble model that achieves the new state-of-the-art performance on
all four datasets (TVr, How2r, YouCook2r, and VATEXr) presented in the VALUE
Challenge.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">UnfairGAN: An Enhanced Generative Adversarial Network for Raindrop Removal from A Single Image. (arXiv:2110.05523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05523">
<div class="article-summary-box-inner">
<span><p>Image deraining is a new challenging problem in real-world applications, such
as autonomous vehicles. In a bad weather condition of heavy rainfall,
raindrops, mainly hitting glasses or windshields, can significantly reduce
observation ability. Moreover, raindrops spreading over the glass can yield
refraction's physical effect, which seriously impedes the sightline or
undermine machine learning systems. In this paper, we propose an enhanced
generative adversarial network to deal with the challenging problems of
raindrops. UnfairGAN is an enhanced generative adversarial network that can
utilize prior high-level information, such as edges and rain estimation, to
boost deraining performance. To demonstrate UnfairGAN, we introduce a large
dataset for training deep learning models of rain removal. The experimental
results show that our proposed method is superior to other state-of-the-art
approaches of deraining raindrops regarding quantitative metrics and visual
quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development and testing of an image transformer for explainable autonomous driving systems. (arXiv:2110.05559v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05559">
<div class="article-summary-box-inner">
<span><p>In the last decade, deep learning (DL) approaches have been used successfully
in computer vision (CV) applications. However, DL-based CV models are generally
considered to be black boxes due to their lack of interpretability. This black
box behavior has exacerbated user distrust and therefore has prevented
widespread deployment DLCV models in autonomous driving tasks even though some
of these models exhibit superiority over human performance. For this reason, it
is essential to develop explainable DL models for autonomous driving task.
Explainable DL models can not only boost user trust in autonomy but also serve
as a diagnostic approach to identify anydefects and weaknesses of the model
during the system development phase. In this paper, we propose an explainable
end-to-end autonomous driving system based on "Transformer", a state-of-the-art
(SOTA) self-attention based model, to map visual features from images collected
by onboard cameras to guide potential driving actions with corresponding
explanations. The model achieves a soft attention over the global features of
the image. The results demonstrate the efficacy of our proposed model as it
exhibits superior performance (in terms of correct prediction of actions and
explanations) compared to the benchmark model by a significant margin with
lower computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UrbanNet: Leveraging Urban Maps for Long Range 3D Object Detection. (arXiv:2110.05561v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05561">
<div class="article-summary-box-inner">
<span><p>Relying on monocular image data for precise 3D object detection remains an
open problem, whose solution has broad implications for cost-sensitive
applications such as traffic monitoring. We present UrbanNet, a modular
architecture for long range monocular 3D object detection with static cameras.
Our proposed system combines commonly available urban maps along with a mature
2D object detector and an efficient 3D object descriptor to accomplish accurate
detection at long range even when objects are rotated along any of their three
axes. We evaluate UrbanNet on a novel challenging synthetic dataset and
highlight the advantages of its design for traffic detection in roads with
changing slope, where the flat ground approximation does not hold. Data and
code are available at https://github.com/TRAILab/UrbanNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EchoVPR: Echo State Networks for Visual Place Recognition. (arXiv:2110.05572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05572">
<div class="article-summary-box-inner">
<span><p>Recognising previously visited locations is an important, but unsolved, task
in autonomous navigation. Current visual place recognition (VPR) benchmarks
typically challenge models to recover the position of a query image (or images)
from sequential datasets that include both spatial and temporal components.
Recently, Echo State Network (ESN) varieties have proven particularly powerful
at solving machine learning tasks that require spatio-temporal modelling. These
networks are simple, yet powerful neural architectures that -- exhibiting
memory over multiple time-scales and non-linear high-dimensional
representations -- can discover temporal relations in the data while still
maintaining linearity in the learning. In this paper, we present a series of
ESNs and analyse their applicability to the VPR problem. We report that the
addition of ESNs to pre-processed convolutional neural networks led to a
dramatic boost in performance in comparison to non-recurrent networks in four
standard benchmarks (GardensPoint, SPEDTest, ESSEX3IN1, Nordland) demonstrating
that ESNs are able to capture the temporal structure inherent in VPR problems.
Moreover, we show that ESNs can outperform class-leading VPR models which also
exploit the sequential dynamics of the data. Finally, our results demonstrate
that ESNs also improve generalisation abilities, robustness, and accuracy
further supporting their suitability to VPR applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo. (arXiv:2110.05594v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05594">
<div class="article-summary-box-inner">
<span><p>We present a modern solution to the multi-view photometric stereo problem
(MVPS). Our work suitably exploits the image formation model in a MVPS
experimental setup to recover the dense 3D reconstruction of an object from
images. We procure the surface orientation using a photometric stereo (PS)
image formation model and blend it with a multi-view neural radiance field
representation to recover the object's surface geometry. Contrary to the
previous multi-staged framework to MVPS, where the position, iso-depth
contours, or orientation measurements are estimated independently and then
fused later, our method is simple to implement and realize. Our method performs
neural rendering of multi-view images while utilizing surface normals estimated
by a deep photometric stereo network. We render the MVPS images by considering
the object's surface normals for each 3D sample point along the viewing
direction rather than explicitly using the density gradient in the volume space
via 3D occupancy information. We optimize the proposed neural radiance field
representation for the MVPS setup efficiently using a fully connected deep
network to recover the 3D geometry of an object. Extensive evaluation on the
DiLiGenT-MV benchmark dataset shows that our method performs better than the
approaches that perform only PS or only multi-view stereo (MVS) and provides
comparable results against the state-of-the-art multi-stage fusion methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architecture Search for Efficient Uncalibrated Deep Photometric Stereo. (arXiv:2110.05621v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05621">
<div class="article-summary-box-inner">
<span><p>We present an automated machine learning approach for uncalibrated
photometric stereo (PS). Our work aims at discovering lightweight and
computationally efficient PS neural networks with excellent surface normal
accuracy. Unlike previous uncalibrated deep PS networks, which are handcrafted
and carefully tuned, we leverage differentiable neural architecture search
(NAS) strategy to find uncalibrated PS architecture automatically. We begin by
defining a discrete search space for a light calibration network and a normal
estimation network, respectively. We then perform a continuous relaxation of
this search space and present a gradient-based optimization strategy to find an
efficient light calibration and normal estimation network. Directly applying
the NAS methodology to uncalibrated PS is not straightforward as certain
task-specific constraints must be satisfied, which we impose explicitly.
Moreover, we search for and train the two networks separately to account for
the Generalized Bas-Relief (GBR) ambiguity. Extensive experiments on the
DiLiGenT dataset show that the automatically searched neural architectures
performance compares favorably with the state-of-the-art uncalibrated PS
methods while having a lower memory footprint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameterizing Activation Functions for Adversarial Robustness. (arXiv:2110.05626v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05626">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are known to be vulnerable to adversarially perturbed
inputs. A commonly used defense is adversarial training, whose performance is
influenced by model capacity. While previous works have studied the impact of
varying model width and depth on robustness, the impact of increasing capacity
by using learnable parametric activation functions (PAFs) has not been studied.
We study how using learnable PAFs can improve robustness in conjunction with
adversarial training. We first ask the question: how should we incorporate
parameters into activation functions to improve robustness? To address this, we
analyze the direct impact of activation shape on robustness through PAFs and
observe that activation shapes with positive outputs on negative inputs and
with high finite curvature can increase robustness. We combine these properties
to create a new PAF, which we call Parametric Shifted Sigmoidal Linear Unit
(PSSiLU). We then combine PAFs (including PReLU, PSoftplus and PSSiLU) with
adversarial training and analyze robust performance. We find that PAFs optimize
towards activation shape properties found to directly affect robustness.
Additionally, we find that while introducing only 1-2 learnable parameters into
the network, smooth PAFs can significantly increase robustness over ReLU. For
instance, when trained on CIFAR-10 with additional synthetic data, PSSiLU
improves robust accuracy by 4.54% over ReLU on ResNet-18 and 2.69% over ReLU on
WRN-28-10 in the $\ell_{\infty}$ threat model while adding only 2 additional
parameters into the network architecture. The PSSiLU WRN-28-10 model achieves
61.96% AutoAttack accuracy, improving over the state-of-the-art robust accuracy
on RobustBench (Croce et al., 2020).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Robust PCA: A Scalable Deep Unfolding Approach for High-Dimensional Outlier Detection. (arXiv:2110.05649v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05649">
<div class="article-summary-box-inner">
<span><p>Robust principal component analysis (RPCA) is a critical tool in modern
machine learning, which detects outliers in the task of low-rank matrix
reconstruction. In this paper, we propose a scalable and learnable non-convex
approach for high-dimensional RPCA problems, which we call Learned Robust PCA
(LRPCA). LRPCA is highly efficient, and its free parameters can be effectively
learned to optimize via deep unfolding. Moreover, we extend deep unfolding from
finite iterations to infinite iterations via a novel
feedforward-recurrent-mixed neural network model. We establish the recovery
guarantee of LRPCA under mild assumptions for RPCA. Numerical experiments show
that LRPCA outperforms the state-of-the-art RPCA algorithms, such as ScaledGD
and AltProj, on both synthetic datasets and real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image. (arXiv:2110.05655v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05655">
<div class="article-summary-box-inner">
<span><p>We present a method that takes as input a single dual-pixel image, and
simultaneously estimates the image's defocus map -- the amount of defocus blur
at each pixel -- and recovers an all-in-focus image. Our method is inspired
from recent works that leverage the dual-pixel sensors available in many
consumer cameras to assist with autofocus, and use them for recovery of defocus
maps or all-in-focus images. These prior works have solved the two recovery
problems independently of each other, and often require large labeled datasets
for supervised training. By contrast, we show that it is beneficial to treat
these two closely-connected problems simultaneously. To this end, we set up an
optimization problem that, by carefully modeling the optics of dual-pixel
images, jointly solves both problems. We use data captured with a consumer
smartphone camera to demonstrate that, after a one-time calibration step, our
approach improves upon prior works for both defocus map estimation and blur
removal, despite being entirely unsupervised.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate and Generalizable Quantitative Scoring of Liver Steatosis from Ultrasound Images via Scalable Deep Learning. (arXiv:2110.05664v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05664">
<div class="article-summary-box-inner">
<span><p>Background &amp; Aims: Hepatic steatosis is a major cause of chronic liver
disease. 2D ultrasound is the most widely used non-invasive tool for screening
and monitoring, but associated diagnoses are highly subjective. We developed a
scalable deep learning (DL) algorithm for quantitative scoring of liver
steatosis from 2D ultrasound images.
</p>
<p>Approach &amp; Results: Using retrospectively collected multi-view ultrasound
data from 3,310 patients, 19,513 studies, and 228,075 images, we trained a DL
algorithm to diagnose steatosis stages (healthy, mild, moderate, or severe)
from ultrasound diagnoses. Performance was validated on two multi-scanner
unblinded and blinded (initially to DL developer) histology-proven cohorts (147
and 112 patients) with histopathology fatty cell percentage diagnoses, and a
subset with FibroScan diagnoses. We also quantified reliability across scanners
and viewpoints. Results were evaluated using Bland-Altman and receiver
operating characteristic (ROC) analysis. The DL algorithm demonstrates
repeatable measurements with a moderate number of images (3 for each viewpoint)
and high agreement across 3 premium ultrasound scanners. High diagnostic
performance was observed across all viewpoints: area under the curves of the
ROC to classify &gt;=mild, &gt;=moderate, =severe steatosis grades were 0.85, 0.90,
and 0.93, respectively. The DL algorithm outperformed or performed at least
comparably to FibroScan with statistically significant improvements for all
levels on the unblinded histology-proven cohort, and for =severe steatosis on
the blinded histology-proven cohort.
</p>
<p>Conclusions: The DL algorithm provides a reliable quantitative steatosis
assessment across view and scanners on two multi-scanner cohorts. Diagnostic
performance was high with comparable or better performance than FibroScan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search. (arXiv:2110.05668v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05668">
<div class="article-summary-box-inner">
<span><p>Most existing neural architecture search (NAS) benchmarks and algorithms
prioritize performance on well-studied tasks, e.g., image classification on
CIFAR and ImageNet. This makes the applicability of NAS approaches in more
diverse areas inadequately understood. In this paper, we present NAS-Bench-360,
a benchmark suite for evaluating state-of-the-art NAS methods for convolutional
neural networks (CNNs). To construct it, we curate a collection of ten tasks
spanning a diverse array of application domains, dataset sizes, problem
dimensionalities, and learning objectives. By carefully selecting tasks that
can both interoperate with modern CNN-based search methods but that are also
far-afield from their original development domain, we can use NAS-Bench-360 to
investigate the following central question: do existing state-of-the-art NAS
methods perform well on diverse tasks? Our experiments show that a modern NAS
procedure designed for image classification can indeed find good architectures
for tasks with other dimensionalities and learning objectives; however, the
same method struggles against more task-specific methods and performs
catastrophically poorly on classification in non-vision domains. The case for
NAS robustness becomes even more dire in a resource-constrained setting, where
a recent NAS method provides little-to-no benefit over much simpler baselines.
These results demonstrate the need for a benchmark such as NAS-Bench-360 to
help develop NAS approaches that work well on a variety of tasks, a crucial
component of a truly robust and automated pipeline. We conclude with a
demonstration of the kind of future research our suite of tasks will enable.
All data and code is made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Heatmap-based Landmark Detection. (arXiv:2110.05676v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05676">
<div class="article-summary-box-inner">
<span><p>Mitral valve repair is a very difficult operation, often requiring
experienced surgeons. The doctor will insert a prosthetic ring to aid in the
restoration of heart function. The location of the prosthesis' sutures is
critical. Obtaining and studying them during the procedure is a valuable
learning experience for new surgeons. This paper proposes a landmark detection
network for detecting sutures in endoscopic pictures, which solves the problem
of a variable number of suture points in the images. Because there are two
datasets, one from the simulated domain and the other from real intraoperative
data, this work uses cycleGAN to interconvert the images from the two domains
to obtain a larger dataset and a better score on real intraoperative data. This
paper performed the tests using a simulated dataset of 2708 photos and a real
dataset of 2376 images. The mean sensitivity on the simulated dataset is about
75.64% and the precision is about 73.62%. The mean sensitivity on the real
dataset is about 50.23% and the precision is about 62.76%. The data is from the
AdaptOR MICCAI Challenge 2021, which can be found at
https://zenodo.org/record/4646979\#.YO1zLUxCQ2x.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No way to crop: On robust image crop localization. (arXiv:2110.05687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05687">
<div class="article-summary-box-inner">
<span><p>Previous image forensics schemes for crop detection are only limited on
predicting whether an image has been cropped. This paper presents a novel
scheme for image crop localization using robust watermarking. We further extend
our scheme to detect tampering attack on the attacked image. We demonstrate
that our scheme is the first to provide high-accuracy and robust image crop
localization. Besides, the accuracy of tamper detection is comparable to many
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inclusive Design: Accessibility Settings for People with Cognitive Disabilities. (arXiv:2110.05688v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05688">
<div class="article-summary-box-inner">
<span><p>The advancement of technology has progressed faster than any other field in
the world and with the development of these new technologies, it is important
to make sure that these tools can be used by everyone, including people with
disabilities. Accessibility options in computing devices help ensure that
everyone has the same access to advanced technologies. Unfortunately, for those
who require more unique and sometimes challenging accommodations, such as
people with Amyotrophic lateral sclerosis ( ALS), the most commonly used
accessibility features are simply not enough. While assistive technology for
those with ALS does exist, it requires multiple peripheral devices that can
become quite expensive collectively. The purpose of this paper is to suggest a
more affordable and readily available option for ALS assistive technology that
can be implemented on a smartphone or tablet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hiding Images into Images with Real-world Robustness. (arXiv:2110.05689v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05689">
<div class="article-summary-box-inner">
<span><p>The existing image embedding networks are basically vulnerable to malicious
attacks such as JPEG compression and noise adding, not applicable for
real-world copyright protection tasks. To solve this problem, we introduce a
generative deep network based method for hiding images into images while
assuring high-quality extraction from the destructive synthesized images. An
embedding network is sequentially concatenated with an attack layer, a
decoupling network and an image extraction network. The addition of decoupling
network learns to extract the embedded watermark from the attacked image. We
also pinpoint the weaknesses of the adversarial training for robustness in
previous works and build our improved real-world attack simulator. Experimental
results demonstrate the superiority of the proposed method against typical
digital attacks by a large margin, as well as the performance boost of the
recovered images with the aid of progressive recovery strategy. Besides, we are
the first to robustly hide three secret images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Modeling for Task Recognition and Action Segmentation in Weakly-Labeled Instructional Videos. (arXiv:2110.05697v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05697">
<div class="article-summary-box-inner">
<span><p>This paper focuses on task recognition and action segmentation in
weakly-labeled instructional videos, where only the ordered sequence of
video-level actions is available during training. We propose a two-stream
framework, which exploits semantic and temporal hierarchies to recognize
top-level tasks in instructional videos. Further, we present a novel top-down
weakly-supervised action segmentation approach, where the predicted task is
used to constrain the inference of fine-grained action sequences. Experimental
results on the popular Breakfast and Cooking 2 datasets show that our
two-stream hierarchical task modeling significantly outperforms existing
methods in top-level task recognition for all datasets and metrics.
Additionally, using our task recognition framework in the proposed top-down
action segmentation approach consistently improves the state of the art, while
also reducing segmentation inference time by 80-90 percent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Exploring and Improving Robustness of Scene Text Detection Models. (arXiv:2110.05700v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05700">
<div class="article-summary-box-inner">
<span><p>It is crucial to understand the robustness of text detection models with
regard to extensive corruptions, since scene text detection techniques have
many practical applications. For systematically exploring this problem, we
propose two datasets from which to evaluate scene text detection models:
ICDAR2015-C (IC15-C) and CTW1500-C (CTW-C). Our study extends the investigation
of the performance and robustness of the proposed region proposal, regression
and segmentation-based scene text detection frameworks. Furthermore, we perform
a robustness analysis of six key components: pre-training data, backbone,
feature fusion module, multi-scale predictions, representation of text
instances and loss function. Finally, we present a simple yet effective
data-based method to destroy the smoothness of text regions by merging
background and foreground, which can significantly increase the robustness of
different text detection networks. We hope that this study will provide valid
data points as well as experience for future research. Benchmark, code and data
will be made available at
\url{https://github.com/wushilian/robust-scene-text-detection-benchmark}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Fusion Prior for Multi-Focus Image Super Resolution Fusion. (arXiv:2110.05706v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05706">
<div class="article-summary-box-inner">
<span><p>This paper unifies the multi-focus images fusion (MFIF) and blind super
resolution (SR) problems as the multi-focus image super resolution fusion
(MFISRF) task, and proposes a novel unified dataset-free unsupervised framework
named deep fusion prior (DFP) to address such MFISRF task. DFP consists of
SKIPnet network, DoubleReblur focus measurement tactic, decision embedding
module and loss functions. In particular, DFP can obtain MFISRF only from two
low-resolution inputs without any extent dataset; SKIPnet implementing
unsupervised learning via deep image prior is an end-to-end generated network
acting as the engine of DFP; DoubleReblur is used to determine the primary
decision map without learning but based on estimated PSF and Gaussian kernels
convolution; decision embedding module optimizes the decision map via learning;
and DFP losses composed of content loss, joint gradient loss and gradient limit
loss can obtain high-quality MFISRF results robustly. Experiments have proved
that our proposed DFP approaches and even outperforms those state-of-art MFIF
and SR method combinations. Additionally, DFP is a general framework, thus its
networks and focus measurement tactics can be continuously updated to further
improve the MFISRF performance. DFP codes are open source and will be available
soon at <a href="http://github.com/GuYuanjie/DeepFusionPrior.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05717">
<div class="article-summary-box-inner">
<span><p>Temporal language grounding in videos aims to localize the temporal span
relevant to the given query sentence. Previous methods treat it either as a
boundary regression task or a span extraction task. This paper will formulate
temporal language grounding into video reading comprehension and propose a
Relation-aware Network (RaNet) to address it. This framework aims to select a
video moment choice from the predefined answer set with the aid of
coarse-and-fine choice-query interaction and choice-choice relation
construction. A choice-query interactor is proposed to match the visual and
textual information simultaneously in sentence-moment and token-moment levels,
leading to a coarse-and-fine cross-modal interaction. Moreover, a novel
multi-choice relation constructor is introduced by leveraging graph convolution
to capture the dependencies among video moment choices for the best choice
selection. Extensive experiments on ActivityNet-Captions, TACoS, and
Charades-STA demonstrate the effectiveness of our solution. Codes will be
released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Spatial Route Prior in Vision-and-Language Navigation. (arXiv:2110.05728v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05728">
<div class="article-summary-box-inner">
<span><p>Vision-and-language navigation (VLN) is a trending topic which aims to
navigate an intelligent agent to an expected position through natural language
instructions. This work addresses the task of VLN from a previously-ignored
aspect, namely the spatial route prior of the navigation scenes. A critically
enabling innovation of this work is explicitly considering the spatial route
prior under several different VLN settings. In a most information-rich case of
knowing environment maps and admitting shortest-path prior, we observe that
given an origin-destination node pair, the internal route can be uniquely
determined. Thus, VLN can be effectively formulated as an ordinary
classification problem over all possible destination nodes in the scenes.
Furthermore, we relax it to other more general VLN settings, proposing a
sequential-decision variant (by abandoning the shortest-path route prior) and
an explore-and-exploit scheme (for addressing the case of not knowing the
environment maps) that curates a compact and informative sub-graph to exploit.
As reported by [34], the performance of VLN methods has been stuck at a plateau
in past two years. Even with increased model complexity, the state-of-the-art
success rate on R2R validation-unseen set has stayed around 62% for single-run
and 73% for beam-search with model-ensemble. We have conducted comprehensive
evaluations on both R2R and R4R, and surprisingly found that utilizing the
spatial route priors may be the key of breaking above-mentioned performance
ceiling. For example, on R2R validation-unseen set, when the number of discrete
nodes explored is about 40, our single-model success rate reaches 73%, and
increases to 78% if a Speaker model is ensembled, which significantly outstrips
previous state-of-the-art VLN-BERT with 3 models ensembled.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Scene Graph Generation by Attention Distillation from Caption. (arXiv:2110.05731v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05731">
<div class="article-summary-box-inner">
<span><p>If an image tells a story, the image caption is the briefest narrator.
Generally, a scene graph prefers to be an omniscient generalist, while the
image caption is more willing to be a specialist, which outlines the gist. Lots
of previous studies have found that a scene graph is not as practical as
expected unless it can reduce the trivial contents and noises. In this respect,
the image caption is a good tutor. To this end, we let the scene graph borrow
the ability from the image caption so that it can be a specialist on the basis
of remaining all-around, resulting in the so-called Topic Scene Graph. What an
image caption pays attention to is distilled and passed to the scene graph for
estimating the importance of partial objects, relationships, and events.
Specifically, during the caption generation, the attention about individual
objects in each time step is collected, pooled, and assembled to obtain the
attention about relationships, which serves as weak supervision for
regularizing the estimated importance scores of relationships. In addition, as
this attention distillation process provides an opportunity for combining the
generation of image caption and scene graph together, we further transform the
scene graph into linguistic form with rich and free-form expressions by sharing
a single generation model with image caption. Experiments show that attention
distillation brings significant improvements in mining important relationships
without strong supervision, and the topic scene graph shows great potential in
subsequent applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Efficient Multi-Agent Cooperative Visual Exploration. (arXiv:2110.05734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05734">
<div class="article-summary-box-inner">
<span><p>We consider the task of visual indoor exploration with multiple agents, where
the agents need to cooperatively explore the entire indoor region using as few
steps as possible. Classical planning-based methods often suffer from
particularly expensive computation at each inference step and a limited
expressiveness of cooperation strategy. By contrast, reinforcement learning
(RL) has become a trending paradigm for tackling this challenge due to its
modeling capability of arbitrarily complex strategies and minimal inference
overhead. We extend the state-of-the-art single-agent RL solution, Active
Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based
global-goal planner, Spatial Coordination Planner (SCP), which leverages
spatial information from each individual agent in an end-to-end manner and
effectively guides the agents to navigate towards different spatial goals with
high exploration efficiency. SCP consists of a transformer-based relation
encoder to capture intra-agent interactions and a spatial action decoder to
produce accurate goals. In addition, we also implement a few multi-agent
enhancements to process local information from each agent for an aligned
spatial representation and more precise planning. Our final solution,
Multi-Agent Active Neural SLAM (MAANS), combines all these techniques and
substantially outperforms 4 different planning-based methods and various RL
baselines in the photo-realistic physical testbed, Habitat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Refinement of Low-level Feature Based Activation Map for Weakly Supervised Object Localization. (arXiv:2110.05741v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05741">
<div class="article-summary-box-inner">
<span><p>We present a two-stage learning framework for weakly supervised object
localization (WSOL). While most previous efforts rely on high-level feature
based CAMs (Class Activation Maps), this paper proposes to localize objects
using the low-level feature based activation maps. In the first stage, an
activation map generator produces activation maps based on the low-level
feature maps in the classifier, such that rich contextual object information is
included in an online manner. In the second stage, we employ an evaluator to
evaluate the activation maps predicted by the activation map generator. Based
on this, we further propose a weighted entropy loss, an attentive erasing, and
an area loss to drive the activation map generator to substantially reduce the
uncertainty of activations between object and background, and explore less
discriminative regions. Based on the low-level object information preserved in
the first stage, the second stage model gradually generates a well-separated,
complete, and compact activation map of object in the image, which can be
easily thresholded for accurate localization. Extensive experiments on
CUB-200-2011 and ImageNet-1K datasets show that our framework surpasses
previous methods by a large margin, which sets a new state-of-the-art for WSOL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seamless Copy Move Manipulation in Digital Images. (arXiv:2110.05747v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05747">
<div class="article-summary-box-inner">
<span><p>The importance and relevance of digital image forensics has attracted
researchers to establish different techniques for creating as well as detecting
forgeries. The core category in passive image forgery is copy-move image
forgery that affects the originality of image by applying a different
transformation. In this paper frequency domain image manipulation method is
being presented.The method exploits the localized nature of discrete wavelet
transform (DWT) to get hold of the region of the host image to be manipulated.
Both the patch and host image are subjected to DWT at the same level $l$ to get
$3l + 1$ sub-bands and each sub-band of the patch is pasted to the identified
region in the corresponding sub-band of the host image. The resultant
manipulated host sub-bands are then subjected to inverse DWT to get the final
manipulated host image. The proposed method shows good resistance against
detection by two frequency domain forgery detection methods from the
literature. The purpose of this research work is to create the forgery and
highlight the need to produce forgery detection methods that are robust against
the malicious copy-move forgery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Damage Building Using Real-time Crowdsourced Images and Transfer Learning. (arXiv:2110.05762v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05762">
<div class="article-summary-box-inner">
<span><p>After significant earthquakes, we can see images posted on social media
platforms by individuals and media agencies owing to the mass usage of
smartphones these days. These images can be utilized to provide information
about the shaking damage in the earthquake region both to the public and
research community, and potentially to guide rescue work. This paper presents
an automated way to extract the damaged building images after earthquakes from
social media platforms such as Twitter and thus identify the particular user
posts containing such images. Using transfer learning and ~6500 manually
labelled images, we trained a deep learning model to recognize images with
damaged buildings in the scene. The trained model achieved good performance
when tested on newly acquired images of earthquakes at different locations and
ran in near real-time on Twitter feed after the 2020 M7.0 earthquake in Turkey.
Furthermore, to better understand how the model makes decisions, we also
implemented the Grad-CAM method to visualize the important locations on the
images that facilitate the decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents. (arXiv:2110.05769v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05769">
<div class="article-summary-box-inner">
<span><p>Communication between embodied AI agents has received increasing attention in
recent years. Despite its use, it is still unclear whether the learned
communication is interpretable and grounded in perception. To study the
grounding of emergent forms of communication, we first introduce the
collaborative multi-object navigation task CoMON. In this task, an oracle agent
has detailed environment information in the form of a map. It communicates with
a navigator agent that perceives the environment visually and is tasked to find
a sequence of goals. To succeed at the task, effective communication is
essential. CoMON hence serves as a basis to study different communication
mechanisms between heterogeneous agents, that is, agents with different
capabilities and roles. We study two common communication mechanisms and
analyze their communication patterns through an egocentric and spatial lens. We
show that the emergent communication can be grounded to the agent observations
and the spatial structure of the 3D environment. Video summary:
https://youtu.be/kLv2rxO9t0g
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperCube: Implicit Field Representations of Voxelized 3D Models. (arXiv:2110.05770v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05770">
<div class="article-summary-box-inner">
<span><p>Recently introduced implicit field representations offer an effective way of
generating 3D object shapes. They leverage implicit decoder trained to take a
3D point coordinate concatenated with a shape encoding and to output a value
which indicates whether the point is outside the shape or not. Although this
approach enables efficient rendering of visually plausible objects, it has two
significant limitations. First, it is based on a single neural network
dedicated for all objects from a training set which results in a cumbersome
training procedure and its application in real life. More importantly, the
implicit decoder takes only points sampled within voxels (and not the entire
voxels) which yields problems at the classification boundaries and results in
empty spaces within the rendered mesh.
</p>
<p>To solve the above limitations, we introduce a new HyperCube architecture
based on interval arithmetic network, that enables direct processing of 3D
voxels, trained using a hypernetwork paradigm to enforce model convergence.
Instead of processing individual 3D samples from within a voxel, our approach
allows to input the entire voxel (3D cube) represented with its convex hull
coordinates, while the target network constructed by a hypernet assigns it to
an inside or outside category. As a result our HyperCube model outperforms the
competing approaches both in terms of training and inference efficiency, as
well as the final mesh quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDWNet: A Straight Dilated Network with Wavelet Transformation for Image Deblurring. (arXiv:2110.05803v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05803">
<div class="article-summary-box-inner">
<span><p>Image deblurring is a classical computer vision problem that aims to recover
a sharp image from a blurred image. To solve this problem, existing methods
apply the Encode-Decode architecture to design the complex networks to make a
good performance. However, most of these methods use repeated up-sampling and
down-sampling structures to expand the receptive field, which results in
texture information loss during the sampling process and some of them design
the multiple stages that lead to difficulties with convergence. Therefore, our
model uses dilated convolution to enable the obtainment of the large receptive
field with high spatial resolution. Through making full use of the different
receptive fields, our method can achieve better performance. On this basis, we
reduce the number of up-sampling and down-sampling and design a simple network
structure. Besides, we propose a novel module using the wavelet transform,
which effectively helps the network to recover clear high-frequency texture
details. Qualitative and quantitative evaluations of real and synthetic
datasets show that our deblurring method is comparable to existing algorithms
in terms of performance with much lower training requirements. The source code
and pre-trained models are available at https://github.com/FlyEgle/SDWNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Satellite Image Semantic Segmentation. (arXiv:2110.05812v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05812">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a method for the automatic semantic segmentation of
satellite images into six classes (sparse forest, dense forest, moor,
herbaceous formation, building, and road). We rely on Swin Transformer
architecture and build the dataset from IGN open data. We report quantitative
and qualitative segmentation results on this dataset and discuss strengths and
limitations. The dataset and the trained model are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-Based high-speed low-latency fiducial marker tracking. (arXiv:2110.05819v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05819">
<div class="article-summary-box-inner">
<span><p>Motion and dynamic environments, especially under challenging lighting
conditions, are still an open issue for robust robotic applications. In this
paper, we propose an end-to-end pipeline for real-time, low latency, 6
degrees-of-freedom pose estimation of fiducial markers. Instead of achieving a
pose estimation through a conventional frame-based approach, we employ the
high-speed abilities of event-based sensors to directly refine the spatial
transformation, using consecutive events. Furthermore, we introduce a novel
two-way verification process for detecting tracking errors by backtracking the
estimated pose, allowing us to evaluate the quality of our tracking. This
approach allows us to achieve pose estimation at a rate up to 156~kHz, while
only relying on CPU resources. The average end-to-end latency of our method is
3~ms. Experimental results demonstrate outstanding potential for robotic tasks,
such as visual servoing in fast action-perception loops.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AVoE: A Synthetic 3D Dataset on Understanding Violation of Expectation for Artificial Cognition. (arXiv:2110.05836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05836">
<div class="article-summary-box-inner">
<span><p>Recent work in cognitive reasoning and computer vision has engendered an
increasing popularity for the Violation-of-Expectation (VoE) paradigm in
synthetic datasets. Inspired by work in infant psychology, researchers have
started evaluating a model's ability to discriminate between expected and
surprising scenes as a sign of its reasoning ability. Existing VoE-based 3D
datasets in physical reasoning only provide vision data. However, current
cognitive models of physical reasoning by psychologists reveal infants create
high-level abstract representations of objects and interactions. Capitalizing
on this knowledge, we propose AVoE: a synthetic 3D VoE-based dataset that
presents stimuli from multiple novel sub-categories for five event categories
of physical reasoning. Compared to existing work, AVoE is armed with
ground-truth labels of abstract features and rules augmented to vision data,
paving the way for high-level symbolic predictions in physical reasoning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLNet: Plane and Line Priors for Unsupervised Indoor Depth Estimation. (arXiv:2110.05839v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05839">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning of depth from indoor monocular videos is challenging as
the artificial environment contains many textureless regions. Fortunately, the
indoor scenes are full of specific structures, such as planes and lines, which
should help guide unsupervised depth learning. This paper proposes PLNet that
leverages the plane and line priors to enhance the depth estimation. We first
represent the scene geometry using local planar coefficients and impose the
smoothness constraint on the representation. Moreover, we enforce the planar
and linear consistency by randomly selecting some sets of points that are
probably coplanar or collinear to construct simple and effective consistency
losses. To verify the proposed method's effectiveness, we further propose to
evaluate the flatness and straightness of the predicted point cloud on the
reliable planar and linear regions. The regularity of these regions indicates
quality indoor reconstruction. Experiments on NYU Depth V2 and ScanNet show
that PLNet outperforms existing methods. The code is available at
\url{https://github.com/HalleyJiang/PLNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Adversarial Semi-supervised Learning. (arXiv:2110.05848v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05848">
<div class="article-summary-box-inner">
<span><p>In this paper we exploit Semi-Supervised Learning (SSL) to increase the
amount of training data to improve the performance of Fine-Grained Visual
Categorization (FGVC). This problem has not been investigated in the past in
spite of prohibitive annotation costs that FGVC requires. Our approach
leverages unlabeled data with an adversarial optimization strategy in which the
internal features representation is obtained with a second-order pooling model.
This combination allows to back-propagate the information of the parts,
represented by second-order pooling, onto unlabeled data in an adversarial
training setting. We demonstrate the effectiveness of the combined use by
conducting experiments on six state-of-the-art fine-grained datasets, which
include Aircrafts, Stanford Cars, CUB-200-2011, Oxford Flowers, Stanford Dogs,
and the recent Semi-Supervised iNaturalist-Aves. Experimental results clearly
show that our proposed method has better performance than the only previous
approach that examined this problem; it also obtained higher classification
accuracy with respect to the supervised learning methods with which we
compared.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Binary Neural Networks through Fully Utilizing Latent Weights. (arXiv:2110.05850v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05850">
<div class="article-summary-box-inner">
<span><p>Binary Neural Networks (BNNs) rely on a real-valued auxiliary variable W to
help binary training. However, pioneering binary works only use W to accumulate
gradient updates during backward propagation, which can not fully exploit its
power and may hinder novel advances in BNNs. In this work, we explore the role
of W in training besides acting as a latent variable. Notably, we propose to
add W into the computation graph, making it perform as a real-valued feature
extractor to aid the binary training. We make different attempts on how to
utilize the real-valued weights and propose a specialized supervision.
Visualization experiments qualitatively verify the effectiveness of our
approach in making it easier to distinguish between different categories.
Quantitative experiments show that our approach outperforms current
state-of-the-arts, further closing the performance gap between floating-point
networks and BNNs. Evaluation on ImageNet with ResNet-18 (Top-1 63.4%),
ResNet-34 (Top-1 67.0%) achieves new state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Learning On The Hierarchy Representation for Fine-Grained Human Action Recognition. (arXiv:2110.05853v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05853">
<div class="article-summary-box-inner">
<span><p>Fine-grained human action recognition is a core research topic in computer
vision. Inspired by the recently proposed hierarchy representation of
fine-grained actions in FineGym and SlowFast network for action recognition, we
propose a novel multi-task network which exploits the FineGym hierarchy
representation to achieve effective joint learning and prediction for
fine-grained human action recognition. The multi-task network consists of three
pathways of SlowOnly networks with gradually increased frame rates for events,
sets and elements of fine-grained actions, followed by our proposed integration
layers for joint learning and prediction. It is a two-stage approach, where it
first learns deep feature representation at each hierarchical level, and is
followed by feature encoding and fusion for multi-task learning. Our empirical
results on the FineGym dataset achieve a new state-of-the-art performance, with
91.80% Top-1 accuracy and 88.46% mean accuracy for element actions, which are
3.40% and 7.26% higher than the previous best results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Networks Are Not Invariant to Translation, but They Can Learn to Be. (arXiv:2110.05861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05861">
<div class="article-summary-box-inner">
<span><p>When seeing a new object, humans can immediately recognize it across
different retinal locations: the internal object representation is invariant to
translation. It is commonly believed that Convolutional Neural Networks (CNNs)
are architecturally invariant to translation thanks to the convolution and/or
pooling operations they are endowed with. In fact, several studies have found
that these networks systematically fail to recognise new objects on untrained
locations. In this work, we test a wide variety of CNNs architectures showing
how, apart from DenseNet-121, none of the models tested was architecturally
invariant to translation. Nevertheless, all of them could learn to be invariant
to translation. We show how this can be achieved by pretraining on ImageNet,
and it is sometimes possible with much simpler data sets when all the items are
fully translated across the input canvas. At the same time, this invariance can
be disrupted by further training due to catastrophic forgetting/interference.
These experiments show how pretraining a network on an environment with the
right `latent' characteristics (a more naturalistic environment) can result in
the network learning deep perceptual rules which would dramatically improve
subsequent generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages. (arXiv:2110.05877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05877">
<div class="article-summary-box-inner">
<span><p>AI technologies for Natural Languages have made tremendous progress recently.
However, commensurate progress has not been made on Sign Languages, in
particular, in recognizing signs as individual words or as complete sentences.
We introduce OpenHands, a library where we take four key ideas from the NLP
community for low-resource languages and apply them to sign languages for
word-level recognition. First, we propose using pose extracted through
pretrained models as the standard modality of data to reduce training time and
enable efficient inference, and we release standardized pose datasets for 6
different sign languages - American, Argentinian, Chinese, Greek, Indian, and
Turkish. Second, we train and release checkpoints of 4 pose-based isolated sign
language recognition models across all 6 languages, providing baselines and
ready checkpoints for deployment. Third, to address the lack of labelled data,
we propose self-supervised pretraining on unlabelled data. We curate and
release the largest pose-based pretraining dataset on Indian Sign Language
(Indian-SL). Fourth, we compare different pretraining strategies and for the
first time establish that pretraining is effective for sign language
recognition by demonstrating (a) improved fine-tuning performance especially in
low-resource settings, and (b) high crosslingual transfer from Indian-SL to few
other sign languages. We open-source all models and datasets in OpenHands with
a hope that it makes research in sign languages more accessible, available here
at https://github.com/AI4Bharat/OpenHands .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fourier-based Video Prediction through Relational Object Motion. (arXiv:2110.05881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05881">
<div class="article-summary-box-inner">
<span><p>The ability to predict future outcomes conditioned on observed video frames
is crucial for intelligent decision-making in autonomous systems. Recently,
deep recurrent architectures have been applied to the task of video prediction.
However, this often results in blurry predictions and requires tedious training
on large datasets. Here, we explore a different approach by (1) using
frequency-domain approaches for video prediction and (2) explicitly inferring
object-motion relationships in the observed scene. The resulting predictions
are consistent with the observed dynamics in a scene and do not suffer from
blur.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Depth Estimation with Sharp Boundary. (arXiv:2110.05885v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05885">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation is the base task in computer vision. It has a
tremendous development in the decade with the development of deep learning. But
the boundary blur of the depth map is still a serious problem. Research finds
the boundary blur problem is mainly caused by two factors, first, the low-level
features containing boundary and structure information may loss in deeper
networks during the convolution process., second, the model ignores the errors
introduced by the boundary area due to the few portions of the boundary in the
whole areas during the backpropagation. In order to mitigate the boundary blur
problem, we focus on the above two impact factors. Firstly, we design a scene
understanding module to learn the global information with low- and high-level
features, and then to transform the global information to different scales with
our proposed scale transform module according to the different phases in the
decoder. Secondly, we propose a boundary-aware depth loss function to pay
attention to the effects of the boundary's depth value. The extensive
experiments show that our method can predict the depth maps with clearer
boundaries, and the performance of the depth accuracy base on NYU-depth v2 and
SUN RGB-D is competitive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGH: Metadata Guided Hypergraph Modeling for Unsupervised Person Re-identification. (arXiv:2110.05886v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05886">
<div class="article-summary-box-inner">
<span><p>As a challenging task, unsupervised person ReID aims to match the same
identity with query images which does not require any labeled information. In
general, most existing approaches focus on the visual cues only, leaving
potentially valuable auxiliary metadata information (e.g., spatio-temporal
context) unexplored. In the real world, such metadata is normally available
alongside captured images, and thus plays an important role in separating
several hard ReID matches. With this motivation in mind, we
propose~\textbf{MGH}, a novel unsupervised person ReID approach that uses meta
information to construct a hypergraph for feature learning and label
refinement. In principle, the hypergraph is composed of camera-topology-aware
hyperedges, which can model the heterogeneous data correlations across cameras.
Taking advantage of label propagation on the hypergraph, the proposed approach
is able to effectively refine the ReID results, such as correcting the wrong
labels or smoothing the noisy labels. Given the refined results, We further
present a memory-based listwise loss to directly optimize the average precision
in an approximate manner. Extensive experiments on three benchmarks demonstrate
the effectiveness of the proposed approach against the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Is Graph: Structured Graph Module for Video Action Recognition. (arXiv:2110.05904v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05904">
<div class="article-summary-box-inner">
<span><p>In the field of action recognition, video clips are always treated as ordered
frames for subsequent processing. To achieve spatio-temporal perception,
existing approaches propose to embed adjacent temporal interaction in the
convolutional layer. The global semantic information can therefore be obtained
by stacking multiple local layers hierarchically. However, such global temporal
accumulation can only reflect the high-level semantics in deep layers,
neglecting the potential low-level holistic clues in shallow layers. In this
paper, we first propose to transform a video sequence into a graph to obtain
direct long-term dependencies among temporal frames. To preserve sequential
information during transformation, we devise a structured graph module (SGM),
achieving fine-grained temporal interactions throughout the entire network. In
particular, SGM divides the neighbors of each node into several temporal
regions so as to extract global structural information with diverse sequential
flows. Extensive experiments are performed on standard benchmark datasets,
i.e., Something-Something V1 &amp; V2, Diving48, Kinetics-400, UCF101, and HMDB51.
The reported performance and analysis demonstrate that SGM can achieve
outstanding precision with less computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rescoring Sequence-to-Sequence Models for Text Line Recognition with CTC-Prefixes. (arXiv:2110.05909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05909">
<div class="article-summary-box-inner">
<span><p>In contrast to Connectionist Temporal Classification (CTC) approaches,
Sequence-To-Sequence (S2S) models for Handwritten Text Recognition (HTR) suffer
from errors such as skipped or repeated words which often occur at the end of a
sequence. In this paper, to combine the best of both approaches, we propose to
use the CTC-Prefix-Score during S2S decoding. Hereby, during beam search, paths
that are invalid according to the CTC confidence matrix are penalised. Our
network architecture is composed of a Convolutional Neural Network (CNN) as
visual backbone, bidirectional Long-Short-Term-Memory-Cells (LSTMs) as encoder,
and a decoder which is a Transformer with inserted mutual attention layers. The
CTC confidences are computed on the encoder while the Transformer is only used
for character-wise S2S decoding. We evaluate this setup on three HTR data sets:
IAM, Rimes, and StAZH. On IAM, we achieve a competitive Character Error Rate
(CER) of 2.95% when pretraining our model on synthetic data and including a
character-based language model for contemporary English. Compared to other
state-of-the-art approaches, our model requires about 10-20 times less
parameters. Access our shared implementations via this link to GitHub:
https://github.com/Planet-AI-GmbH/tfaip-hybrid-ctc-s2s.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trivial or impossible -- dichotomous data difficulty masks model differences (on ImageNet and beyond). (arXiv:2110.05922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05922">
<div class="article-summary-box-inner">
<span><p>"The power of a generalization system follows directly from its biases"
(Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems --
but to what degree have we understood how their inductive bias influences model
decisions? We here attempt to disentangle the various aspects that determine
how a model decides. In particular, we ask: what makes one model decide
differently from another? In a meticulously controlled setting, we find that
(1.) irrespective of the network architecture or objective (e.g.
self-supervised, semi-supervised, vision transformers, recurrent models) all
models end up with a similar decision boundary. (2.) To understand these
findings, we analysed model decisions on the ImageNet validation set from epoch
to epoch and image by image. We find that the ImageNet validation set, among
others, suffers from dichotomous data difficulty (DDD): For the range of
investigated models and their accuracies, it is dominated by 46.0% "trivial"
and 11.5% "impossible" images (beyond label errors). Only 42.5% of the images
could possibly be responsible for the differences between two models' decision
boundaries. (3.) Only removing the "impossible" and "trivial" images allows us
to see pronounced differences between models. (4.) Humans are highly accurate
at predicting which images are "trivial" and "impossible" for CNNs (81.4%).
This implies that in future comparisons of brains, machines and behaviour, much
may be gained from investigating the decisive role of images and the
distribution of their difficulties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Semantic Segmentation by Learning Label Uncertainty. (arXiv:2110.05926v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05926">
<div class="article-summary-box-inner">
<span><p>Since the rise of deep learning, many computer vision tasks have seen
significant advancements. However, the downside of deep learning is that it is
very data-hungry. Especially for segmentation problems, training a deep neural
net requires dense supervision in the form of pixel-perfect image labels, which
are very costly. In this paper, we present a new loss function to train a
segmentation network with only a small subset of pixel-perfect labels, but take
the advantage of weakly-annotated training samples in the form of cheap
bounding-box labels. Unlike recent works which make use of box-to-mask proposal
generators, our loss trains the network to learn a label uncertainty within the
bounding-box, which can be leveraged to perform online bootstrapping (i.e.
transforming the boxes to segmentation masks), while training the network. We
evaluated our method on binary segmentation tasks, as well as a multi-class
segmentation task (CityScapes vehicles and persons). We trained each task on a
dataset comprised of only 18% pixel-perfect and 82% bounding-box labels, and
compared the results to a baseline model trained on a completely pixel-perfect
dataset. For the binary segmentation tasks, our method achieves an IoU score
which is ~98.33% as good as our baseline model, while for the multi-class task,
our method is 97.12% as good as our baseline model (77.5 vs. 79.8 mIoU).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoising Diffusion Gamma Models. (arXiv:2110.05948v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05948">
<div class="article-summary-box-inner">
<span><p>Generative diffusion processes are an emerging and effective tool for image
and speech generation. In the existing methods, the underlying noise
distribution of the diffusion process is Gaussian noise. However, fitting
distributions with more degrees of freedom could improve the performance of
such generative models. In this work, we investigate other types of noise
distribution for the diffusion process. Specifically, we introduce the
Denoising Diffusion Gamma Model (DDGM) and show that noise from Gamma
distribution provides improved results for image and speech generation. Our
approach preserves the ability to efficiently sample state in the training
diffusion process while using Gamma noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imitating Deep Learning Dynamics via Locally Elastic Stochastic Differential Equations. (arXiv:2110.05960v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05960">
<div class="article-summary-box-inner">
<span><p>Understanding the training dynamics of deep learning models is perhaps a
necessary step toward demystifying the effectiveness of these models. In
particular, how do data from different classes gradually become separable in
their feature spaces when training neural networks using stochastic gradient
descent? In this study, we model the evolution of features during deep learning
training using a set of stochastic differential equations (SDEs) that each
corresponds to a training sample. As a crucial ingredient in our modeling
strategy, each SDE contains a drift term that reflects the impact of
backpropagation at an input on the features of all samples. Our main finding
uncovers a sharp phase transition phenomenon regarding the {intra-class impact:
if the SDEs are locally elastic in the sense that the impact is more
significant on samples from the same class as the input, the features of the
training data become linearly separable, meaning vanishing training loss;
otherwise, the features are not separable, regardless of how long the training
time is. Moreover, in the presence of local elasticity, an analysis of our SDEs
shows that the emergence of a simple geometric structure called the neural
collapse of the features. Taken together, our results shed light on the
decisive role of local elasticity in the training dynamics of neural networks.
We corroborate our theoretical analysis with experiments on a synthesized
dataset of geometric shapes and CIFAR-10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can machines learn to see without visual databases?. (arXiv:2110.05973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05973">
<div class="article-summary-box-inner">
<span><p>This paper sustains the position that the time has come for thinking of
learning machines that conquer visual skills in a truly human-like context,
where a few human-like object supervisions are given by vocal interactions and
pointing aids only. This likely requires new foundations on computational
processes of vision with the final purpose of involving machines in tasks of
visual description by living in their own visual environment under simple
man-machine linguistic interactions. The challenge consists of developing
machines that learn to see without needing to handle visual databases. This
might open the doors to a truly orthogonal competitive track concerning deep
learning technologies for vision which does not rely on the accumulation of
huge visual databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early Melanoma Diagnosis with Sequential Dermoscopic Images. (arXiv:2110.05976v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05976">
<div class="article-summary-box-inner">
<span><p>Dermatologists often diagnose or rule out early melanoma by evaluating the
follow-up dermoscopic images of skin lesions. However, existing algorithms for
early melanoma diagnosis are developed using single time-point images of
lesions. Ignoring the temporal, morphological changes of lesions can lead to
misdiagnosis in borderline cases. In this study, we propose a framework for
automated early melanoma diagnosis using sequential dermoscopic images. To this
end, we construct our method in three steps. First, we align sequential
dermoscopic images of skin lesions using estimated Euclidean transformations,
extract the lesion growth region by computing image differences among the
consecutive images, and then propose a spatio-temporal network to capture the
dermoscopic changes from aligned lesion images and the corresponding difference
images. Finally, we develop an early diagnosis module to compute probability
scores of malignancy for lesion images over time. We collected 179 serial
dermoscopic imaging data from 122 patients to verify our method. Extensive
experiments show that the proposed model outperforms other commonly used
sequence models. We also compared the diagnostic results of our model with
those of seven experienced dermatologists and five registrars. Our model
achieved higher diagnostic accuracy than clinicians (63.69% vs. 54.33%,
respectively) and provided an earlier diagnosis of melanoma (60.7% vs. 32.7% of
melanoma correctly diagnosed on the first follow-up images). These results
demonstrate that our model can be used to identify melanocytic lesions that are
at high-risk of malignant transformation earlier in the disease process and
thereby redefine what is possible in the early detection of melanoma.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking supervised pre-training for better downstream transferring. (arXiv:2110.06014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06014">
<div class="article-summary-box-inner">
<span><p>The pretrain-finetune paradigm has shown outstanding performance on many
applications of deep learning, where a model is pre-trained on a upstream large
dataset (e.g. ImageNet), and is then fine-tuned to different downstream tasks.
Though for most cases, the pre-training stage is conducted based on supervised
methods, recent works on self-supervised pre-training have shown powerful
transferability and even outperform supervised pre-training on multiple
downstream tasks. It thus remains an open question how to better generalize
supervised pre-training model to downstream tasks. In this paper, we argue that
the worse transferability of existing supervised pre-training methods arise
from the negligence of valuable intra-class semantic difference. This is
because these methods tend to push images from the same class close to each
other despite of the large diversity in their visual contents, a problem to
which referred as "overfit of upstream tasks". To alleviate this problem, we
propose a new supervised pre-training method based on Leave-One-Out
K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting
upstream tasks by only requiring each image to share its class label with most
of its k nearest neighbors, thus allowing each class to exhibit a multi-mode
distribution and consequentially preserving part of intra-class difference for
better transferring to downstream tasks. We developed efficient implementation
of the proposed method that scales well to large datasets. Experimental studies
on multiple downstream tasks show that LOOK outperforms other state-of-the-art
methods for supervised and self-supervised pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Security Risks of AutoML. (arXiv:2110.06018v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06018">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) represents an emerging machine learning (ML)
paradigm that automatically searches for models tailored to given tasks, which
greatly simplifies the development of ML systems and propels the trend of ML
democratization. Yet, little is known about the potential security risks
incurred by NAS, which is concerning given the increasing use of NAS-generated
models in critical domains.
</p>
<p>This work represents a solid initial step towards bridging the gap. Through
an extensive empirical study of 10 popular NAS methods, we show that compared
with their manually designed counterparts, NAS-generated models tend to suffer
greater vulnerability to various malicious attacks (e.g., adversarial evasion,
model poisoning, and functionality stealing). Further, with both empirical and
analytical evidence, we provide possible explanations for such phenomena: given
the prohibitive search space and training cost, most NAS methods favor models
that converge fast at early training stages; this preference results in
architectural properties associated with attack vulnerability (e.g., high loss
smoothness and low gradient variance). Our findings not only reveal the
relationships between model characteristics and attack vulnerability but also
suggest the inherent connections underlying different attacks. Finally, we
discuss potential remedies to mitigate such drawbacks, including increasing
cell depth and suppressing skip connects, which lead to several promising
research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoftNeuro: Fast Deep Inference using Multi-platform Optimization. (arXiv:2110.06037v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06037">
<div class="article-summary-box-inner">
<span><p>Faster inference of deep learning models is highly demanded on edge devices
and even servers, for both financial and environmental reasons. To address this
issue, we propose SoftNeuro, a novel, high-performance inference framework with
efficient performance tuning. The key idea is to separate algorithmic routines
from network layers. Our framework maximizes the inference performance by
profiling various routines for each layer and selecting the fastest path. To
efficiently find the best path, we propose a routine-selection algorithm based
on dynamic programming. Experiments show that the proposed framework achieves
both fast inference and efficient tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SlideGraph+: Whole Slide Image Level Graphs to Predict HER2Status in Breast Cancer. (arXiv:2110.06042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06042">
<div class="article-summary-box-inner">
<span><p>Human epidermal growth factor receptor 2 (HER2) is an important prognostic
and predictive factor which is overexpressed in 15-20% of breast cancer (BCa).
The determination of its status is a key clinical decision making step for
selection of treatment regimen and prognostication. HER2 status is evaluated
using transcroptomics or immunohistochemistry (IHC) through situ hybridisation
(ISH) which require additional costs and tissue burden in addition to
analytical variabilities in terms of manual observational biases in scoring. In
this study, we propose a novel graph neural network (GNN) based model (termed
SlideGraph+) to predict HER2 status directly from whole-slide images of routine
Haematoxylin and Eosin (H&amp;E) slides. The network was trained and tested on
slides from The Cancer Genome Atlas (TCGA) in addition to two independent test
datasets. We demonstrate that the proposed model outperforms the
state-of-the-art methods with area under the ROC curve (AUC) values &gt; 0.75 on
TCGA and 0.8 on independent test sets. Our experiments show that the proposed
approach can be utilised for case triaging as well as pre-ordering diagnostic
tests in a diagnostic setting. It can also be used for other weakly supervised
prediction problems in computational pathology. The SlideGraph+ code is
available at https://github.com/wenqi006/SlideGraph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Pillar with Fine-grained Feature for 3D Object Detection. (arXiv:2110.06049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06049">
<div class="article-summary-box-inner">
<span><p>3D object detection with LiDAR point clouds plays an important role in
autonomous driving perception module that requires high speed, stability and
accuracy. However, the existing point-based methods are challenging to reach
the speed requirements because of too many raw points, and the voxel-based
methods are unable to ensure stable speed because of the 3D sparse convolution.
In contrast, the 2D grid-based methods, such as PointPillar, can easily achieve
a stable and efficient speed based on simple 2D convolution, but it is hard to
get the competitive accuracy limited by the coarse-grained point clouds
representation. So we propose an improved pillar with fine-grained feature
based on PointPillar that can significantly improve detection accuracy. It
consists of two modules, including height-aware sub-pillar and sparsity-based
tiny-pillar, which get fine-grained representation respectively in the vertical
and horizontal direction of 3D space. For height-aware sub-pillar, we introduce
a height position encoding to keep height information of each sub-pillar during
projecting to a 2D pseudo image. For sparsity-based tiny-pillar, we introduce
sparsity-based CNN backbone stacked by dense feature and sparse attention
module to extract feature with larger receptive field efficiently. Experimental
results show that our proposed method significantly outperforms previous
state-of-the-art 3D detection methods on the Waymo Open Dataset. The related
code will be released to facilitate the academic and industrial study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Interaction Graph Convolutional Network for Temporal Language Localization in Videos. (arXiv:2110.06058v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06058">
<div class="article-summary-box-inner">
<span><p>This paper focuses on tackling the problem of temporal language localization
in videos, which aims to identify the start and end points of a moment
described by a natural language sentence in an untrimmed video. However, it is
non-trivial since it requires not only the comprehensive understanding of the
video and sentence query, but also the accurate semantic correspondence capture
between them. Existing efforts are mainly centered on exploring the sequential
relation among video clips and query words to reason the video and sentence
query, neglecting the other intra-modal relations (e.g., semantic similarity
among video clips and syntactic dependency among the query words). Towards this
end, in this work, we propose a Multi-modal Interaction Graph Convolutional
Network (MIGCN), which jointly explores the complex intra-modal relations and
inter-modal interactions residing in the video and sentence query to facilitate
the understanding and semantic correspondence capture of the video and sentence
query. In addition, we devise an adaptive context-aware localization method,
where the context information is taken into the candidate moments and the
multi-scale fully connected layers are designed to rank and adjust the boundary
of the generated coarse candidate moments with different lengths. Extensive
experiments on Charades-STA and ActivityNet datasets demonstrate the promising
performance and superior efficiency of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEDUSA: Multi-scale Encoder-Decoder Self-Attention Deep Neural Network Architecture for Medical Image Analysis. (arXiv:2110.06063v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06063">
<div class="article-summary-box-inner">
<span><p>Medical image analysis continues to hold interesting challenges given the
subtle characteristics of certain diseases and the significant overlap in
appearance between diseases. In this work, we explore the concept of
self-attention for tackling such subtleties in and between diseases. To this
end, we introduce MEDUSA, a multi-scale encoder-decoder self-attention
mechanism tailored for medical image analysis. While self-attention deep
convolutional neural network architectures in existing literature center around
the notion of multiple isolated lightweight attention mechanisms with limited
individual capacities being incorporated at different points in the network
architecture, MEDUSA takes a significant departure from this notion by
possessing a single, unified self-attention mechanism with significantly higher
capacity with multiple attention heads feeding into different scales in the
network architecture. To the best of the authors' knowledge, this is the first
"single body, multi-scale heads" realization of self-attention and enables
explicit global context amongst selective attention at different levels of
representational abstractions while still enabling differing local attention
context at individual levels of abstractions. With MEDUSA, we obtain
state-of-the-art performance on multiple challenging medical image analysis
benchmarks including COVIDx, RSNA RICORD, and RSNA Pneumonia Challenge when
compared to previous work. Our MEDUSA model is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral analysis of re-parameterized light fields. (arXiv:2110.06064v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06064">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the spectral properties of re-parameterized light
field. Following previous studies of the light field spectrum, which notably
provided sampling guidelines, we focus on the two plane parameterization of the
light field. However, we introduce additional flexibility by allowing the image
plane to be tilted and not only parallel. A formal theoretical analysis is
first presented, which shows that more flexible sampling guidelines (i.e. wider
camera baselines) can be used to sample the light field when adapting the image
plane orientation to the scene geometry. We then present our simulations and
results to support these theoretical findings. While the work introduced in
this paper is mostly theoretical, we believe these new findings open exciting
avenues for more practical application of light fields, such as view synthesis
or compact representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expressivity and Trainability of Quadratic Networks. (arXiv:2110.06081v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06081">
<div class="article-summary-box-inner">
<span><p>Inspired by diversity of biological neurons, quadratic artificial neurons can
play an important role in deep learning models. The type of quadratic neurons
of our interest replaces the inner-product operation in the conventional neuron
with a quadratic function. Despite promising results so far achieved by
networks of quadratic neurons, there are important issues not well addressed.
Theoretically, the superior expressivity of a quadratic network over either a
conventional network or a conventional network via quadratic activation is not
fully elucidated, which makes the use of quadratic networks not well grounded.
Practically, although a quadratic network can be trained via generic
backpropagation, it can be subject to a higher risk of collapse than the
conventional counterpart. To address these issues, we first apply the spline
theory and a measure from algebraic geometry to give two theorems that
demonstrate better model expressivity of a quadratic network than the
conventional counterpart with or without quadratic activation. Then, we propose
an effective and efficient training strategy referred to as ReLinear to
stabilize the training process of a quadratic network, thereby unleashing the
full potential in its associated machine learning tasks. Comprehensive
experiments on popular datasets are performed to support our findings and
evaluate the performance of quadratic deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Conditional Random Field Convolution for Point Cloud Segmentation. (arXiv:2110.06085v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06085">
<div class="article-summary-box-inner">
<span><p>Point cloud segmentation is the foundation of 3D environmental perception for
modern intelligent systems. To solve this problem and image segmentation,
conditional random fields (CRFs) are usually formulated as discrete models in
label space to encourage label consistency, which is actually a kind of
postprocessing. In this paper, we reconsider the CRF in feature space for point
cloud segmentation because it can capture the structure of features well to
improve the representation ability of features rather than simply smoothing.
Therefore, we first model the point cloud features with a continuous quadratic
energy model and formulate its solution process as a message-passing graph
convolution, by which it can be easily integrated into a deep network. We
theoretically demonstrate that the message passing in the graph convolution is
equivalent to the mean-field approximation of a continuous CRF model.
Furthermore, we build an encoder-decoder network based on the proposed
continuous CRF graph convolution (CRFConv), in which the CRFConv embedded in
the decoding layers can restore the details of high-level features that were
lost in the encoding stage to enhance the location ability of the network,
thereby benefiting segmentation. Analogous to the CRFConv, we show that the
classical discrete CRF can also work collaboratively with the proposed network
via another graph convolution to further improve the segmentation results.
Experiments on various point cloud benchmarks demonstrate the effectiveness and
robustness of the proposed method. Compared with the state-of-the-art methods,
the proposed method can also achieve competitive segmentation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sign Language Recognition via Skeleton-Aware Multi-Model Ensemble. (arXiv:2110.06161v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06161">
<div class="article-summary-box-inner">
<span><p>Sign language is commonly used by deaf or mute people to communicate but
requires extensive effort to master. It is usually performed with the fast yet
delicate movement of hand gestures, body posture, and even facial expressions.
Current Sign Language Recognition (SLR) methods usually extract features via
deep neural networks and suffer overfitting due to limited and noisy data.
Recently, skeleton-based action recognition has attracted increasing attention
due to its subject-invariant and background-invariant nature, whereas
skeleton-based SLR is still under exploration due to the lack of hand
annotations. Some researchers have tried to use off-line hand pose trackers to
obtain hand keypoints and aid in recognizing sign language via recurrent neural
networks. Nevertheless, none of them outperforms RGB-based approaches yet. To
this end, we propose a novel Skeleton Aware Multi-modal Framework with a Global
Ensemble Model (GEM) for isolated SLR (SAM-SLR-v2) to learn and fuse
multi-modal feature representations towards a higher recognition rate.
Specifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to
model the embedded dynamics of skeleton keypoints and a Separable
Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. The
skeleton-based predictions are fused with other RGB and depth based modalities
by the proposed late-fusion GEM to provide global information and make a
faithful SLR prediction. Experiments on three isolated SLR datasets demonstrate
that our proposed SAM-SLR-v2 framework is exceedingly effective and achieves
state-of-the-art performance with significant margins. Our code will be
available at https://github.com/jackyjsy/SAM-SLR-v2
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M2GAN: A Multi-Stage Self-Attention Network for Image Rain Removal on Autonomous Vehicles. (arXiv:2110.06164v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06164">
<div class="article-summary-box-inner">
<span><p>Image deraining is a new challenging problem in applications of autonomous
vehicles. In a bad weather condition of heavy rainfall, raindrops, mainly
hitting the vehicle's windshield, can significantly reduce observation ability
even though the windshield wipers might be able to remove part of it. Moreover,
rain flows spreading over the windshield can yield the physical effect of
refraction, which seriously impede the sightline or undermine the machine
learning system equipped in the vehicle. In this paper, we propose a new
multi-stage multi-task recurrent generative adversarial network (M2GAN) to deal
with challenging problems of raindrops hitting the car's windshield. This
method is also applicable for removing raindrops appearing on a glass window or
lens. M2GAN is a multi-stage multi-task generative adversarial network that can
utilize prior high-level information, such as semantic segmentation, to boost
deraining performance. To demonstrate M2GAN, we introduce the first real-world
dataset for rain removal on autonomous vehicles. The experimental results show
that our proposed method is superior to other state-of-the-art approaches of
deraining raindrops in respect of quantitative metrics and visual quality.
M2GAN is considered the first method to deal with challenging problems of
real-world rains under unconstrained environments such as autonomous vehicles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAda! Temporally-Adaptive Convolutions for Video Understanding. (arXiv:2110.06178v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06178">
<div class="article-summary-box-inner">
<span><p>Spatial convolutions are widely used in numerous deep video models. It
fundamentally assumes spatio-temporal invariance, i.e., using shared weights
for every location in different frames. This work presents Temporally-Adaptive
Convolutions (TAdaConv) for video understanding, which shows that adaptive
weight calibration along the temporal dimension is an efficient way to
facilitate modelling complex temporal dynamics in videos. Specifically,
TAdaConv empowers the spatial convolutions with temporal modelling abilities by
calibrating the convolution weights for each frame according to its local and
global temporal context. Compared to previous temporal modelling operations,
TAdaConv is more efficient as it operates over the convolution kernels instead
of the features, whose dimension is an order of magnitude smaller than the
spatial resolutions. Further, the kernel calibration also brings an increased
model capacity. We construct TAda2D networks by replacing the spatial
convolutions in ResNet with TAdaConv, which leads to on par or better
performance compared to state-of-the-art approaches on multiple video action
recognition and localization benchmarks. We also demonstrate that as a readily
plug-in operation with negligible computation overhead, TAdaConv can
effectively improve many existing video models with a convincing margin. Codes
and models will be made available at
https://github.com/alibaba-mmai-research/pytorch-video-understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ABO: Dataset and Benchmarks for Real-World 3D Object Understanding. (arXiv:2110.06199v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06199">
<div class="article-summary-box-inner">
<span><p>We introduce Amazon-Berkeley Objects (ABO), a new large-scale dataset of
product images and 3D models corresponding to real household objects. We use
this realistic, object-centric 3D dataset to measure the domain gap for
single-view 3D reconstruction networks trained on synthetic objects. We also
use multi-view images from ABO to measure the robustness of state-of-the-art
metric learning approaches to different camera viewpoints. Finally, leveraging
the physically-based rendering materials in ABO, we perform single- and
multi-view material estimation for a variety of complex, real-world geometries.
The full dataset is available for download at
https://amazon-berkeley-objects.s3.amazonaws.com/index.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Set Recognition: A Good Closed-Set Classifier is All You Need. (arXiv:2110.06207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06207">
<div class="article-summary-box-inner">
<span><p>The ability to identify whether or not a test sample belongs to one of the
semantic classes in a classifier's training set is critical to practical
deployment of the model. This task is termed open-set recognition (OSR) and has
received significant attention in recent years. In this paper, we first
demonstrate that the ability of a classifier to make the 'none-of-above'
decision is highly correlated with its accuracy on the closed-set classes. We
find that this relationship holds across loss objectives and architectures, and
further demonstrate the trend both on the standard OSR benchmarks as well as on
a large-scale ImageNet evaluation. Second, we use this correlation to boost the
performance of the cross-entropy OSR 'baseline' by improving its closed-set
accuracy, and with this strong baseline achieve a new state-of-the-art on the
most challenging OSR benchmark. Similarly, we boost the performance of the
existing state-of-the-art method by improving its closed-set accuracy, but this
does not surpass the strong baseline on the most challenging dataset. Our third
contribution is to reappraise the datasets used for OSR evaluation, and
construct new benchmarks which better respect the task of detecting semantic
novelty, as opposed to low-level distributional shifts as tackled by
neighbouring machine learning fields. In this new setting, we again demonstrate
that there is negligible difference between the strong baseline and the
existing state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PX-NET: Simple and Efficient Pixel-Wise Training of Photometric Stereo Networks. (arXiv:2008.04933v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.04933">
<div class="article-summary-box-inner">
<span><p>Retrieving accurate 3D reconstructions of objects from the way they reflect
light is a very challenging task in computer vision. Despite more than four
decades since the definition of the Photometric Stereo problem, most of the
literature has had limited success when global illumination effects such as
cast shadows, self-reflections and ambient light come into play, especially for
specular surfaces.
</p>
<p>Recent approaches have leveraged the power of deep learning in conjunction
with computer graphics in order to cope with the need of a vast number of
training data in order to invert the image irradiance equation and retrieve the
geometry of the object. However, rendering global illumination effects is a
slow process which can limit the amount of training data that can be generated.
</p>
<p>In this work we propose a novel pixel-wise training procedure for normal
prediction by replacing the training data (observation maps) of globally
rendered images with independent per-pixel generated data. We show that global
physical effects can be approximated on the observation map domain and this
simplifies and speeds up the data creation procedure.
</p>
<p>Our network, PX-NET, achieves the state-of-the-art performance compared to
other pixelwise methods on synthetic datasets, as well as the Diligent real
dataset on both dense and sparse light settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallax Attention for Unsupervised Stereo Correspondence Learning. (arXiv:2009.08250v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08250">
<div class="article-summary-box-inner">
<span><p>Stereo image pairs encode 3D scene cues into stereo correspondences between
the left and right images. To exploit 3D cues within stereo images, recent CNN
based methods commonly use cost volume techniques to capture stereo
correspondence over large disparities. However, since disparities can vary
significantly for stereo cameras with different baselines, focal lengths and
resolutions, the fixed maximum disparity used in cost volume techniques hinders
them to handle different stereo image pairs with large disparity variations. In
this paper, we propose a generic parallax-attention mechanism (PAM) to capture
stereo correspondence regardless of disparity variations. Our PAM integrates
epipolar constraints with attention mechanism to calculate feature similarities
along the epipolar line to capture stereo correspondence. Based on our PAM, we
propose a parallax-attention stereo matching network (PASMnet) and a
parallax-attention stereo image super-resolution network (PASSRnet) for stereo
matching and stereo image super-resolution tasks. Moreover, we introduce a new
and large-scale dataset named Flickr1024 for stereo image super-resolution.
Experimental results show that our PAM is generic and can effectively learn
stereo correspondence under large disparity variations in an unsupervised
manner. Comparative results show that our PASMnet and PASSRnet achieve the
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Transformer-based Set Prediction for Object Detection. (arXiv:2011.10881v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10881">
<div class="article-summary-box-inner">
<span><p>DETR is a recently proposed Transformer-based method which views object
detection as a set prediction problem and achieves state-of-the-art performance
but demands extra-long training time to converge. In this paper, we investigate
the causes of the optimization difficulty in the training of DETR. Our
examinations reveal several factors contributing to the slow convergence of
DETR, primarily the issues with the Hungarian loss and the Transformer
cross-attention mechanism. To overcome these issues we propose two solutions,
namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN
(Transformer-based Set Prediction with RCNN). Experimental results show that
the proposed methods not only converge much faster than the original DETR, but
also significantly outperform DETR and other baselines in terms of detection
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Aggregation in Test-Time Augmentation. (arXiv:2011.11156v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11156">
<div class="article-summary-box-inner">
<span><p>Test-time augmentation -- the aggregation of predictions across transformed
versions of a test input -- is a common practice in image classification.
Traditionally, predictions are combined using a simple average. In this paper,
we present 1) experimental analyses that shed light on cases in which the
simple average is suboptimal and 2) a method to address these shortcomings. A
key finding is that even when test-time augmentation produces a net improvement
in accuracy, it can change many correct predictions into incorrect predictions.
We delve into when and why test-time augmentation changes a prediction from
being correct to incorrect and vice versa. Building on these insights, we
present a learning-based method for aggregating test-time augmentations.
Experiments across a diverse set of models, datasets, and augmentations show
that our method delivers consistent improvements over existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Regularization Prediction in Diffeomorphic Image Registration. (arXiv:2011.14229v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14229">
<div class="article-summary-box-inner">
<span><p>This paper presents a predictive model for estimating regularization
parameters of diffeomorphic image registration. We introduce a novel framework
that automatically determines the parameters controlling the smoothness of
diffeomorphic transformations. Our method significantly reduces the effort of
parameter tuning, which is time and labor-consuming. To achieve the goal, we
develop a predictive model based on deep convolutional neural networks (CNN)
that learns the mapping between pairwise images and the regularization
parameter of image registration. In contrast to previous methods that estimate
such parameters in a high-dimensional image space, our model is built in an
efficient bandlimited space with much lower dimensions. We demonstrate the
effectiveness of our model on both 2D synthetic data and 3D real brain images.
Experimental results show that our model not only predicts appropriate
regularization parameters for image registration, but also improving the
network training in terms of time and memory efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Attribute Learning. (arXiv:2012.05895v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05895">
<div class="article-summary-box-inner">
<span><p>Semantic concepts are frequently defined by combinations of underlying
attributes. As mappings from attributes to classes are often simple,
attribute-based representations facilitate novel concept learning with zero or
few examples. A significant limitation of existing attribute-based learning
paradigms, such as zero-shot learning, is that the attributes are assumed to be
known and fixed. In this work we study the rapid learning of attributes that
were not previously labeled. Compared to standard few-shot learning of semantic
classes, in which novel classes may be defined by attributes that were relevant
at training time, learning new attributes imposes a stiffer challenge. We found
that supervised learning with training attributes does not generalize well to
new test attributes, whereas self-supervised pre-training brings significant
improvement. We further experimented with random splits of the attribute space
and found that predictability of test attributes provides an informative
estimate of a model's generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Representation Learning from Flow Equivariance. (arXiv:2101.06553v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06553">
<div class="article-summary-box-inner">
<span><p>Self-supervised representation learning is able to learn semantically
meaningful features; however, much of its recent success relies on multiple
crops of an image with very few objects. Instead of learning view-invariant
representation from simple images, humans learn representations in a complex
world with changing scenes by observing object movement, deformation, pose
variation, and ego motion. Motivated by this ability, we present a new
self-supervised learning representation framework that can be directly deployed
on a video stream of complex scenes with many moving objects. Our framework
features a simple flow equivariance objective that encourages the network to
predict the features of another frame by applying a flow transformation to the
features of the current frame. Our representations, learned from
high-resolution raw video, can be readily used for downstream tasks on static
images. Readout experiments on challenging semantic segmentation, instance
segmentation, and object detection benchmarks show that we are able to
outperform representations obtained from previous state-of-the-art methods
including SimCLR and BYOL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attacks On Multi-Agent Communication. (arXiv:2101.06560v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06560">
<div class="article-summary-box-inner">
<span><p>Growing at a fast pace, modern autonomous systems will soon be deployed at
scale, opening up the possibility for cooperative multi-agent systems. Sharing
information and distributing workloads allow autonomous agents to better
perform tasks and increase computation efficiency. However, shared information
can be modified to execute adversarial attacks on deep learning models that are
widely employed in modern systems. Thus, we aim to study the robustness of such
systems and focus on exploring adversarial attacks in a novel multi-agent
setting where communication is done through sharing learned intermediate
representations of neural networks. We observe that an indistinguishable
adversarial message can severely degrade performance, but becomes weaker as the
number of benign agents increases. Furthermore, we show that black-box transfer
attacks are more difficult in this setting when compared to directly perturbing
the inputs, as it is necessary to align the distribution of learned
representations with domain adaptation. Our work studies robustness at the
neural network level to contribute an additional layer of fault tolerance to
modern security protocols for more secure multi-agent systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Monocular Hand Pose Estimation on Embedded Systems. (arXiv:2102.07067v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07067">
<div class="article-summary-box-inner">
<span><p>Hand pose estimation is a fundamental task in many human-robot
interaction-related applications. However, previous approaches suffer from
unsatisfying hand landmark predictions in real-world scenes and high
computation burden. This paper proposes a fast and accurate framework for hand
pose estimation, dubbed as "FastHand". Using a lightweight encoder-decoder
network architecture, FastHand fulfills the requirements of practical
applications running on embedded devices. The encoder consists of deep layers
with a small number of parameters, while the decoder makes use of spatial
location information to obtain more accurate results. The evaluation took place
on two publicly available datasets demonstrating the improved performance of
the proposed pipeline compared to other state-of-the-art approaches. FastHand
offers high accuracy scores while reaching a speed of 25 frames per second on
an NVIDIA Jetson TX2 graphics processing unit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Low-Rank Simplicity Bias in Deep Networks. (arXiv:2103.10427v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10427">
<div class="article-summary-box-inner">
<span><p>Modern deep neural networks are highly over-parameterized compared to the
data on which they are trained, yet they often generalize remarkably well. A
flurry of recent work has asked: why do deep networks not overfit to their
training data? In this work, we make a series of empirical observations that
investigate the hypothesis that deeper networks are inductively biased to find
solutions with lower rank embeddings. We conjecture that this bias exists
because the volume of functions that maps to low-rank embedding increases with
depth. We show empirically that our claim holds true on finite width linear and
non-linear models and show that these are the solutions that generalize well.
We then show that the low-rank simplicity bias exists even after training,
using a wide variety of commonly used optimizers. We found this phenomenon to
be resilient to initialization, hyper-parameters, and learning methods. We
further demonstrate how linear over-parameterization of deep non-linear models
can be used to induce low-rank bias, improving generalization performance
without changing the effective model capacity. Practically, we demonstrate that
simply linearly over-parameterizing standard models at training time can
improve performance on image classification tasks, including ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Depth Estimation through Virtual-world Supervision and Real-world SfM Self-Supervision. (arXiv:2103.12209v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12209">
<div class="article-summary-box-inner">
<span><p>Depth information is essential for on-board perception in autonomous driving
and driver assistance. Monocular depth estimation (MDE) is very appealing since
it allows for appearance and depth being on direct pixelwise correspondence
without further calibration. Best MDE models are based on Convolutional Neural
Networks (CNNs) trained in a supervised manner, i.e., assuming pixelwise ground
truth (GT). Usually, this GT is acquired at training time through a calibrated
multi-modal suite of sensors. However, also using only a monocular system at
training time is cheaper and more scalable. This is possible by relying on
structure-from-motion (SfM) principles to generate self-supervision.
Nevertheless, problems of camouflaged objects, visibility changes,
static-camera intervals, textureless areas, and scale ambiguity, diminish the
usefulness of such self-supervision. In this paper, we perform monocular depth
estimation by virtual-world supervision (MonoDEVS) and real-world SfM
self-supervision. We compensate the SfM self-supervision limitations by
leveraging virtual-world images with accurate semantic and depth supervision
and addressing the virtual-to-real domain gap. Our MonoDEVSNet outperforms
previous MDE CNNs trained on monocular and even stereo sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling the Power of Mixup for Stronger Classifiers. (arXiv:2103.13027v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13027">
<div class="article-summary-box-inner">
<span><p>Mixup-based data augmentations have achieved great success as regularizers
for deep neural networks. However, existing methods rely on deliberately
handcrafted mixup policies, which ignore or oversell the semantic matching
between mixed samples and labels. Driven by their prior assumptions, early
methods attempt to smooth decision boundaries by random linear interpolation
while others focus on maximizing class-related information via offline saliency
optimization. As a result, the issue of label mismatch has not been well
addressed. Additionally, the optimization stability of mixup training is
constantly troubled by the label mismatch. To address these challenges, we
first reformulate mixup for supervised classification as two sub-tasks, mixup
sample generation and classification, then propose Automatic Mixup (AutoMix), a
revolutionary mixup framework. Specifically, a learnable lightweight Mix Block
(MB) with a cross-attention mechanism is proposed to generate a mixed sample by
modeling a fair relationship between the pair of samples under direct
supervision of the corresponding mixed label. Moreover, the proposed Momentum
Pipeline (MP) enhances training stability and accelerates convergence on top of
making the Mix Block fully trained end-to-end. Extensive experiments on five
popular classification benchmarks show that the proposed approach consistently
outperforms leading methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Graphs: A Survey of Generations and Applications. (arXiv:2104.01111v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01111">
<div class="article-summary-box-inner">
<span><p>Scene graph is a structured representation of a scene that can clearly
express the objects, attributes, and relationships between objects in the
scene. As computer vision technology continues to develop, people are no longer
satisfied with simply detecting and recognizing objects in images; instead,
people look forward to a higher level of understanding and reasoning about
visual scenes. For example, given an image, we want to not only detect and
recognize objects in the image, but also know the relationship between objects
(visual relationship detection), and generate a text description (image
captioning) based on the image content. Alternatively, we might want the
machine to tell us what the little girl in the image is doing (Visual Question
Answering (VQA)), or even remove the dog from the image and find similar images
(image editing and retrieval), etc. These tasks require a higher level of
understanding and reasoning for image vision tasks. The scene graph is just
such a powerful tool for scene understanding. Therefore, scene graphs have
attracted the attention of a large number of researchers, and related research
is often cross-modal, complex, and rapidly developing. However, no relatively
systematic survey of scene graphs exists at present. To this end, this survey
conducts a comprehensive investigation of the current scene graph research.
More specifically, we first summarized the general definition of the scene
graph, then conducted a comprehensive and systematic discussion on the
generation method of the scene graph (SGG) and the SGG with the aid of prior
knowledge. We then investigated the main applications of scene graphs and
summarized the most commonly used datasets. Finally, we provide some insights
into the future development of scene graphs. We believe this will be a very
helpful foundation for future research on scene graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct Differentiable Augmentation Search. (arXiv:2104.04282v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04282">
<div class="article-summary-box-inner">
<span><p>Data augmentation has been an indispensable tool to improve the performance
of deep neural networks, however the augmentation can hardly transfer among
different tasks and datasets. Consequently, a recent trend is to adopt AutoML
technique to learn proper augmentation policy without extensive hand-crafted
tuning. In this paper, we propose an efficient differentiable search algorithm
called Direct Differentiable Augmentation Search (DDAS). It exploits
meta-learning with one-step gradient update and continuous relaxation to the
expected training loss for efficient search. Our DDAS can achieve efficient
augmentation search without relying on approximations such as Gumbel Softmax or
second order gradient approximation. To further reduce the adverse effect of
improper augmentations, we organize the search space into a two level
hierarchy, in which we first decide whether to apply augmentation, and then
determine the specific augmentation policy. On standard image classification
benchmarks, our DDAS achieves state-of-the-art performance and efficiency
tradeoff while reducing the search cost dramatically, e.g. 0.15 GPU hours for
CIFAR-10. In addition, we also use DDAS to search augmentation for object
detection task and achieve comparable performance with AutoAugment, while being
1000x faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Subjective Ratings Using Auto-Decoded Deep Latent Embeddings. (arXiv:2104.05570v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05570">
<div class="article-summary-box-inner">
<span><p>Depending on the application, radiological diagnoses can be associated with
high inter- and intra-rater variabilities. Most computer-aided diagnosis (CAD)
solutions treat such data as incontrovertible, exposing learning algorithms to
considerable and possibly contradictory label noise and biases. Thus, managing
subjectivity in labels is a fundamental problem in medical imaging analysis. To
address this challenge, we introduce auto-decoded deep latent embeddings
(ADDLE), which explicitly models the tendencies of each rater using an
auto-decoder framework. After a simple linear transformation, the latent
variables can be injected into any backbone at any and multiple points,
allowing the model to account for rater-specific effects on the diagnosis.
Importantly, ADDLE does not expect multiple raters per image in training,
meaning it can readily learn from data mined from hospital archives. Moreover,
the complexity of training ADDLE does not increase as more raters are added.
During inference each rater can be simulated and a 'mean' or 'greedy' virtual
rating can be produced. We test ADDLE on the problem of liver steatosis
diagnosis from 2D ultrasound (US) by collecting 46 084 studies along with
clinical US diagnoses originating from 65 different raters. We evaluated
diagnostic performance using a separate dataset with gold-standard biopsy
diagnoses. ADDLE can improve the partial areas under the curve (AUCs) for
diagnosing severe steatosis by 10.5% over standard classifiers while
outperforming other annotator-noise approaches, including those requiring 65
times the parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PARE: Part Attention Regressor for 3D Human Body Estimation. (arXiv:2104.08527v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08527">
<div class="article-summary-box-inner">
<span><p>Despite significant progress, we show that state of the art 3D human pose and
shape estimation methods remain sensitive to partial occlusion and can produce
dramatically wrong predictions although much of the body is observable. To
address this, we introduce a soft attention mechanism, called the Part
Attention REgressor (PARE), that learns to predict body-part-guided attention
masks. We observe that state-of-the-art methods rely on global feature
representations, making them sensitive to even small occlusions. In contrast,
PARE's part-guided attention mechanism overcomes these issues by exploiting
information about the visibility of individual body parts while leveraging
information from neighboring body-parts to predict occluded parts. We show
qualitatively that PARE learns sensible attention masks, and quantitative
evaluation confirms that PARE achieves more accurate and robust reconstruction
results than existing approaches on both occlusion-specific and standard
benchmarks. The code and data are available for research purposes at {\small
\url{https://pare.is.tue.mpg.de/}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaLaLoc: Latent Layout Localisation in Dynamic, Unvisited Environments. (arXiv:2104.09169v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09169">
<div class="article-summary-box-inner">
<span><p>We present LaLaLoc to localise in environments without the need for prior
visitation, and in a manner that is robust to large changes in scene
appearance, such as a full rearrangement of furniture. Specifically, LaLaLoc
performs localisation through latent representations of room layout. LaLaLoc
learns a rich embedding space shared between RGB panoramas and layouts inferred
from a known floor plan that encodes the structural similarity between
locations. Further, LaLaLoc introduces direct, cross-modal pose optimisation in
its latent space. Thus, LaLaLoc enables fine-grained pose estimation in a scene
without the need for prior visitation, as well as being robust to dynamics,
such as a change in furniture configuration. We show that in a domestic
environment LaLaLoc is able to accurately localise a single RGB panorama image
to within 8.3cm, given only a floor plan as a prior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding. (arXiv:2104.12763v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12763">
<div class="article-summary-box-inner">
<span><p>Multi-modal reasoning systems rely on a pre-trained object detector to
extract regions of interest from the image. However, this crucial module is
typically used as a black box, trained independently of the downstream task and
on a fixed vocabulary of objects and attributes. This makes it challenging for
such systems to capture the long tail of visual concepts expressed in free form
text. In this paper we propose MDETR, an end-to-end modulated detector that
detects objects in an image conditioned on a raw text query, like a caption or
a question. We use a transformer-based architecture to reason jointly over text
and image by fusing the two modalities at an early stage of the model. We
pre-train the network on 1.3M text-image pairs, mined from pre-existing
multi-modal datasets having explicit alignment between phrases in text and
objects in the image. We then fine-tune on several downstream tasks such as
phrase grounding, referring expression comprehension and segmentation,
achieving state-of-the-art results on popular benchmarks. We also investigate
the utility of our model as an object detector on a given label set when
fine-tuned in a few-shot setting. We show that our pre-training approach
provides a way to handle the long tail of object categories which have very few
labelled instances. Our approach can be easily extended for visual question
answering, achieving competitive performance on GQA and CLEVR. The code and
models are available at https://github.com/ashkamath/mdetr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LUCES: A Dataset for Near-Field Point Light Source Photometric Stereo. (arXiv:2104.13135v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13135">
<div class="article-summary-box-inner">
<span><p>Three-dimensional reconstruction of objects from shading information is a
challenging task in computer vision. As most of the approaches facing the
Photometric Stereo problem use simplified far-field assumptions, real-world
scenarios have essentially more complex physical effects that need to be
handled for accurately reconstructing the 3D shape. An increasing number of
methods have been proposed to address the problem when point light sources are
assumed to be nearby the target object. The proximity of the light sources
complicates the modeling of the image formation as the light behaviour requires
non-linear parameterisation to describe its propagation and attenuation.
</p>
<p>To understand the capability of the approaches dealing with this near-field
scenario, the literature till now has used synthetically rendered photometric
images or minimal and very customised real-world data. In order to fill the gap
in evaluating near-field photometric stereo methods, we introduce LUCES the
first real-world 'dataset for near-fieLd point light soUrCe photomEtric Stereo'
of 14 objects of a varying of materials. A device counting 52 LEDs has been
designed to lit each object positioned 10 to 30 centimeters away from the
camera. Together with the raw images, in order to evaluate the 3D
reconstructions, the dataset includes both normal and depth maps for comparing
different features of the retrieved 3D geometry. Furthermore, we evaluate the
performance of the latest near-field Photometric Stereo algorithms on the
proposed dataset to assess the SOTA method with respect to actual close range
effects and object materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COMISR: Compression-Informed Video Super-Resolution. (arXiv:2105.01237v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01237">
<div class="article-summary-box-inner">
<span><p>Most video super-resolution methods focus on restoring high-resolution video
frames from low-resolution videos without taking into account compression.
However, most videos on the web or mobile devices are compressed, and the
compression can be severe when the bandwidth is limited. In this paper, we
propose a new compression-informed video super-resolution model to restore
high-resolution content without introducing artifacts caused by compression.
The proposed model consists of three modules for video super-resolution:
bi-directional recurrent warping, detail-preserving flow estimation, and
Laplacian enhancement. All these three modules are used to deal with
compression properties such as the location of the intra-frames in the input
and smoothness in the output frames. For thorough performance evaluation, we
conducted extensive experiments on standard datasets with a wide range of
compression rates, covering many real video use cases. We showed that our
method not only recovers high-resolution content on uncompressed frames from
the widely-used benchmark datasets, but also achieves state-of-the-art
performance in super-resolving compressed videos based on numerous quantitative
metrics. We also evaluated the proposed method by simulating streaming from
YouTube to demonstrate its effectiveness and robustness. The source codes and
trained models are available at
https://github.com/google-research/google-research/tree/master/comisr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations. (arXiv:2106.01548v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01548">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) and MLPs signal further efforts on replacing
hand-wired features or inductive biases with general-purpose neural
architectures. Existing works empower the models by massive data, such as
large-scale pre-training and/or repeated strong data augmentations, and still
report optimization-related problems (e.g., sensitivity to initialization and
learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the
lens of loss geometry, intending to improve the models' data efficiency at
training and generalization at inference. Visualization and Hessian reveal
extremely sharp local minima of converged models. By promoting smoothness with
a recently proposed sharpness-aware optimizer, we substantially improve the
accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning
supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and
+11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,
with the simple Inception-style preprocessing). We show that the improved
smoothness attributes to sparser active neurons in the first few layers. The
resultant ViTs outperform ResNets of similar size and throughput when trained
from scratch on ImageNet without large-scale pre-training or strong data
augmentations. They also possess more perceptive attention maps. Our model
checkpoints are released at
\url{https://github.com/google-research/vision_transformer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12423">
<div class="article-summary-box-inner">
<span><p>We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Align Yourself: Self-supervised Pre-training for Fine-grained Recognition via Saliency Alignment. (arXiv:2106.15788v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15788">
<div class="article-summary-box-inner">
<span><p>Self-supervised contrastive learning has demonstrated great potential in
learning visual representations. Despite their success on various downstream
tasks such as image classification and object detection, self-supervised
pre-training for fine-grained scenarios is not fully explored. In this paper,
we first point out that current contrastive methods are prone to memorizing
background/foreground texture and therefore have a limitation in localizing the
foreground object. Analysis suggests that learning to extract discriminative
texture information and localization are equally crucial for self-supervised
pre-training in fine-grained scenarios. Based on our findings, we introduce
cross-view saliency alignment (CVSA), a contrastive learning framework that
first crops and swaps saliency regions of images as a novel view generation and
then guides the model to localize on the foreground object via a cross-view
alignment loss. Extensive experiments on four popular fine-grained
classification benchmarks show that CVSA significantly improves the learned
representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResViT: Residual vision transformers for multi-modal medical image synthesis. (arXiv:2106.16031v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16031">
<div class="article-summary-box-inner">
<span><p>Multi-modal imaging is a key healthcare technology that is often
underutilized due to costs associated with multiple separate scans. This
limitation yields the need for synthesis of unacquired modalities from the
subset of available modalities. In recent years, generative adversarial network
(GAN) models with superior depiction of structural details have been
established as state-of-the-art in numerous medical image synthesis tasks. GANs
are characteristically based on convolutional neural network (CNN) backbones
that perform local processing with compact filters. This inductive bias in turn
compromises learning of contextual features. Here, we propose a novel
generative adversarial approach for medical image synthesis, ResViT, to combine
local precision of convolution operators with contextual sensitivity of vision
transformers. ResViT employs a central bottleneck comprising novel aggregated
residual transformer (ART) blocks that synergistically combine convolutional
and transformer modules. Comprehensive demonstrations are performed for
synthesizing missing sequences in multi-contrast MRI, and CT images from MRI.
Our results indicate superiority of ResViT against competing methods in terms
of qualitative observations and quantitative metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Positional Encoding. (arXiv:2107.02561v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02561">
<div class="article-summary-box-inner">
<span><p>It is well noted that coordinate based MLPs benefit -- in terms of preserving
high-frequency information -- through the encoding of coordinate positions as
an array of Fourier features. Hitherto, the rationale for the effectiveness of
these positional encodings has been solely studied through a Fourier lens. In
this paper, we strive to broaden this understanding by showing that alternative
non-Fourier embedding functions can indeed be used for positional encoding.
Moreover, we show that their performance is entirely determined by a trade-off
between the stable rank of the embedded matrix and the distance preservation
between embedded coordinates. We further establish that the now ubiquitous
Fourier feature mapping of position is a special case that fulfills these
conditions. Consequently, we present a more general theory to analyze
positional encoding in terms of shifted basis functions. To this end, we
develop the necessary theoretical formulae and empirically verify that our
theoretical claims hold in practice. Codes available at
https://github.com/osiriszjq/Rethinking-positional-encoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Brain Reconstruction by Hierarchical Shape-Perception Network from a Single Incomplete Image. (arXiv:2107.11010v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11010">
<div class="article-summary-box-inner">
<span><p>3D shape reconstruction is essential in the navigation of minimally-invasive
and auto robot-guided surgeries whose operating environments are indirect and
narrow, and there have been some works that focused on reconstructing the 3D
shape of the surgical organ through limited 2D information available. However,
the lack and incompleteness of such information caused by intraoperative
emergencies (such as bleeding) and risk control conditions have not been
considered. In this paper, a novel hierarchical shape-perception network (HSPN)
is proposed to reconstruct the 3D point clouds (PCs) of specific brains from
one single incomplete image with low latency. A branching predictor and several
hierarchical attention pipelines are constructed to generate point clouds that
accurately describe the incomplete images and then complete these point clouds
with high quality. Meanwhile, attention gate blocks (AGBs) are designed to
efficiently aggregate geometric local features of incomplete PCs transmitted by
hierarchical attention pipelines and internal features of reconstructing point
clouds. With the proposed HSPN, 3D shape perception and completion can be
achieved spontaneously. Comprehensive results measured by Chamfer distance and
PC-to-PC error demonstrate that the performance of the proposed HSPN
outperforms other competitive methods in terms of qualitative displays,
quantitative experiment, and classification evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04212">
<div class="article-summary-box-inner">
<span><p>Action recognition is a crucial task for video understanding. In this paper,
we present AutoVideo, a Python system for automated video action recognition.
It currently supports seven action recognition algorithms and various
pre-processing modules. Unlike the existing libraries that only provide model
zoos, AutoVideo is built with the standard pipeline language. The basic
building block is primitive, which wraps a pre-processing module or an
algorithm with some hyperparameters. AutoVideo is highly modular and
extendable. It can be easily combined with AutoML searchers. The pipeline
language is quite general so that we can easily enrich AutoVideo with
algorithms for various other video-related tasks in the future. AutoVideo is
released under MIT license at https://github.com/datamllab/autovideo
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation. (arXiv:2108.07482v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07482">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the knowledge distillation (KD) strategy for
object detection and propose an effective framework applicable to both
homogeneous and heterogeneous student-teacher pairs. The conventional feature
imitation paradigm introduces imitation masks to focus on informative
foreground areas while excluding the background noises. However, we find that
those methods fail to fully utilize the semantic information in all feature
pyramid levels, which leads to inefficiency for knowledge distillation between
FPN-based detectors. To this end, we propose a novel semantic-guided feature
imitation technique, which automatically performs soft matching between feature
pairs across all pyramid levels to provide the optimal guidance to the student.
To push the envelop even further, we introduce contrastive distillation to
effectively capture the information encoded in the relationship between
different feature regions. Finally, we propose a generalized detection KD
pipeline, which is capable of distilling both homogeneous and heterogeneous
detector pairs. Our method consistently outperforms the existing detection KD
techniques, and works when (1) components in the framework are used separately
and in conjunction; (2) for both homogeneous and heterogenous student-teacher
pairs and (3) on multiple detection benchmarks. With a powerful
X101-FasterRCNN-Instaboost detector as the teacher, R50-FasterRCNN reaches
44.0% AP, R50-RetinaNet reaches 43.3% AP and R50-FCOS reaches 43.1% AP on COCO
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full-Cycle Energy Consumption Benchmark for Low-Carbon Computer Vision. (arXiv:2108.13465v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13465">
<div class="article-summary-box-inner">
<span><p>The energy consumption of deep learning models is increasing at a
breathtaking rate, which raises concerns due to potential negative effects on
carbon neutrality in the context of global warming and climate change. With the
progress of efficient deep learning techniques, e.g., model compression,
researchers can obtain efficient models with fewer parameters and smaller
latency. However, most of the existing efficient deep learning methods do not
explicitly consider energy consumption as a key performance indicator.
Furthermore, existing methods mostly focus on the inference costs of the
resulting efficient models, but neglect the notable energy consumption
throughout the entire life cycle of the algorithm. In this paper, we present
the first large-scale energy consumption benchmark for efficient computer
vision models, where a new metric is proposed to explicitly evaluate the
full-cycle energy consumption under different model usage intensity. The
benchmark can provide insights for low carbon emission when selecting efficient
deep learning algorithms in different model usage scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System. (arXiv:2109.03144v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03144">
<div class="article-summary-box-inner">
<span><p>Optical Character Recognition (OCR) systems have been widely used in various
of application scenarios. Designing an OCR system is still a challenging task.
In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR)
to balance the accuracy against the efficiency. In order to improve the
accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more
robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better
text detector and a better text recognizer, which include Collaborative Mutual
Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual
Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the
precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost.
It is also comparable to the server models of the PP-OCR which uses ResNet
series as backbones. All of the above mentioned models are open-sourced and the
code is available in the GitHub repository PaddleOCR which is powered by
PaddlePaddle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Unsupervised Learning of Visual Representations and Categories. (arXiv:2109.05675v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05675">
<div class="article-summary-box-inner">
<span><p>Real world learning scenarios involve a nonstationary distribution of classes
with sequential dependencies among the samples, in contrast to the standard
machine learning formulation of drawing samples independently from a fixed,
typically uniform distribution. Furthermore, real world interactions demand
learning on-the-fly from few or no class labels. In this work, we propose an
unsupervised model that simultaneously performs online visual representation
learning and few-shot learning of new categories without relying on any class
labels. Our model is a prototype-based memory network with a control component
that determines when to form a new class prototype. We formulate it as an
online Gaussian mixture model, where components are created online with only a
single new example, and assignments do not have to be balanced, which permits
an approximation to natural imbalanced distributions from uncurated raw data.
Learning includes a contrastive loss that encourages different views of the
same image to be assigned to the same prototype. The result is a mechanism that
forms categorical representations of objects in nonstationary environments.
Experiments show that our method can learn from an online stream of visual
input data and is significantly better at category recognition compared to
state-of-the-art self-supervised learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Discrete Wavelet Pooling (LDW-Pooling) For Convolutional Networks. (arXiv:2109.06638v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06638">
<div class="article-summary-box-inner">
<span><p>Pooling is a simple but essential layer in modern deep CNN architectures for
feature aggregation and extraction. Typical CNN design focuses on the conv
layers and activation functions, while leaving the pooling layers with fewer
options. We introduce the Learning Discrete Wavelet Pooling (LDW-Pooling) that
can be applied universally to replace standard pooling operations to better
extract features with improved accuracy and efficiency. Motivated from the
wavelet theory, we adopt the low-pass (L) and high-pass (H) filters
horizontally and vertically for pooling on a 2D feature map. Feature signals
are decomposed into four (LL, LH, HL, HH) subbands to retain features better
and avoid information dropping. The wavelet transform ensures features after
pooling can be fully preserved and recovered. We next adopt an energy-based
attention learning to fine-select crucial and representative features.
LDW-Pooling is effective and efficient when compared with other
state-of-the-art pooling techniques such as WaveletPooling and LiftPooling.
Extensive experimental validation shows that LDW-Pooling can be applied to a
wide range of standard CNN architectures and consistently outperform standard
(max, mean, mixed, and stochastic) pooling operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An optimised deep spiking neural network architecture without gradients. (arXiv:2109.12813v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12813">
<div class="article-summary-box-inner">
<span><p>We present an end-to-end trainable modular event-driven neural architecture
that uses local synaptic and threshold adaptation rules to perform
transformations between arbitrary spatio-temporal spike patterns. The
architecture represents a highly abstracted model of existing Spiking Neural
Network (SNN) architectures. The proposed Optimized Deep Event-driven Spiking
neural network Architecture (ODESA) can simultaneously learn hierarchical
spatio-temporal features at multiple arbitrary time scales. ODESA performs
online learning without the use of error back-propagation or the calculation of
gradients. Through the use of simple local adaptive selection thresholds at
each node, the network rapidly learns to appropriately allocate its neuronal
resources at each layer for any given problem without using a real-valued error
measure. These adaptive selection thresholds are the central feature of ODESA,
ensuring network stability and remarkable robustness to noise as well as to the
selection of initial system parameters. Network activations are inherently
sparse due to a hard Winner-Take-All (WTA) constraint at each layer. We
evaluate the architecture on existing spatio-temporal datasets, including the
spike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based
on International Morse Code that we created. These tests demonstrate the
hierarchical spatio-temporal learning capabilities of ODESA. Through these
tests, we demonstrate ODESA can optimally solve practical and highly
challenging hierarchical spatio-temporal learning tasks with the minimum
possible number of computing nodes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Landmark Detection Based Spatiotemporal Motion Estimation for 4D Dynamic Medical Images. (arXiv:2109.14805v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14805">
<div class="article-summary-box-inner">
<span><p>Motion estimation is a fundamental step in dynamic medical image processing
for the assessment of target organ anatomy and function. However, existing
image-based motion estimation methods, which optimize the motion field by
evaluating the local image similarity, are prone to produce implausible
estimation, especially in the presence of large motion. In this study, we
provide a novel motion estimation framework of Dense-Sparse-Dense (DSD), which
comprises two stages. In the first stage, we process the raw dense image to
extract sparse landmarks to represent the target organ anatomical topology and
discard the redundant information that is unnecessary for motion estimation.
For this purpose, we introduce an unsupervised 3D landmark detection network to
extract spatially sparse but representative landmarks for the target organ
motion estimation. In the second stage, we derive the sparse motion
displacement from the extracted sparse landmarks of two images of different
time points. Then, we present a motion reconstruction network to construct the
motion field by projecting the sparse landmarks displacement back into the
dense image domain. Furthermore, we employ the estimated motion field from our
two-stage DSD framework as initialization and boost the motion estimation
quality in light-weight yet effective iterative optimization. We evaluate our
method on two dynamic medical imaging tasks to model cardiac motion and lung
respiratory motion, respectively. Our method has produced superior motion
estimation accuracy compared to existing comparative methods. Besides, the
extensive experimental results demonstrate that our solution can extract well
representative anatomical landmarks without any requirement of manual
annotation. Our code is publicly available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PubTables-1M: Towards a universal dataset and metrics for training and evaluating table extraction models. (arXiv:2110.00061v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00061">
<div class="article-summary-box-inner">
<span><p>Recently, interest has grown in applying machine learning to the problem of
table structure inference and extraction from unstructured documents. However,
progress in this area has been challenging both to make and to measure, due to
several issues that arise in training and evaluating models from labeled data.
This includes challenges as fundamental as the lack of a single definitive
ground truth output for each input sample and the lack of an ideal metric for
measuring partial correctness for this task. To address these issues we propose
a new dataset, PubMed Tables One Million (PubTables-1M), and a new class of
metric, grid table similarity (GriTS). PubTables-1M is nearly twice as large as
the previous largest comparable dataset, contains highly-detailed structure
annotations, and can be used for models across multiple architectures and
modalities. Further, it addresses issues such as ambiguity and lack of
consistency in the annotations via a novel canonicalization and quality control
procedure. We apply DETR to table extraction for the first time and show that
object detection models trained on PubTables-1M produce excellent results
out-of-the-box for all three tasks of detection, structure recognition, and
functional analysis. It is our hope that PubTables-1M and GriTS can further
progress in this area by creating data and metrics suitable for training and
evaluating a wide variety of models for table extraction. Data and code will be
released at https://github.com/microsoft/table-transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From SLAM to Situational Awareness: Challenges and Survey. (arXiv:2110.00273v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00273">
<div class="article-summary-box-inner">
<span><p>The knowledge that an intelligent and autonomous mobile robot has and is able
to acquire of itself and the environment, namely the situation, limits its
reasoning, decision-making, and execution skills to efficiently and safely
perform complex missions. Situational awareness is a basic capability of humans
that has been deeply studied in fields like Psychology, Military, Aerospace,
Education, etc., but it has barely been considered in robotics, which has
focused on ideas such as sensing, perception, sensor fusion, state estimation,
localization and mapping, spatial AI, etc. In our research, we connected the
broad multidisciplinary existing knowledge on situational awareness with its
counterpart in mobile robotics. In this paper, we survey the state-of-the-art
robotics algorithms, we analyze the situational awareness aspects that have
been covered by them, and we discuss their missing points. We found out that
the existing robotics algorithms are still missing manifold important aspects
of situational awareness. As a consequence, we conclude that these missing
features are limiting the performance of robotic situational awareness, and
further research is needed to overcome this challenge. We see this as an
opportunity, and provide our vision for future research on robotic situational
awareness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MovingFashion: a Benchmark for the Video-to-Shop Challenge. (arXiv:2110.02627v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02627">
<div class="article-summary-box-inner">
<span><p>Retrieving clothes which are worn in social media videos (Instagram, TikTok)
is the latest frontier of e-fashion, referred to as "video-to-shop" in the
computer vision literature. In this paper we present MovingFashion, the first
publicly available dataset to cope with this challenge. MovingFashion is
composed of 14855 social videos, each one of them associated to e-commerce
"shop" images where the corresponding clothing items are clearly portrayed. In
addition, we present a network for retrieving the shop images in this scenario,
dubbed SEAM Match-RCNN. The model is trained by image-to-video domain
adaptation, allowing to use video sequences where only their association with a
shop image is given, eliminating the need of millions of annotated bounding
boxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted
sum of few frames (10) of a social video is enough to individuate the correct
product within the first 5 retrieved items in a 14K+ shop element gallery with
an accuracy of 80%. This provides the best performance on MovingFashion,
comparing exhaustively against the related state-of-the-art approaches and
alternative baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Thing to Fool them All: Generating Interpretable, Universal, and Physically-Realizable Adversarial Features. (arXiv:2110.03605v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03605">
<div class="article-summary-box-inner">
<span><p>It is well understood that modern deep networks are vulnerable to adversarial
attacks. However, conventional methods fail to produce adversarial
perturbations that are intelligible to humans, and they pose limited threats in
the physical world. To study feature-class associations in networks and better
understand the real-world threats they face, we develop feature-level
adversarial perturbations using deep image generators and a novel optimization
objective. We term these feature-fool attacks. We show that they are versatile
and use them to generate targeted feature-level attacks at the ImageNet scale
that are simultaneously interpretable, universal to any source image, and
physically-realizable. These attacks can also reveal spurious,
semantically-describable feature/class associations, and we use them to guide
the design of "copy/paste" adversaries in which one natural image is pasted
into another to cause a targeted misclassification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVG-Net: An SVG-based Trajectory Prediction Model. (arXiv:2110.03706v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03706">
<div class="article-summary-box-inner">
<span><p>Anticipating motions of vehicles in a scene is an essential problem for safe
autonomous driving systems. To this end, the comprehension of the scene's
infrastructure is often the main clue for predicting future trajectories. Most
of the proposed approaches represent the scene with a rasterized format and
some of the more recent approaches leverage custom vectorized formats. In
contrast, we propose representing the scene's information by employing Scalable
Vector Graphics (SVG). SVG is a well-established format that matches the
problem of trajectory prediction better than rasterized formats while being
more general than arbitrary vectorized formats. SVG has the potential to
provide the convenience and generality of raster-based solutions if coupled
with a powerful tool such as CNNs, for which we introduce SVG-Net. SVG-Net is a
Transformer-based Neural Network that can effectively capture the scene's
information from SVG inputs. Thanks to the self-attention mechanism in its
Transformers, SVG-Net can also adequately apprehend relations amongst the scene
and the agents. We demonstrate SVG-Net's effectiveness by evaluating its
performance on the publicly available Argoverse forecasting dataset. Finally,
we illustrate how, by using SVG, one can benefit from datasets and advancements
in other research fronts that also utilize the same input format. Our code is
available at https://vita-epfl.github.io/SVGNet/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Identity Preserving Landmark Synthesis for Face Reenactment. (arXiv:2110.04708v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04708">
<div class="article-summary-box-inner">
<span><p>Recent face reenactment works are limited by the coarse reference landmarks,
leading to unsatisfactory identity preserving performance due to the
distribution gap between the manipulated landmarks and those sampled from a
real person. To address this issue, we propose a fine-grained
identity-preserving landmark-guided face reenactment approach. The proposed
method has two novelties. First, a landmark synthesis network which is designed
to generate fine-grained landmark faces with more details. The network refines
the manipulated landmarks and generates a smooth and gradually changing face
landmark sequence with good identity preserving ability. Second, several novel
loss functions including synthesized face identity preserving loss,
foreground/background mask loss as well as boundary loss are designed, which
aims at synthesizing clear and sharp high-quality faces. Experiments are
conducted on our self-collected BeautySelfie and the public VoxCeleb1 datasets.
The presented qualitative and quantitative results show that our method can
reenact fine-grained higher quality faces with good ID-preserved appearance
details, fewer artifacts and clearer boundaries than state-of-the-art works.
Code will be released for reproduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Dual Relation Graph for Multi-label Image Recognition. (arXiv:2110.04722v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04722">
<div class="article-summary-box-inner">
<span><p>The simultaneous recognition of multiple objects in one image remains a
challenging task, spanning multiple events in the recognition field such as
various object scales, inconsistent appearances, and confused inter-class
relationships. Recent research efforts mainly resort to the statistic label
co-occurrences and linguistic word embedding to enhance the unclear semantics.
Different from these researches, in this paper, we propose a novel
Transformer-based Dual Relation learning framework, constructing complementary
relationships by exploring two aspects of correlation, i.e., structural
relation graph and semantic relation graph. The structural relation graph aims
to capture long-range correlations from object context, by developing a
cross-scale transformer-based architecture. The semantic graph dynamically
models the semantic meanings of image objects with explicit semantic-aware
constraints. In addition, we also incorporate the learnt structural
relationship into the semantic graph, constructing a joint relation graph for
robust representations. With the collaborative learning of these two effective
relation graphs, our approach achieves new state-of-the-art on two popular
multi-label recognition benchmarks, i.e., MS-COCO and VOC 2007 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEV-Net: Assessing Social Distancing Compliance by Joint People Localization and Geometric Reasoning. (arXiv:2110.04931v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04931">
<div class="article-summary-box-inner">
<span><p>Social distancing, an essential public health measure to limit the spread of
contagious diseases, has gained significant attention since the outbreak of the
COVID-19 pandemic. In this work, the problem of visual social distancing
compliance assessment in busy public areas, with wide field-of-view cameras, is
considered. A dataset of crowd scenes with people annotations under a bird's
eye view (BEV) and ground truth for metric distances is introduced, and several
measures for the evaluation of social distance detection systems are proposed.
A multi-branch network, BEV-Net, is proposed to localize individuals in world
coordinates and identify high-risk regions where social distancing is violated.
BEV-Net combines detection of head and feet locations, camera pose estimation,
a differentiable homography module to map image into BEV coordinates, and
geometric reasoning to produce a BEV map of the people locations in the scene.
Experiments on complex crowded scenes demonstrate the power of the approach and
show superior performance over baselines derived from methods in the
literature. Applications of interest for public health decision makers are
finally discussed. Datasets, code and pretrained models are publicly available
at GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DANIEL: A Fast and Robust Consensus Maximization Method for Point Cloud Registration with High Outlier Ratios. (arXiv:2110.05075v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05075">
<div class="article-summary-box-inner">
<span><p>Correspondence-based point cloud registration is a cornerstone in geometric
computer vision, robotics perception, photogrammetry and remote sensing, which
seeks to estimate the best rigid transformation between two point clouds from
the correspondences established over 3D keypoints. However, due to limited
robustness and accuracy, current 3D keypoint matching techniques are very prone
to yield outliers, probably even in very large numbers, making robust
estimation for point cloud registration of great importance. Unfortunately,
existing robust methods may suffer from high computational cost or insufficient
robustness when encountering high (or even extreme) outlier ratios, hardly
ideal enough for practical use. In this paper, we present a novel
time-efficient RANSAC-type consensus maximization solver, named DANIEL
(Double-layered sAmpliNg with consensus maximization based on stratIfied
Element-wise compatibiLity), for robust registration. DANIEL is designed with
two layers of random sampling, in order to find inlier subsets with the lowest
computational cost possible. Specifically, we: (i) apply the rigidity
constraint to prune raw outliers in the first layer of one-point sampling, (ii)
introduce a series of stratified element-wise compatibility tests to conduct
rapid compatibility checking between minimal models so as to realize more
efficient consensus maximization in the second layer of two-point sampling, and
(iii) probabilistic termination conditions are employed to ensure the timely
return of the final inlier set. Based on a variety of experiments over multiple
real datasets, we show that DANIEL is robust against over 99% outliers and also
significantly faster than existing state-of-the-art robust solvers (e.g.
RANSAC, FGR, GORE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Prototype Classifier for Few-shot Image Classification. (arXiv:2110.05076v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05076">
<div class="article-summary-box-inner">
<span><p>The prototypical network is a prototype classifier based on meta-learning and
is widely used for few-shot learning because it classifies unseen examples by
constructing class-specific prototypes without adjusting hyper-parameters
during meta-testing. Interestingly, recent research has attracted a lot of
attention, showing that a linear classifier with fine-tuning, which does not
use a meta-learning algorithm, performs comparably with the prototypical
network. However, fine-tuning requires additional hyper-parameters when
adapting a model to a new environment. In addition, although the purpose of
few-shot learning is to enable the model to quickly adapt to a new environment,
fine-tuning needs to be applied every time a new class appears, making fast
adaptation difficult. In this paper, we analyze how a prototype classifier
works equally well without fine-tuning and meta-learning. We experimentally
found that directly using the feature vector extracted using standard
pre-trained models to construct a prototype classifier in meta-testing does not
perform as well as the prototypical network and linear classifiers with
fine-tuning and feature vectors of pre-trained models. Thus, we derive a novel
generalization bound for the prototypical network and show that focusing on the
variance of the norm of a feature vector can improve performance. We
experimentally investigated several normalization methods for minimizing the
variance of the norm and found that the same performance can be obtained by
using the L2 normalization and embedding space transformation without
fine-tuning or meta-learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViSeRet: A simple yet effective approach to moment retrieval via fine-grained video segmentation. (arXiv:2110.05146v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05146">
<div class="article-summary-box-inner">
<span><p>Video-text retrieval has many real-world applications such as media
analytics, surveillance, and robotics. This paper presents the 1st place
solution to the video retrieval track of the ICCV VALUE Challenge 2021. We
present a simple yet effective approach to jointly tackle two video-text
retrieval tasks (video retrieval and video corpus moment retrieval) by
leveraging the model trained only on the video retrieval task. In addition, we
create an ensemble model that achieves the new state-of-the-art performance on
all four datasets (TVr, How2r, YouCook2r, and VATEXr) presented in the VALUE
Challenge.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-13 23:02:45.840744425 UTC">2021-10-13 23:02:45 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>