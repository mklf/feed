<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-17T01:30:00Z">02-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonverbal Sound Detection for Disordered Speech. (arXiv:2202.07750v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07750">
<div class="article-summary-box-inner">
<span><p>Voice assistants have become an essential tool for people with various
disabilities because they enable complex phone- or tablet-based interactions
without the need for fine-grained motor control, such as with touchscreens.
However, these systems are not tuned for the unique characteristics of
individuals with speech disorders, including many of those who have a
motor-speech disorder, are deaf or hard of hearing, have a severe stutter, or
are minimally verbal. We introduce an alternative voice-based input system
which relies on sound event detection using fifteen nonverbal mouth sounds like
"pop," "click," or "eh." This system was designed to work regardless of ones'
speech abilities and allows full access to existing technology. In this paper,
we describe the design of a dataset, model considerations for real-world
deployment, and efforts towards model personalization. Our fully-supervised
model achieves segment-level precision and recall of 88.6% and 88.4% on an
internal dataset of 710 adults, while achieving 0.31 false positives per hour
on aggressors such as speech. Five-shot personalization enables satisfactory
performance in 84.5% of cases where the generic model fails.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Russian SuperGLUE 1.1: Revising the Lessons not Learned by Russian NLP models. (arXiv:2202.07791v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07791">
<div class="article-summary-box-inner">
<span><p>In the last year, new neural architectures and multilingual pre-trained
models have been released for Russian, which led to performance evaluation
problems across a range of language understanding tasks.
</p>
<p>This paper presents Russian SuperGLUE 1.1, an updated benchmark styled after
GLUE for Russian NLP models. The new version includes a number of technical,
user experience and methodological improvements, including fixes of the
benchmark vulnerabilities unresolved in the previous version: novel and
improved tests for understanding the meaning of a word in context (RUSSE) along
with reading comprehension and common sense reasoning (DaNetQA, RuCoS, MuSeRC).
Together with the release of the updated datasets, we improve the benchmark
toolkit based on \texttt{jiant} framework for consistent training and
evaluation of NLP-models of various architectures which now supports the most
recent models for Russian. Finally, we provide the integration of Russian
SuperGLUE with a framework for industrial evaluation of the open-source models,
MOROCCO (MOdel ResOurCe COmparison), in which the models are evaluated
according to the weighted average metric over all tasks, the inference speed,
and the occupied amount of RAM. Russian SuperGLUE is publicly available at
https://russiansuperglue.com/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Generation for Unknown Libraries via Reading API Documentations. (arXiv:2202.07806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07806">
<div class="article-summary-box-inner">
<span><p>Open-domain code generation is a challenging problem because the set of
functions and classes that we use are frequently changed and extended in
programming communities. We consider the challenge of code generation for
unknown libraries without additional training. In this paper, we explore a
framework of code generation that can refer to relevant API documentations like
human programmers to handle unknown libraries. As a first step of this
direction, we implement a model that can extract relevant code signatures from
API documentations based on a natural language intent and copy primitives from
the extracted signatures. Moreover, to evaluate code generation for unknown
libraries and our framework, we extend an existing dataset of open-domain code
generation and resplit it so that the evaluation data consist of only examples
using the libraries that do not appear in the training data. Experiments on our
new split show that baseline encoder-decoder models cannot generate code using
primitives of unknown libraries as expected. In contrast, our model outperforms
the baseline on the new split and can properly generate unknown primitives when
extracted code signatures are noiseless.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech. (arXiv:2202.07816v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07816">
<div class="article-summary-box-inner">
<span><p>Expressive text-to-speech (TTS) has become a hot research topic recently,
mainly focusing on modeling prosody in speech. Prosody modeling has several
challenges: 1) the extracted pitch used in previous prosody modeling works have
inevitable errors, which hurts the prosody modeling; 2) different attributes of
prosody (e.g., pitch, duration and energy) are dependent on each other and
produce the natural prosody together; and 3) due to high variability of prosody
and the limited amount of high-quality data for TTS training, the distribution
of prosody cannot be fully shaped. To tackle these issues, we propose
ProsoSpeech, which enhances the prosody using quantized latent vectors
pre-trained on large-scale unpaired and low-quality text and speech data.
Specifically, we first introduce a word-level prosody encoder, which quantizes
the low-frequency band of the speech and compresses prosody attributes in the
latent prosody vector (LPV). Then we introduce an LPV predictor, which predicts
LPV given word sequence. We pre-train the LPV predictor on large-scale text and
low-quality speech data and fine-tune it on the high-quality TTS dataset.
Finally, our model can generate expressive speech conditioned on the predicted
LPV. Experimental results show that ProsoSpeech can generate speech with richer
prosody compared with baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Speech Recognition By Learning Conversation-level Characteristics. (arXiv:2202.07855v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07855">
<div class="article-summary-box-inner">
<span><p>Conversational automatic speech recognition (ASR) is a task to recognize
conversational speech including multiple speakers. Unlike sentence-level ASR,
conversational ASR can naturally take advantages from specific characteristics
of conversation, such as role preference and topical coherence. This paper
proposes a conversational ASR model which explicitly learns conversation-level
characteristics under the prevalent end-to-end neural framework. The highlights
of the proposed model are twofold. First, a latent variational module (LVM) is
attached to a conformer-based encoder-decoder ASR backbone to learn role
preference and topical coherence. Second, a topic model is specifically adopted
to bias the outputs of the decoder to words in the predicted topics.
Experiments on two Mandarin conversational ASR tasks show that the proposed
model achieves a maximum 12% relative character error rate (CER) reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The NLP Task Effectiveness of Long-Range Transformers. (arXiv:2202.07856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07856">
<div class="article-summary-box-inner">
<span><p>Transformer models cannot easily scale to long sequences due to their O(N^2)
time and space complexity. This has led to Transformer variants seeking to
lessen computational complexity, such as Longformer and Performer. While such
models have theoretically greater efficiency, their effectiveness on real NLP
tasks has not been well studied. We benchmark 7 variants of Transformer models
on 5 difficult NLP tasks and 7 datasets. We design experiments to isolate the
effect of pretraining and hyperparameter settings, to focus on their capacity
for long-range attention. Moreover, we present various methods to investigate
attention behaviors, to illuminate model details beyond metric scores. We find
that attention of long-range transformers has advantages on content selection
and query-guided decoding, but they come with previously unrecognized drawbacks
such as insufficient attention to distant tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ITTC @ TREC 2021 Clinical Trials Track. (arXiv:2202.07858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07858">
<div class="article-summary-box-inner">
<span><p>This paper describes the submissions of the Natural Language Processing (NLP)
team from the Australian Research Council Industrial Transformation Training
Centre (ITTC) for Cognitive Computing in Medical Technologies to the TREC 2021
Clinical Trials Track. The task focuses on the problem of matching eligible
clinical trials to topics constituting a summary of a patient's admission
notes. We explore different ways of representing trials and topics using NLP
techniques, and then use a common retrieval model to generate the ranked list
of relevant trials for each topic. The results from all our submitted runs are
well above the median scores for all topics, but there is still plenty of scope
for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIS2: A Simplified Commonsense Inference Evaluation for Story Prose. (arXiv:2202.07880v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07880">
<div class="article-summary-box-inner">
<span><p>Transformers have been showing near-human performance on a variety of tasks,
but they are not without their limitations. We discuss the issue of conflating
results of transformers that are instructed to do multiple tasks
simultaneously. In particular, we focus on the domain of commonsense reasoning
within story prose, which we call contextual commonsense inference (CCI). We
look at the GLUCOSE (Mostafazadeh et al 2020) dataset and task for predicting
implicit commonsense inferences between story sentences. Since the GLUCOSE task
simultaneously generates sentences and predicts the CCI relation, there is a
conflation in the results. Is the model really measuring CCI or is its ability
to generate grammatical text carrying the results? In this paper, we introduce
the task contextual commonsense inference in sentence selection (CIS$^2$), a
simplified task that avoids conflation by eliminating language generation
altogether. Our findings emphasize the necessity of future work to disentangle
language generation from the desired NLP tasks at hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Transfer from Large-scale Pretrained Language Models to End-to-end Speech Recognizers. (arXiv:2202.07894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07894">
<div class="article-summary-box-inner">
<span><p>End-to-end speech recognition is a promising technology for enabling compact
automatic speech recognition (ASR) systems since it can unify the acoustic and
language model into a single neural network. However, as a drawback, training
of end-to-end speech recognizers always requires transcribed utterances. Since
end-to-end models are also known to be severely data hungry, this constraint is
crucial especially because obtaining transcribed utterances is costly and can
possibly be impractical or impossible. This paper proposes a method for
alleviating this issue by transferring knowledge from a language model neural
network that can be pretrained with text-only data. Specifically, this paper
attempts to transfer semantic knowledge acquired in embedding vectors of
large-scale language models. Since embedding vectors can be assumed as implicit
representations of linguistic information such as part-of-speech, intent, and
so on, those are also expected to be useful modeling cues for ASR decoders.
This paper extends two types of ASR decoders, attention-based decoders and
neural transducers, by modifying training loss functions to include embedding
prediction terms. The proposed systems were shown to be effective for error
rate reduction without incurring extra computational costs in the decoding
phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroGen: Efficient Zero-shot Learning via Dataset Generation. (arXiv:2202.07922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07922">
<div class="article-summary-box-inner">
<span><p>There is a growing interest in dataset generation recently due to the
superior generative capacity of large pre-trained language models (PLMs). In
this paper, we study a flexible and efficient zero-short learning method,
ZeroGen. Given a zero-shot task, we first generate a dataset from scratch using
PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM)
under the supervision of the synthesized dataset. This approach allows highly
efficient inference as the final task model only has orders of magnitude fewer
parameters comparing to PLMs (e.g., GPT2-XL). Apart from being annotation-free
and efficient, we argue that ZeroGen can also provide useful insights from the
perspective of data-free model-agnostic knowledge distillation, and
unreferenced text generation evaluation. Experiments and analysis on different
NLP tasks, namely, text classification, question answering, and natural
language inference), show the effectiveness of ZeroGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation. (arXiv:2202.07959v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07959">
<div class="article-summary-box-inner">
<span><p>We propose EdgeFormer -- a parameter-efficient Transformer of the
encoder-decoder architecture for on-device seq2seq generation, which is
customized under the strict computation and memory constraints. EdgeFormer
proposes two novel principles for cost-effective parameterization and further
enhance the model with efficient layer adaptation. We conduct extensive
experiments on two practical on-device seq2seq tasks: Machine Translation and
Grammatical Error Correction, and show that EdgeFormer can effectively
outperform previous parameter-efficient Transformer baselines and achieve very
competitive results with knowledge distillation under both the computation and
memory constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Parameter-Efficient Tuning: Are We Really There Yet?. (arXiv:2202.07962v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07962">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient tuning (PETuning) methods have been deemed by many as the
new paradigm for using pretrained language models (PLMs). By tuning just a
fraction amount of parameters comparing to full model finetuning, PETuning
methods claim to have achieved performance on par with or even better than
finetuning. In this work, we take a step back and re-examine these PETuning
methods by conducting the first comprehensive investigation into the training
and evaluation of PETuning methods. We found the problematic validation and
testing practice in current studies, when accompanied by the instability nature
of PETuning methods, has led to unreliable conclusions. When being compared
under a truly fair evaluation protocol, PETuning cannot yield consistently
competitive performance while finetuning remains to be the best-performing
method in medium- and high-resource settings. We delve deeper into the cause of
the instability and observed that model size does not explain the phenomenon
but training iteration positively correlates with the stability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">m-Nearly k-Universal Words -- Investigating Simon Congruence. (arXiv:2202.07981v1 [math.CO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07981">
<div class="article-summary-box-inner">
<span><p>Determining the index of the Simon congruence is a long outstanding open
problem. Two words $u$ and $v$ are called Simon congruent if they have the same
set of scattered factors, which are parts of the word in the correct order but
not necessarily consecutive, e.g., $\mathtt{oath}$ is a scattered factor of
$\mathtt{logarithm}$. Following the idea of scattered factor $k$-universality,
we investigate $m$-nearly $k$-universality, i.e., words where $m$ scattered
factors of length $k$ are absent, w.r.t. Simon congruence. We present a full
characterisation as well as the index of the congruence for $m=1$. For $m\neq
1$, we show some results if in addition $w$ is $(k-1)$-universal as well as
some further insights for different $m$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Self Shuffle Language. (arXiv:2202.07988v1 [math.CO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07988">
<div class="article-summary-box-inner">
<span><p>The shuffle product \(u\shuffle v\) of two words \(u\) and \(v\) is the set
of all words which can be obtained by interleaving \(u\) and \(v\). Motivated
by the paper \emph{The Shuffle Product: New Research Directions} by Restivo
(2015) we investigate a special case of the shuffle product. In this work we
consider the shuffle of a word with itself called the \emph{self shuffle} or
\emph{shuffle square}, showing first that the self shuffle language and the
shuffle of the language are in general different sets. We prove that the
language of all words arising as a self shuffle of some word is context
sensitive but not context free. Furthermore, we show that the self shuffle \(w
\shuffle w\) uniquely determines \(w\).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADIMA: Abuse Detection In Multilingual Audio. (arXiv:2202.07991v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07991">
<div class="article-summary-box-inner">
<span><p>Abusive content detection in spoken text can be addressed by performing
Automatic Speech Recognition (ASR) and leveraging advancements in natural
language processing. However, ASR models introduce latency and often perform
sub-optimally for profane words as they are underrepresented in training
corpora and not spoken clearly or completely. Exploration of this problem
entirely in the audio domain has largely been limited by the lack of audio
datasets. Building on these challenges, we propose ADIMA, a novel,
linguistically diverse, ethically sourced, expert annotated and well-balanced
multilingual profanity detection audio dataset comprising of 11,775 audio
samples in 10 Indic languages spanning 65 hours and spoken by 6,446 unique
users. Through quantitative experiments across monolingual and cross-lingual
zero-shot settings, we take the first step in democratizing audio based content
moderation in Indic languages and set forth our dataset to pave future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Should You Mask 15% in Masked Language Modeling?. (arXiv:2202.08005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08005">
<div class="article-summary-box-inner">
<span><p>Masked language models conventionally use a masking rate of 15% due to the
belief that more masking would provide insufficient context to learn good
representations, and less masking would make training too expensive.
Surprisingly, we find that masking up to 40% of input tokens can outperform the
15% baseline, and even masking 80% can preserve most of the performance, as
measured by fine-tuning on downstream tasks. Increasing the masking rates has
two distinct effects, which we investigate through careful ablations: (1) A
larger proportion of input tokens are corrupted, reducing the context size and
creating a harder task, and (2) models perform more predictions, which benefits
training. We observe that larger models in particular favor higher masking
rates, as they have more capacity to perform the harder task. We also connect
our findings to sophisticated masking schemes such as span masking and PMI
masking, as well as BERT's curious 80-10-10 corruption strategy, and find that
simple uniform masking with [MASK] replacements can be competitive at higher
masking rates. Our results contribute to a better understanding of masked
language modeling and point to new avenues for efficient pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks. (arXiv:2202.08011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08011">
<div class="article-summary-box-inner">
<span><p>The research of open-domain dialog systems has been greatly prospered by
neural models trained on large-scale corpora, however, such corpora often
introduce various safety problems (e.g., offensive languages, biases, and toxic
behaviors) that significantly hinder the deployment of dialog systems in
practice. Among all these unsafe issues, addressing social bias is more complex
as its negative impact on marginalized populations is usually expressed
implicitly, thus requiring normative reasoning and rigorous analysis. In this
paper, we focus our investigation on social bias detection of dialog safety
problems. We first propose a novel Dial-Bias Frame for analyzing the social
bias in conversations pragmatically, which considers more comprehensive
bias-related analyses rather than simple dichotomy annotations. Based on the
proposed framework, we further introduce CDail-Bias Dataset that, to our
knowledge, is the first well-annotated Chinese social bias dialog dataset. In
addition, we establish several dialog bias detection benchmarks at different
label granularities and input types (utterance-level and context-level). We
show that the proposed in-depth analyses together with these benchmarks in our
Dial-Bias Frame are necessary and essential to bias detection tasks and can
benefit building safe dialog systems in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decorrelate Irrelevant, Purify Relevant: Overcome Textual Spurious Correlations from a Feature Perspective. (arXiv:2202.08048v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08048">
<div class="article-summary-box-inner">
<span><p>Natural language understanding (NLU) models tend to rely on spurious
correlations (\emph{i.e.}, dataset bias) to achieve high performance on
in-distribution datasets but poor performance on out-of-distribution ones. Most
of the existing debiasing methods often identify and weaken these samples with
biased features (\emph{i.e.}, superficial surface features that cause such
spurious correlations). However, down-weighting these samples obstructs the
model in learning from the non-biased parts of these samples. To tackle this
challenge, in this paper, we propose to eliminate spurious correlations in a
fine-grained manner from a feature space perspective. Specifically, we
introduce Random Fourier Features and weighted re-sampling to decorrelate the
dependencies between features to mitigate spurious correlations. After
obtaining decorrelated features, we further design a mutual-information-based
method to purify them, which forces the model to learn features that are more
relevant to tasks. Extensive experiments on two well-studied NLU tasks
including Natural Language Inference and Fact Verification demonstrate that our
method is superior to other comparative approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08063">
<div class="article-summary-box-inner">
<span><p>Knowledge Extraction (KE) which aims to extract structural information from
unstructured texts often suffers from data scarcity and emerging unseen types,
i.e., low-resource scenarios. Many neural approaches on low-resource KE have
been widely investigated and achieved impressive performance. In this paper, we
present a literature review towards KE in low-resource scenarios, and
systematically categorize existing works into three paradigms: (1) exploiting
higher-resource data, (2) exploiting stronger models, and (3) exploiting data
and models together. In addition, we describe promising applications and
outline some potential directions for future research. We hope that our survey
can help both the academic and industrial community to better understand this
field, inspire more ideas and boost broader applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XFBoost: Improving Text Generation with Controllable Decoders. (arXiv:2202.08124v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08124">
<div class="article-summary-box-inner">
<span><p>Multimodal conditionality in transformer-based natural language models has
demonstrated state-of-the-art performance in the task of product description
generation. Recent approaches condition a language model on one or more images
and other textual metadata to achieve near-human performance for describing
products from e-commerce stores. However, generated descriptions may exhibit
degrees of inaccuracy or even contradictory claims relative to the inputs of a
given product. In this paper, we propose a controllable language generation
framework called Extract-Finetune-Boost (XFBoost), which addresses the problem
of inaccurate low-quality inference. By using visual semantic attributes as
constraints at the decoding stage of the generation process and finetuning the
language model with policy gradient techniques, the XFBoost framework is found
to produce significantly more descriptive text with higher image relevancy,
outperforming baselines and lowering the frequency of factually inaccurate
descriptions. We further demonstrate the application of XFBoost to online
learning wherein human-in-the-loop critics improve language models with active
feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Processing the structure of documents: Logical Layout Analysis of historical newspapers in French. (arXiv:2202.08125v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08125">
<div class="article-summary-box-inner">
<span><p>Background. In recent years, libraries and archives led important
digitisation campaigns that opened the access to vast collections of historical
documents. While such documents are often available as XML ALTO documents, they
lack information about their logical structure. In this paper, we address the
problem of Logical Layout Analysis applied to historical documents in French.
We propose a rule-based method, that we evaluate and compare with two
Machine-Learning models, namely RIPPER and Gradient Boosting. Our data set
contains French newspapers, periodicals and magazines, published in the first
half of the twentieth century in the Franche-Comt\'e Region. Results. Our
rule-based system outperforms the two other models in nearly all evaluations.
It has especially better Recall results, indicating that our system covers more
types of every logical label than the other two models. When comparing RIPPER
with Gradient Boosting, we can observe that Gradient Boosting has better
Precision scores but RIPPER has better Recall scores. Conclusions. The
evaluation shows that our system outperforms the two Machine Learning models,
and provides significantly higher Recall. It also confirms that our system can
be used to produce annotated data sets that are large enough to envisage
Machine Learning or Deep Learning approaches for the task of Logical Layout
Analysis. Combining rules and Machine Learning models into hybrid systems could
potentially provide even better performances. Furthermore, as the layout in
historical documents evolves rapidly, one possible solution to overcome this
problem would be to apply Rule Learning algorithms to bootstrap rule sets
adapted to different publication periods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs. (arXiv:2202.08138v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08138">
<div class="article-summary-box-inner">
<span><p>We consider the task of temporal human action localization in lifestyle
vlogs. We introduce a novel dataset consisting of manual annotations of
temporal localization for 13,000 narrated actions in 1,200 video clips. We
present an extensive analysis of this data, which allows us to better
understand how the language and visual modalities interact throughout the
videos. We propose a simple yet effective method to localize the narrated
actions based on their expected duration. Through several experiments and
analyses, we show that our method brings complementary information with respect
to previous methods, and leads to improvements over previous work for the task
of temporal action localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voice Filter: Few-shot text-to-speech speaker adaptation using voice conversion as a post-processing module. (arXiv:2202.08164v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08164">
<div class="article-summary-box-inner">
<span><p>State-of-the-art text-to-speech (TTS) systems require several hours of
recorded speech data to generate high-quality synthetic speech. When using
reduced amounts of training data, standard TTS models suffer from speech
quality and intelligibility degradations, making training low-resource TTS
systems problematic. In this paper, we propose a novel extremely low-resource
TTS method called Voice Filter that uses as little as one minute of speech from
a target speaker. It uses voice conversion (VC) as a post-processing module
appended to a pre-existing high-quality TTS system and marks a conceptual shift
in the existing TTS paradigm, framing the few-shot TTS problem as a VC task.
Furthermore, we propose to use a duration-controllable TTS system to create a
parallel speech corpus to facilitate the VC task. Results show that the Voice
Filter outperforms state-of-the-art few-shot speech synthesis techniques in
terms of objective and subjective metrics on one minute of speech on a diverse
set of voices, while being competitive against a TTS model built on 30 times
more data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capitalization Normalization for Language Modeling with an Accurate and Efficient Hierarchical RNN Model. (arXiv:2202.08171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08171">
<div class="article-summary-box-inner">
<span><p>Capitalization normalization (truecasing) is the task of restoring the
correct case (uppercase or lowercase) of noisy text. We propose a fast,
accurate and compact two-level hierarchical word-and-character-based recurrent
neural network model. We use the truecaser to normalize user-generated text in
a Federated Learning framework for language modeling. A case-aware language
model trained on this normalized text achieves the same perplexity as a model
trained on text with gold capitalization. In a real user A/B experiment, we
demonstrate that the improvement translates to reduced prediction error rates
in a virtual keyboard application. Similarly, in an ASR language model fusion
experiment, we show reduction in uppercase character error rate and word error
rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphNLI: A Graph-based Natural Language Inference Model for Polarity Prediction in Online Debates. (arXiv:2202.08175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08175">
<div class="article-summary-box-inner">
<span><p>Online forums that allow participatory engagement between users have been
transformative for public discussion of important issues. However, debates on
such forums can sometimes escalate into full blown exchanges of hate or
misinformation. An important tool in understanding and tackling such problems
is to be able to infer the argumentative relation of whether a reply is
supporting or attacking the post it is replying to. This so called polarity
prediction task is difficult because replies may be based on external context
beyond a post and the reply whose polarity is being predicted. We propose
GraphNLI, a novel graph-based deep learning architecture that uses graph walk
techniques to capture the wider context of a discussion thread in a principled
fashion. Specifically, we propose methods to perform root-seeking graph walks
that start from a post and captures its surrounding context to generate
additional embeddings for the post. We then use these embeddings to predict the
polarity relation between a reply and the post it is replying to. We evaluate
the performance of our models on a curated debate dataset from Kialo, an online
debating platform. Our model outperforms relevant baselines, including S-BERT,
with an overall accuracy of 83%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to summarize from human feedback. (arXiv:2009.01325v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.01325">
<div class="article-summary-box-inner">
<span><p>As language models become more powerful, training and evaluation are
increasingly bottlenecked by the data and metrics used for a particular task.
For example, summarization models are often trained to predict human reference
summaries and evaluated using ROUGE, but both of these metrics are rough
proxies for what we really care about -- summary quality. In this work, we show
that it is possible to significantly improve summary quality by training a
model to optimize for human preferences. We collect a large, high-quality
dataset of human comparisons between summaries, train a model to predict the
human-preferred summary, and use that model as a reward function to fine-tune a
summarization policy using reinforcement learning. We apply our method to a
version of the TL;DR dataset of Reddit posts and find that our models
significantly outperform both human reference summaries and much larger models
fine-tuned with supervised learning alone. Our models also transfer to CNN/DM
news articles, producing summaries nearly as good as the human reference
without any news-specific fine-tuning. We conduct extensive analyses to
understand our human feedback dataset and fine-tuned models We establish that
our reward model generalizes to new datasets, and that optimizing our reward
model results in better summaries than optimizing ROUGE according to humans. We
hope the evidence from our paper motivates machine learning researchers to pay
closer attention to how their training loss affects the model behavior they
actually want.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning for on-line Sequence Transformation. (arXiv:2105.14097v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14097">
<div class="article-summary-box-inner">
<span><p>A number of problems in the processing of sound and natural language, as well
as in other areas, can be reduced to simultaneously reading an input sequence
and writing an output sequence of generally different length. There are well
developed methods that produce the output sequence based on the entirely known
input. However, efficient methods that enable such transformations on-line do
not exist. In this paper we introduce an architecture that learns with
reinforcement to make decisions about whether to read a token or write another
token. This architecture is able to transform potentially infinite sequences
on-line. In an experimental study we compare it with state-of-the-art methods
for neural machine translation. While it produces slightly worse translations
than Transformer, it outperforms the autoencoder with attention, even though
our architecture translates texts on-line thereby solving a more difficult
problem than both reference methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v8 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08597">
<div class="article-summary-box-inner">
<span><p>Answering complex questions over knowledge bases (KB-QA) faces huge input
data with billions of facts, involving millions of entities and thousands of
predicates. For efficiency, QA systems first reduce the answer search space by
identifying a set of facts that is likely to contain all answers and relevant
cues. The most common technique for doing this is to apply named entity
disambiguation (NED) systems to the question, and retrieve KB facts for the
disambiguated entities. This work presents CLOCQ, an efficient method that
prunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses
a top-k query processor over score-ordered lists of KB items that combine
signals about lexical matching, relevance to the question, coherence among
candidate items, and connectivity in the KB graph. Experiments with two recent
QA benchmarks for complex questions demonstrate the superiority of CLOCQ over
state-of-the-art baselines with respect to answer presence, size of the search
space, and runtimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v5 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00165">
<div class="article-summary-box-inner">
<span><p>Self- and semi-supervised learning methods have been actively investigated to
reduce labeled training data or enhance the model performance. However, the
approach mostly focus on in-domain performance for public datasets. In this
study, we utilize the combination of self- and semi-supervised learning methods
to solve unseen domain adaptation problem in a large-scale production setting
for online ASR model. This approach demonstrates that using the source domain
data with a small fraction of the target domain data (3%) can recover the
performance gap compared to a full data baseline: relative 13.5% WER
improvement for target domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Automated Unit Tests for Unsupervised Code Translation. (arXiv:2110.06773v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06773">
<div class="article-summary-box-inner">
<span><p>With little to no parallel data available for programming languages,
unsupervised methods are well-suited to source code translation. However, the
majority of unsupervised machine translation approaches rely on
back-translation, a method developed in the context of natural language
translation and one that inherently involves training on noisy inputs.
Unfortunately, source code is highly sensitive to small changes; a single token
can result in compilation failures or erroneous programs, unlike natural
languages where small inaccuracies may not change the meaning of a sentence. To
address this issue, we propose to leverage an automated unit-testing system to
filter out invalid translations, thereby creating a fully tested parallel
corpus. We found that fine-tuning an unsupervised model with this filtered data
set significantly reduces the noise in the translations so-generated,
comfortably outperforming the state-of-the-art for all language pairs studied.
In particular, for Java $\to$ Python and Python $\to$ C++ we outperform the
best previous methods by more than 16% and 24% respectively, reducing the error
rate by more than 35%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Language Model Integration for RNN Transducer based Speech Recognition. (arXiv:2110.06841v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06841">
<div class="article-summary-box-inner">
<span><p>The mismatch between an external language model (LM) and the implicitly
learned internal LM (ILM) of RNN-Transducer (RNN-T) can limit the performance
of LM integration such as simple shallow fusion. A Bayesian interpretation
suggests to remove this sequence prior as ILM correction. In this work, we
study various ILM correction-based LM integration methods formulated in a
common RNN-T framework. We provide a decoding interpretation on two major
reasons for performance improvement with ILM correction, which is further
experimentally verified with detailed analysis. We also propose an exact-ILM
training framework by extending the proof given in the hybrid autoregressive
transducer, which enables a theoretical justification for other ILM approaches.
Systematic comparison is conducted for both in-domain and cross-domain
evaluation on the Librispeech and TED-LIUM Release 2 corpora, respectively. Our
proposed exact-ILM training can further improve the best ILM method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FacTeR-Check: Semi-automated fact-checking through Semantic Similarity and Natural Language Inference. (arXiv:2110.14532v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14532">
<div class="article-summary-box-inner">
<span><p>Our society produces and shares overwhelming amounts of information through
Online Social Networks (OSNs). Within this environment, misinformation and
disinformation have proliferated, becoming a public safety concern in most
countries. Allowing the public and professionals to efficiently find reliable
evidences about the factual veracity of a claim is a crucial step to mitigate
this harmful spread. To this end, we propose FacTeR-Check, a multilingual
architecture for semi-automated fact-checking that can be used for either
applications designed for the general public and by fact-checking
organisations. FacTeR-Check enables retrieving fact-checked information,
unchecked claims verification and tracking dangerous information over social
media. This architectures involves several modules developed to evaluate
semantic similarity, to calculate natural language inference and to retrieve
information from Online Social Networks. The union of all these components
builds a semi-automated fact-checking tool able of verifying new claims, to
extract related evidence, and to track the evolution of a hoax on a OSN. While
individual modules are validated on related benchmarks (mainly MSTS and SICK),
the complete architecture is validated using a new dataset called NLI19-SP that
is publicly released with COVID-19 related hoaxes and tweets from Spanish
social media. Our results show state-of-the-art performance on the individual
benchmarks, as well as producing a useful analysis of the evolution over time
of 61 different hoaxes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Language Models Learn Commonsense Knowledge?. (arXiv:2111.00607v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00607">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) trained on large amounts of data have shown impressive
performance on many NLP tasks under the zero-shot and few-shot setup. Here we
aim to better understand the extent to which such models learn commonsense
knowledge -- a critical component of many NLP applications. To that end, we
conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation
of pre-trained LMs, where we: (i) carefully control for the LM's ability to
exploit potential surface cues and annotation artefacts, and (ii) account for
variations in model performance that arise from non-commonsense related
factors. Our findings highlight the limitations of pre-trained LMs in acquiring
commonsense knowledge without task-specific supervision; furthermore, using
larger models -- or augmenting the LMs with commonsense knowledge bases at
test-time -- did not substantially improve their performance. More broadly, our
findings offer valuable lessons and best practices for conducting more rigorous
multiple-choice evaluations of pre-trained LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone. (arXiv:2112.02418v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02418">
<div class="article-summary-box-inner">
<span><p>YourTTS brings the power of a multilingual approach to the task of zero-shot
multi-speaker TTS. Our method builds upon the VITS model and adds several novel
modifications for zero-shot multi-speaker and multilingual training. We
achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and
results comparable to SOTA in zero-shot voice conversion on the VCTK dataset.
Additionally, our approach achieves promising results in a target language with
a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS
and zero-shot voice conversion systems in low-resource languages. Finally, it
is possible to fine-tune the YourTTS model with less than 1 minute of speech
and achieve state-of-the-art results in voice similarity and with reasonable
quality. This is important to allow synthesis for speakers with a very
different voice or recording characteristics from those seen during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethink the Evaluation for Attack Strength of Backdoor Attacks in Natural Language Processing. (arXiv:2201.02993v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02993">
<div class="article-summary-box-inner">
<span><p>It has been shown that natural language processing (NLP) models are
vulnerable to a kind of security threat called the Backdoor Attack, which
utilizes a `backdoor trigger' paradigm to mislead the models. The most
threatening backdoor attack is the stealthy backdoor, which defines the
triggers as text style or syntactic. Although they have achieved an incredible
high attack success rate (ASR), we find that the principal factor contributing
to their ASR is not the `backdoor trigger' paradigm. Thus the capacity of these
stealthy backdoor attacks is overestimated when categorized as backdoor
attacks. Therefore, to evaluate the real attack power of backdoor attacks, we
propose a new metric called attack successful rate difference (ASRD), which
measures the ASR difference between clean state and poison state models.
Besides, since the defenses against stealthy backdoor attacks are absent, we
propose Trigger Breaker, consisting of two too simple tricks that can defend
against stealthy backdoor attacks effectively. Experiments show that our method
achieves significantly better performance than state-of-the-art defense methods
against stealthy backdoor attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The ABBE Corpus: Animate Beings Being Emotional. (arXiv:2201.10618v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10618">
<div class="article-summary-box-inner">
<span><p>Emotion detection is an established NLP task of demonstrated utility for text
understanding. However, basic emotion detection leaves out key information,
namely, who is experiencing the emotion in question. For example, it may be the
author, the narrator, or a character; or the emotion may correspond to
something the audience is supposed to feel, or even be unattributable to a
specific being, e.g., when emotions are being discussed per se. We provide the
ABBE corpus -- Animate Beings Being Emotional -- a new double-annotated corpus
of texts that captures this key information for one class of emotion
experiencer, namely, animate beings in the world described by the text. Such a
corpus is useful for developing systems that seek to model or understand this
specific type of expressed emotion. Our corpus contains 30 chapters, comprising
134,513 words, drawn from the Corpus of English Novels, and contains 2,010
unique emotion expressions attributable to 2,227 animate beings. The emotion
expressions are categorized according to Plutchik's 8-category emotion model,
and the overall inter-annotator agreement for the annotations was 0.83 Cohen's
Kappa, indicating excellent agreement. We describe in detail our annotation
scheme and procedure, and also release the corpus for use by other researchers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10890">
<div class="article-summary-box-inner">
<span><p>Human education system trains one student by multiple experts.
Mixture-of-experts (MoE) is a powerful sparse architecture including multiple
experts. However, sparse MoE model is hard to implement, easy to overfit, and
not hardware-friendly. In this work, inspired by human education model, we
propose a novel task, knowledge integration, to obtain a dense student model
(OneS) as knowledgeable as one sparse MoE. We investigate this task by
proposing a general training framework including knowledge gathering and
knowledge distillation. Specifically, we first propose Singular Value
Decomposition Knowledge Gathering (SVD-KG) to gather key knowledge from
different pretrained experts. We then refine the dense student model by
knowledge distillation to offset the noise from gathering. On ImageNet, our
OneS preserves $61.7\%$ benefits from MoE. OneS can achieve $78.4\%$ top-1
accuracy with only $15$M parameters. On four natural language processing
datasets, OneS obtains $88.2\%$ MoE benefits and outperforms SoTA by $51.7\%$
using the same architecture and training data. In addition, compared with the
MoE counterpart, OneS can achieve $3.7 \times$ inference speedup due to the
hardware-friendly architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Scaling and Transfer of Language Model Architectures for Machine Translation. (arXiv:2202.00528v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00528">
<div class="article-summary-box-inner">
<span><p>Natural language understanding and generation models follow one of the two
dominant architectural paradigms: language models (LMs) that process
concatenated sequences in a single stack of layers, and encoder-decoder models
(EncDec) that utilize separate layer stacks for input and output processing. In
machine translation, EncDec has long been the favoured approach, but with few
studies investigating the performance of LMs. In this work, we thoroughly
examine the role of several architectural design choices on the performance of
LMs on bilingual, (massively) multilingual and zero-shot translation tasks,
under systematic variations of data conditions and model sizes. Our results
show that: (i) Different LMs have different scaling properties, where
architectural differences often have a significant impact on model performance
at small scales, but the performance gap narrows as the number of parameters
increases, (ii) Several design choices, including causal masking and
language-modeling objectives for the source sequence, have detrimental effects
on translation quality, and (iii) When paired with full-visible masking for
source sequences, LMs could perform on par with EncDec on supervised bilingual
and multilingual translation tasks, and improve greatly on zero-shot directions
by facilitating the reduction of off-target translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs. (arXiv:2202.05331v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05331">
<div class="article-summary-box-inner">
<span><p>Several services for people with visual disabilities have emerged recently
due to achievements in Assistive Technologies and Artificial Intelligence
areas. Despite the growth in assistive systems availability, there is a lack of
services that support specific tasks, such as understanding the image context
presented in online content, e.g., webinars. Image captioning techniques and
their variants are limited as Assistive Technologies as they do not match the
needs of visually impaired people when generating specific descriptions. We
propose an approach for generating context of webinar images combining a dense
captioning technique with a set of filters, to fit the captions in our domain,
and a language model for the abstractive summary task. The results demonstrated
that we can produce descriptions with higher interpretability and focused on
the relevant information for that group of people by combining image analysis
methods and neural language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Task Framework for Improving Persona-grounded Dialogue Dataset. (arXiv:2202.05435v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05435">
<div class="article-summary-box-inner">
<span><p>This paper introduces a simple yet effective data-centric approach for the
task of improving persona-conditioned dialogue agents. Prior model-centric
approaches unquestioningly depend on the raw crowdsourced benchmark datasets
such as Persona-Chat. In contrast, we aim to fix annotation artifacts in
benchmarking, which is orthogonally applicable to any dialogue model.
Specifically, we augment relevant personas to improve dialogue dataset/agent,
by leveraging the primal-dual structure of the two tasks, predicting dialogue
responses and personas based on each other. Experiments on Persona-Chat show
that our approach outperforms pre-trained LMs by an 11.7 point gain in terms of
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deduplicating Training Data Mitigates Privacy Risks in Language Models. (arXiv:2202.06539v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06539">
<div class="article-summary-box-inner">
<span><p>Past work has shown that large language models are susceptible to privacy
attacks, where adversaries generate sequences from a trained model and detect
which sequences are memorized from the training set. In this work, we show that
the success of these attacks is largely due to duplication in commonly used
web-scraped training sets. We first show that the rate at which language models
regenerate training sequences is superlinearly related to a sequence's count in
the training set. For instance, a sequence that is present 10 times in the
training data is on average generated ~1000 times more often than a sequence
that is present only once. We next show that existing methods for detecting
memorized sequences have near-chance accuracy on non-duplicated training
sequences. Finally, we find that after applying methods to deduplicate training
data, language models are considerably more secure against these types of
privacy attacks. Taken together, our results motivate an increased focus on
deduplication in privacy-sensitive applications and a reevaluation of the
practicality of existing privacy attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Memory as a Differentiable Search Index. (arXiv:2202.06991v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06991">
<div class="article-summary-box-inner">
<span><p>In this paper, we demonstrate that information retrieval can be accomplished
with a single Transformer, in which all information about the corpus is encoded
in the parameters of the model. To this end, we introduce the Differentiable
Search Index (DSI), a new paradigm that learns a text-to-text model that maps
string queries directly to relevant docids; in other words, a DSI model answers
queries directly using only its parameters, dramatically simplifying the whole
retrieval process. We study variations in how documents and their identifiers
are represented, variations in training procedures, and the interplay between
models and corpus sizes. Experiments demonstrate that given appropriate design
choices, DSI significantly outperforms strong baselines such as dual encoder
models. Moreover, DSI demonstrates strong generalization capabilities,
outperforming a BM25 baseline in a zero-shot setup.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Misinformation Detection in Social Media Video Posts. (arXiv:2202.07706v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07706">
<div class="article-summary-box-inner">
<span><p>With the growing adoption of short-form video by social media platforms,
reducing the spread of misinformation through video posts has become a critical
challenge for social media providers. In this paper, we develop methods to
detect misinformation in social media posts, exploiting modalities such as
video and text. Due to the lack of large-scale public data for misinformation
detection in multi-modal datasets, we collect 160,000 video posts from Twitter,
and leverage self-supervised learning to learn expressive representations of
joint visual and textual data. In this work, we propose two new methods for
detecting semantic inconsistencies within short-form social media video posts,
based on contrastive learning and masked language modeling. We demonstrate that
our new approaches outperform current state-of-the-art methods on both
artificial data generated by random-swapping of positive samples and in the
wild on a new manually-labeled test set for semantic misinformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Preserving Visual Question Answering. (arXiv:2202.07712v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07712">
<div class="article-summary-box-inner">
<span><p>We introduce a novel privacy-preserving methodology for performing Visual
Question Answering on the edge. Our method constructs a symbolic representation
of the visual scene, using a low-complexity computer vision model that jointly
predicts classes, attributes and predicates. This symbolic representation is
non-differentiable, which means it cannot be used to recover the original
image, thereby keeping the original image private. Our proposed hybrid solution
uses a vision model which is more than 25 times smaller than the current
state-of-the-art (SOTA) vision models, and 100 times smaller than end-to-end
SOTA VQA models. We report detailed error analysis and discuss the trade-offs
of using a distilled vision model and a symbolic representation of the visual
scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Subjective Quality Study for Video Frame Interpolation. (arXiv:2202.07727v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07727">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation (VFI) is one of the fundamental research areas in
video processing and there has been extensive research on novel and enhanced
interpolation algorithms. The same is not true for quality assessment of the
interpolated content. In this paper, we describe a subjective quality study for
VFI based on a newly developed video database, BVI-VFI. BVI-VFI contains 36
reference sequences at three different frame rates and 180 distorted videos
generated using five conventional and learning based VFI algorithms. Subjective
opinion scores have been collected from 60 human participants, and then
employed to evaluate eight popular quality metrics, including PSNR, SSIM and
LPIPS which are all commonly used for assessing VFI methods. The results
indicate that none of these metrics provide acceptable correlation with the
perceived quality on interpolated content, with the best-performing metric,
LPIPS, offering a SROCC value below 0.6. Our findings show that there is an
urgent need to develop a bespoke perceptual quality metric for VFI. The BVI-VFI
dataset is publicly available and can be accessed at
https://danielism97.github.io/BVI-VFI/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis. (arXiv:2202.07728v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07728">
<div class="article-summary-box-inner">
<span><p>A variety of methods have been proposed to try to explain how deep neural
networks make their decisions. Key to those approaches is the need to sample
the pixel space efficiently in order to derive importance maps. However, it has
been shown that the sampling methods used to date introduce biases and other
artifacts, leading to inaccurate estimates of the importance of individual
pixels and severely limit the reliability of current explainability methods.
Unfortunately, the alternative -- to exhaustively sample the image space is
computationally prohibitive. In this paper, we introduce EVA (Explaining using
Verified perturbation Analysis) -- the first explainability method guarantee to
have an exhaustive exploration of a perturbation space. Specifically, we
leverage the beneficial properties of verified perturbation analysis -- time
efficiency, tractability and guaranteed complete coverage of a manifold -- to
efficiently characterize the input variables that are most likely to drive the
model decision. We evaluate the approach systematically and demonstrate
state-of-the-art results on multiple benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN. (arXiv:2202.07731v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07731">
<div class="article-summary-box-inner">
<span><p>This paper presents a new deformable convolution-based video frame
interpolation (VFI) method, using a coarse to fine 3D CNN to enhance the
multi-flow prediction. This model first extracts spatio-temporal features at
multiple scales using a 3D CNN, and estimates multi-flows using these features
in a coarse-to-fine manner. The estimated multi-flows are then used to warp the
original input frames as well as context maps, and the warped results are fused
by a synthesis network to produce the final output. This VFI approach has been
fully evaluated against 12 state-of-the-art VFI methods on three commonly used
test databases. The results evidently show the effectiveness of the proposed
method, which offers superior interpolation performance over other state of the
art algorithms, with PSNR gains up to 0.19dB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ab-initio Contrast Estimation and Denoising of Cryo-EM Images. (arXiv:2202.07737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07737">
<div class="article-summary-box-inner">
<span><p>Background and Objective: The contrast of cryo-EM images vary from one to
another, primarily due to the uneven thickness of ice layers. The variation of
contrast can affect the quality of 2-D class averaging, 3-D ab-initio modeling,
and 3-D heterogeneity analysis. Contrast estimation is currently performed
during 3-D iterative refinement. As a result, the estimates are not available
for class averaging and ab-initio modeling. However, these methods require good
initial estimates of 3-D volumes and 3-D rotations of molecules. This paper
aims to solve the contrast estimation problem in the ab-initio stage, without
estimating the 3-D volume.
</p>
<p>Methods: The key observation underlying our analysis is that the 2-D
covariance matrix of the raw images is related to the covariance of the
underlying clean images, the noise variance, and the contrast variability
between images. We show that the contrast variability can be derived from the
2-D covariance matrix and use the existing Covariance Wiener Filtering (CWF)
framework to estimate it. We also demonstrate a modification of CWF to estimate
the contrast of individual images.
</p>
<p>Results: Our method improves the contrast estimation by a large margin,
compared to the previous CWF method. Its estimation accuracy is often
comparable to that of an oracle that knows the ground truth covariance of the
clean images. The more accurate contrast estimation also improves the quality
of image denoising as demonstrated in both synthetic and experimental datasets.
</p>
<p>Conclusions: This paper proposes an effective method for contrast estimation
directly from noisy images without using any 3-D volume information. It enables
contrast correction in the earlier stage of single particle analysis, and may
improve the accuracy of downstream processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Normalized K-Means for Noise-Insensitive Multi-Dimensional Feature Learning. (arXiv:2202.07754v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07754">
<div class="article-summary-box-inner">
<span><p>Many measurement modalities which perform imaging by probing an object
pixel-by-pixel, such as via Photoacoustic Microscopy, produce a
multi-dimensional feature (typically a time-domain signal) at each pixel. In
principle, the many degrees of freedom in the time-domain signal would admit
the possibility of significant multi-modal information being implicitly
present, much more than a single scalar "brightness", regarding the underlying
targets being observed. However, the measured signal is neither a weighted-sum
of basis functions (such as principal components) nor one of a set of
prototypes (K-means), which has motivated the novel clustering method proposed
here, capable of learning centroids (signal shapes) that are related to the
underlying, albeit unknown, target characteristics in a scalable and
noise-robust manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-Assisted Co-registration of Full-Spectral Autofluorescence Lifetime Microscopic Images with H&E-Stained Histology Images. (arXiv:2202.07755v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07755">
<div class="article-summary-box-inner">
<span><p>Autofluorescence lifetime images reveal unique characteristics of endogenous
fluorescence in biological samples. Comprehensive understanding and clinical
diagnosis rely on co-registration with the gold standard, histology images,
which is extremely challenging due to the difference of both images. Here, we
show an unsupervised image-to-image translation network that significantly
improves the success of the co-registration using a conventional
optimisation-based regression network, applicable to autofluorescence lifetime
images at different emission wavelengths. A preliminary blind comparison by
experienced researchers shows the superiority of our method on co-registration.
The results also indicate that the approach is applicable to various image
formats, like fluorescence intensity images. With the registration, stitching
outcomes illustrate the distinct differences of the spectral lifetime across an
unstained tissue, enabling macro-level rapid visual identification of lung
cancer and cellular-level characterisation of cell variants and common types.
The approach could be effortlessly extended to lifetime images beyond this
range and other staining technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General-purpose, long-context autoregressive modeling with Perceiver AR. (arXiv:2202.07765v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07765">
<div class="article-summary-box-inner">
<span><p>Real-world data is high-dimensional: a book, image, or musical performance
can easily contain hundreds of thousands of elements even after compression.
However, the most commonly used autoregressive models, Transformers, are
prohibitively expensive to scale to the number of inputs and layers needed to
capture this long-range structure. We develop Perceiver AR, an autoregressive,
modality-agnostic architecture which uses cross-attention to map long-range
inputs to a small number of latents while also maintaining end-to-end causal
masking. Perceiver AR can directly attend to over a hundred thousand tokens,
enabling practical long-context density estimation without the need for
hand-crafted sparsity patterns or memory mechanisms. When trained on images or
music, Perceiver AR generates outputs with clear long-term coherence and
structure. Our architecture also obtains state-of-the-art likelihood on
long-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Deterministic Translation for Unsupervised Domain Adaptation. (arXiv:2202.07778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07778">
<div class="article-summary-box-inner">
<span><p>In this work we challenge the common approach of using a one-to-one mapping
('translation') between the source and target domains in unsupervised domain
adaptation (UDA). Instead, we rely on stochastic translation to capture
inherent translation ambiguities. This allows us to (i) train more accurate
target networks by generating multiple outputs conditioned on the same source
image, leveraging both accurate translation and data augmentation for
appearance variability, (ii) impute robust pseudo-labels for the target data by
averaging the predictions of a source network on multiple translated versions
of a single target image and (iii) train and ensemble diverse networks in the
target domain by modulating the degree of stochasticity in the translations. We
report improvements over strong recent baselines, leading to state-of-the-art
UDA results on two challenging semantic segmentation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations. (arXiv:2202.07800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07800">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) take all the image patches as tokens and construct
multi-head self-attention (MHSA) among them. Complete leverage of these image
tokens brings redundant computations since not all the tokens are attentive in
MHSA. Examples include that tokens containing semantically meaningless or
distractive image backgrounds do not positively contribute to the ViT
predictions. In this work, we propose to reorganize image tokens during the
feed-forward process of ViT models, which is integrated into ViT during
training. For each forward inference, we identify the attentive image tokens
between MHSA and FFN (i.e., feed-forward network) modules, which is guided by
the corresponding class token attention. Then, we reorganize image tokens by
preserving attentive image tokens and fusing inattentive ones to expedite
subsequent MHSA and FFN computations. To this end, our method EViT improves
ViTs from two perspectives. First, under the same amount of input image tokens,
our method reduces MHSA and FFN computation for efficient inference. For
instance, the inference speed of DeiT-S is increased by 50% while its
recognition accuracy is decreased by only 0.3% for ImageNet classification.
Second, by maintaining the same computational cost, our method empowers ViTs to
take more image tokens as input for recognition accuracy improvement, where the
image tokens are from higher resolution images. An example is that we improve
the recognition accuracy of DeiT-S by 1% for ImageNet classification at the
same computational cost of a vanilla DeiT-S. Meanwhile, our method does not
introduce more parameters to ViTs. Experiments on the standard benchmarks show
the effectiveness of our method. The code is available at
https://github.com/youweiliang/evit
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying adversarial networks to increase the data efficiency and reliability of Self-Driving Cars. (arXiv:2202.07815v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07815">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) are vulnerable to misclassifying images
when small perturbations are present. With the increasing prevalence of CNNs in
self-driving cars, it is vital to ensure these algorithms are robust to prevent
collisions from occurring due to failure in recognizing a situation. In the
Adversarial Self-Driving framework, a Generative Adversarial Network (GAN) is
implemented to generate realistic perturbations in an image that cause a
classifier CNN to misclassify data. This perturbed data is then used to train
the classifier CNN further. The Adversarial Self-driving framework is applied
to an image classification algorithm to improve the classification accuracy on
perturbed images and is later applied to train a self-driving car to drive in a
simulation. A small-scale self-driving car is also built to drive around a
track and classify signs. The Adversarial Self-driving framework produces
perturbed images through learning a dataset, as a result removing the need to
train on significant amounts of data. Experiments demonstrate that the
Adversarial Self-driving framework identifies situations where CNNs are
vulnerable to perturbations and generates new examples of these situations for
the CNN to train on. The additional data generated by the Adversarial
Self-driving framework provides sufficient data for the CNN to generalize to
the environment. Therefore, it is a viable tool to increase the resilience of
CNNs to perturbations. Particularly, in the real-world self-driving car, the
application of the Adversarial Self-Driving framework resulted in an 18 %
increase in accuracy, and the simulated self-driving model had no collisions in
30 minutes of driving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-view and Cross-domain Underwater Localization based on Optical Aerial and Acoustic Underwater Images. (arXiv:2202.07817v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07817">
<div class="article-summary-box-inner">
<span><p>Cross-view image matches have been widely explored on terrestrial image
localization using aerial images from drones or satellites. This study expands
the cross-view image match idea and proposes a cross-domain and cross-view
localization framework. The method identifies the correlation between color
aerial images and underwater acoustic images to improve the localization of
underwater vehicles that travel in partially structured environments such as
harbors and marinas. The approach is validated on a real dataset acquired by an
underwater vehicle in a marina. The results show an improvement in the
localization when compared to the dead reckoning of the vehicle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Semen Quality Evaluation in Microscopic Videos Using Computer Assisted Sperm Analysis. (arXiv:2202.07820v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07820">
<div class="article-summary-box-inner">
<span><p>The Computer Assisted Sperm Analysis (CASA) plays a crucial role in male
reproductive health diagnosis and Infertility treatment. With the development
of the computer industry in recent years, a great of accurate algorithms are
proposed. With the assistance of those novel algorithms, it is possible for
CASA to achieve a faster and higher quality result. Since image processing is
the technical basis of CASA, including pre-processing,feature extraction,
target detection and tracking, these methods are important technical steps in
dealing with CASA. The various works related to Computer Assisted Sperm
Analysis methods in the last 30 years (since 1988) are comprehensively
introduced and analysed in this survey. To facilitate understanding, the
methods involved are analysed in the sequence of general steps in sperm
analysis. In other words, the methods related to sperm detection (localization)
are first analysed, and then the methods of sperm tracking are analysed. Beside
this, we analyse and prospect the present situation and future of CASA.
According to our work, the feasible for applying in sperm microscopic video of
methods mentioned in this review is explained. Moreover, existing challenges of
object detection and tracking in microscope video are potential to be solved
inspired by this survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation and Risk Score Prediction of Head and Neck Cancers in PET/CT Volumes with 3D U-Net and Cox Proportional Hazard Neural Networks. (arXiv:2202.07823v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07823">
<div class="article-summary-box-inner">
<span><p>We utilized a 3D nnU-Net model with residual layers supplemented by squeeze
and excitation (SE) normalization for tumor segmentation from PET/CT images
provided by the Head and Neck Tumor segmentation chal-lenge (HECKTOR). Our
proposed loss function incorporates the Unified Fo-cal and Mumford-Shah losses
to take the advantage of distribution, region, and boundary-based loss
functions. The results of leave-one-out-center-cross-validation performed on
different centers showed a segmentation performance of 0.82 average Dice score
(DSC) and 3.16 median Hausdorff Distance (HD), and our results on the test set
achieved 0.77 DSC and 3.01 HD. Following lesion segmentation, we proposed
training a case-control proportional hazard Cox model with an MLP neural net
backbone to predict the hazard risk score for each discrete lesion. This hazard
risk prediction model (CoxCC) was to be trained on a number of PET/CT radiomic
features extracted from the segmented lesions, patient and lesion demographics,
and encoder features provided from the penultimate layer of a multi-input 2D
PET/CT convolutional neural network tasked with predicting time-to-event for
each lesion. A 10-fold cross-validated CoxCC model resulted in a c-index
validation score of 0.89, and a c-index score of 0.61 on the HECKTOR challenge
test dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RNGDet: Road Network Graph Detection by Transformer in Aerial Images. (arXiv:2202.07824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07824">
<div class="article-summary-box-inner">
<span><p>Road network graphs provide critical information for autonomous vehicle
applications, such as motion planning on drivable areas. However, manually
annotating road network graphs is inefficient and labor-intensive.
Automatically detecting road network graphs could alleviate this issue, but
existing works are either segmentation-based approaches that could not ensure
satisfactory topology correctness, or graph-based approaches that could not
present precise enough detection results. To provide a solution to these
problems, we propose a novel approach based on transformer and imitation
learning named RNGDet (\underline{R}oad \underline{N}etwork \underline{G}raph
\underline{Det}ection by Transformer) in this paper. In view of that
high-resolution aerial images could be easily accessed all over the world
nowadays, we make use of aerial images in our approach. Taken as input an
aerial image, our approach iteratively generates road network graphs
vertex-by-vertex. Our approach can handle complicated intersection points of
various numbers of road segments. We evaluate our approach on a publicly
available dataset. The superiority of our approach is demonstrated through the
comparative experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Overconfidence Predictions for Autonomous Driving Perception. (arXiv:2202.07825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07825">
<div class="article-summary-box-inner">
<span><p>In state-of-the-art deep learning for object recognition, SoftMax and Sigmoid
functions are most commonly employed as the predictor outputs. Such layers
often produce overconfident predictions rather than proper probabilistic
scores, which can thus harm the decision-making of `critical' perception
systems applied in autonomous driving and robotics. Given this, the experiments
in this work propose a probabilistic approach based on distributions calculated
out of the Logit layer scores of pre-trained networks. We demonstrate that
Maximum Likelihood (ML) and Maximum a-Posteriori (MAP) functions are more
suitable for probabilistic interpretations than SoftMax and Sigmoid-based
predictions for object recognition. We explore distinct sensor modalities via
RGB images and LiDARs (RV: range-view) data from the KITTI and Lyft Level-5
datasets, where our approach shows promising performance compared to the usual
SoftMax and Sigmoid layers, with the benefit of enabling interpretable
probabilistic predictions. Another advantage of the approach introduced in this
paper is that the ML and MAP functions can be implemented in existing trained
networks, that is, the approach benefits from the output of the Logit layer of
pre-trained networks. Thus, there is no need to carry out a new training phase
since the ML and MAP functions are used in the test/prediction phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Transformer K-Means. (arXiv:2202.07829v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07829">
<div class="article-summary-box-inner">
<span><p>K-means defines one of the most employed centroid-based clustering algorithms
with performances tied to the data's embedding. Intricate data embeddings have
been designed to push $K$-means performances at the cost of reduced theoretical
guarantees and interpretability of the results. Instead, we propose preserving
the intrinsic data space and augment K-means with a similarity measure
invariant to non-rigid transformations. This enables (i) the reduction of
intrinsic nuisances associated with the data, reducing the complexity of the
clustering task and increasing performances and producing state-of-the-art
results, (ii) clustering in the input space of the data, leading to a fully
interpretable clustering algorithm, and (iii) the benefit of convergence
guarantees.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCRP: Unsupervised Point Cloud Object Retrieval and Pose Estimation. (arXiv:2202.07843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07843">
<div class="article-summary-box-inner">
<span><p>An unsupervised point cloud object retrieval and pose estimation method,
called PCRP, is proposed in this work. It is assumed that there exists a
gallery point cloud set that contains point cloud objects with given pose
orientation information. PCRP attempts to register the unknown point cloud
object with those in the gallery set so as to achieve content-based object
retrieval and pose estimation jointly, where the point cloud registration task
is built upon an enhanced version of the unsupervised R-PointHop method.
Experiments on the ModelNet40 dataset demonstrate the superior performance of
PCRP in comparison with traditional and learning based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deeply-Supervised Knowledge Distillation. (arXiv:2202.07846v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07846">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation aims to enhance the performance of a lightweight
student model by exploiting the knowledge from a pre-trained cumbersome teacher
model. However, in the traditional knowledge distillation, teacher predictions
are only used to provide the supervisory signal for the last layer of the
student model, which may result in those shallow student layers lacking
accurate training guidance in the layer-by-layer back propagation and thus
hinders effective knowledge transfer. To address this issue, we propose
Deeply-Supervised Knowledge Distillation (DSKD), which fully utilizes class
predictions and feature maps of the teacher model to supervise the training of
shallow student layers. A loss-based weight allocation strategy is developed in
DSKD to adaptively balance the learning process of each shallow layer, so as to
further improve the student performance. Extensive experiments show that the
performance of DSKD consistently exceeds state-of-the-art methods on various
teacher-student models, confirming the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Network Acceleration with Tiny Sets. (arXiv:2202.07861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07861">
<div class="article-summary-box-inner">
<span><p>Network compression is effective in accelerating the inference of deep neural
networks, but often requires finetuning with all the training data to recover
from the accuracy loss. It is impractical in some applications, however, due to
data privacy issues or constraints in compression time budget. To deal with the
above issues, we propose a method named PRACTISE to accelerate the network with
tiny sets of training images. By considering both the pruned part and the
unpruned part of a compressed model, PRACTISE alleviates layer-wise error
accumulation, which is the main drawback of previous methods. Furthermore,
existing methods are confined to few compression schemes, have limited speedup
in terms of latency, and are unstable. In contrast, PRACTISE is stable, fast to
train, versatile to handle various compression schemes, and achieves low
latency. We also propose that dropping entire blocks is a better way than
existing compression schemes when only tiny sets of training data are
available. Extensive experiments demonstrate that PRACTISE achieves much higher
accuracy and more stable models than state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IPD:An Incremental Prototype based DBSCAN for large-scale data with cluster representatives. (arXiv:2202.07870v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07870">
<div class="article-summary-box-inner">
<span><p>DBSCAN is a fundamental density-based clustering technique that identifies
any arbitrary shape of the clusters. However, it becomes infeasible while
handling big data. On the other hand, centroid-based clustering is important
for detecting patterns in a dataset since unprocessed data points can be
labeled to their nearest centroid. However, it can not detect non-spherical
clusters. For a large data, it is not feasible to store and compute labels of
every samples. These can be done as and when the information is required. The
purpose can be accomplished when clustering act as a tool to identify cluster
representatives and query is served by assigning cluster labels of nearest
representative. In this paper, we propose an Incremental Prototype-based DBSCAN
(IPD) algorithm which is designed to identify arbitrary-shaped clusters for
large-scale data. Additionally, it chooses a set of representatives for each
cluster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Common Representation Learning with Triplet Loss Functions. (arXiv:2202.07901v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07901">
<div class="article-summary-box-inner">
<span><p>Common representation learning (CRL) learns a shared embedding between two or
more modalities to improve in a given task over using only one of the
modalities. CRL from different data types such as images and time-series data
(e.g., audio or text data) requires a deep metric learning loss that minimizes
the distance between the modality embeddings. In this paper, we propose to use
the triplet loss, which uses positive and negative identities to create sample
pairs with different labels, for CRL between image and time-series modalities.
By adapting the triplet loss for CRL, higher accuracy in the main (time-series
classification) task can be achieved by exploiting additional information of
the auxiliary (image classification) task. Our experiments on synthetic data
and handwriting recognition data from sensor-enhanced pens show an improved
classification accuracy, faster convergence, and a better generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Deep Learning be Applied to Model-Based Multi-Object Tracking?. (arXiv:2202.07909v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07909">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) is the problem of tracking the state of an
unknown and time-varying number of objects using noisy measurements, with
important applications such as autonomous driving, tracking animal behavior,
defense systems, and others. In recent years, deep learning (DL) has been
increasingly used in MOT for improving tracking performance, but mostly in
settings where the measurements are high-dimensional and there are no available
models of the measurement likelihood and the object dynamics. The model-based
setting instead has not attracted as much attention, and it is still unclear if
DL methods can outperform traditional model-based Bayesian methods, which are
the state of the art (SOTA) in this context. In this paper, we propose a
Transformer-based DL tracker and evaluate its performance in the model-based
setting, comparing it to SOTA model-based Bayesian methods in a variety of
different tasks. Our results show that the proposed DL method can match the
performance of the model-based methods in simple tasks, while outperforming
them when the task gets more complicated, either due to an increase in the data
association complexity, or to stronger nonlinearities of the models of the
environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ActionFormer: Localizing Moments of Actions with Transformers. (arXiv:2202.07925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07925">
<div class="article-summary-box-inner">
<span><p>Self-attention based Transformer models have demonstrated impressive results
for image classification and object detection, and more recently for video
understanding. Inspired by this success, we investigate the application of
Transformer networks for temporal action localization in videos. To this end,
we present ActionFormer -- a simple yet powerful model to identify actions in
time and recognize their categories in a single shot, without using action
proposals or relying on pre-defined anchor windows. ActionFormer combines a
multiscale feature representation with local self-attention, and uses a
light-weighted decoder to classify every moment in time and estimate the
corresponding action boundaries. We show that this orchestrated design results
in major improvements upon prior works. Without bells and whistles,
ActionFormer achieves 65.6% mAP at tIoU=0.5 on THUMOS14, outperforming the best
prior model by 8.7 absolute percentage points and crossing the 60% mAP for the
first time. Further, ActionFormer demonstrates strong results on ActivityNet
1.3 (36.0% average mAP) and the more recent EPIC-Kitchens 100 (+13.5% average
mAP over prior works). Our code is available at
<a href="http://github.com/happyharrycn/actionformer_release">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Knowledge Distillation. (arXiv:2202.07940v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07940">
<div class="article-summary-box-inner">
<span><p>Recent studies pointed out that knowledge distillation (KD) suffers from two
degradation problems, the teacher-student gap and the incompatibility with
strong data augmentations, making it not applicable to training
state-of-the-art models, which are trained with advanced augmentations.
However, we observe that a key factor, i.e., the temperatures in the softmax
functions for generating probabilities of both the teacher and student models,
was mostly overlooked in previous methods. With properly tuned temperatures,
such degradation problems of KD can be much mitigated. However, instead of
relying on a naive grid search, which shows poor transferability, we propose
Meta Knowledge Distillation (MKD) to meta-learn the distillation with learnable
meta temperature parameters. The meta parameters are adaptively adjusted during
training according to the gradients of the learning objective. We validate that
MKD is robust to different dataset scales, different teacher/student
architectures, and different types of data augmentation. With MKD, we achieve
the best performance with popular ViT architectures among compared methods that
use only ImageNet-1K as training data, ranging from tiny to large models. With
ViT-L, we achieve 86.5% with 600 epochs of training, 0.6% better than MAE that
trains for 1,650 epochs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified smoke and fire detection in an evolutionary framework with self-supervised progressive data augment. (arXiv:2202.07954v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07954">
<div class="article-summary-box-inner">
<span><p>Few researches have studied simultaneous detection of smoke and flame
accompanying fires due to their different physical natures that lead to
uncertain fluid patterns. In this study, we collect a large image data set to
re-label them as a multi-label image classification problem so as to identify
smoke and flame simultaneously. In order to solve the generalization ability of
the detection model on account of the movable fluid objects with uncertain
shapes like fire and smoke, and their not compactible natures as well as the
complex backgrounds with high variations, we propose a data augment method by
random image stitch to deploy resizing, deforming, position variation, and
background altering so as to enlarge the view of the learner. Moreover, we
propose a self-learning data augment method by using the class activation map
to extract the highly trustable region as new data source of positive examples
to further enhance the data augment. By the mutual reinforcement between the
data augment and the detection model that are performed iteratively, both
modules make progress in an evolutionary manner. Experiments show that the
proposed method can effectively improve the generalization performance of the
model for concurrent smoke and fire detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images. (arXiv:2202.07983v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07983">
<div class="article-summary-box-inner">
<span><p>Age-related macular degeneration (AMD) is the leading cause of visual
impairment among elderly in the world. Early detection of AMD is of great
importance as the vision loss caused by AMD is irreversible and permanent.
Color fundus photography is the most cost-effective imaging modality to screen
for retinal disorders. \textcolor{red}{Recently, some algorithms based on deep
learning had been developed for fundus image analysis and automatic AMD
detection. However, a comprehensive annotated dataset and a standard evaluation
benchmark are still missing.} To deal with this issue, we set up the Automatic
Detection challenge on Age-related Macular degeneration (ADAM) for the first
time, held as a satellite event of the ISBI 2020 conference. The ADAM challenge
consisted of four tasks which cover the main topics in detecting AMD from
fundus images, including classification of AMD, detection and segmentation of
optic disc, localization of fovea, and detection and segmentation of lesions.
The ADAM challenge has released a comprehensive dataset of 1200 fundus images
with the category labels of AMD, the pixel-wise segmentation masks of the full
optic disc and lesions (drusen, exudate, hemorrhage, scar, and other), as well
as the location coordinates of the macular fovea. A uniform evaluation
framework has been built to make a fair comparison of different models. During
the ADAM challenge, 610 results were submitted for online evaluation, and
finally, 11 teams participated in the onsite challenge. This paper introduces
the challenge, dataset, and evaluation methods, as well as summarizes the
methods and analyzes the results of the participating teams of each task. In
particular, we observed that ensembling strategy and clinical prior knowledge
can better improve the performances of the deep learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Planckian jitter: enhancing the color quality of self-supervised visual representations. (arXiv:2202.07993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07993">
<div class="article-summary-box-inner">
<span><p>Several recent works on self-supervised learning are trained by mapping
different augmentations of the same image to the same feature representation.
The set of used data augmentations is of crucial importance for the quality of
the learned feature representation. We analyze how the traditionally used color
jitter negatively impacts the quality of the color features in the learned
feature representation. To address this problem, we replace this module with
physics-based color augmentation, called Planckian jitter, which creates
realistic variations in chromaticity, producing a model robust to llumination
changes that can be commonly observed in real life, while maintaining the
ability to discriminate the image content based on color information. We
further improve the performance by introducing a latent space combination of
color-sensitive and non-color-sensitive features. These are found to be
complementary and the combination leads to large absolute performance gains
over the default data augmentation on color classification tasks, including on
Flowers-102 (+15%), Cub200 (+11%), VegFru (+15%), and T1K+ (+12%). Finally, we
present a color sensitivity analysis to document the impact of different
training methods on the model neurons and we show that the performance of the
learned features is robust with respect to illuminant variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">360 Depth Estimation in the Wild -- The Depth360 Dataset and the SegFuse Network. (arXiv:2202.08010v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08010">
<div class="article-summary-box-inner">
<span><p>Single-view depth estimation from omnidirectional images has gained
popularity with its wide range of applications such as autonomous driving and
scene reconstruction. Although data-driven learning-based methods demonstrate
significant potential in this field, scarce training data and ineffective 360
estimation algorithms are still two key limitations hindering accurate
estimation across diverse domains. In this work, we first establish a
large-scale dataset with varied settings called Depth360 to tackle the training
data problem. This is achieved by exploring the use of a plenteous source of
data, 360 videos from the internet, using a test-time training method that
leverages unique information in each omnidirectional sequence. With novel
geometric and temporal constraints, our method generates consistent and
convincing depth samples to facilitate single-view estimation. We then propose
an end-to-end two-branch multi-task learning network, SegFuse, that mimics the
human eye to effectively learn from the dataset and estimate high-quality depth
maps from diverse monocular RGB images. With a peripheral branch that uses
equirectangular projection for depth estimation and a foveal branch that uses
cubemap projection for semantic segmentation, our method predicts consistent
global depth while maintaining sharp details at local regions. Experimental
results show favorable performance against the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Detect People on the Fly: A Bio-inspired Event-based Visual System for Drones. (arXiv:2202.08023v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08023">
<div class="article-summary-box-inner">
<span><p>We demonstrate for the first time that a biologicallyplausible spiking neural
network (SNN) equipped with Spike- Timing-Dependent Plasticity (STDP) learning
can continuously learn to detect walking people on the fly using
retina-inspired, event-based camera data. Our pipeline works as follows. First,
a short sequence of event data (&lt; 2 minutes), capturing a walking human from a
flying drone, is shown to a convolutional SNNSTDP system which also receives
teacher spiking signals from a convolutional readout (forming a semi-supervised
system). Then, STDP adaptation is stopped and the learned system is assessed on
testing sequences. We conduct several experiments to study the effect of key
mechanisms in our system and we compare our precision-recall performance to
conventionally-trained CNNs working with either RGB or event-based camera
frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diagnosing Batch Normalization in Class Incremental Learning. (arXiv:2202.08025v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08025">
<div class="article-summary-box-inner">
<span><p>Extensive researches have applied deep neural networks (DNNs) in class
incremental learning (Class-IL). As building blocks of DNNs, batch
normalization (BN) standardizes intermediate feature maps and has been widely
validated to improve training stability and convergence. However, we claim that
the direct use of standard BN in Class-IL models is harmful to both the
representation learning and the classifier training, thus exacerbating
catastrophic forgetting. In this paper we investigate the influence of BN on
Class-IL models by illustrating such BN dilemma. We further propose BN Tricks
to address the issue by training a better feature extractor while eliminating
classification bias. Without inviting extra hyperparameters, we apply BN Tricks
to three baseline rehearsal-based methods, ER, DER++ and iCaRL. Through
comprehensive experiments conducted on benchmark datasets of Seq-CIFAR-10,
Seq-CIFAR-100 and Seq-Tiny-ImageNet, we show that BN Tricks can bring
significant performance gains to all adopted baselines, revealing its potential
generality along this line of research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generalize across Domains on Single Test Samples. (arXiv:2202.08045v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08045">
<div class="article-summary-box-inner">
<span><p>We strive to learn a model from a set of source domains that generalizes well
to unseen target domains. The main challenge in such a domain generalization
scenario is the unavailability of any target domain data during training,
resulting in the learned model not being explicitly adapted to the unseen
target domains. We propose learning to generalize across domains on single test
samples. We leverage a meta-learning paradigm to learn our model to acquire the
ability of adaptation with single samples at training time so as to further
adapt itself to each single test sample at test time. We formulate the
adaptation to the single test sample as a variational Bayesian inference
problem, which incorporates the test sample as a conditional into the
generation of model parameters. The adaptation to each test sample requires
only one feed-forward computation at test time without any fine-tuning or
self-supervised training on additional data from the unseen domains. Extensive
ablation studies demonstrate that our model learns the ability to adapt models
to each single sample by mimicking domain shifts during training. Further, our
model achieves at least comparable -- and often better -- performance than
state-of-the-art methods on multiple benchmarks for domain generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image translation of Ultrasound to Pseudo Anatomical Display Using Artificial Intelligence. (arXiv:2202.08053v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08053">
<div class="article-summary-box-inner">
<span><p>Ultrasound is the second most used modality in medical imaging. It is cost
effective, hazardless, portable and implemented routinely in numerous clinical
procedures. Nonetheless, image quality is characterized by granulated
appearance, poor SNR and speckle noise. Specific for malignant tumors, the
margins are blurred and indistinct. Thus, there is a great need for improving
ultrasound image quality. We hypothesize that this can be achieved by
translation into a more realistic anatomic display, using neural networks. In
order to achieve this goal, the preferable approach would be to use a set of
paired images. However, this is practically impossible in our case. Therefore,
CycleGAN was used, to learn each domain properties separately and enforce cross
domain cycle consistency. The two datasets which were used for training the
model were "Breast Ultrasound Images" (BUSI) and a set of optic images of
poultry breast tissue samples acquired at our lab. The generated pseudo
anatomical images provide improved visual discrimination of the lesions with
clearer border definition and pronounced contrast. Furthermore, the algorithm
manages to overcome the acoustic shadows artifacts commonly appearing in
ultrasonic images. In order to evaluate the preservation of the anatomical
features, the lesions in the ultrasonic images and the generated pseudo
anatomical images were both automatically segmented and compared. This
comparison yielded median dice score of 0.78 for the benign tumors and 0.43 for
the malignancies. Median lesion center error of 2.38% and 8.42% for the benign
and malignancies respectively and median area error index of 0.77% and 5.06%
for the benign and malignancies respectively. In conclusion, these generated
pseudo anatomical images, which are presented in a more intuitive way, preserve
tissue anatomy and can potentially simplify the diagnosis and improve the
clinical outcome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Adapt to Light. (arXiv:2202.08098v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08098">
<div class="article-summary-box-inner">
<span><p>Light adaptation or brightness correction is a key step in improving the
contrast and visual appeal of an image. There are multiple light-related tasks
(for example, low-light enhancement and exposure correction) and previous
studies have mainly investigated these tasks individually. However, it is
interesting to consider whether these light-related tasks can be executed by a
unified model, especially considering that our visual system adapts to external
light in such way. In this study, we propose a biologically inspired method to
handle light-related image-enhancement tasks with a unified network (called
LA-Net). First, a frequency-based decomposition module is designed to decouple
the common and characteristic sub-problems of light-related tasks into two
pathways. Then, a new module is built inspired by biological visual adaptation
to achieve unified light adaptation in the low-frequency pathway. In addition,
noise suppression or detail enhancement is achieved effectively in the
high-frequency pathway regardless of the light levels. Extensive experiments on
three tasks -- low-light enhancement, exposure correction, and tone mapping --
demonstrate that the proposed method almost obtains state-of-the-art
performance compared with recent methods designed for these individual tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reversible data hiding with dual pixel-value-ordering and1minimum prediction error expansion. (arXiv:2202.08100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08100">
<div class="article-summary-box-inner">
<span><p>Pixel Value Ordering (PVO) holds an impressive property for high fidelity
Reversible Data Hiding (RDH). In this paper, we introduce a dual-PVO (dPVO) for
Prediction Error Expansion(PEE), and thereby develop a new RDH scheme to offer
a better rate-distortion performance. Particularly, we propose to embed in two
phases: forward and backward. In the forward phase, PVO with classic PEE is
applied to every non-overlapping image block of size 1x3. In the backward
phase,minimum-set and maximum-set of pixels are determined from the pixels
predicted in the forward phase. The minimum set only contains the lowest
predicted pixels and the maximum set contains the largest predicted pixels of
each image block. Proposed dPVO withPEE is then applied to both sets, so that
the pixel values of the minimum set are increased and that of the maximum set
are decreased by a unit value. Thereby, the pixels predicted in the forward
embedding can partially be restored to their original values resulting in both
better-embedded image quality and a higher embedding rate. Experimental results
have recorded a promising rate-distortion performance of our scheme with a
significant improvement of embedded image quality at higher embedding rates
compared to the popular and state-of-the-art PVO-based RDHschemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrasing Magritte's Observation. (arXiv:2202.08103v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08103">
<div class="article-summary-box-inner">
<span><p>Contrast Sensitivity of the human visual system can be explained from certain
low-level vision tasks (like retinal noise and optical blur removal), but not
from others (like chromatic adaptation or pure reconstruction after simple
bottlenecks). This conclusion still holds even under substantial change in
stimulus statistics, as for instance considering cartoon-like images as opposed
to natural images (Li et al. Journal of Vision, 2022, Preprint
<a href="/abs/2103.00481">arXiv:2103.00481</a>).
</p>
<p>In this note we present a method to generate original cartoon-like images
compatible with the statistical training used in (Li et al., 2022). Following
the classical observation in (Magritte, 1929), the stimuli generated by the
proposed method certainly are not what they represent: Ceci n'est pas une pipe.
The clear distinction between representation (the stimuli generated by the
proposed method) and reality (the actual object) avoids eventual problems for
the use of the generated stimuli in academic, non-profit, publications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Typography-MNIST (TMNIST): an MNIST-Style Image Dataset to Categorize Glyphs and Font-Styles. (arXiv:2202.08112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08112">
<div class="article-summary-box-inner">
<span><p>We present Typography-MNIST (TMNIST), a dataset comprising of 565,292
MNIST-style grayscale images representing 1,812 unique glyphs in varied styles
of 1,355 Google-fonts. The glyph-list contains common characters from over 150
of the modern and historical language scripts with symbol sets, and each
font-style represents varying subsets of the total unique glyphs. The dataset
has been developed as part of the CognitiveType project which aims to develop
eye-tracking tools for real-time mapping of type to cognition and to create
computational tools that allow for the easy design of typefaces with cognitive
properties such as readability. The dataset and scripts to generate MNIST-style
images for glyphs in different font styles are freely available at
https://github.com/aiskunks/CognitiveType.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Navigational Information to Learn Visual Representations. (arXiv:2202.08114v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08114">
<div class="article-summary-box-inner">
<span><p>Children learn to build a visual representation of the world from
unsupervised exploration and we hypothesize that a key part of this learning
ability is the use of self-generated navigational information as a similarity
label to drive a learning objective for self-supervised learning. The goal of
this work is to exploit navigational information in a visual environment to
provide performance in training that exceeds the state-of-the-art
self-supervised training. Here, we show that using spatial and temporal
information in the pretraining stage of contrastive learning can improve the
performance of downstream classification relative to conventional contrastive
learning approaches that use instance discrimination to discriminate between
two alterations of the same image or two different images. We designed a
pipeline to generate egocentric-vision images from a photorealistic ray-tracing
environment (ThreeDWorld) and record relevant navigational information for each
image. Modifying the Momentum Contrast (MoCo) model, we introduced spatial and
temporal information to evaluate the similarity of two views in the pretraining
stage instead of instance discrimination. This work reveals the effectiveness
and efficiency of contextual information for improving representation learning.
The work informs our understanding of the means by which children might learn
to see the world without external supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs. (arXiv:2202.08138v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08138">
<div class="article-summary-box-inner">
<span><p>We consider the task of temporal human action localization in lifestyle
vlogs. We introduce a novel dataset consisting of manual annotations of
temporal localization for 13,000 narrated actions in 1,200 video clips. We
present an extensive analysis of this data, which allows us to better
understand how the language and visual modalities interact throughout the
videos. We propose a simple yet effective method to localize the narrated
actions based on their expected duration. Through several experiments and
analyses, we show that our method brings complementary information with respect
to previous methods, and leads to improvements over previous work for the task
of temporal action localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FUN-SIS: a Fully UNsupervised approach for Surgical Instrument Segmentation. (arXiv:2202.08141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08141">
<div class="article-summary-box-inner">
<span><p>Automatic surgical instrument segmentation of endoscopic images is a crucial
building block of many computer-assistance applications for minimally invasive
surgery. So far, state-of-the-art approaches completely rely on the
availability of a ground-truth supervision signal, obtained via manual
annotation, thus expensive to collect at large scale. In this paper, we present
FUN-SIS, a Fully-UNsupervised approach for binary Surgical Instrument
Segmentation. FUN-SIS trains a per-frame segmentation model on completely
unlabelled endoscopic videos, by solely relying on implicit motion information
and instrument shape-priors. We define shape-priors as realistic segmentation
masks of the instruments, not necessarily coming from the same dataset/domain
as the videos. The shape-priors can be collected in various and convenient
ways, such as recycling existing annotations from other datasets. We leverage
them as part of a novel generative-adversarial approach, allowing to perform
unsupervised instrument segmentation of optical-flow images during training. We
then use the obtained instrument masks as pseudo-labels in order to train a
per-frame segmentation model; to this aim, we develop a
learning-from-noisy-labels architecture, designed to extract a clean
supervision signal from these pseudo-labels, leveraging their peculiar noise
properties. We validate the proposed contributions on three surgical datasets,
including the MICCAI 2017 EndoVis Robotic Instrument Segmentation Challenge
dataset. The obtained fully-unsupervised results for surgical instrument
segmentation are almost on par with the ones of fully-supervised
state-of-the-art approaches. This suggests the tremendous potential of the
proposed method to leverage the great amount of unlabelled data produced in the
context of minimally invasive surgery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias in Automated Image Colorization: Metrics and Error Types. (arXiv:2202.08143v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08143">
<div class="article-summary-box-inner">
<span><p>We measure the color shifts present in colorized images from the ADE20K
dataset, when colorized by the automatic GAN-based DeOldify model. We introduce
fine-grained local and regional bias measurements between the original and the
colorized images, and observe many colorization effects. We confirm a general
desaturation effect, and also provide novel observations: a shift towards the
training average, a pervasive blue shift, different color shifts among image
categories, and a manual categorization of colorization errors in three
classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Class-Cognizant Few-Shot Classification. (arXiv:2202.08149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08149">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning is argued to be the dark matter of human intelligence.
To build in this direction, this paper focuses on unsupervised learning from an
abundance of unlabeled data followed by few-shot fine-tuning on a downstream
classification task. To this aim, we extend a recent study on adopting
contrastive learning for self-supervised pre-training by incorporating
class-level cognizance through iterative clustering and re-ranking and by
expanding the contrastive optimization loss to account for it. To our
knowledge, our experimentation both in standard and cross-domain scenarios
demonstrate that we set a new state-of-the-art (SoTA) in (5-way, 1 and 5-shot)
settings of standard mini-ImageNet benchmark as well as the (5-way, 5 and
20-shot) settings of cross-domain CDFSL benchmark. Our code and experimentation
can be found in our GitHub repository: https://github.com/ojss/c3lr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative modeling with projected entangled-pair states. (arXiv:2202.08177v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08177">
<div class="article-summary-box-inner">
<span><p>We argue and demonstrate that projected entangled-pair states (PEPS)
outperform matrix product states significantly for the task of generative
modeling of datasets with an intrinsic two-dimensional structure such as
images. Our approach builds on a recently introduced algorithm for sampling
PEPS, which allows for the efficient optimization and sampling of the
distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible-Modal Face Anti-Spoofing: A Benchmark. (arXiv:2202.08192v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08192">
<div class="article-summary-box-inner">
<span><p>Face anti-spoofing (FAS) plays a vital role in securing face recognition
systems from presentation attacks. Benefitted from the maturing camera sensors,
single-modal (RGB) and multi-modal (e.g., RGB+Depth) FAS has been applied in
various scenarios with different configurations of sensors/modalities. Existing
single- and multi-modal FAS methods usually separately train and deploy models
for each possible modality scenario, which might be redundant and inefficient.
Can we train a unified model, and flexibly deploy it under various modality
scenarios? In this paper, we establish the first flexible-modal FAS benchmark
with the principle `train one for all'. To be specific, with trained
multi-modal (RGB+Depth+IR) FAS models, both intra- and cross-dataset testings
are conducted on four flexible-modal sub-protocols (RGB, RGB+Depth, RGB+IR, and
RGB+Depth+IR). We also investigate prevalent deep models and feature fusion
strategies for flexible-modal FAS. We hope this new benchmark will facilitate
the future research of the multi-modal FAS. The protocols and codes are
available at https://github.com/ZitongYu/Flex-Modal-FAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Propagation for Annotation-Efficient Nuclei Segmentation from Pathology Images. (arXiv:2202.08195v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08195">
<div class="article-summary-box-inner">
<span><p>Nuclei segmentation is a crucial task for whole slide image analysis in
digital pathology. Generally, the segmentation performance of fully-supervised
learning heavily depends on the amount and quality of the annotated data.
However, it is time-consuming and expensive for professional pathologists to
provide accurate pixel-level ground truth, while it is much easier to get
coarse labels such as point annotations. In this paper, we propose a
weakly-supervised learning method for nuclei segmentation that only requires
point annotations for training. The proposed method achieves label propagation
in a coarse-to-fine manner as follows. First, coarse pixel-level labels are
derived from the point annotations based on the Voronoi diagram and the k-means
clustering method to avoid overfitting. Second, a co-training strategy with an
exponential moving average method is designed to refine the incomplete
supervision of the coarse labels. Third, a self-supervised visual
representation learning method is tailored for nuclei segmentation of pathology
images that transforms the hematoxylin component images into the H\&amp;E stained
images to gain better understanding of the relationship between the nuclei and
cytoplasm. We comprehensively evaluate the proposed method using two public
datasets. Both visual and quantitative results demonstrate the superiority of
our method to the state-of-the-art methods, and its competitive performance
compared to the fully-supervised methods. The source codes for implementing the
experiments will be released after acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Surgical Phase Recognition from Timestamp Supervision. (arXiv:2202.08199v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08199">
<div class="article-summary-box-inner">
<span><p>Surgical phase recognition is a fundamental task in computer-assisted surgery
systems. Most existing works require expensive frame-wise annotations, which is
very time-consuming. In this paper, we introduce timestamp supervision to
surgical phase recognition for the first time, which only requires randomly
labeling one frame for each phase in a video. With timestamp supervision,
current methods in natural videos aim to generate pseudo labels of full frames.
However, due to the surgical videos containing ambiguous boundaries, these
methods would generate many noisy and inconsistent pseudo labels, leading to
limited performance. We argue that less is more in surgical phase
recognition,~\ie, less but discriminative pseudo labels outperform full but
ambiguous frames. To this end, we propose a novel method called
uncertainty-aware temporal diffusion to generate trustworthy pseudo labels. Our
approach evaluates the confidence of generated pseudo labels based on
uncertainty estimation. Then, we treat the annotated frames as anchors and make
pseudo labels diffuse to both sides, starting from anchors and stopping at the
high-uncertainty frames. In this way, our proposed method can generate
contiguous confident pseudo labels while discarding the uncertain ones.
Extensive experiments demonstrate that our method not only significantly save
annotation cost, but also outperforms fully supervised methods. Moreover, our
proposed approach can be used to clean noisy labels near boundaries and improve
the performance of the current surgical phase recognition methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08227">
<div class="article-summary-box-inner">
<span><p>Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A multi-reconstruction study of breast density estimation using Deep Learning. (arXiv:2202.08238v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08238">
<div class="article-summary-box-inner">
<span><p>Breast density estimation is one of the key tasks in recognizing individuals
predisposed to breast cancer. It is often challenging because of low contrast
and fluctuations in mammograms' fatty tissue background. Most of the time, the
breast density is estimated manually where a radiologist assigns one of the
four density categories decided by the Breast Imaging and Reporting Data
Systems (BI-RADS). There have been efforts in the direction of automating a
breast density classification pipeline.
</p>
<p>Breast density estimation is one of the key tasks performed during a
screening exam. Dense breasts are more susceptible to breast cancer. The
density estimation is challenging because of low contrast and fluctuations in
mammograms' fatty tissue background. Traditional mammograms are being replaced
by tomosynthesis and its other low radiation dose variants (for example
Hologic' Intelligent 2D and C-View). Because of the low-dose requirement,
increasingly more screening centers are favoring the Intelligent 2D view and
C-View. Deep-learning studies for breast density estimation use only a single
modality for training a neural network. However, doing so restricts the number
of images in the dataset. In this paper, we show that a neural network trained
on all the modalities at once performs better than a neural network trained on
any single modality. We discuss these results using the area under the receiver
operator characteristics curves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.09147">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has surpassed traditional methods for
medical image classification. However, CNN is vulnerable to adversarial attacks
which may lead to disastrous consequences in medical applications. Although
adversarial noises are usually generated by attack algorithms,
white-noise-induced adversarial samples can exist, and therefore the threats
are real. In this study, we propose a novel training method, named IMA, to
improve the robust-ness of CNN against adversarial noises. During training, the
IMA method increases the margins of training samples in the input space, i.e.,
moving CNN decision boundaries far away from the training samples to improve
robustness. The IMA method is evaluated on publicly available datasets under
strong 100-PGD white-box adversarial attacks, and the results show that the
proposed method significantly improved CNN classification and segmentation
accuracy on noisy data while keeping a high accuracy on clean data. We hope our
approach may facilitate the development of robust applications in medical
field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Frequency and Image Space Learning for MRI Reconstruction and Analysis. (arXiv:2007.01441v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01441">
<div class="article-summary-box-inner">
<span><p>We propose neural network layers that explicitly combine frequency and image
feature representations and show that they can be used as a versatile building
block for reconstruction from frequency space data. Our work is motivated by
the challenges arising in MRI acquisition where the signal is a corrupted
Fourier transform of the desired image. The proposed joint learning schemes
enable both correction of artifacts native to the frequency space and
manipulation of image space representations to reconstruct coherent image
structures at every layer of the network. This is in contrast to most current
deep learning approaches for image reconstruction that treat frequency and
image space features separately and often operate exclusively in one of the two
spaces. We demonstrate the advantages of joint convolutional learning for a
variety of tasks, including motion correction, denoising, reconstruction from
undersampled acquisitions, and combined undersampling and motion correction on
simulated and real world multicoil MRI data. The joint models produce
consistently high quality output images across all tasks and datasets. When
integrated into a state of the art unrolled optimization network with
physics-inspired data consistency constraints for undersampled reconstruction,
the proposed architectures significantly improve the optimization landscape,
which yields an order of magnitude reduction of training time. This result
suggests that joint representations are particularly well suited for MRI
signals in deep learning networks. Our code and pretrained models are publicly
available at https://github.com/nalinimsingh/interlacer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Learning: A Survey. (arXiv:2007.08745v5 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08745">
<div class="article-summary-box-inner">
<span><p>Backdoor attack intends to embed hidden backdoor into deep neural networks
(DNNs), so that the attacked models perform well on benign samples, whereas
their predictions will be maliciously changed if the hidden backdoor is
activated by attacker-specified triggers. This threat could happen when the
training process is not fully controlled, such as training on third-party
datasets or adopting third-party models, which poses a new and realistic
threat. Although backdoor learning is an emerging and rapidly growing research
area, its systematic review, however, remains blank. In this paper, we present
the first comprehensive survey of this realm. We summarize and categorize
existing backdoor attacks and defenses based on their characteristics, and
provide a unified framework for analyzing poisoning-based backdoor attacks.
Besides, we also analyze the relation between backdoor attacks and relevant
fields ($i.e.,$ adversarial attacks and data poisoning), and summarize widely
adopted benchmark datasets. Finally, we briefly outline certain future research
directions relying upon reviewed works. A curated list of backdoor-related
resources is also available at
\url{https://github.com/THUYimingLi/backdoor-learning-resources}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing Generation Networks with Structure Prediction for Recipe Generation. (arXiv:2007.13374v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.13374">
<div class="article-summary-box-inner">
<span><p>Recipe generation from food images and ingredients is a challenging task,
which requires the interpretation of the information from another modality.
Different from the image captioning task, where the captions usually have one
sentence, cooking instructions contain multiple sentences and have obvious
structures. To help the model capture the recipe structure and avoid missing
some cooking details, we propose a novel framework: Decomposing Generation
Networks (DGN) with structure prediction, to get more structured and complete
recipe generation outputs. Specifically, we split each cooking instruction into
several phases, and assign different sub-generators to each phase. Our approach
includes two novel ideas: (i) learning the recipe structures with the global
structure prediction component and (ii) producing recipe phases in the
sub-generator output component based on the predicted structure. Extensive
experiments on the challenging large-scale Recipe1M dataset validate the
effectiveness of our proposed model, which improves the performance over the
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex-valued Iris Recognition Network. (arXiv:2011.11198v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11198">
<div class="article-summary-box-inner">
<span><p>In this work, we design a fully complex-valued neural network for the task of
iris recognition. Unlike the problem of general object recognition, where
real-valued neural networks can be used to extract pertinent features, iris
recognition depends on the extraction of both phase and magnitude information
from the input iris texture in order to better represent its biometric content.
This necessitates the extraction and processing of phase information that
cannot be effectively handled by a real-valued neural network. In this regard,
we design a fully complex-valued neural network that can better capture the
multi-scale, multi-resolution, and multi-orientation phase and amplitude
features of the iris texture. We show a strong correspondence of the proposed
complex-valued iris recognition network with Gabor wavelets that are used to
generate the classical IrisCode; however, the proposed method enables a new
capability of automatic complex-valued feature learning that is tailored for
iris recognition. We conduct experiments on three benchmark datasets -
ND-CrossSensor-2013, CASIA-Iris-Thousand and UBIRIS.v2 - and show the benefit
of the proposed network for the task of iris recognition. We exploit
visualization schemes to convey how the complex-valued network, when compared
to standard real-valued networks, extracts fundamentally different features
from the iris texture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16938">
<div class="article-summary-box-inner">
<span><p>Real-time estimation of actual environment depth is an essential module for
various autonomous system tasks such as localization, obstacle detection and
pose estimation. During the last decade of machine learning, extensive
deployment of deep learning methods to computer vision tasks yielded successful
approaches for realistic depth synthesis out of a simple RGB modality. While
most of these models rest on paired depth data or availability of video
sequences and stereo images, there is a lack of methods facing single-image
depth synthesis in an unsupervised manner. Therefore, in this study, latest
advancements in the field of generative neural networks are leveraged to fully
unsupervised single-image depth synthesis. To be more exact, two
cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are
implemented and simultaneously optimized using the Wasserstein-1 distance. To
ensure plausibility of the proposed method, we apply the models to a self
acquised industrial data set as well as to the renown NYU Depth v2 data set,
which allows comparison with existing approaches. The observed success in this
study suggests high potential for unpaired single-image depth estimation in
real world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics. (arXiv:2106.01981v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01981">
<div class="article-summary-box-inner">
<span><p>Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preservation of the Global Knowledge by Not-True Self Knowledge Distillation in Federated Learning. (arXiv:2106.03097v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03097">
<div class="article-summary-box-inner">
<span><p>In federated learning, a strong global model is collaboratively learned by
aggregating clients' locally trained models. Although this precludes the need
to access clients' data directly, the global model's convergence often suffers
from data heterogeneity. This study starts from an analogy to continual
learning and suggests that forgetting could be the bottleneck of federated
learning. We observe that the global model forgets the knowledge from previous
rounds, and the local training induces forgetting the knowledge outside of the
local distribution. Based on our findings, we hypothesize that tackling down
forgetting will relieve the data heterogeneity problem. To this end, we propose
a novel and effective algorithm, Federated Not-True Distillation (FedNTD),
which preserves the global perspective on locally available data only for the
not-true classes. In the experiments, FedNTD shows state-of-the-art performance
on various setups without compromising data privacy or incurring additional
communication costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-based Style Randomization for Domain Generalization. (arXiv:2106.03171v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03171">
<div class="article-summary-box-inner">
<span><p>As a recent noticeable topic, domain generalization (DG) aims to first learn
a generic model on multiple source domains and then directly generalize to an
arbitrary unseen target domain without any additional adaption. In previous DG
models, by generating virtual data to supplement observed source domains, the
data augmentation based methods have shown its effectiveness. To simulate the
possible unseen domains, most of them enrich the diversity of original data via
image-level style transformation. However, we argue that the potential styles
are hard to be exhaustively illustrated and fully augmented due to the limited
referred styles, leading the diversity could not be always guaranteed. Unlike
image-level augmentation, we in this paper develop a simple yet effective
feature-based style randomization module to achieve feature-level augmentation,
which can produce random styles via integrating random noise into the original
style. Compared with existing image-level augmentation, our feature-level
augmentation favors a more goal-oriented and sample-diverse way. Furthermore,
to sufficiently explore the efficacy of the proposed module, we design a novel
progressive training strategy to enable all parameters of the network to be
fully trained. Extensive experiments on three standard benchmark datasets,
i.e., PACS, VLCS and Office-Home, highlight the superiority of our method
compared to the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Steerable 3D Spherical Neurons. (arXiv:2106.13863v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13863">
<div class="article-summary-box-inner">
<span><p>Emerging from low-level vision theory, steerable filters found their
counterpart in prior work on steerable convolutional neural networks
equivariant to rigid transformations. In our work, we propose a steerable
feed-forward learning-based approach that consists of neurons with spherical
decision surfaces and operates on point clouds. Such spherical neurons are
obtained by conformal embedding of Euclidean space and have recently been
revisited in the context of learning representations of point sets. Focusing on
3D geometry, we exploit the isometry property of spherical neurons and derive a
3D steerability constraint. After training spherical neurons to classify point
clouds in a canonical orientation, we use a tetrahedron basis to quadruplicate
the neurons and construct rotation-equivariant spherical filter banks. We then
apply the derived constraint to interpolate the filter bank outputs and, thus,
obtain a rotation-invariant network. Finally, we use a synthetic point set and
real-world 3D skeleton data to verify our theoretical findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans. (arXiv:2109.11941v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11941">
<div class="article-summary-box-inner">
<span><p>Accurately segmenting teeth and identifying the corresponding anatomical
landmarks on dental mesh models are essential in computer-aided orthodontic
treatment. Manually performing these two tasks is time-consuming, tedious, and,
more importantly, highly dependent on orthodontists' experiences due to the
abnormality and large-scale variance of patients' teeth. Some machine
learning-based methods have been designed and applied in the orthodontic field
to automatically segment dental meshes (e.g., intraoral scans). In contrast,
the number of studies on tooth landmark localization is still limited. This
paper proposes a two-stage framework based on mesh deep learning (called
TS-MDL) for joint tooth labeling and landmark identification on raw intraoral
scans. Our TS-MDL first adopts an end-to-end \emph{i}MeshSegNet method (i.e., a
variant of the existing MeshSegNet with both improved accuracy and efficiency)
to label each tooth on the downsampled scan. Guided by the segmentation
outputs, our TS-MDL further selects each tooth's region of interest (ROI) on
the original mesh to construct a light-weight variant of the pioneering
PointNet (i.e., PointNet-Reg) for regressing the corresponding landmark
heatmaps. Our TS-MDL was evaluated on a real-clinical dataset, showing
promising segmentation and localization performance. Specifically,
\emph{i}MeshSegNet in the first stage of TS-MDL reached an averaged Dice
similarity coefficient (DSC) at $0.964\pm0.054$, significantly outperforming
the original MeshSegNet. In the second stage, PointNet-Reg achieved a mean
absolute error (MAE) of $0.597\pm0.761 \, mm$ in distances between the
prediction and ground truth for $66$ landmarks, which is superior compared with
other networks for landmark detection. All these results suggest the potential
usage of our TS-MDL in clinical practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Graph-theoretic Algorithm for Small Bowel Path Tracking in CT Scans. (arXiv:2110.00466v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00466">
<div class="article-summary-box-inner">
<span><p>We present a novel graph-theoretic method for small bowel path tracking. It
is formulated as finding the minimum cost path between given start and end
nodes on a graph that is constructed based on the bowel wall detection. We
observed that a trivial solution with many short-cuts is easily made even with
the wall detection, where the tracked path penetrates indistinct walls around
the contact between different parts of the small bowel. Thus, we propose to
include must-pass nodes in finding the path to better cover the entire course
of the small bowel. The proposed method does not entail training with
ground-truth paths while the previous methods do. We acquired ground-truth
paths that are all connected from start to end of the small bowel for 10
abdominal CT scans, which enables the evaluation of the path tracking for the
entire course of the small bowel. The proposed method showed clear improvements
in terms of several metrics compared to the baseline method. The maximum length
of the path that is tracked without an error per scan, by the proposed method,
is above 800mm on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Objects in Semantic Topology. (arXiv:2110.02687v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02687">
<div class="article-summary-box-inner">
<span><p>A more realistic object detection paradigm, Open-World Object Detection, has
arisen increasing research interests in the community recently. A qualified
open-world object detector can not only identify objects of known categories,
but also discover unknown objects, and incrementally learn to categorize them
when their annotations progressively arrive. Previous works rely on independent
modules to recognize unknown categories and perform incremental learning,
respectively. In this paper, we provide a unified perspective: Semantic
Topology. During the life-long learning of an open-world object detector, all
object instances from the same category are assigned to their corresponding
pre-defined node in the semantic topology, including the `unknown' category.
This constraint builds up discriminative feature representations and consistent
relationships among objects, thus enabling the detector to distinguish unknown
objects out of the known categories, as well as making learned features of
known objects undistorted when learning new categories incrementally. Extensive
experiments demonstrate that semantic topology, either randomly-generated or
derived from a well-trained language model, could outperform the current
state-of-the-art open-world object detectors by a large margin, e.g., the
absolute open-set error is reduced from 7832 to 2546, exhibiting the inherent
superiority of semantic topology on open-world object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Strong Gravitational Lenses Through Self-Attention. (arXiv:2110.09202v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09202">
<div class="article-summary-box-inner">
<span><p>The upcoming large scale surveys are expected to find approximately $10^5$
strong gravitational systems by analyzing data of many orders of magnitude than
the current era. In this scenario, non-automated techniques will be highly
challenging and time-consuming. We propose a new automated architecture based
on the principle of self-attention to find strong gravitational lensing. The
advantages of self-attention based encoder models over convolution neural
networks are investigated and encoder models are analyzed to optimize
performance. We constructed 21 self-attention based encoder models and four
convolution neural networks trained to identify gravitational lenses from the
Bologna Lens Challenge. Each model is trained separately using 18,000 simulated
images, cross-validated using 2 000 images, and then applied to a test set with
100 000 images. We used four different metrics for evaluation: classification
accuracy, the area under the receiver operating characteristic curve (AUROC),
the $TPR_0$ score and the $TPR_{10}$ score. The performance of the
self-attention based encoder models and CNN's participated in the challenge are
compared. The encoder models performed better than the CNNs and surpassed the
CNN models that participated in the bologna lens challenge by a high margin for
the $TPR_0$ and $TPR_{10}$. In terms of the AUROC, the encoder models scored
equivalent to the top CNN model by only using one-sixth parameters to that of
the CNN. Self-Attention based models have a clear advantage compared to simpler
CNNs. A low computational cost and complexity make it a highly competing
architecture to currently used residual neural networks. Moreover, introducing
the encoder layers can also tackle the over-fitting problem present in the
CNN's by acting as effective filters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lung Cancer Lesion Detection in Histopathology Images Using Graph-Based Sparse PCA Network. (arXiv:2110.14728v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14728">
<div class="article-summary-box-inner">
<span><p>Early detection of lung cancer is critical for improvement of patient
survival. To address the clinical need for efficacious treatments, genetically
engineered mouse models (GEMM) have become integral in identifying and
evaluating the molecular underpinnings of this complex disease that may be
exploited as therapeutic targets. Assessment of GEMM tumor burden on
histopathological sections performed by manual inspection is both time
consuming and prone to subjective bias. Therefore, an interplay of needs and
challenges exists for computer-aided diagnostic tools, for accurate and
efficient analysis of these histopathology images. In this paper, we propose a
simple machine learning approach called the graph-based sparse principal
component analysis (GS-PCA) network, for automated detection of cancerous
lesions on histological lung slides stained by hematoxylin and eosin (H&amp;E). Our
method comprises four steps: 1) cascaded graph-based sparse PCA, 2) PCA binary
hashing, 3) block-wise histograms, and 4) support vector machine (SVM)
classification. In our proposed architecture, graph-based sparse PCA is
employed to learn the filter banks of the multiple stages of a convolutional
network. This is followed by PCA hashing and block histograms for indexing and
pooling. The meaningful features extracted from this GS-PCA are then fed to an
SVM classifier. We evaluate the performance of the proposed algorithm on H&amp;E
slides obtained from an inducible K-rasG12D lung cancer mouse model using
precision/recall rates, F-score, Tanimoto coefficient, and area under the curve
(AUC) of the receiver operator characteristic (ROC) and show that our algorithm
is efficient and provides improved detection accuracy compared to existing
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The self-supervised spectral-spatial attention-based transformer network for automated, accurate prediction of crop nitrogen status from UAV imagery. (arXiv:2111.06839v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06839">
<div class="article-summary-box-inner">
<span><p>Nitrogen (N) fertilizer is routinely applied by farmers to increase crop
yields. At present, farmers often over-apply N fertilizer in some locations or
at certain times because they do not have high-resolution crop N status data.
N-use efficiency can be low, with the remaining N lost to the environment,
resulting in higher production costs and environmental pollution. Accurate and
timely estimation of N status in crops is crucial to improving cropping
systems' economic and environmental sustainability. Destructive approaches
based on plant tissue analysis are time consuming and impractical over large
fields. Recent advances in remote sensing and deep learning have shown promise
in addressing the aforementioned challenges in a non-destructive way. In this
work, we propose a novel deep learning framework: a self-supervised
spectral-spatial attention-based vision transformer (SSVT). The proposed SSVT
introduces a Spectral Attention Block (SAB) and a Spatial Interaction Block
(SIB), which allows for simultaneous learning of both spatial and spectral
features from UAV digital aerial imagery, for accurate N status prediction in
wheat fields. Moreover, the proposed framework introduces local-to-global
self-supervised learning to help train the model from unlabelled data. The
proposed SSVT has been compared with five state-of-the-art models including:
ResNet, RegNet, EfficientNet, EfficientNetV2 and the original vision
transformer on both testing and independent datasets. The proposed approach
achieved high accuracy (0.96) with good generalizability and reproducibility
for wheat N status estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hamiltonian Operator Disentanglement of Content and Motion in Image Sequences. (arXiv:2112.01641v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01641">
<div class="article-summary-box-inner">
<span><p>We introduce a deep generative model for image sequences that reliably
factorise the latent space into content and motion variables. To model the
diverse dynamics, we split the motion space into subspaces and introduce a
unique Hamiltonian operator for each subspace. The Hamiltonian formulation
provides reversible dynamics that constrain the evolution of the motion path
along the low-dimensional manifold and conserves learnt invariant properties.
The explicit split of the motion space decomposes the Hamiltonian into symmetry
groups and gives long-term separability of the dynamics. This split also means
we can learn content representations that are easy to interpret and control. We
demonstrate the utility of our model by swapping the motion of two videos,
generating long term sequences of various actions from a given image,
unconditional sequence generation and image rotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Holistic Interpretation of Public Scenes Using Computer Vision and Temporal Graphs to Identify Social Distancing Violations. (arXiv:2112.06428v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06428">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has caused an unprecedented global public health
crisis. Given its inherent nature, social distancing measures are proposed as
the primary strategies to curb the spread of this pandemic. Therefore,
identifying situations where these protocols are violated, has implications for
curtailing the spread of the disease and promoting a sustainable lifestyle.
This paper proposes a novel computer vision-based system to analyze CCTV
footage to provide a threat level assessment of COVID-19 spread. The system
strives to holistically capture and interpret the information content of CCTV
footage spanning multiple frames to recognize instances of various violations
of social distancing protocols, across time and space, as well as
identification of group behaviors. This functionality is achieved primarily by
utilizing a temporal graph-based structure to represent the information of the
CCTV footage and a strategy to holistically interpret the graph and quantify
the threat level of the given scene. The individual components are tested and
validated on a range of scenarios and the complete system is tested against
human expert opinion. The results reflect the dependence of the threat level on
people, their physical proximity, interactions, protective clothing, and group
dynamics. The system performance has an accuracy of 76%, thus enabling a
deployable threat monitoring system in cities, to permit normalcy and
sustainability in the society.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirPose: Multi-View Fusion Network for Aerial 3D Human Pose and Shape Estimation. (arXiv:2201.08093v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08093">
<div class="article-summary-box-inner">
<span><p>In this letter, we present a novel markerless 3D human motion capture (MoCap)
system for unstructured, outdoor environments that uses a team of autonomous
unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation.
Existing methods are limited by calibrated cameras and off-line processing.
Thus, we present the first method (AirPose) to estimate human pose and shape
using images captured by multiple extrinsically uncalibrated flying cameras.
AirPose itself calibrates the cameras relative to the person instead of relying
on any pre-calibration. It uses distributed neural networks running on each UAV
that communicate viewpoint-independent information with each other about the
person (i.e., their 3D shape and articulated pose). The person's shape and pose
are parameterized using the SMPL-X body model, resulting in a compact
representation, that minimizes communication between the UAVs. The network is
trained using synthetic images of realistic virtual environments, and
fine-tuned on a small set of real images. We also introduce an
optimization-based post-processing method (AirPose$^{+}$) for offline
applications that require higher MoCap quality. We make our method's code and
data available for research at
https://github.com/robot-perception-group/AirPose. A video describing the
approach and results is available at https://youtu.be/xLYe1TNHsfs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Array Network for Low-light Image Enhancement. (arXiv:2201.08996v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08996">
<div class="article-summary-box-inner">
<span><p>Convolution neural networks (CNNs) based methods have dominated the low-light
image enhancement tasks due to their outstanding performance. However, the
convolution operation is based on a local sliding window mechanism, which is
difficult to construct the long-range dependencies of the feature maps.
Meanwhile, the self-attention based global relationship aggregation methods
have been widely used in computer vision, but these methods are difficult to
handle high-resolution images because of the high computational complexity. To
solve this problem, this paper proposes a Linear Array Self-attention (LASA)
mechanism, which uses only two 2-D feature encodings to construct 3-D global
weights and then refines feature maps generated by convolution layers. Based on
LASA, Linear Array Network (LAN) is proposed, which is superior to the existing
state-of-the-art (SOTA) methods in both RGB and RAW based low-light enhancement
tasks with a smaller amount of parameters. The code is released in
https://github.com/cuiziteng/LASA_enhancement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10890">
<div class="article-summary-box-inner">
<span><p>Human education system trains one student by multiple experts.
Mixture-of-experts (MoE) is a powerful sparse architecture including multiple
experts. However, sparse MoE model is hard to implement, easy to overfit, and
not hardware-friendly. In this work, inspired by human education model, we
propose a novel task, knowledge integration, to obtain a dense student model
(OneS) as knowledgeable as one sparse MoE. We investigate this task by
proposing a general training framework including knowledge gathering and
knowledge distillation. Specifically, we first propose Singular Value
Decomposition Knowledge Gathering (SVD-KG) to gather key knowledge from
different pretrained experts. We then refine the dense student model by
knowledge distillation to offset the noise from gathering. On ImageNet, our
OneS preserves $61.7\%$ benefits from MoE. OneS can achieve $78.4\%$ top-1
accuracy with only $15$M parameters. On four natural language processing
datasets, OneS obtains $88.2\%$ MoE benefits and outperforms SoTA by $51.7\%$
using the same architecture and training data. In addition, compared with the
MoE counterpart, OneS can achieve $3.7 \times$ inference speedup due to the
hardware-friendly architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DynaMixer: A Vision MLP Architecture with Dynamic Mixing. (arXiv:2201.12083v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12083">
<div class="article-summary-box-inner">
<span><p>Recently, MLP-like vision models have achieved promising performances on
mainstream visual recognition tasks. In contrast with vision transformers and
CNNs, the success of MLP-like models shows that simple information fusion
operations among tokens and channels can yield a good representation power for
deep recognition models. However, existing MLP-like models fuse tokens through
static fusion operations, lacking adaptability to the contents of the tokens to
be mixed. Thus, customary information fusion procedures are not effective
enough. To this end, this paper presents an efficient MLP-like network
architecture, dubbed DynaMixer, resorting to dynamic information fusion.
Critically, we propose a procedure, on which the DynaMixer model relies, to
dynamically generate mixing matrices by leveraging the contents of all the
tokens to be mixed. To reduce the time complexity and improve the robustness, a
dimensionality reduction technique and a multi-segment fusion mechanism are
adopted. Our proposed DynaMixer model (97M parameters) achieves 84.3\% top-1
accuracy on the ImageNet-1K dataset without extra training data, performing
favorably against the state-of-the-art vision MLP models. When the number of
parameters is reduced to 26M, it still achieves 82.7\% top-1 accuracy,
surpassing the existing MLP-like models with a similar capacity. The
implementation of DynaMixer will be made available to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">O-ViT: Orthogonal Vision Transformer. (arXiv:2201.12133v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12133">
<div class="article-summary-box-inner">
<span><p>Inspired by the tremendous success of the self-attention mechanism in natural
language processing, the Vision Transformer (ViT) creatively applies it to
image patch sequences and achieves incredible performance. However, the scaled
dot-product self-attention of ViT brings about scale ambiguity to the structure
of the original feature space. To address this problem, we propose a novel
method named Orthogonal Vision Transformer (O-ViT), to optimize ViT from the
geometric perspective. O-ViT limits parameters of self-attention blocks to be
on the norm-keeping orthogonal manifold, which can keep the geometry of the
feature space. Moreover, O-ViT achieves both orthogonal constraints and cheap
optimization overhead by adopting a surjective mapping between the orthogonal
group and its Lie algebra.We have conducted comparative experiments on image
recognition tasks to demonstrate O-ViT's validity and experiments show that
O-ViT can boost the performance of ViT by up to 3.6%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12170">
<div class="article-summary-box-inner">
<span><p>Real-time estimation of actual object depth is a module that is essential to
performing various autonomous system tasks such as 3D reconstruction, scene
understanding and condition assessment of machinery parts. During the last
decade of machine learning, extensive deployment of deep learning methods to
computer vision tasks has yielded approaches that succeed in achieving
realistic depth synthesis out of a simple RGB modality. While most of these
models are based on paired depth data or availability of video sequences and
stereo images, methods for single-view depth synthesis in a fully unsupervised
setting have hardly been explored. This study presents the most recent advances
in the field of generative neural networks, leveraging them to perform fully
unsupervised single-shot depth synthesis. Two generators for RGB-to-depth and
depth-to-RGB transfer are implemented and simultaneously optimized using the
Wasserstein-1 distance and a novel perceptual reconstruction term. To ensure
that the proposed method is plausible, we comprehensively evaluate the models
using industrial surface depth data as well as the Texas 3D Face Recognition
Database and the SURREAL dataset that records body depth. The success observed
in this study suggests the great potential for unsupervised single-shot depth
estimation in real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss. (arXiv:2201.12589v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12589">
<div class="article-summary-box-inner">
<span><p>The existence of completely aligned and paired multi-modal neuroimaging data
has proved its effectiveness in the diagnosis of brain diseases. However,
collecting the full set of well-aligned and paired data is impractical or even
luxurious, since the practical difficulties may include high cost, long time
acquisition, image corruption, and privacy issues. Previously, the misaligned
unpaired neuroimaging data (termed as MUD) are generally treated as noisy
label. However, such a noisy label-based method could not work very well when
misaligned data occurs distortions severely, for example, different angles of
rotation. In this paper, we propose a novel federated self-supervised learning
(FedMed) for brain image synthesis. An affine transform loss (ATL) was
formulated to make use of severely distorted images without violating privacy
legislation for the hospital. We then introduce a new data augmentation
procedure for self-supervised training and fed it into three auxiliary heads,
namely auxiliary rotation, auxiliary translation, and auxiliary scaling heads.
The proposed method demonstrates advanced performance in both the quality of
synthesized results under a severely misaligned and unpaired data setting, and
better stability than other GAN-based algorithms. The proposed method also
reduces the demand for deformable registration while encouraging to realize the
usage of those misaligned and unpaired data. Experimental results verify the
outstanding ability of our learning paradigm compared to other state-of-the-art
approaches. Our code is available on the website:
https://github.com/FedMed-Meta/FedMed-ATL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms. (arXiv:2201.12728v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12728">
<div class="article-summary-box-inner">
<span><p>Unlike the conventional facial expressions, micro-expressions are involuntary
and transient facial expressions capable of revealing the genuine emotions that
people attempt to hide. Therefore, they can provide important information in a
broad range of applications such as lie detection, criminal detection, etc.
Since micro-expressions are transient and of low intensity, however, their
detection and recognition is difficult and relies heavily on expert
experiences. Due to its intrinsic particularity and complexity, video-based
micro-expression analysis is attractive but challenging, and has recently
become an active area of research. Although there have been numerous
developments in this area, thus far there has been no comprehensive survey that
provides researchers with a systematic overview of these developments with a
unified evaluation. Accordingly, in this survey paper, we first highlight the
key differences between macro- and micro-expressions, then use these
differences to guide our research survey of video-based micro-expression
analysis in a cascaded structure, encompassing the neuropsychological basis,
datasets, features, spotting algorithms, recognition algorithms, applications
and evaluation of state-of-the-art approaches. For each aspect, the basic
techniques, advanced developments and major challenges are addressed and
discussed. Furthermore, after considering the limitations of existing
micro-expression datasets, we present and release a new dataset - called
micro-and-macro expression warehouse (MMEW) - containing more video samples and
more labeled emotion types. We then perform a unified comparison of
representative methods on CAS(ME)2 for spotting, and on MMEW and SAMM for
recognition, respectively. Finally, some potential future research directions
are explored and outlined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13392">
<div class="article-summary-box-inner">
<span><p>The mortality of lung cancer has ranked high among cancers for many years.
Early detection of lung cancer is critical for disease prevention, cure, and
mortality rate reduction. However, existing detection methods on pulmonary
nodules introduce an excessive number of false positive proposals in order to
achieve high sensitivity, which is not practical in clinical situations. In
this paper, we propose the multi-head detection and spatial
squeeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to
aid doctors in the early diagnosis of lung cancers. Specifically, we first
introduce multi-head detectors and skip connections to customize for the
variety of nodules in sizes, shapes and types and capture multi-scale features.
Then, we implement a spatial attention module to enable the network to focus on
different regions differently inspired by how experienced clinicians screen CT
images, which results in fewer false positive proposals. Lastly, we present a
lightweight but effective false positive reduction module with the Linear
Regression model to cut down the number of false positive proposals, without
any constraints on the front network. Extensive experimental results compared
with the state-of-the-art models have shown the superiority of the MHSnet in
terms of the average FROC, sensitivity and especially false discovery rate
(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%
and 28.33% decrease in terms of false discovery rate and average candidates per
scan). The false positive reduction module significantly decreases the average
number of candidates generated per scan by 68.11% and the false discovery rate
by 13.48%, which is promising to reduce distracted proposals for the downstream
tasks based on the detection results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boundary-aware Information Maximization for Self-supervised Medical Image Segmentation. (arXiv:2202.02371v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02371">
<div class="article-summary-box-inner">
<span><p>Unsupervised pre-training has been proven as an effective approach to boost
various downstream tasks given limited labeled data. Among various methods,
contrastive learning learns a discriminative representation by constructing
positive and negative pairs. However, it is not trivial to build reasonable
pairs for a segmentation task in an unsupervised way. In this work, we propose
a novel unsupervised pre-training framework that avoids the drawback of
contrastive learning. Our framework consists of two principles: unsupervised
over-segmentation as a pre-train task using mutual information maximization and
boundary-aware preserving learning. Experimental results on two benchmark
medical segmentation datasets reveal our method's effectiveness in improving
segmentation performance when few annotated images are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Computational Cytology: A Survey. (arXiv:2202.05126v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05126">
<div class="article-summary-box-inner">
<span><p>Computational cytology is a critical, rapid-developing, yet challenging topic
in the field of medical image computing which analyzes the digitized cytology
image by computer-aided technologies for cancer screening. Recently, an
increasing number of deep learning (DL) algorithms have made significant
progress in medical image analysis, leading to the boosting publications of
cytological studies. To investigate the advanced methods and comprehensive
applications, we survey more than 120 publications of DL-based cytology image
analysis in this article. We first introduce various deep learning methods,
including fully supervised, weakly supervised, unsupervised, and transfer
learning. Then, we systematically summarize the public datasets, evaluation
metrics, versatile cytology image analysis applications including
classification, detection, segmentation, and other related tasks. Finally, we
discuss current challenges and potential research directions of computational
cytology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs. (arXiv:2202.05331v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05331">
<div class="article-summary-box-inner">
<span><p>Several services for people with visual disabilities have emerged recently
due to achievements in Assistive Technologies and Artificial Intelligence
areas. Despite the growth in assistive systems availability, there is a lack of
services that support specific tasks, such as understanding the image context
presented in online content, e.g., webinars. Image captioning techniques and
their variants are limited as Assistive Technologies as they do not match the
needs of visually impaired people when generating specific descriptions. We
propose an approach for generating context of webinar images combining a dense
captioning technique with a set of filters, to fit the captions in our domain,
and a language model for the abstractive summary task. The results demonstrated
that we can produce descriptions with higher interpretability and focused on
the relevant information for that group of people by combining image analysis
methods and neural language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges. (arXiv:2202.06511v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06511">
<div class="article-summary-box-inner">
<span><p>Color fundus photography and Optical Coherence Tomography (OCT) are the two
most cost-effective tools for glaucoma screening. Both two modalities of images
have prominent biomarkers to indicate glaucoma suspected. Clinically, it is
often recommended to take both of the screenings for a more accurate and
reliable diagnosis. However, although numerous algorithms are proposed based on
fundus images or OCT volumes in computer-aided diagnosis, there are still few
methods leveraging both of the modalities for the glaucoma assessment. Inspired
by the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held
previously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA)
Challenge to encourage the development of fundus \&amp; OCT-based glaucoma grading.
The primary task of the challenge is to grade glaucoma from both the 2D fundus
images and 3D OCT scanning volumes. As part of GAMMA, we have publicly released
a glaucoma annotated dataset with both 2D fundus color photography and 3D OCT
volumes, which is the first multi-modality dataset for glaucoma grading. In
addition, an evaluation framework is also established to evaluate the
performance of the submitted methods. During the challenge, 1272 results were
submitted, and finally, top-10 teams were selected to the final stage. We
analysis their results and summarize their methods in the paper. Since all
these teams submitted their source code in the challenge, a detailed ablation
study is also conducted to verify the effectiveness of the particular modules
proposed. We find many of the proposed techniques are practical for the
clinical diagnosis of glaucoma. As the first in-depth study of fundus \&amp; OCT
multi-modality glaucoma grading, we believe the GAMMA Challenge will be an
essential starting point for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Cross-Modality Brain Image Synthesis. (arXiv:2202.06997v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06997">
<div class="article-summary-box-inner">
<span><p>The existence of completely aligned and paired multi-modal neuroimaging data
has proved its effectiveness in diagnosis of brain diseases. However,
collecting the full set of well-aligned and paired data is impractical or even
luxurious, since the practical difficulties may include high cost, long time
acquisition, image corruption, and privacy issues. A realistic solution is to
explore either an unsupervised learning or a semi-supervised learning to
synthesize the absent neuroimaging data. In this paper, we tend to approach
multi-modality brain image synthesis task from different perspectives, which
include the level of supervision, the range of modality synthesis, and the
synthesis-based downstream tasks. Particularly, we provide in-depth analysis on
how cross-modality brain image synthesis can improve the performance of
different downstream tasks. Finally, we evaluate the challenges and provide
several open directions for this community. All resources are available at
https://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Domain Experts for Long-Tailed Camera-Trap Recognition. (arXiv:2202.07215v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07215">
<div class="article-summary-box-inner">
<span><p>Label distributions in camera-trap images are highly imbalanced and
long-tailed, resulting in neural networks tending to be biased towards
head-classes that appear frequently. Although long-tail learning has been
extremely explored to address data imbalances, few studies have been conducted
to consider camera-trap characteristics, such as multi-domain and multi-frame
setup. Here, we propose a unified framework and introduce two datasets for
long-tailed camera-trap recognition. We first design domain experts, where each
expert learns to balance imperfect decision boundaries caused by data
imbalances and complement each other to generate domain-balanced decision
boundaries. Also, we propose a flow consistency loss to focus on moving
objects, expecting class activation maps of multi-frame matches the flow with
optical flow maps for input images. Moreover, two long-tailed camera-trap
datasets, WCS-LT and DMZ-LT, are introduced to validate our methods.
Experimental results show the effectiveness of our framework, and proposed
methods outperform previous methods on recessive domain samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks. (arXiv:2202.07261v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07261">
<div class="article-summary-box-inner">
<span><p>3D dynamic point clouds provide a discrete representation of real-world
objects or scenes in motion, which have been widely applied in immersive
telepresence, autonomous driving, surveillance, \textit{etc}. However, point
clouds acquired from sensors are usually perturbed by noise, which affects
downstream tasks such as surface reconstruction and analysis. Although many
efforts have been made for static point cloud denoising, few works address
dynamic point cloud denoising. In this paper, we propose a novel gradient-based
dynamic point cloud denoising method, exploiting the temporal correspondence
for the estimation of gradient fields -- also a fundamental problem in dynamic
point cloud processing and analysis. The gradient field is the gradient of the
log-probability function of the noisy point cloud, based on which we perform
gradient ascent so as to converge each point to the underlying clean surface.
We estimate the gradient of each surface patch by exploiting the temporal
correspondence, where the temporally corresponding patches are searched
leveraging on rigid motion in classical mechanics. In particular, we treat each
patch as a rigid object, which moves in the gradient field of an adjacent frame
via force until reaching a balanced state, i.e., when the sum of gradients over
the patch reaches 0. Since the gradient would be smaller when the point is
closer to the underlying surface, the balanced patch would fit the underlying
surface well, thus leading to the temporal correspondence. Finally, the
position of each point in the patch is updated along the direction of the
gradient averaged from corresponding patches in adjacent frames. Experimental
results demonstrate that the proposed model outperforms state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-17 23:07:16.532334738 UTC">2022-02-17 23:07:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>