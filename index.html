<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-29T01:30:00Z">03-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas. (arXiv:2203.13838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13838">
<div class="article-summary-box-inner">
<span><p>Vision and language navigation (VLN) is a challenging visually-grounded
language understanding task. Given a natural language navigation instruction, a
visual agent interacts with a graph-based environment equipped with panorama
images and tries to follow the described route. Most prior work has been
conducted in indoor scenarios where best results were obtained for navigation
on routes that are similar to the training routes, with sharp drops in
performance when testing on unseen environments. We focus on VLN in outdoor
scenarios and find that in contrast to indoor VLN, most of the gain in outdoor
VLN on unseen data is due to features like junction type embedding or heading
delta that are specific to the respective environment graph, while image
information plays a very minor role in generalizing VLN to unseen outdoor
areas. These findings show a bias to specifics of graph representations of
urban environments, demanding that VLN tasks grow in scale and diversity of
geographical environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Selection Curriculum for Neural Machine Translation. (arXiv:2203.13867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13867">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation (NMT) models are typically trained on
heterogeneous data that are concatenated and randomly shuffled. However, not
all of the training data are equally useful to the model. Curriculum training
aims to present the data to the NMT models in a meaningful order. In this work,
we introduce a two-stage curriculum training framework for NMT where we
fine-tune a base NMT model on subsets of data, selected by both deterministic
scoring using pre-trained methods and online scoring that considers prediction
scores of the emerging NMT model. Through comprehensive experiments on six
language pairs comprising low- and high-resource languages from WMT'21, we have
shown that our curriculum strategies consistently demonstrate better quality
(up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer
updates).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AUTOLEX: An Automatic Framework for Linguistic Exploration. (arXiv:2203.13901v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13901">
<div class="article-summary-box-inner">
<span><p>Each language has its own complex systems of word, phrase, and sentence
construction, the guiding principles of which are often summarized in grammar
descriptions for the consumption of linguists or language learners. However,
manual creation of such descriptions is a fraught process, as creating
descriptions which describe the language in "its own terms" without bias or
error requires both a deep understanding of the language at hand and
linguistics as a whole. We propose an automatic framework AutoLEX that aims to
ease linguists' discovery and extraction of concise descriptions of linguistic
phenomena. Specifically, we apply this framework to extract descriptions for
three phenomena: morphological agreement, case marking, and word order, across
several languages. We evaluate the descriptions with the help of language
experts and propose a method for automated evaluation when human evaluation is
infeasible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Canary Extraction in Natural Language Understanding Models. (arXiv:2203.13920v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13920">
<div class="article-summary-box-inner">
<span><p>Natural Language Understanding (NLU) models can be trained on sensitive
information such as phone numbers, zip-codes etc. Recent literature has focused
on Model Inversion Attacks (ModIvA) that can extract training data from model
parameters. In this work, we present a version of such an attack by extracting
canaries inserted in NLU training data. In the attack, an adversary with
open-box access to the model reconstructs the canaries contained in the model's
training set. We evaluate our approach by performing text completion on
canaries and demonstrate that by using the prefix (non-sensitive) tokens of the
canary, we can generate the full canary. As an example, our attack is able to
reconstruct a four digit code in the training dataset of the NLU model with a
probability of 0.5 in its best configuration. As countermeasures, we identify
several defense mechanisms that, when combined, effectively eliminate the risk
of ModIvA in our experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues. (arXiv:2203.13926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13926">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of dialogue reasoning with contextualized
commonsense inference. We curate CICERO, a dataset of dyadic conversations with
five types of utterance-level reasoning-based inferences: cause, subsequent
event, prerequisite, motivation, and emotional reaction. The dataset contains
53,105 of such inferences from 5,672 dialogues. We use this dataset to solve
relevant generative and discriminative tasks: generation of cause and
subsequent event; generation of prerequisite, motivation, and listener's
emotional reaction; and selection of plausible alternatives. Our results
ascertain the value of such dialogue-centric commonsense knowledge datasets. It
is our hope that CICERO will open new research avenues into commonsense-based
dialogue reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation. (arXiv:2203.13927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13927">
<div class="article-summary-box-inner">
<span><p>Accurate automatic evaluation metrics for open-domain dialogs are in high
demand. Existing model-based metrics for system response evaluation are trained
on human annotated data, which is cumbersome to collect. In this work, we
propose to use information that can be automatically extracted from the next
user utterance, such as its sentiment or whether the user explicitly ends the
conversation, as a proxy to measure the quality of the previous system
response. This allows us to train on a massive set of dialogs with weak
supervision, without requiring manual system turn quality annotations.
Experiments show that our model is comparable to models trained on human
annotated data. Furthermore, our model generalizes across both spoken and
written open-domain dialog corpora collected from real and paid users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations. (arXiv:2203.13928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13928">
<div class="article-summary-box-inner">
<span><p>Multiple metrics have been introduced to measure fairness in various natural
language processing tasks. These metrics can be roughly categorized into two
categories: 1) \emph{extrinsic metrics} for evaluating fairness in downstream
applications and 2) \emph{intrinsic metrics} for estimating fairness in
upstream contextualized language representation models. In this paper, we
conduct an extensive correlation study between intrinsic and extrinsic metrics
across bias notions using 19 contextualized language models. We find that
intrinsic and extrinsic metrics do not necessarily correlate in their original
setting, even when correcting for metric misalignments, noise in evaluation
datasets, and confounding factors such as experiment configuration for
extrinsic metrics. %al
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fantastic Questions and Where to Find Them: FairytaleQA -- An Authentic Dataset for Narrative Comprehension. (arXiv:2203.13947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13947">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) is a fundamental means to facilitate assessment and
training of narrative comprehension skills for both machines and young
children, yet there is scarcity of high-quality QA datasets carefully designed
to serve this purpose. In particular, existing datasets rarely distinguish
fine-grained reading skills, such as the understanding of varying narrative
elements. Drawing on the reading education research, we introduce FairytaleQA,
a dataset focusing on narrative comprehension of kindergarten to eighth-grade
students. Generated by educational experts based on an evidence-based
theoretical framework, FairytaleQA consists of 10,580 explicit and implicit
questions derived from 278 children-friendly stories, covering seven types of
narrative elements or relations. Our dataset is valuable in two folds: First,
we ran existing QA models on our dataset and confirmed that this annotation
helps assess models' fine-grained learning skills. Second, the dataset supports
question generation (QG) task in the education domain. Through benchmarking
with QG models, we show that the QG model trained on FairytaleQA is capable of
asking high-quality and more diverse questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Densely Connected Criss-Cross Attention Network for Document-level Relation Extraction. (arXiv:2203.13953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13953">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (RE) aims to identify relations between
two entities in a given document. Compared with its sentence-level counterpart,
document-level RE requires complex reasoning. Previous research normally
completed reasoning through information propagation on the mention-level or
entity-level document-graph, but rarely considered reasoning at the
entity-pair-level.In this paper, we propose a novel model, called Densely
Connected Criss-Cross Attention Network (Dense-CCNet), for document-level RE,
which can complete logical reasoning at the entity-pair-level. Specifically,
the Dense-CCNet performs entity-pair-level logical reasoning through the
Criss-Cross Attention (CCA), which can collect contextual information in
horizontal and vertical directions on the entity-pair matrix to enhance the
corresponding entity-pair representation. In addition, we densely connect
multiple layers of the CCA to simultaneously capture the features of single-hop
and multi-hop logical reasoning.We evaluate our Dense-CCNet model on three
public document-level RE datasets, DocRED, CDR, and GDA. Experimental results
demonstrate that our model achieves state-of-the-art performance on these three
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive Linguistic Steganography Based on BERT and Consistency Coding. (arXiv:2203.13972v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13972">
<div class="article-summary-box-inner">
<span><p>Linguistic steganography (LS) conceals the presence of communication by
embedding secret information into a text. How to generate a high-quality text
carrying secret information is a key problem. With the widespread application
of deep learning in natural language processing, recent algorithms use a
language model (LM) to generate the steganographic text, which provides a
higher payload compared with many previous arts. However, the security still
needs to be enhanced. To tackle with this problem, we propose a novel
autoregressive LS algorithm based on BERT and consistency coding, which
achieves a better trade-off between embedding payload and system security. In
the proposed work, based on the introduction of the masked LM, given a text, we
use consistency coding to make up for the shortcomings of block coding used in
the previous work so that we can encode arbitrary-size candidate token set and
take advantages of the probability distribution for information hiding. The
masked positions to be embedded are filled with tokens determined by an
autoregressive manner to enhance the connection between contexts and therefore
maintain the quality of the text. Experimental results have shown that,
compared with related works, the proposed work improves the fluency of the
steganographic text while guaranteeing security, and also increases the
embedding payload to a certain extent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Transformer/RNN Architecture for Gesture Typing in Indic Languages. (arXiv:2203.14049v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14049">
<div class="article-summary-box-inner">
<span><p>Gesture typing is a method of typing words on a touch-based keyboard by
creating a continuous trace passing through the relevant keys. This work is
aimed at developing a keyboard that supports gesture typing in Indic languages.
We begin by noting that when dealing with Indic languages, one needs to cater
to two different sets of users: (i) users who prefer to type in the native
Indic script (Devanagari, Bengali, etc.) and (ii) users who prefer to type in
the English script but want the output transliterated into the native script.
In both cases, we need a model that takes a trace as input and maps it to the
intended word. To enable the development of these models, we create and release
two datasets. First, we create a dataset containing keyboard traces for 193,658
words from 7 Indic languages. Second, we curate 104,412 English-Indic
transliteration pairs from Wikidata across these languages. Using these
datasets we build a model that performs path decoding, transliteration, and
transliteration correction. Unlike prior approaches, our proposed model does
not make co-character independence assumptions during decoding. The overall
accuracy of our model across the 7 languages varies from 70-95%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MQDD -- Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain. (arXiv:2203.14093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14093">
<div class="article-summary-box-inner">
<span><p>This work proposes a new pipeline for leveraging data collected on the Stack
Overflow website for pre-training a multimodal model for searching duplicates
on question answering websites. Our multimodal model is trained on question
descriptions and source codes in multiple programming languages. We design two
new learning objectives to improve duplicate detection capabilities. The result
of this work is a mature, fine-tuned Multimodal Question Duplicity Detection
(MQDD) model, ready to be integrated into a Stack Overflow search system, where
it can help users find answers for already answered questions. Alongside the
MQDD model, we release two datasets related to the software engineering domain.
The first Stack Overflow Dataset (SOD) represents a massive corpus of paired
questions and answers. The second Stack Overflow Duplicity Dataset (SODD)
contains data for training duplicate detection models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Roadmap for Big Model. (arXiv:2203.14101v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14101">
<div class="article-summary-box-inner">
<span><p>With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&amp;Interpretability, Commonsense Reasoning, Reliability&amp;Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lite Unified Modeling for Discriminative Reading Comprehension. (arXiv:2203.14103v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14103">
<div class="article-summary-box-inner">
<span><p>As a broad and major category in machine reading comprehension (MRC), the
generalized goal of discriminative MRC is answer prediction from the given
materials. However, the focuses of various discriminative MRC tasks may be
diverse enough: multi-choice MRC requires model to highlight and integrate all
potential critical evidence globally; while extractive MRC focuses on higher
local boundary preciseness for answer extraction. Among previous works, there
lacks a unified design with pertinence for the overall discriminative MRC
tasks. To fill in above gap, we propose a lightweight POS-Enhanced Iterative
Co-Attention Network (POI-Net) as the first attempt of unified modeling with
pertinence, to handle diverse discriminative MRC tasks synchronously. Nearly
without introducing more parameters, our lite unified design brings model
significant improvement with both encoder and decoder components. The
evaluation results on four discriminative MRC benchmarks consistently indicate
the general effectiveness and applicability of our model, and the code is
available at https://github.com/Yilin1111/poi-net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comparative analysis of Graph Neural Networks and commonly used machine learning algorithms on fake news detection. (arXiv:2203.14132v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14132">
<div class="article-summary-box-inner">
<span><p>Fake news on social media is increasingly regarded as one of the most
concerning issues. Low cost, simple accessibility via social platforms, and a
plethora of low-budget online news sources are some of the factors that
contribute to the spread of false news. Most of the existing fake news
detection algorithms are solely focused on the news content only but engaged
users prior posts or social activities provide a wealth of information about
their views on news and have significant ability to improve fake news
identification. Graph Neural Networks are a form of deep learning approach that
conducts prediction on graph-described data. Social media platforms are
followed graph structure in their representation, Graph Neural Network are
special types of neural networks that could be usually applied to graphs,
making it much easier to execute edge, node, and graph-level prediction.
Therefore, in this paper, we present a comparative analysis among some commonly
used machine learning algorithms and Graph Neural Networks for detecting the
spread of false news on social media platforms. In this study, we take the UPFD
dataset and implement several existing machine learning algorithms on text data
only. Besides this, we create different GNN layers for fusing graph-structured
news propagation data and the text data as the node feature in our GNN models.
GNNs provide the best solutions to the dilemma of identifying false news in our
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages. (arXiv:2203.14139v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14139">
<div class="article-summary-box-inner">
<span><p>Human languages are full of metaphorical expressions. Metaphors help people
understand the world by connecting new concepts and domains to more familiar
ones. Large pre-trained language models (PLMs) are therefore assumed to encode
metaphorical knowledge useful for NLP systems. In this paper, we investigate
this hypothesis for PLMs, by probing metaphoricity information in their
encodings, and by measuring the cross-lingual and cross-dataset generalization
of this information. We present studies in multiple metaphor detection datasets
and in four languages (i.e., English, Spanish, Russian, and Farsi). Our
extensive experiments suggest that contextual representations in PLMs do encode
metaphorical knowledge, and mostly in their middle layers. The knowledge is
transferable between languages and datasets, especially when the annotation is
consistent across training and testing sets. Our findings give helpful insights
for both cognitive and NLP scientists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demonstrating CAT: Synthesizing Data-Aware Conversational Agents for Transactional Databases. (arXiv:2203.14144v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14144">
<div class="article-summary-box-inner">
<span><p>Databases for OLTP are often the backbone for applications such as hotel room
or cinema ticket booking applications. However, developing a conversational
agent (i.e., a chatbot-like interface) to allow end-users to interact with an
application using natural language requires both immense amounts of training
data and NLP expertise. This motivates CAT, which can be used to easily create
conversational agents for transactional databases. The main idea is that, for a
given OLTP database, CAT uses weak supervision to synthesize the required
training data to train a state-of-the-art conversational agent, allowing users
to interact with the OLTP database. Furthermore, CAT provides an out-of-the-box
integration of the resulting agent with the database. As a major difference to
existing conversational agents, agents synthesized by CAT are data-aware. This
means that the agent decides which information should be requested from the
user based on the current data distributions in the database, which typically
results in markedly more efficient dialogues compared with non-data-aware
agents. We publish the code for CAT as open source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Word Information with Multi-Level Word Adapter for Chinese Spoken Language Understanding. (arXiv:2010.03903v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03903">
<div class="article-summary-box-inner">
<span><p>In this paper, we improve Chinese spoken language understanding (SLU) by
injecting word information. Previous studies on Chinese SLU do not consider the
word information, failing to detect word boundaries that are beneficial for
intent detection and slot filling. To address this issue, we propose a
multi-level word adapter to inject word information for Chinese SLU, which
consists of (1) sentence-level word adapter, which directly fuses the sentence
representations of the word information and character information to perform
intent detection and (2) character-level word adapter, which is applied at each
character for selectively controlling weights on word information as well as
character information. Experimental results on two Chinese SLU datasets show
that our model can capture useful word information and achieve state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaM-Gen:Causally-aware Metric-guided Text Generation. (arXiv:2010.12795v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12795">
<div class="article-summary-box-inner">
<span><p>Content is created for a well-defined purpose, often described by a metric or
signal represented in the form of structured information. The relationship
between the goal (metrics) of target content and the content itself is
non-trivial. While large-scale language models show promising text generation
capabilities, guiding the generated text with external metrics is challenging.
These metrics and content tend to have inherent relationships and not all of
them may be of consequence. We introduce CaM-Gen: Causally aware Generative
Networks guided by user-defined target metrics incorporating the causal
relationships between the metric and content features. We leverage causal
inference techniques to identify causally significant aspects of a text that
lead to the target metric and then explicitly guide generative models towards
these by a feedback mechanism. We propose this mechanism for variational
autoencoder and Transformer-based generative models. The proposed models beat
baselines in terms of the target metric control while maintaining fluency and
language quality of the generated text. To the best of our knowledge, this is
one of the early attempts at controlled generation incorporating a metric guide
using causal inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reference Knowledgeable Network for Machine Reading Comprehension. (arXiv:2012.03709v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03709">
<div class="article-summary-box-inner">
<span><p>Multi-choice Machine Reading Comprehension (MRC) as a challenge requires
models to select the most appropriate answer from a set of candidates with a
given passage and question. Most of the existing researches focus on the
modeling of specific tasks or complex networks, without explicitly referring to
relevant and credible external knowledge sources, which are supposed to greatly
make up for the deficiency of the given passage. Thus we propose a novel
reference-based knowledge enhancement model called Reference Knowledgeable
Network (RekNet), which simulates human reading strategies to refine critical
information from the passage and quote explicit knowledge in necessity. In
detail, RekNet refines finegrained critical information and defines it as
Reference Span, then quotes explicit knowledge quadruples by the co-occurrence
information of Reference Span and candidates. The proposed RekNet is evaluated
on three multi-choice MRC benchmarks: RACE, DREAM and Cosmos QA, obtaining
consistent and remarkable performance improvement with observable statistical
significance level over strong baselines. Our code is available at
https://github.com/Yilin1111/RekNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04631">
<div class="article-summary-box-inner">
<span><p>Machine translation between many languages at once is highly challenging,
since training with ground truth requires supervision between all language
pairs, which is difficult to obtain. Our key insight is that, while languages
may vary drastically, the underlying visual appearance of the world remains
consistent. We introduce a method that uses visual observations to bridge the
gap between languages, rather than relying on parallel corpora or topological
properties of the representations. We train a model that aligns segments of
text from different languages if and only if the images associated with them
are similar and each image in turn is well-aligned with its textual
description. We train our model from scratch on a new dataset of text in over
fifty languages with accompanying images. Experiments show that our method
outperforms previous work on unsupervised word and sentence translation using
retrieval. Code, models and data are available on globetrotter.cs.columbia.edu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06669">
<div class="article-summary-box-inner">
<span><p>Many implicit inferences exist in text depending on how it is structured that
can critically impact the text's interpretation and meaning. One such
structural aspect present in text with chronology is the order of its
presentation. For narratives or stories, this is known as the narrative order.
Reordering a narrative can impact the temporal, causal, event-based, and other
inferences readers draw from it, which in turn can have strong effects both on
its interpretation and interestingness. In this paper, we propose and
investigate the task of Narrative Reordering (NAREOR) which involves rewriting
a given story in a different narrative order while preserving its plot. We
present a dataset, NAREORC, with human rewritings of stories within ROCStories
in non-linear orders, and conduct a detailed analysis of it. Further, we
propose novel task-specific training methods with suitable evaluation metrics.
We perform experiments on NAREORC using state-of-the-art models such as BART
and T5 and conduct extensive automatic and human evaluations. We demonstrate
that although our models can perform decently, NAREOR is a challenging task
with potential for further exploration. We also investigate two applications of
NAREOR: generation of more interesting variations of stories and serving as
adversarial sets for temporal/event-related tasks, besides discussing other
prospective ones, such as for pedagogical setups related to language skills
like essay writing and applications to medicine involving clinical narratives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Feature and Instance Attribution to Detect Artifacts. (arXiv:2107.00323v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00323">
<div class="article-summary-box-inner">
<span><p>Training the deep neural networks that dominate NLP requires large datasets.
These are often collected automatically or via crowdsourcing, and may exhibit
systematic biases or annotation artifacts. By the latter we mean spurious
correlations between inputs and outputs that do not represent a generally held
causal relationship between features and classes; models that exploit such
correlations may appear to perform a given task well, but fail on out of sample
data. In this paper we evaluate use of different attribution methods for aiding
identification of training data artifacts. We propose new hybrid approaches
that combine saliency maps (which highlight important input features) with
instance attribution methods (which retrieve training samples influential to a
given prediction). We show that this proposed training-feature attribution can
be used to efficiently uncover artifacts in training data when a challenging
validation set is available. We also carry out a small user study to evaluate
whether these methods are useful to NLP researchers in practice, with promising
results. We make code for all methods and experiments in this paper available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. (arXiv:2108.06712v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06712">
<div class="article-summary-box-inner">
<span><p>Tables are often created with hierarchies, but existing works on table
reasoning mainly focus on flat tables and neglect hierarchical tables.
Hierarchical tables challenge existing methods by hierarchical indexing, as
well as implicit relationships of calculation and semantics. This work presents
HiTab, a free and open dataset to study question answering (QA) and natural
language generation (NLG) over hierarchical tables. HiTab is a cross-domain
dataset constructed from a wealth of statistical reports (analyses) and
Wikipedia pages, and has unique characteristics: (1) nearly all tables are
hierarchical, and (2) both target sentences for NLG and questions for QA are
revised from original, meaningful, and diverse descriptive sentences authored
by analysts and professions of reports. (3) to reveal complex numerical
reasoning in statistical analyses, we provide fine-grained annotations of
entity and quantity alignment. HiTab provides 10,686 QA pairs and descriptive
sentences with well-annotated quantity and entity alignment on 3,597 tables
with broad coverage of table hierarchies and numerical reasoning types.
</p>
<p>Targeting hierarchical structure, we devise a novel hierarchy-aware logical
form for symbolic reasoning over tables, which shows high effectiveness.
Targeting complex numerical reasoning, we propose partially supervised training
given annotations of entity and quantity alignment, which helps models to
largely reduce spurious predictions in the QA task. In the NLG task, we find
that entity and quantity alignment also helps NLG models to generate better
results in a conditional generation setting. Experiment results of
state-of-the-art baselines suggest that this dataset presents a strong
challenge and a valuable benchmark for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain. (arXiv:2108.06955v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06955">
<div class="article-summary-box-inner">
<span><p>We present MobIE, a German-language dataset, which is human-annotated with 20
coarse- and fine-grained entity types and entity linking information for
geographically linkable entities. The dataset consists of 3,232 social media
texts and traffic reports with 91K tokens, and contains 20.5K annotated
entities, 13.1K of which are linked to a knowledge base. A subset of the
dataset is human-annotated with seven mobility-related, n-ary relation types,
while the remaining documents are annotated using a weakly-supervised labeling
approach implemented with the Snorkel framework. To the best of our knowledge,
this is the first German-language dataset that combines annotations for NER, EL
and RE, and thus can be used for joint and multi-task learning of these
fundamental information extraction tasks. We make MobIE public at
https://github.com/dfki-nlp/mobie.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08614">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good systems providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, systems from the IR
and NLP communities have addressed QA over text, but such systems barely
utilize semantic data and knowledge. This paper presents the first QA system
that can seamlessly operate over RDF datasets and text corpora, or both
together, in a unified framework. Our method, called UNIQORN, builds a context
graph on-the-fly, by retrieving question-relevant evidences from the RDF data
and/or a text corpus, using fine-tuned BERT models. The resulting graph is
typically rich but highly noisy. UNIQORN copes with this input by a graph
algorithm for Group Steiner Trees, that identifies the best answer candidates
in the context graph. Experimental results on several benchmarks of complex
questions with multiple entities and relations, show that UNIQORN significantly
outperforms state-of-the-art methods for QA over heterogeneous sources. The
graph-based methodology provides user-interpretable evidence for the complete
answering process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Multi-Aspect Explainability Analyses on Machine Reading Comprehension Models. (arXiv:2108.11574v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11574">
<div class="article-summary-box-inner">
<span><p>Achieving human-level performance on some of the Machine Reading
Comprehension (MRC) datasets is no longer challenging with the help of powerful
Pre-trained Language Models (PLMs). However, the internal mechanism of these
artifacts remains unclear, placing an obstacle for further understanding these
models. This paper focuses on conducting a series of analytical experiments to
examine the relations between the multi-head self-attention and the final MRC
system performance, revealing the potential explainability in PLM-based MRC
models. To ensure the robustness of the analyses, we perform our experiments in
a multilingual way on top of various PLMs. We discover that passage-to-question
and passage understanding attentions are the most important ones in the
question answering process, showing strong correlations to the final
performance than other parts. Through comprehensive visualizations and case
studies, we also observe several general findings on the attention maps, which
can be helpful to understand how these models solve the questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00590">
<div class="article-summary-box-inner">
<span><p>Scaling Visual Question Answering (VQA) to the open-domain and multi-hop
nature of web searches, requires fundamental advances in visual representation
learning, knowledge aggregation, and language generation. In this work, we
introduce WebQA, a challenging new benchmark that proves difficult for
large-scale state-of-the-art models which lack language groundable visual
representations for novel objects and the ability to reason, yet trivial for
humans. WebQA mirrors the way humans use the web: 1) Ask a question, 2) Choose
sources to aggregate, and 3) Produce a fluent language response. This is the
behavior we should be expecting from IoT devices and digital assistants.
Existing work prefers to assume that a model can either reason about knowledge
in images or in text. WebQA includes a secondary text-only QA task to ensure
improved visual performance does not come at the cost of language
understanding. Our challenge for the community is to create unified multimodal
reasoning models that answer questions regardless of the source modality,
moving us closer to digital assistants that not only query language knowledge,
but also the richer visual online world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It is AI's Turn to Ask Humans a Question: Question-Answer Pair Generation for Children's Story Books. (arXiv:2109.03423v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03423">
<div class="article-summary-box-inner">
<span><p>Existing question answering (QA) techniques are created mainly to answer
questions asked by humans. But in educational applications, teachers often need
to decide what questions they should ask, in order to help students to improve
their narrative understanding capabilities. We design an automated
question-answer generation (QAG) system for this education scenario: given a
story book at the kindergarten to eighth-grade level as input, our system can
automatically generate QA pairs that are capable of testing a variety of
dimensions of a student's comprehension skills. Our proposed QAG model
architecture is demonstrated using a new expert-annotated FairytaleQA dataset,
which has 278 child-friendly storybooks with 10,580 QA pairs. Automatic and
human evaluations show that our model outperforms state-of-the-art QAG baseline
systems. On top of our QAG system, we also start to build an interactive
story-telling application for the future real-world deployment in this
educational scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning. (arXiv:2109.07589v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07589">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) in Few-Shot setting is imperative for entity
tagging in low resource domains. Existing approaches only learn class-specific
semantic features and intermediate representations from source domains. This
affects generalizability to unseen target domains, resulting in suboptimal
performances. To this end, we present CONTaiNER, a novel contrastive learning
technique that optimizes the inter-token distribution distance for Few-Shot
NER. Instead of optimizing class-specific attributes, CONTaiNER optimizes a
generalized objective of differentiating between token categories based on
their Gaussian-distributed embeddings. This effectively alleviates overfitting
issues originating from training domains. Our experiments in several
traditional test domains (OntoNotes, CoNLL'03, WNUT '17, GUM) and a new large
scale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER
outperforms previous methods by 3%-13% absolute F1 points while showing
consistent performance trends, even in challenging scenarios where previous
approaches could not achieve appreciable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10852">
<div class="article-summary-box-inner">
<span><p>We present Pix2Seq, a simple and generic framework for object detection.
Unlike existing approaches that explicitly integrate prior knowledge about the
task, we cast object detection as a language modeling task conditioned on the
observed pixel inputs. Object descriptions (e.g., bounding boxes and class
labels) are expressed as sequences of discrete tokens, and we train a neural
network to perceive the image and generate the desired sequence. Our approach
is based mainly on the intuition that if a neural network knows about where and
what the objects are, we just need to teach it how to read them out. Beyond the
use of task-specific data augmentations, our approach makes minimal assumptions
about the task, yet it achieves competitive results on the challenging COCO
dataset, compared to highly specialized and well optimized detection
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Non-local Features for Neural Constituency Parsing. (arXiv:2109.12814v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12814">
<div class="article-summary-box-inner">
<span><p>Thanks to the strong representation power of neural encoders, neural
chart-based parsers have achieved highly competitive performance by using local
features. Recently, it has been shown that non-local features in CRF structures
lead to improvements. In this paper, we investigate injecting non-local
features into the training process of a local span-based parser, by predicting
constituent n-gram non-local patterns and ensuring consistency between
non-local patterns and local constituents. Results show that our simple method
gives better results than the self-attentive parser on both PTB and CTB.
Besides, our method achieves state-of-the-art BERT-based performance on PTB
(95.92 F1) and strong performance on CTB (92.31 F1). Our parser also achieves
better or competitive performance in multilingual and zero-shot cross-domain
settings compared with the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Channel End-to-End Neural Diarization with Distributed Microphones. (arXiv:2110.04694v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04694">
<div class="article-summary-box-inner">
<span><p>Recent progress on end-to-end neural diarization (EEND) has enabled
overlap-aware speaker diarization with a single neural network. This paper
proposes to enhance EEND by using multi-channel signals from distributed
microphones. We replace Transformer encoders in EEND with two types of encoders
that process a multi-channel input: spatio-temporal and co-attention encoders.
Both are independent of the number and geometry of microphones and suitable for
distributed microphone settings. We also propose a model adaptation method
using only single-channel recordings. With simulated and real-recorded
datasets, we demonstrated that the proposed method outperformed conventional
EEND when a multi-channel input was given while maintaining comparable
performance with a single-channel input. We also showed that the proposed
method performed well even when spatial information is inoperative given
multi-channel inputs, such as in hybrid meetings in which the utterances of
multiple remote participants are played back from the same loudspeaker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Model Supervised by Understanding Map. (arXiv:2110.06043v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06043">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end translation of human neural activity to speech with a dual-dual generative adversarial network. (arXiv:2110.06634v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06634">
<div class="article-summary-box-inner">
<span><p>In a recent study of auditory evoked potential (AEP) based brain-computer
interface (BCI), it was shown that, with an encoder-decoder framework, it is
possible to translate human neural activity to speech (T-CAS). However, current
encoder-decoder-based methods achieve T-CAS often with a two-step method where
the information is passed between the encoder and decoder with a shared
dimension reduction vector, which may result in a loss of information. A
potential approach to this problem is to design an end-to-end method by using a
dual generative adversarial network (DualGAN) without dimension reduction of
passing information, but it cannot realize one-to-one signal-to-signal
translation (see Fig.1 (a) and (b)). In this paper, we propose an end-to-end
model to translate human neural activity to speech directly, create a new
electroencephalogram (EEG) datasets for participants with good attention by
design a device to detect participants' attention, and introduce a dual-dual
generative adversarial network (Dual-DualGAN) (see Fig. 1 (c) and (d)) to
address an end-to-end translation of human neural activity to speech (ET-CAS)
problem by group labelling EEG signals and speech signals, inserting a
transition domain to realize cross-domain mapping. In the transition domain,
the transition signals are cascaded by the corresponding EEG and speech signals
in a certain proportion, which can build bridges for EEG and speech signals
without corresponding features, and realize one-to-one cross-domain
EEG-to-speech translation. The proposed method can translate word-length and
sentence-length sequences of neural activity to speech. Experimental evaluation
has been conducted to show that the proposed method significantly outperforms
state-of-the-art methods on both words and sentences of auditory stimulus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MReD: A Meta-Review Dataset for Controllable Text Generation. (arXiv:2110.07474v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07474">
<div class="article-summary-box-inner">
<span><p>When directly using existing text generation datasets for controllable
generation, we are facing the problem of not having the domain knowledge and
thus the aspects that could be controlled are limited. A typical example is
when using CNN/Daily Mail dataset for controllable text summarization, there is
no guided information on the emphasis of summary sentences. A more useful text
generator should leverage both the input text and the control signal to guide
the generation, which can only be built with a deep understanding of the domain
knowledge. Motivated by this vision, our paper introduces a new text generation
dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its
45k meta-review sentences are manually annotated with one of the 9 carefully
defined categories, including abstract, strength, decision, etc. We present
experimental results on start-of-the-art summarization models, and propose
methods for structure-controlled generation with both extractive and
abstractive models using our annotated data. By exploring various settings and
analyzing the model behavior with respect to the control signal, we demonstrate
the challenges of our proposed task and the values of our dataset MReD.
Meanwhile, MReD also allows us to have a better understanding of the
meta-review domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction. (arXiv:2110.08345v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08345">
<div class="article-summary-box-inner">
<span><p>Existing studies on semantic parsing focus primarily on mapping a
natural-language utterance to a corresponding logical form in one turn.
However, because natural language can contain a great deal of ambiguity and
variability, this is a difficult challenge. In this work, we investigate an
interactive semantic parsing framework that explains the predicted logical form
step by step in natural language and enables the user to make corrections
through natural-language feedback for individual steps. We focus on question
answering over knowledge bases (KBQA) as an instantiation of our framework,
aiming to increase the transparency of the parsing process and help the user
appropriately trust the final answer. To do so, we construct INSPIRED, a
crowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. Our
experiments show that the interactive framework with human feedback has the
potential to greatly improve overall parse accuracy. Furthermore, we develop a
pipeline for dialogue simulation to evaluate our framework w.r.t. a variety of
state-of-the-art KBQA models without involving further crowdsourcing effort.
The results demonstrate that our interactive semantic parsing framework
promises to be effective across such models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Up Vision-Language Pre-training for Image Captioning. (arXiv:2111.12233v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12233">
<div class="article-summary-box-inner">
<span><p>In recent years, we have witnessed significant performance boost in the image
captioning task based on vision-language pre-training (VLP). Scale is believed
to be an important factor for this advance. However, most existing work only
focuses on pre-training transformers with moderate sizes (e.g., 12 or 24
layers) on roughly 4 million images. In this paper, we present LEMON, a
LargE-scale iMage captiONer, and provide the first empirical study on the
scaling behavior of VLP for image captioning. We use the state-of-the-art VinVL
model as our reference model, which consists of an image feature extractor and
a transformer model, and scale the transformer both up and down, with model
sizes ranging from 13 to 675 million parameters. In terms of data, we conduct
experiments with up to 200 million image-text pairs which are automatically
collected from web based on the alt attribute of the image (dubbed as ALT200M).
Extensive analysis helps to characterize the performance trend as the model
size and the pre-training data size increase. We also compare different
training recipes, especially for training on large-scale noisy data. As a
result, LEMON achieves new state of the arts on several major image captioning
benchmarks, including COCO Caption, nocaps, and Conceptual Captions. We also
show LEMON can generate captions with long-tail visual concepts when used in a
zero-shot manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIREX: A Unified Learning Framework for Language Model Rationale Extraction. (arXiv:2112.08802v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08802">
<div class="article-summary-box-inner">
<span><p>An extractive rationale explains a language model's (LM's) prediction on a
given task instance by highlighting the text inputs that most influenced the
prediction. Ideally, rationale extraction should be faithful (reflective of
LM's actual behavior) and plausible (convincing to humans), without
compromising the LM's (i.e., task model's) task performance. Although
attribution algorithms and select-predict pipelines are commonly used in
rationale extraction, they both rely on certain heuristics that hinder them
from satisfying all three desiderata. In light of this, we propose UNIREX, a
flexible learning framework which generalizes rationale extractor optimization
as follows: (1) specify architecture for a learned rationale extractor; (2)
select explainability objectives (i.e., faithfulness and plausibility
criteria); and (3) jointly the train task model and rationale extractor on the
task using selected objectives. UNIREX enables replacing prior works' heuristic
design choices with a generic learned rationale extractor in (1) and optimizing
it for all three desiderata in (2)-(3). To facilitate comparison between
methods w.r.t. multiple desiderata, we introduce the Normalized Relative Gain
(NRG) metric. Across five text classification datasets, our best UNIREX
configuration outperforms baselines by an average of 32.9% NRG. Plus, we find
that UNIREX-trained rationale extractors can even generalize to unseen datasets
and tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logically at Factify 2022: Multimodal Fact Verification. (arXiv:2112.09253v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09253">
<div class="article-summary-box-inner">
<span><p>This paper describes our participant system for the multi-modal fact
verification (Factify) challenge at AAAI 2022. Despite the recent advance in
text based verification techniques and large pre-trained multimodal models
cross vision and language, very limited work has been done in applying
multimodal techniques to automate fact checking process, particularly
considering the increasing prevalence of claims and fake news about images and
videos on social media. In our work, the challenge is treated as multimodal
entailment task and framed as multi-class classification. Two baseline
approaches are proposed and explored including an ensemble model (combining two
uni-modal models) and a multi-modal attention network (modeling the interaction
between image and text pair from claim and evidence document). We conduct
several experiments investigating and benchmarking different SoTA pre-trained
transformers and vision models in this work. Our best model is ranked first in
leaderboard which obtains a weighted average F-measure of 0.77 on both
validation and test set. Exploratory analysis of dataset is also carried out on
the Factify data set and uncovers salient patterns and issues (e.g., word
overlapping, visual entailment correlation, source bias) that motivates our
hypothesis. Finally, we highlight challenges of the task and multimodal dataset
for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-ABSA: Automatic Detection of Aspects in Aspect-Based Sentiment Analysis. (arXiv:2202.00484v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00484">
<div class="article-summary-box-inner">
<span><p>After transformer is proposed, lots of pre-trained language models have been
come up with and sentiment analysis (SA) task has been improved. In this paper,
we proposed a method that uses an auxiliary sentence about aspects that the
sentence contains to help sentiment prediction. The first is aspect detection,
which uses a multi-aspects detection model to predict all aspects that the
sentence has. Combining the predicted aspects and the original sentence as
Sentiment Analysis (SA) model's input. The second is to do out-of-domain
aspect-based sentiment analysis(ABSA), train sentiment classification model
with one kind of dataset and validate it with another kind of dataset. Finally,
we created two baselines, they use no aspect and all aspects as sentiment
classification model's input, respectively. Compare two baselines performance
to our method, found that our method really makes sense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07028">
<div class="article-summary-box-inner">
<span><p>We study the problem of developing autonomous agents that can follow human
instructions to infer and perform a sequence of actions to complete the
underlying task. Significant progress has been made in recent years, especially
for tasks with short horizons. However, when it comes to long-horizon tasks
with extended sequences of actions, an agent can easily ignore some
instructions or get stuck in the middle of the long instructions and eventually
fail the task. To address this challenge, we propose a model-agnostic
milestone-based task tracker (M-TRACK) to guide the agent and monitor its
progress. Specifically, we propose a milestone builder that tags the
instructions with navigation and interaction milestones which the agent needs
to complete step by step, and a milestone checker that systemically checks the
agent's progress in its current milestone and determines when to proceed to the
next. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and
52% relative improvement in unseen success rate over two competitive base
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. (arXiv:2202.11705v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11705">
<div class="article-summary-box-inner">
<span><p>Many applications of text generation require incorporating different
constraints to control the semantics or style of generated text. These
constraints can be hard (e.g., ensuring certain keywords are included in the
output) and soft (e.g., contextualizing the output with the left- or right-hand
context). In this paper, we present Energy-based Constrained Decoding with
Langevin Dynamics (COLD), a decoding framework which unifies constrained
generation as specifying constraints through an energy function, then
performing efficient differentiable reasoning over the constraints through
gradient-based sampling. COLD decoding is a flexible framework that can be
applied directly to off-the-shelf left-to-right language models without the
need for any task-specific fine-tuning, as demonstrated through three
challenging text generation applications: lexically-constrained generation,
abductive reasoning, and counterfactual reasoning. Our experiments on these
constrained generation tasks point to the effectiveness of our approach, both
in terms of automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction. (arXiv:2202.12109v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12109">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an effective yet efficient model PAIE for both
sentence-level and document-level Event Argument Extraction (EAE), which also
generalizes well when there is a lack of training data. On the one hand, PAIE
utilizes prompt tuning for extractive objectives to take the best advantages of
Pre-trained Language Models (PLMs). It introduces two span selectors based on
the prompt to select start/end tokens among input texts for each role. On the
other hand, it captures argument interactions via multi-role prompts and
conducts joint optimization with optimal span assignments via a bipartite
matching loss. Also, with a flexible prompt design, PAIE can extract multiple
arguments with the same role instead of conventional heuristic threshold
tuning. We have conducted extensive experiments on three benchmarks, including
both sentence- and document-level EAE. The results present promising
improvements from PAIE (3.5\% and 2.3\% F1 gains in average on three
benchmarks, for PAIE-base and PAIE-large respectively). Further analysis
demonstrates the efficiency, generalization to few-shot settings, and
effectiveness of different extractive prompt tuning strategies. Our code is
available at https://github.com/mayubo2333/PAIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Korean Tokenization for Beam Search Rescoring in Speech Recognition. (arXiv:2203.03583v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03583">
<div class="article-summary-box-inner">
<span><p>The performance of automatic speech recognition (ASR) models can be greatly
improved by proper beam-search decoding with external language model (LM).
There has been an increasing interest in Korean speech recognition, but not
many studies have been focused on the decoding procedure. In this paper, we
propose a Korean tokenization method for neural network-based LM used for
Korean ASR. Although the common approach is to use the same tokenization method
for external LM as the ASR model, we show that it may not be the best choice
for Korean. We propose a new tokenization method that inserts a special token,
SkipTC, when there is no trailing consonant in a Korean syllable. By utilizing
the proposed SkipTC token, the input sequence for LM becomes very regularly
patterned so that the LM can better learn the linguistic characteristics. Our
experiments show that the proposed approach achieves a lower word error rate
compared to the same LM model without SkipTC. In addition, we are the first to
report the ASR performance for the recently introduced large-scale 7,600h
Korean speech dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering. (arXiv:2203.04911v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04911">
<div class="article-summary-box-inner">
<span><p>Spoken Question Answering (SQA) is to find the answer from a spoken document
given a question, which is crucial for personal assistants when replying to the
queries from the users. Existing SQA methods all rely on Automatic Speech
Recognition (ASR) transcripts. Not only does ASR need to be trained with
massive annotated data that are time and cost-prohibitive to collect for
low-resourced languages, but more importantly, very often the answers to the
questions include name entities or out-of-vocabulary words that cannot be
recognized correctly. Also, ASR aims to minimize recognition errors equally
over all words, including many function words irrelevant to the SQA task.
Therefore, SQA without ASR transcripts (textless) is always highly desired,
although known to be very difficult.
</p>
<p>This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging
unlabeled data for pre-training and fine-tuned by the SQA downstream task. The
time intervals of spoken answers can be directly predicted from spoken
documents. We also release a new SQA benchmark corpus, NMSQA, for data with
more realistic scenarios. We empirically showed that DUAL yields results
comparable to those obtained by cascading ASR and text QA model and robust to
real-world data. Our code and model will be open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
derived from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps of the semantic features
between the textual question and visual answer, existing methods adopting
visual span predictor perform poorly in the TAGV task. To bridge these gaps, we
propose a visual-prompt text span localizing (VPTSL) method, which introduces
the timestamped subtitles as a passage to perform the text span localization
for the input text question, and prompts the visual highlight features into the
pre-trained language model (PLM) for enhancing the joint semantic
representations. Specifically, the context query attention is utilized to
perform cross-modal interaction between the extracted textual and visual
features. Then, the highlight features are obtained through the video-text
highlighting for the visual prompt. To alleviate semantic differences between
textual and visual features, we design the text span predictor by encoding the
question, the subtitles, and the prompted visual highlight features with the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the visual answer. Extensive experiments on the medical instructional
dataset, namely MedVidQA, show that the proposed VPTSL outperforms the
state-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,
which demonstrates the effectiveness of the proposed visual prompt and the text
span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07996">
<div class="article-summary-box-inner">
<span><p>Training Transformer-based models demands a large amount of data, while
obtaining aligned and labelled data in multimodality is rather cost-demanding,
especially for audio-visual speech recognition (AVSR). Thus it makes a lot of
sense to make use of unlabelled unimodal data. On the other side, although the
effectiveness of large-scale self-supervised learning is well established in
both audio and visual modalities, how to integrate those pre-trained models
into a multimodal scenario remains underexplored. In this work, we successfully
leverage unimodal self-supervised learning to promote the multimodal AVSR. In
particular, audio and visual front-ends are trained on large-scale unimodal
datasets, then we integrate components of both front-ends into a larger
multimodal framework which learns to recognize parallel audio-visual data into
characters through a combination of CTC and seq2seq decoding. We show that both
components inherited from unimodal self-supervised learning cooperate well,
resulting in that the multimodal framework yields competitive results through
fine-tuning. Our model is experimentally validated on both word-level and
sentence-level tasks. Especially, even without an external language model, our
proposed model raises the state-of-the-art performances on the widely accepted
Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative
improvement of 30%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Word Translation via Two-Stage Contrastive Learning. (arXiv:2203.08307v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08307">
<div class="article-summary-box-inner">
<span><p>Word translation or bilingual lexicon induction (BLI) is a key cross-lingual
task, aiming to bridge the lexical gap between different languages. In this
work, we propose a robust and effective two-stage contrastive learning
framework for the BLI task. At Stage C1, we propose to refine standard
cross-lingual linear maps between static word embeddings (WEs) via a
contrastive learning objective; we also show how to integrate it into the
self-learning procedure for even more refined cross-lingual maps. In Stage C2,
we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word
translation capability. We also show that static WEs induced from the
`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments
on standard BLI datasets for diverse languages and different experimental
setups demonstrate substantial gains achieved by our framework. While the BLI
method from Stage C1 already yields substantial gains over all state-of-the-art
BLI methods in our comparison, even stronger improvements are met with the full
two-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28
language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake News Detection Using Majority Voting Technique. (arXiv:2203.09936v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09936">
<div class="article-summary-box-inner">
<span><p>Due to the evolution of the Web and social network platforms it becomes very
easy to disseminate the information. Peoples are creating and sharing more
information than ever before, which may be misleading, misinformation or fake
information. Fake news detection is a crucial and challenging task due to the
unstructured nature of the available information. In the recent years,
researchers have provided significant solutions to tackle with the problem of
fake news detection, but due to its nature there are still many open issues. In
this paper, we have proposed majority voting approach to detect fake news
articles. We have used different textual properties of fake and real news. We
have used publicly available fake news dataset, comprising of 20,800 news
articles among which 10,387 are real and 10,413 are fake news labeled as binary
0 and 1. For the evaluation of our approach, we have used commonly used machine
learning classifiers like, Decision Tree, Logistic Regression, XGBoost, Random
Forest, Extra Trees, AdaBoost, SVM, SGD and Naive Bayes. Using the
aforementioned classifiers, we built a multi-model fake news detection system
using Majority Voting technique to achieve the more accurate results. The
experimental results show that, our proposed approach achieved accuracy of
96.38%, precision of 96%, recall of 96% and F1-measure of 96%. The evaluation
confirms that, Majority Voting technique achieved more acceptable results as
compare to individual learning technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaMEL: Case Marker Extraction without Labels. (arXiv:2203.10010v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10010">
<div class="article-summary-box-inner">
<span><p>We introduce CaMEL (Case Marker Extraction without Labels), a novel and
challenging task in computational morphology that is especially relevant for
low-resource languages. We propose a first model for CaMEL that uses a
massively multilingual corpus to extract case markers in 83 languages based
only on a noun phrase chunker and an alignment system. To evaluate CaMEL, we
automatically construct a silver standard from UniMorph. The case markers
extracted by our model can be used to detect and visualise similarities and
differences between the case systems of different languages as well as to
annotate fine-grained deep cases in languages in which they are not overtly
marked.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity and Content-based Phonetic Self Attention for Speech Recognition. (arXiv:2203.10252v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10252">
<div class="article-summary-box-inner">
<span><p>Transformer-based speech recognition models have achieved great success due
to the self-attention (SA) mechanism that utilizes every frame in the feature
extraction process. Especially, SA heads in lower layers capture various
phonetic characteristics by the query-key dot product, which is designed to
compute the pairwise relationship between frames. In this paper, we propose a
variant of SA to extract more representative phonetic features. The proposed
phonetic self-attention (phSA) is composed of two different types of phonetic
attention; one is similarity-based and the other is content-based. In short,
similarity-based attention utilizes the correlation between frames while
content-based attention only considers each frame without being affected by
others. We identify which parts of the original dot product are related to two
different attention patterns and improve each part by simple modifications. Our
experiments on phoneme classification and speech recognition show that
replacing SA with phSA for lower layers improves the recognition performance
without increasing the latency and the parameter size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses. (arXiv:2203.10750v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10750">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop a new multi-singer Chinese neural singing voice
synthesis (SVS) system named WeSinger. To improve the accuracy and naturalness
of synthesized singing voice, we design several specifical modules and
techniques: 1) A deep bi-directional LSTM based duration model with multi-scale
rhythm loss and post-processing step; 2) A Transformer-alike acoustic model
with progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet
neural vocoder to produce high-quality singing waveforms; 4) A novel data
augmentation method with multi-singer pre-training for stronger robustness and
naturalness. To our knowledge, WeSinger is the first SVS system to adopt 24 kHz
LPCNet and multi-singer pre-training simultaneously. Both quantitative and
qualitative evaluation results demonstrate the effectiveness of WeSinger in
terms of accuracy and naturalness, and WeSinger achieves state-of-the-art
performance on the recently public Chinese singing corpus Opencpop. Some
synthesized singing samples are available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The VoicePrivacy 2022 Challenge Evaluation Plan. (arXiv:2203.12468v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12468">
<div class="article-summary-box-inner">
<span><p>For new participants - Executive summary: (1) The task is to develop a voice
anonymization system for speech data which conceals the speaker's voice
identity while protecting linguistic content, paralinguistic attributes,
intelligibility and naturalness. (2) Training, development and evaluation
datasets are provided in addition to 3 different baseline anonymization
systems, evaluation scripts, and metrics. Participants apply their developed
anonymization systems, run evaluation scripts and submit objective evaluation
results and anonymized speech data to the organizers. (3) Results will be
presented at a workshop held in conjunction with INTERSPEECH 2022 to which all
participants are invited to present their challenge systems and to submit
additional workshop papers.
</p>
<p>For readers familiar with the VoicePrivacy Challenge - Changes w.r.t. 2020:
(1) A stronger, semi-informed attack model in the form of an automatic speaker
verification (ASV) system trained on anonymized (per-utterance) speech data.
(2) Complementary metrics comprising the equal error rate (EER) as a privacy
metric, the word error rate (WER) as a primary utility metric, and the pitch
correlation and gain of voice distinctiveness as secondary utility metrics. (3)
A new ranking policy based upon a set of minimum target privacy requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linearizing Transformer with Key-Value Memory Bank. (arXiv:2203.12644v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12644">
<div class="article-summary-box-inner">
<span><p>Transformer has brought great success to a wide range of natural language
processing tasks. Nevertheless, the computational overhead of the vanilla
transformer scales quadratically with sequence length. Many efforts have been
made to develop more efficient transformer variants. A line of work (e.g.,
Linformer) projects the input sequence into a low-rank space, achieving linear
time complexity. However, Linformer does not suit well for text generation
tasks as the sequence length must be pre-specified. We propose MemSizer, an
approach also projects the source sequence into lower dimension representation
but can take input with dynamic length, with a different perspective of the
attention mechanism. MemSizer not only achieves the same linear time complexity
but also enjoys efficient recurrent-style autoregressive generation, which
yields constant memory complexity and reduced computation at inference. We
demonstrate that MemSizer provides an improved tradeoff between efficiency and
accuracy over the vanilla transformer and other linear variants in language
modeling and machine translation tasks, revealing a viable direction towards
further inference efficiency improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Conversational Paradigm for Program Synthesis. (arXiv:2203.13474v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13474">
<div class="article-summary-box-inner">
<span><p>Program synthesis strives to generate a computer program as a solution to a
given problem specification. We propose a conversational program synthesis
approach via large language models, which addresses the challenges of searching
over a vast program space and user intent specification faced in prior
approaches. Our new approach casts the process of writing a specification and
program as a multi-turn conversation between a user and a system. It treats
program synthesis as a sequence prediction problem, in which the specification
is expressed in natural language and the desired program is conditionally
sampled. We train a family of large language models, called CodeGen, on natural
language and programming language data. With weak supervision in the data and
the scaling up of data size and model size, conversational capacities emerge
from the simple autoregressive language modeling. To study the model behavior
on conversational program synthesis, we develop a multi-turn programming
benchmark (MTPB), where solving each problem requires multi-step synthesis via
multi-turn conversation between the user and the model. Our findings show the
emergence of conversational capabilities and the effectiveness of the proposed
conversational program synthesis paradigm. In addition, our model CodeGen (with
up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the
HumanEval benchmark. We plan to make the training library JaxFormer including
checkpoints available as open source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZS4IE: A toolkit for Zero-Shot Information Extraction with simple Verbalizations. (arXiv:2203.13602v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13602">
<div class="article-summary-box-inner">
<span><p>The current workflow for Information Extraction (IE) analysts involves the
definition of the entities/relations of interest and a training corpus with
annotated examples. In this demonstration we introduce a new workflow where the
analyst directly verbalizes the entities/relations, which are then used by a
Textual Entailment model to perform zero-shot IE. We present the design and
implementation of a toolkit with a user interface, as well as experiments on
four IE tasks that show that the system achieves very good performance at
zero-shot learning using only 5--15 minutes per type of a user's effort. Our
demonstration system is open-sourced at https://github.com/BBN-E/ZS4IE . A
demonstration video is available at https://vimeo.<a href="/abs/com/6761383">com/6761383</a>40 .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13687">
<div class="article-summary-box-inner">
<span><p>In our previous work, we proposed a discriminative autoencoder (DcAE) for
speech recognition. DcAE combines two training schemes into one. First, since
DcAE aims to learn encoder-decoder mappings, the squared error between the
reconstructed speech and the input speech is minimized. Second, in the code
layer, frame-based phonetic embeddings are obtained by minimizing the
categorical cross-entropy between ground truth labels and predicted
triphone-state scores. DcAE is developed based on the Kaldi toolkit by treating
various TDNN models as encoders. In this paper, we further propose three new
versions of DcAE. First, a new objective function that considers both
categorical cross-entropy and mutual information between ground truth and
predicted triphone-state sequences is used. The resulting DcAE is called a
chain-based DcAE (c-DcAE). For application to robust speech recognition, we
further extend c-DcAE to hierarchical and parallel structures, resulting in
hc-DcAE and pc-DcAE. In these two models, both the error between the
reconstructed noisy speech and the input noisy speech and the error between the
enhanced speech and the reference clean speech are taken into the objective
function. Experimental results on the WSJ and Aurora-4 corpora show that our
DcAE models outperform baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UKP-SQUARE: An Online Platform for Question Answering Research. (arXiv:2203.13693v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13693">
<div class="article-summary-box-inner">
<span><p>Recent advances in NLP and information retrieval have given rise to a diverse
set of question answering tasks that are of different formats (e.g.,
extractive, abstractive), require different model architectures (e.g.,
generative, discriminative), and setups (e.g., with or without retrieval).
Despite having a large number of powerful, specialized QA pipelines (which we
refer to as Skills) that consider a single domain, model or setup, there exists
no framework where users can easily explore and compare such pipelines and can
extend them according to their needs. To address this issue, we present
UKP-SQUARE, an extensible online QA platform for researchers which allows users
to query and analyze a large collection of modern Skills via a user-friendly
web interface and integrated behavioural tests. In addition, QA researchers can
develop, manage, and share their custom Skills using our microservices that
support a wide range of models (Transformers, Adapters, ONNX), datastores and
retrieval techniques (e.g., sparse and dense). UKP-SQUARE is available on
https://square.ukp-lab.de.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration. (arXiv:2203.13834v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13834">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks ( DNN s) are known to make overconfident mistakes, which
makes their use problematic in safety-critical applications. State-of-the-art (
SOTA ) calibration techniques improve on the confidence of predicted labels
alone and leave the confidence of non-max classes (e.g. top-2, top-5)
uncalibrated. Such calibration is not suitable for label refinement using
post-processing. Further, most SOTA techniques learn a few hyper-parameters
post-hoc, leaving out the scope for image, or pixel specific calibration. This
makes them unsuitable for calibration under domain shift, or for dense
prediction tasks like semantic segmentation. In this paper, we argue for
intervening at the train time itself, so as to directly produce calibrated DNN
models. We propose a novel auxiliary loss function: Multi-class Difference in
Confidence and Accuracy ( MDCA ), to achieve the same MDCA can be used in
conjunction with other application/task-specific loss functions. We show that
training with MDCA leads to better-calibrated models in terms of Expected
Calibration Error ( ECE ), and Static Calibration Error ( SCE ) on image
classification, and segmentation tasks. We report ECE ( SCE ) score of 0.72
(1.60) on the CIFAR 100 dataset, in comparison to 1.90 (1.71) by the SOTA.
Under domain shift, a ResNet-18 model trained on PACS dataset using MDCA gives
an average ECE ( SCE ) score of 19.7 (9.7) across all domains, compared to 24.2
(11.8) by the SOTA. For the segmentation task, we report a 2X reduction in
calibration error on PASCAL - VOC dataset in comparison to Focal Loss. Finally,
MDCA training improves calibration even on imbalanced data, and for natural
language classification tasks. We have released the code here: code is
available at https://github.com/mdca-loss
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas. (arXiv:2203.13838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13838">
<div class="article-summary-box-inner">
<span><p>Vision and language navigation (VLN) is a challenging visually-grounded
language understanding task. Given a natural language navigation instruction, a
visual agent interacts with a graph-based environment equipped with panorama
images and tries to follow the described route. Most prior work has been
conducted in indoor scenarios where best results were obtained for navigation
on routes that are similar to the training routes, with sharp drops in
performance when testing on unseen environments. We focus on VLN in outdoor
scenarios and find that in contrast to indoor VLN, most of the gain in outdoor
VLN on unseen data is due to features like junction type embedding or heading
delta that are specific to the respective environment graph, while image
information plays a very minor role in generalizing VLN to unseen outdoor
areas. These findings show a bias to specifics of graph representations of
urban environments, demanding that VLN tasks grow in scale and diversity of
geographical environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets. (arXiv:2203.13856v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13856">
<div class="article-summary-box-inner">
<span><p>Deep learning has been proposed for the assessment and classification of
medical images. However, many medical image databases with appropriately
labeled and annotated images are small and imbalanced, and thus unsuitable to
train and validate such models. The option is to generate synthetic images and
one successful technique has been patented which limits its use for others. We
have developed a free-access, alternate method for generating synthetic
high-resolution images using Generative Adversarial Networks (GAN) for data
augmentation and showed their effectiveness using eye-fundus images for
Age-Related Macular Degeneration (AMD) identification. Ten different GAN
architectures were compared to generate synthetic eye-fundus images with and
without AMD. Data from three public databases were evaluated using the
Fr\'echet Inception Distance (FID), two clinical experts and deep-learning
classification. The results show that StyleGAN2 reached the lowest FID
(166.17), and clinicians could not accurately differentiate between real and
synthetic images. ResNet-18 architecture obtained the best performance with 85%
accuracy and outperformed the two experts in detecting AMD fundus images, whose
average accuracy was 77.5%. These results are similar to a recently patented
method, and will provide an alternative to generating high-quality synthetic
medical images. Free access has been provided to the entire method to
facilitate the further development of this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TimeReplayer: Unlocking the Potential of Event Cameras for Video Interpolation. (arXiv:2203.13859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13859">
<div class="article-summary-box-inner">
<span><p>Recording fast motion in a high FPS (frame-per-second) requires expensive
high-speed cameras. As an alternative, interpolating low-FPS videos from
commodity cameras has attracted significant attention. If only low-FPS videos
are available, motion assumptions (linear or quadratic) are necessary to infer
intermediate frames, which fail to model complex motions. Event camera, a new
camera with pixels producing events of brightness change at the temporal
resolution of $\mu s$ $(10^{-6}$ second $)$, is a game-changing device to
enable video interpolation at the presence of arbitrarily complex motion. Since
event camera is a novel sensor, its potential has not been fulfilled due to the
lack of processing algorithms. The pioneering work Time Lens introduced event
cameras to video interpolation by designing optical devices to collect a large
amount of paired training data of high-speed frames and events, which is too
costly to scale. To fully unlock the potential of event cameras, this paper
proposes a novel TimeReplayer algorithm to interpolate videos captured by
commodity cameras with events. It is trained in an unsupervised
cycle-consistent style, canceling the necessity of high-speed training data and
bringing the additional ability of video extrapolation. Its state-of-the-art
results and demo videos in supplementary reveal the promising future of
event-based vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FD-SLAM: 3-D Reconstruction Using Features and Dense Matching. (arXiv:2203.13861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13861">
<div class="article-summary-box-inner">
<span><p>It is well known that visual SLAM systems based on dense matching are locally
accurate but are also susceptible to long-term drift and map corruption. In
contrast, feature matching methods can achieve greater long-term consistency
but can suffer from inaccurate local pose estimation when feature information
is sparse. Based on these observations, we propose an RGB-D SLAM system that
leverages the advantages of both approaches: using dense frame-to-model
odometry to build accurate sub-maps and on-the-fly feature-based matching
across sub-maps for global map optimisation. In addition, we incorporate a
learning-based loop closure component based on 3-D features which further
stabilises map building. We have evaluated the approach on indoor sequences
from public datasets, and the results show that it performs on par or better
than state-of-the-art systems in terms of map reconstruction quality and pose
estimation. The approach can also scale to large scenes where other systems
often fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intelligent Masking: Deep Q-Learning for Context Encoding in Medical Image Analysis. (arXiv:2203.13865v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13865">
<div class="article-summary-box-inner">
<span><p>The need for a large amount of labeled data in the supervised setting has led
recent studies to utilize self-supervised learning to pre-train deep neural
networks using unlabeled data. Many self-supervised training strategies have
been investigated especially for medical datasets to leverage the information
available in the much fewer unlabeled data. One of the fundamental strategies
in image-based self-supervision is context prediction. In this approach, a
model is trained to reconstruct the contents of an arbitrary missing region of
an image based on its surroundings. However, the existing methods adopt a
random and blind masking approach by focusing uniformly on all regions of the
images. This approach results in a lot of unnecessary network updates that
cause the model to forget the rich extracted features. In this work, we develop
a novel self-supervised approach that occludes targeted regions to improve the
pre-training procedure. To this end, we propose a reinforcement learning-based
agent which learns to intelligently mask input images through deep Q-learning.
We show that training the agent against the prediction model can significantly
improve the semantic features extracted for downstream classification tasks. We
perform our experiments on two public datasets for diagnosing breast cancer in
the ultrasound images and detecting lower-grade glioma with MR images. In our
experiments, we show that our novel masking strategy advances the learned
features according to the performance on the classification task in terms of
accuracy, macro F1, and AUROC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Semantic Segmentation Grounded in Visual Concepts. (arXiv:2203.13868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13868">
<div class="article-summary-box-inner">
<span><p>Unsupervised semantic segmentation requires assigning a label to every pixel
without any human annotations. Despite recent advances in self-supervised
representation learning for individual images, unsupervised semantic
segmentation with pixel-level representations is still a challenging task and
remains underexplored. In this work, we propose a self-supervised pixel
representation learning method for semantic segmentation by using visual
concepts (i.e., groups of pixels with semantic meanings, such as parts,
objects, and scenes) extracted from images. To guide self-supervised learning,
we leverage three types of relationships between pixels and concepts, including
the relationships between pixels and local concepts, local and global concepts,
as well as the co-occurrence of concepts. We evaluate the learned pixel
embeddings and visual concepts on three datasets, including PASCAL VOC 2012,
COCO 2017, and DAVIS 2017. Our results show that the proposed method gains
consistent and substantial improvements over recent unsupervised semantic
segmentation approaches, and also demonstrate that visual concepts can reveal
insights into image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised machine learning model for analysis of nanowire morphologies from transmission electron microscopy images. (arXiv:2203.13875v1 [cond-mat.mtrl-sci])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13875">
<div class="article-summary-box-inner">
<span><p>In the field of soft materials, microscopy is the first and often only
accessible method for structural characterization. There is a growing interest
in the development of machine learning methods that can automate the analysis
and interpretation of microscopy images. Typically training of machine learning
models require large numbers of images with associated structural labels,
however, manual labeling of images requires domain knowledge and is prone to
human error and subjectivity. To overcome these limitations, we present a
self-supervised transfer learning approach that uses a small number of labeled
microscopy images for training and performs as effectively as methods trained
on significantly larger data sets. Specifically, we train an image encoder with
unlabeled images and use that encoder for transfer learning of different
downstream image tasks (classification and segmentation) with a minimal number
of labeled images for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning with Action-Free Pre-Training from Videos. (arXiv:2203.13880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13880">
<div class="article-summary-box-inner">
<span><p>Recent unsupervised pre-training methods have shown to be effective on
language and vision domains by learning useful representations for multiple
downstream tasks. In this paper, we investigate if such unsupervised
pre-training methods can also be effective for vision-based reinforcement
learning (RL). To this end, we introduce a framework that learns
representations useful for understanding the dynamics via generative
pre-training on videos. Our framework consists of two phases: we pre-train an
action-free latent video prediction model, and then utilize the pre-trained
representations for efficiently learning action-conditional world models on
unseen environments. To incorporate additional action inputs during
fine-tuning, we introduce a new architecture that stacks an action-conditional
latent prediction model on top of the pre-trained action-free prediction model.
Moreover, for better exploration, we propose a video-based intrinsic bonus that
leverages pre-trained representations. We demonstrate that our framework
significantly improves both final performances and sample-efficiency of
vision-based RL in a variety of manipulation and locomotion tasks. Code is
available at https://github.com/younggyoseo/apv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13883">
<div class="article-summary-box-inner">
<span><p>As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
changing accordingly. Taking advantage of the fact that visual modalities such
as images and videos are more favorable and attractive to the users, and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual correlations between modalities e.g., text
and image. Thus, many research efforts have been put into development of
automatic techniques for detecting possible cross-modal discordances in
web-based media. In this work, we aim to analyze, categorize and identify
existing approaches in addition to challenges and shortcomings they face in
order to unearth new opportunities in furthering the research in the field of
multi-modal misinformation detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sylph: A Hypernetwork Framework for Incremental Few-shot Object Detection. (arXiv:2203.13903v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13903">
<div class="article-summary-box-inner">
<span><p>We study the challenging incremental few-shot object detection (iFSD)
setting. Recently, hypernetwork-based approaches have been studied in the
context of continuous and finetune-free iFSD with limited success. We take a
closer look at important design choices of such methods, leading to several key
improvements and resulting in a more accurate and flexible framework, which we
call Sylph. In particular, we demonstrate the effectiveness of decoupling
object classification from localization by leveraging a base detector that is
pretrained for class-agnostic localization on large-scale dataset. Contrary to
what previous results have suggested, we show that with a carefully designed
class-conditional hypernetwork, finetune-free iFSD can be highly effective,
especially when a large number of base categories with abundant data are
available for meta-training, almost approaching alternatives that undergo
test-time-training. This result is even more significant considering its many
practical advantages: (1) incrementally learning new classes in sequence
without additional training, (2) detecting both novel and seen classes in a
single pass, and (3) no forgetting of previously seen classes. We benchmark our
model on both COCO and LVIS, reporting as high as $17\%$ AP on the long-tail
rare classes on LVIS, indicating the promise of hypernetwork-based iFSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept Embedding Analysis: A Review. (arXiv:2203.13909v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13909">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have found their way into many applications with
potential impact on the safety, security, and fairness of
human-machine-systems. Such require basic understanding and sufficient trust by
the users. This motivated the research field of explainable artificial
intelligence (XAI), i.e. finding methods for opening the "black-boxes" DNNs
represent. For the computer vision domain in specific, practical assessment of
DNNs requires a globally valid association of human interpretable concepts with
internals of the model. The research field of concept (embedding) analysis (CA)
tackles this problem: CA aims to find global, assessable associations of
humanly interpretable semantic concepts (e.g., eye, bearded) with internal
representations of a DNN. This work establishes a general definition of CA and
a taxonomy for CA methods, uniting several ideas from literature. That allows
to easily position and compare CA approaches. Guided by the defined notions,
the current state-of-the-art research regarding CA methods and interesting
applications are reviewed. More than thirty relevant methods are discussed,
compared, and categorized. Finally, for practitioners, a survey of fifteen
datasets is provided that have been used for supervised concept analysis. Open
challenges and research directions are pointed out at the end.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cross-Domain Approach for Continuous Impression Recognition from Dyadic Audio-Visual-Physio Signals. (arXiv:2203.13932v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13932">
<div class="article-summary-box-inner">
<span><p>The impression we make on others depends not only on what we say, but also,
to a large extent, on how we say it. As a sub-branch of affective computing and
social signal processing, impression recognition has proven critical in both
human-human conversations and spoken dialogue systems. However, most research
has studied impressions only from the signals expressed by the emitter,
ignoring the response from the receiver. In this paper, we perform impression
recognition using a proposed cross-domain architecture on the dyadic IMPRESSION
dataset. This improved architecture makes use of cross-domain attention and
regularization. The cross-domain attention consists of intra- and
inter-attention mechanisms, which capture intra- and inter-domain relatedness,
respectively. The cross-domain regularization includes knowledge distillation
and similarity enhancement losses, which strengthen the feature connections
between the emitter and receiver. The experimental evaluation verified the
effectiveness of our approach. Our approach achieved a concordance correlation
coefficient of 0.770 in competence dimension and 0.748 in warmth dimension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SolidGen: An Autoregressive Model for Direct B-rep Synthesis. (arXiv:2203.13944v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13944">
<div class="article-summary-box-inner">
<span><p>The Boundary representation (B-rep) format is the de-facto shape
representation in computer-aided design (CAD) to model watertight solid
objects. Recent approaches to generating CAD models have focused on learning
sketch-and-extrude modeling sequences that are executed by a solid modeling
kernel in postprocess to recover a B-rep. In this paper we present a new
approach that enables learning from and synthesizing B-reps without the need
for supervision through CAD modeling sequence data. Our method SolidGen, is an
autoregressive neural network that models the B-rep directly by predicting the
vertices, edges and faces using Transformer-based and pointer neural networks.
Key to achieving this is our Indexed Boundary Representation that references
B-rep vertices, edges and faces in a well-defined hierarchy to capture the
geometric and topological relations suitable for use with machine learning.
SolidGen can be easily conditioned on contexts e.g., class labels thanks to its
probabilistic modeling of the B-rep distribution. We demonstrate qualitatively,
quantitatively and through perceptual evaluation by human subjects that
SolidGen can produce high quality, realistic looking CAD models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-augmented histopathologic review using image analysis to optimize DNA yield and tumor purity from FFPE slides. (arXiv:2203.13948v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13948">
<div class="article-summary-box-inner">
<span><p>To achieve minimum DNA input and tumor purity requirements for
next-generation sequencing (NGS), pathologists visually estimate
macrodissection and slide count decisions. Misestimation may cause tissue waste
and increased laboratory costs. We developed an AI-augmented smart pathology
review system (SmartPath) to empower pathologists with quantitative metrics for
determining tissue extraction parameters. Using digitized H&amp;E-stained FFPE
slides as inputs, SmartPath segments tumors, extracts cell-based features, and
suggests a macrodissection areas. To predict DNA yield per slide, the extracted
features are correlated with known DNA yields. Then, a pathologist-defined
target yield divided by the predicted DNA yield/slide gives the number of
slides to scrape. Following model development, an internal validation trial was
conducted within the Tempus Labs molecular sequencing laboratory. We evaluated
our system on 501 clinical colorectal cancer slides, where half received
SmartPath-augmented review and half traditional pathologist review. The
SmartPath cohort had 25% more DNA yields within a desired target range of
100-2000ng. The SmartPath system recommended fewer slides to scrape for large
tissue sections, saving tissue in these cases. Conversely, SmartPath
recommended more slides to scrape for samples with scant tissue sections,
helping prevent costly re-extraction due to insufficient extraction yield. A
statistical analysis was performed to measure the impact of covariates on the
results, offering insights on how to improve future applications of SmartPath.
Overall, the study demonstrated that AI-augmented histopathologic review using
SmartPath could decrease tissue waste, sequencing time, and laboratory costs by
optimizing DNA yields and tumor purity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection. (arXiv:2203.13954v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13954">
<div class="article-summary-box-inner">
<span><p>The task of Human-Object Interaction~(HOI) detection could be divided into
two core problems, i.e., human-object association and interaction
understanding. In this paper, we reveal and address the disadvantages of the
conventional query-driven HOI detectors from the two aspects. For the
association, previous two-branch methods suffer from complex and costly
post-matching, while single-branch methods ignore the features distinction in
different tasks. We propose Guided-Embedding Network~(GEN) to attain a
two-branch pipeline without post-matching. In GEN, we design an instance
decoder to detect humans and objects with two independent query sets and a
position Guided Embedding~(p-GE) to mark the human and object in the same
position as a pair. Besides, we design an interaction decoder to classify
interactions, where the interaction queries are made of instance Guided
Embeddings (i-GE) generated from the outputs of each instance decoder layer.
For the interaction understanding, previous methods suffer from long-tailed
distribution and zero-shot discovery. This paper proposes a Visual-Linguistic
Knowledge Transfer (VLKT) training strategy to enhance interaction
understanding by transferring knowledge from a visual-linguistic pre-trained
model CLIP. In specific, we extract text embeddings for all labels with CLIP to
initialize the classifier and adopt a mimic loss to minimize the visual feature
distance between GEN and CLIP. As a result, GEN-VLKT outperforms the state of
the art by large margins on multiple datasets, e.g., +5.05 mAP on HICO-Det. The
source codes are available at https://github.com/YueLiao/gen-vlkt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution. (arXiv:2203.13963v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13963">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) can present multi-contrast images of the
same anatomical structures, enabling multi-contrast super-resolution (SR)
techniques. Compared with SR reconstruction using a single-contrast,
multi-contrast SR reconstruction is promising to yield SR images with higher
quality by leveraging diverse yet complementary information embedded in
different imaging modalities. However, existing methods still have two
shortcomings: (1) they neglect that the multi-contrast features at different
scales contain different anatomical details and hence lack effective mechanisms
to match and fuse these features for better reconstruction; and (2) they are
still deficient in capturing long-range dependencies, which are essential for
the regions with complicated anatomical structures. We propose a novel network
to comprehensively address these problems by developing a set of innovative
Transformer-empowered multi-scale contextual matching and aggregation
techniques; we call it McMRSR. Firstly, we tame transformers to model
long-range dependencies in both reference and target images. Then, a new
multi-scale contextual matching method is proposed to capture corresponding
contexts from reference features at different scales. Furthermore, we introduce
a multi-scale aggregation mechanism to gradually and interactively aggregate
multi-scale matched features for reconstructing the target SR MR image.
Extensive experiments demonstrate that our network outperforms state-of-the-art
approaches and has great potential to be applied in clinical practice. Codes
are available at https://github.com/XAIMI-Lab/McMRSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing Global and Local Features for Generalized AI-Synthesized Image Detection. (arXiv:2203.13964v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13964">
<div class="article-summary-box-inner">
<span><p>With the development of the Generative Adversarial Networks (GANs) and
DeepFakes, AI-synthesized images are now of such high quality that humans can
hardly distinguish them from real images. It is imperative for media forensics
to develop detectors to expose them accurately. Existing detection methods have
shown high performance in generated images detection, but they tend to
generalize poorly in the real-world scenarios, where the synthetic images are
usually generated with unseen models using unknown source data. In this work,
we emphasize the importance of combining information from the whole image and
informative patches in improving the generalization ability of AI-synthesized
image detection. Specifically, we design a two-branch model to combine global
spatial information from the whole image and local informative features from
multiple patches selected by a novel patch selection module. Multi-head
attention mechanism is further utilized to fuse the global and local features.
We collect a highly diverse dataset synthesized by 19 models with various
objects and resolutions to evaluate our model. Experimental results demonstrate
the high accuracy and good generalization ability of our method in detecting
generated images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Self-Attention for Visual Intersection Classification. (arXiv:2203.13977v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13977">
<div class="article-summary-box-inner">
<span><p>In robot vision, self-attention has recently emerged as a technique for
capturing non-local contexts. In this study, we introduced a self-attention
mechanism into the intersection recognition system as a method to capture the
non-local contexts behind the scenes. An intersection classification system
comprises two distinctive modules: (a) a first-person vision (FPV) module,
which uses a short egocentric view sequence as the intersection is passed, and
(b) a third-person vision (TPV) module, which uses a single view immediately
before entering the intersection. The self-attention mechanism is effective in
the TPV module because most parts of the local pattern (e.g., road edges,
buildings, and sky) are similar to each other, and thus the use of a non-local
context (e.g., the angle between two diagonal corners around an intersection)
would be effective. This study makes three major contributions. First, we
proposed a self-attention-based approach for intersection classification using
TPVs. Second, we presented a practical system in which a self-attention-based
TPV module is combined with an FPV module to improve the overall recognition
performance. Finally, experiments using the public KITTI dataset show that the
above self-attention-based system outperforms conventional recognition based on
local patterns and recognition based on convolution operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Current Source Localization Using Deep Prior with Depth Weighting. (arXiv:2203.13981v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13981">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel neuronal current source localization method based
on Deep Prior that represents a more complicated prior distribution of current
source using convolutional networks. Deep Prior has been suggested as a means
of an unsupervised learning approach that does not require learning using
training data, and randomly-initialized neural networks are used to update a
source location using a single observation. In our previous work, a
Deep-Prior-based current source localization method in the brain has been
proposed but the performance was not almost the same as those of conventional
approaches, such as sLORETA. In order to improve the Deep-Prior-based approach,
in this paper, a depth weight of the current source is introduced for Deep
Prior, where depth weighting amounts to assigning more penalty to the
superficial currents. Its effectiveness is confirmed by experiments of current
source estimation on simulated MEG data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Monocular Depth Estimation Provide Better Pre-training than Classification for Semantic Segmentation?. (arXiv:2203.13987v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13987">
<div class="article-summary-box-inner">
<span><p>Training a deep neural network for semantic segmentation is labor-intensive,
so it is common to pre-train it for a different task, and then fine-tune it
with a small annotated dataset. State-of-the-art methods use image
classification for pre-training, which introduces uncontrolled biases. We test
the hypothesis that depth estimation from unlabeled videos may provide better
pre-training. Despite the absence of any semantic information, we argue that
estimating scene geometry is closer to the task of semantic segmentation than
classifying whole images into semantic classes. Since analytical validation is
intractable, we test the hypothesis empirically by introducing a pre-training
scheme that yields an improvement of 5.7% mIoU and 4.1% pixel accuracy over
classification-based pre-training. While annotation is not needed for
pre-training, it is needed for testing the hypothesis. We use the KITTI
(outdoor) and NYU-V2 (indoor) benchmarks to that end, and provide an extensive
discussion of the benefits and limitations of the proposed scheme in relation
to existing unsupervised, self-supervised, and semi-supervised pre-training
protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RSCFed: Random Sampling Consensus Federated Semi-supervised Learning. (arXiv:2203.13993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13993">
<div class="article-summary-box-inner">
<span><p>Federated semi-supervised learning (FSSL) aims to derive a global model by
training fully-labeled and fully-unlabeled clients or training partially
labeled clients. The existing approaches work well when local clients have
independent and identically distributed (IID) data but fail to generalize to a
more practical FSSL setting, i.e., Non-IID setting. In this paper, we present a
Random Sampling Consensus Federated learning, namely RSCFed, by considering the
uneven reliability among models from fully-labeled clients, fully-unlabeled
clients or partially labeled clients. Our key motivation is that given models
with large deviations from either labeled clients or unlabeled clients, the
consensus could be reached by performing random sub-sampling over clients. To
achieve it, instead of directly aggregating local models, we first distill
several sub-consensus models by random sub-sampling over clients and then
aggregating the sub-consensus models to the global model. To enhance the
robustness of sub-consensus models, we also develop a novel distance-reweighted
model aggregation method. Experimental results show that our method outperforms
state-of-the-art methods on three benchmarked datasets, including both natural
and medical images. The code is available at
https://github.com/XMed-Lab/RSCFed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Predict RNA Sequence Expressions from Whole Slide Images with Applications for Search and Classification. (arXiv:2203.13997v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13997">
<div class="article-summary-box-inner">
<span><p>Deep learning methods are widely applied in digital pathology to address
clinical challenges such as prognosis and diagnosis. As one of the most recent
applications, deep models have also been used to extract molecular features
from whole slide images. Although molecular tests carry rich information, they
are often expensive, time-consuming, and require additional tissue to sample.
In this paper, we propose tRNAsfomer, an attention-based topology that can
learn both to predict the bulk RNA-seq from an image and represent the whole
slide image of a glass slide simultaneously. The tRNAsfomer uses multiple
instance learning to solve a weakly supervised problem while the pixel-level
annotation is not available for an image. We conducted several experiments and
achieved better performance and faster convergence in comparison to the
state-of-the-art algorithms. The proposed tRNAsfomer can assist as a
computational pathology tool to facilitate a new generation of search and
classification methods by combining the tissue morphology and the molecular
fingerprint of the biopsy samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation with the Reused Teacher Classifier. (arXiv:2203.14001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14001">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation aims to compress a powerful yet cumbersome teacher
model into a lightweight student model without much sacrifice of performance.
For this purpose, various approaches have been proposed over the past few
years, generally with elaborately designed knowledge representations, which in
turn increase the difficulty of model development and interpretation. In
contrast, we empirically show that a simple knowledge distillation technique is
enough to significantly narrow down the teacher-student performance gap. We
directly reuse the discriminative classifier from the pre-trained teacher model
for student inference and train a student encoder through feature alignment
with a single $\ell_2$ loss. In this way, the student model is able to achieve
exactly the same performance as the teacher model provided that their extracted
features are perfectly aligned. An additional projector is developed to help
the student encoder match with the teacher classifier, which renders our
technique applicable to various teacher and student architectures. Extensive
experiments demonstrate that our technique achieves state-of-the-art results at
the modest cost of compression ratio due to the added projector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn to Adapt for Monocular Depth Estimation. (arXiv:2203.14005v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14005">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation is one of the fundamental tasks in environmental
perception and has achieved tremendous progress in virtue of deep learning.
However, the performance of trained models tends to degrade or deteriorate when
employed on other new datasets due to the gap between different datasets.
Though some methods utilize domain adaptation technologies to jointly train
different domains and narrow the gap between them, the trained models cannot
generalize to new domains that are not involved in training. To boost the
transferability of depth estimation models, we propose an adversarial depth
estimation task and train the model in the pipeline of meta-learning. Our
proposed adversarial task mitigates the issue of meta-overfitting, since the
network is trained in an adversarial manner and aims to extract domain
invariant representations. In addition, we propose a constraint to impose upon
cross-task depth consistency to compel the depth estimation to be identical in
different adversarial tasks, which improves the performance of our method and
smoothens the training process. Experiments demonstrate that our method adapts
well to new datasets after few training steps during the test procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EYNet: Extended YOLO for Airport Detection in Remote Sensing Images. (arXiv:2203.14007v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14007">
<div class="article-summary-box-inner">
<span><p>Nowadays, airport detection in remote sensing images has attracted
considerable attention due to its strategic role in civilian and military
scopes. In particular, uncrewed and operated aerial vehicles must immediately
detect safe areas to land in emergencies. The previous schemes suffered from
various aspects, including complicated backgrounds, scales, and shapes of the
airport. Meanwhile, the rapid action and accuracy of the method are confronted
with significant concerns. Hence, this study proposes an effective scheme by
extending YOLOV3 and ShearLet transform. In this way, MobileNet and ResNet18,
with fewer layers and parameters retrained on a similar dataset, are parallelly
trained as base networks. According to airport geometrical characteristics, the
ShearLet filters with different scales and directions are considered in the
first convolution layers of ResNet18 as a visual attention mechanism. Besides,
the major extended in YOLOV3 concerns the detection Sub-Networks with novel
structures which boost object expression ability and training efficiency. In
addition, novel augmentation and negative mining strategies are presented to
significantly increase the localization phase's performance. The experimental
results on the DIOR dataset reveal that the framework reliably detects
different types of airports in a varied area and acquires robust results in
complex scenes compared to traditional YOLOV3 and state-of-the-art schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGDR: Semantic-guided Disentangled Representation for Unsupervised Cross-modality Medical Image Segmentation. (arXiv:2203.14025v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14025">
<div class="article-summary-box-inner">
<span><p>Disentangled representation is a powerful technique to tackle domain shift
problem in medical image analysis in unsupervised domain adaptation
setting.However, previous methods only focus on exacting domain-invariant
feature and ignore whether exacted feature is meaningful for downstream
tasks.We propose a novel framework, called semantic-guided disentangled
representation (SGDR), an effective method to exact semantically meaningful
feature for segmentation task to improve performance of cross modality medical
image segmentation in unsupervised domain adaptation setting.To exact the
meaningful domain-invariant features of different modality, we introduce a
content discriminator to force the content representation to be embedded to the
same space and a feature discriminator to exact the meaningful
representation.We also use pixel-level annotations to guide the encoder to
learn features that are meaningful for segmentation task.We validated our
method on two public datasets and experiment results show that our approach
outperforms the state of the art methods on two evaluation metrics by a
significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medicinal Boxes Recognition on a Deep Transfer Learning Augmented Reality Mobile Application. (arXiv:2203.14031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14031">
<div class="article-summary-box-inner">
<span><p>Taking medicines is a fundamental aspect to cure illnesses. However, studies
have shown that it can be hard for patients to remember the correct posology.
More aggravating, a wrong dosage generally causes the disease to worsen.
Although, all relevant instructions for a medicine are summarized in the
corresponding patient information leaflet, the latter is generally difficult to
navigate and understand. To address this problem and help patients with their
medication, in this paper we introduce an augmented reality mobile application
that can present to the user important details on the framed medicine. In
particular, the app implements an inference engine based on a deep neural
network, i.e., a densenet, fine-tuned to recognize a medicinal from its
package. Subsequently, relevant information, such as posology or a simplified
leaflet, is overlaid on the camera feed to help a patient when taking a
medicine. Extensive experiments to select the best hyperparameters were
performed on a dataset specifically collected to address this task; ultimately
obtaining up to 91.30\% accuracy as well as real-time capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Abductive Reasoning. (arXiv:2203.14040v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14040">
<div class="article-summary-box-inner">
<span><p>Abductive reasoning seeks the likeliest possible explanation for partial
observations. Although abduction is frequently employed in human daily
reasoning, it is rarely explored in computer vision literature. In this paper,
we propose a new task and dataset, Visual Abductive Reasoning (VAR), for
examining abductive reasoning ability of machine intelligence in everyday
visual situations. Given an incomplete set of visual events, AI systems are
required to not only describe what is observed, but also infer the hypothesis
that can best explain the visual premise. Based on our large-scale VAR dataset,
we devise a strong baseline model, Reasoner (causal-and-cascaded reasoning
Transformer). First, to capture the causal structure of the observations, a
contextualized directional position embedding strategy is adopted in the
encoder, that yields discriminative representations for the premise and
hypothesis. Then, multiple decoders are cascaded to generate and progressively
refine the premise and hypothesis sentences. The prediction scores of the
sentences are used to guide cross-sentence information flow in the cascaded
reasoning procedure. Our VAR benchmarking results show that Reasoner surpasses
many famous video-language models, while still being far behind human
performance. This work is expected to foster future efforts in the
reasoning-beyond-observation paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Segmentation by Early Region Proxy. (arXiv:2203.14043v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14043">
<div class="article-summary-box-inner">
<span><p>Typical vision backbones manipulate structured features. As a compromise,
semantic segmentation has long been modeled as per-point prediction on dense
regular grids. In this work, we present a novel and efficient modeling that
starts from interpreting the image as a tessellation of learnable regions, each
of which has flexible geometrics and carries homogeneous semantics. To model
region-wise context, we exploit Transformer to encode regions in a
sequence-to-sequence manner by applying multi-layer self-attention on the
region embeddings, which serve as proxies of specific regions. Semantic
segmentation is now carried out as per-region prediction on top of the encoded
region embeddings using a single linear classifier, where a decoder is no
longer needed. The proposed RegProxy model discards the common Cartesian
feature layout and operates purely at region level. Hence, it exhibits the most
competitive performance-efficiency trade-off compared with the conventional
dense prediction methods. For example, on ADE20K, the small-sized RegProxy-S/16
outperforms the best CNN model using 25% parameters and 4% computation, while
the largest RegProxy-L/16 achieves 52.9mIoU which outperforms the
state-of-the-art by 2.1% with fewer resources. Codes and models are available
at https://github.com/YiF-Zhang/RegionProxy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptively Lighting up Facial Expression Crucial Regions via Local Non-Local Joint Network. (arXiv:2203.14045v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14045">
<div class="article-summary-box-inner">
<span><p>Facial expression recognition (FER) is still one challenging research due to
the small inter-class discrepancy in the facial expression data. In view of the
significance of facial crucial regions for FER, many existing researches
utilize the prior information from some annotated crucial points to improve the
performance of FER. However, it is complicated and time-consuming to manually
annotate facial crucial points, especially for vast wild expression images.
Based on this, a local non-local joint network is proposed to adaptively light
up the facial crucial regions in feature learning of FER in this paper. In the
proposed method, two parts are constructed based on facial local and non-local
information respectively, where an ensemble of multiple local networks are
proposed to extract local features corresponding to multiple facial local
regions and a non-local attention network is addressed to explore the
significance of each local region. Especially, the attention weights obtained
by the non-local network is fed into the local part to achieve the interactive
feedback between the facial global and local information. Interestingly, the
non-local weights corresponding to local regions are gradually updated and
higher weights are given to more crucial regions. Moreover, U-Net is employed
to extract the integrated features of deep semantic information and low
hierarchical detail information of expression images. Finally, experimental
results illustrate that the proposed method achieves more competitive
performance compared with several state-of-the art methods on five benchmark
datasets. Noticeably, the analyses of the non-local weights corresponding to
local regions demonstrate that the proposed method can automatically enhance
some crucial regions in the process of feature learning without any facial
landmark information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and Methodologies. (arXiv:2203.14046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14046">
<div class="article-summary-box-inner">
<span><p>In the last a few decades, deep neural networks have achieved remarkable
success in machine learning, computer vision, and pattern recognition. Recent
studies however show that neural networks (both shallow and deep) may be easily
fooled by certain imperceptibly perturbed input samples called adversarial
examples. Such security vulnerability has resulted in a large body of research
in recent years because real-world threats could be introduced due to vast
applications of neural networks. To address the robustness issue to adversarial
examples particularly in pattern recognition, robust adversarial training has
become one mainstream. Various ideas, methods, and applications have boomed in
the field. Yet, a deep understanding of adversarial training including
characteristics, interpretations, theories, and connections among different
models has still remained elusive. In this paper, we present a comprehensive
survey trying to offer a systematic and structured investigation on robust
adversarial training in pattern recognition. We start with fundamentals
including definition, notations, and properties of adversarial examples. We
then introduce a unified theoretical framework for defending against
adversarial samples - robust adversarial training with visualizations and
interpretations on why adversarial training can lead to model robustness.
Connections will be also established between adversarial training and other
traditional learning theories. After that, we summarize, review, and discuss
various methodologies with adversarial attack and defense/training algorithms
in a structured way. Finally, we present analysis, outlook, and remarks of
adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Deep Implicit Functions for 3D Shapes with Dynamic Code Clouds. (arXiv:2203.14048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14048">
<div class="article-summary-box-inner">
<span><p>Deep Implicit Function (DIF) has gained popularity as an efficient 3D shape
representation. To capture geometry details, current methods usually learn DIF
using local latent codes, which discretize the space into a regular 3D grid (or
octree) and store local codes in grid points (or octree nodes). Given a query
point, the local feature is computed by interpolating its neighboring local
codes with their positions. However, the local codes are constrained at
discrete and regular positions like grid points, which makes the code positions
difficult to be optimized and limits their representation ability. To solve
this problem, we propose to learn DIF with Dynamic Code Cloud, named DCC-DIF.
Our method explicitly associates local codes with learnable position vectors,
and the position vectors are continuous and can be dynamically optimized, which
improves the representation ability. In addition, we propose a novel code
position loss to optimize the code positions, which heuristically guides more
local codes to be distributed around complex geometric details. In contrast to
previous methods, our DCC-DIF represents 3D shapes more efficiently with a
small amount of local codes, and improves the reconstruction quality.
Experiments demonstrate that DCC-DIF achieves better performance over previous
methods. Code and data are available at https://github.com/lity20/DCCDIF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceVerse: a Fine-grained and Detail-changeable 3D Neural Face Model from a Hybrid Dataset. (arXiv:2203.14057v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14057">
<div class="article-summary-box-inner">
<span><p>We present FaceVerse, a fine-grained 3D Neural Face Model, which is built
from hybrid East Asian face datasets containing 60K fused RGB-D images and 2K
high-fidelity 3D head scan models. A novel coarse-to-fine structure is proposed
to take better advantage of our hybrid dataset. In the coarse module, we
generate a base parametric model from large-scale RGB-D images, which is able
to predict accurate rough 3D face models in different genders, ages, etc. Then
in the fine module, a conditional StyleGAN architecture trained with
high-fidelity scan models is introduced to enrich elaborate facial geometric
and texture details. Note that different from previous methods, our base and
detailed modules are both changeable, which enables an innovative application
of adjusting both the basic attributes and the facial details of 3D face
models. Furthermore, we propose a single-image fitting framework based on
differentiable rendering. Rich experiments show that our method outperforms the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture. (arXiv:2203.14065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14065">
<div class="article-summary-box-inner">
<span><p>Due to the visual ambiguity, purely kinematic formulations on monocular human
motion capture are often physically incorrect, biomechanically implausible, and
can not reconstruct accurate interactions. In this work, we focus on exploiting
the high-precision and non-differentiable physics simulator to incorporate
dynamical constraints in motion capture. Our key-idea is to use real physical
supervisions to train a target pose distribution prior for sampling-based
motion control to capture physically plausible human motion. To obtain accurate
reference motion with terrain interactions for the sampling, we first introduce
an interaction constraint based on SDF (Signed Distance Field) to enforce
appropriate ground contact modeling. We then design a novel two-branch decoder
to avoid stochastic error from pseudo ground-truth and train a distribution
prior with the non-differentiable physics simulator. Finally, we regress the
sampling distribution from the current state of the physical character with the
trained prior and sample satisfied target poses to track the estimated
reference motion. Qualitative and quantitative results show that we can obtain
physically plausible human motion with complex terrain interactions, human
shape variations, and diverse behaviors. More information can be found
at~\url{https://www.yangangwang.com/papers/HBZ-NM-2022-03.html}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Answer Questions in Dynamic Audio-Visual Scenarios. (arXiv:2203.14072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14072">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on the Audio-Visual Question Answering (AVQA) task,
which aims to answer questions regarding different visual objects, sounds, and
their associations in videos. The problem requires comprehensive multimodal
understanding and spatio-temporal reasoning over audio-visual scenes. To
benchmark this task and facilitate our study, we introduce a large-scale
MUSIC-AVQA dataset, which contains more than 45K question-answer pairs covering
33 different question templates spanning over different modalities and question
types. We develop several baselines and introduce a spatio-temporal grounded
audio-visual network for the AVQA problem. Our results demonstrate that AVQA
benefits from multisensory perception and our model outperforms recent A-, V-,
and AVQA approaches. We believe that our built dataset has the potential to
serve as testbed for evaluating and promoting progress in audio-visual scene
understanding and spatio-temporal reasoning. Code and dataset:
<a href="http://gewu-lab.github.io/MUSIC-AVQA/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V3GAN: Decomposing Background, Foreground and Motion for Video Generation. (arXiv:2203.14074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14074">
<div class="article-summary-box-inner">
<span><p>Video generation is a challenging task that requires modeling plausible
spatial and temporal dynamics in a video. Inspired by how humans perceive a
video by grouping a scene into moving and stationary components, we propose a
method that decomposes the task of video generation into the synthesis of
foreground, background and motion. Foreground and background together describe
the appearance, whereas motion specifies how the foreground moves in a video
over time. We propose V3GAN, a novel three-branch generative adversarial
network where two branches model foreground and background information, while
the third branch models the temporal information without any supervision. The
foreground branch is augmented with our novel feature-level masking layer that
aids in learning an accurate mask for foreground and background separation. To
encourage motion consistency, we further propose a shuffling loss for the video
discriminator. Extensive quantitative and qualitative analysis on synthetic as
well as real-world benchmark datasets demonstrates that V3GAN outperforms the
state-of-the-art methods by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Point Cloud Representation Learning with Occlusion Auto-Encoder. (arXiv:2203.14084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14084">
<div class="article-summary-box-inner">
<span><p>Learning representations for point clouds is an important task in 3D computer
vision, especially without manually annotated supervision. Previous methods
usually take the common aid from auto-encoders to establish the
self-supervision by reconstructing the input itself. However, the existing
self-reconstruction based auto-encoders merely focus on the global shapes, and
ignore the hierarchical context between the local and global geometries, which
is a crucial supervision for 3D representation learning. To resolve this issue,
we present a novel self-supervised point cloud representation learning
framework, named 3D Occlusion Auto-Encoder (3D-OAE). Our key idea is to
randomly occlude some local patches of the input point cloud and establish the
supervision via recovering the occluded patches using the remaining visible
ones. Specifically, we design an encoder for learning the features of visible
local patches, and a decoder for leveraging these features to predict the
occluded patches. In contrast with previous methods, our 3D-OAE can remove a
large proportion of patches and predict them only with a small number of
visible patches, which enable us to significantly accelerate training and yield
a nontrivial self-supervisory performance. The trained encoder can be further
transferred to various downstream tasks. We demonstrate our superior
performances over the state-of-the-art methods in different discriminant and
generative applications under widely used benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Near-Infrared Depth-Independent Image Dehazing using Haar Wavelets. (arXiv:2203.14085v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14085">
<div class="article-summary-box-inner">
<span><p>We propose a fusion algorithm for haze removal that combines color
information from an RGB image and edge information extracted from its
corresponding NIR image using Haar wavelets. The proposed algorithm is based on
the key observation that NIR edge features are more prominent in the hazy
regions of the image than the RGB edge features in those same regions. To
combine the color and edge information, we introduce a haze-weight map which
proportionately distributes the color and edge information during the fusion
process. Because NIR images are, intrinsically, nearly haze-free, our work
makes no assumptions like existing works that rely on a scattering model and
essentially designing a depth-independent method. This helps in minimizing
artifacts and gives a more realistic sense to the restored haze-free image.
Extensive experiments show that the proposed algorithm is both qualitatively
and quantitatively better on several key metrics when compared to existing
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition. (arXiv:2203.14092v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14092">
<div class="article-summary-box-inner">
<span><p>The physical and textural attributes of objects have been widely studied for
recognition, detection and segmentation tasks in computer vision. A number of
datasets, such as large scale ImageNet, have been proposed for feature learning
using data hungry deep neural networks and for hand-crafted feature extraction.
To intelligently interact with objects, robots and intelligent machines need
the ability to infer beyond the traditional physical/textural attributes, and
understand/learn visual cues, called visual affordances, for affordance
recognition, detection and segmentation. To date there is no publicly available
large dataset for visual affordance understanding and learning. In this paper,
we introduce a large scale multi-view RGBD visual affordance learning dataset,
a benchmark of 47210 RGBD images from 37 object categories, annotated with 15
visual affordance categories and 35 cluttered/complex scenes with different
objects and multiple affordances. To the best of our knowledge, this is the
first ever and the largest multi-view RGBD visual affordance learning dataset.
We benchmark the proposed dataset for affordance recognition and segmentation.
To achieve this we propose an Affordance Recognition Network a.k.a ARNet. In
addition, four state-of-the-art deep learning networks are evaluated for
affordance segmentation task. Our experimental results showcase the challenging
nature of the dataset and present definite prospects for new and robust
affordance learning algorithms. The dataset is available at:
https://sites.google.com/view/afaqshah/dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation. (arXiv:2203.14098v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14098">
<div class="article-summary-box-inner">
<span><p>A fundamental and challenging problem in deep learning is catastrophic
forgetting, i.e. the tendency of neural networks to fail to preserve the
knowledge acquired from old tasks when learning new tasks. This problem has
been widely investigated in the research community and several Incremental
Learning (IL) approaches have been proposed in the past years. While earlier
works in computer vision have mostly focused on image classification and object
detection, more recently some IL approaches for semantic segmentation have been
introduced. These previous works showed that, despite its simplicity, knowledge
distillation can be effectively employed to alleviate catastrophic forgetting.
In this paper, we follow this research direction and, inspired by recent
literature on contrastive learning, we propose a novel distillation framework,
Uncertainty-aware Contrastive Distillation (\method). In a nutshell, \method~is
operated by introducing a novel distillation loss that takes into account all
the images in a mini-batch, enforcing similarity between features associated to
all the pixels from the same classes, and pulling apart those corresponding to
pixels from different classes. In order to mitigate catastrophic forgetting, we
contrast features of the new model with features extracted by a frozen model
learned at the previous incremental step. Our experimental results demonstrate
the advantage of the proposed distillation technique, which can be used in
synergy with previous IL approaches, and leads to state-of-art performance on
three commonly adopted benchmarks for incremental semantic segmentation. The
code is available at \url{https://github.com/ygjwd12345/UCD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos. (arXiv:2203.14104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14104">
<div class="article-summary-box-inner">
<span><p>Action recognition models have shown a promising capability to classify human
actions in short video clips. In a real scenario, multiple correlated human
actions commonly occur in particular orders, forming semantically meaningful
human activities. Conventional action recognition approaches focus on analyzing
single actions. However, they fail to fully reason about the contextual
relations between adjacent actions, which provide potential temporal logic for
understanding long videos. In this paper, we propose a prompt-based framework,
Bridge-Prompt (Br-Prompt), to model the semantics across adjacent actions, so
that it simultaneously exploits both out-of-context and contextual information
from a series of ordinal actions in instructional videos. More specifically, we
reformulate the individual action labels as integrated text prompts for
supervision, which bridge the gap between individual action semantics. The
generated text prompts are paired with corresponding video clips, and together
co-train the text encoder and the video encoder via a contrastive approach. The
learned vision encoder has a stronger capability for ordinal-action-related
downstream tasks, e.g. action segmentation and human activity recognition. We
evaluate the performances of our approach on several video datasets: Georgia
Tech Egocentric Activities (GTEA), 50Salads, and the Breakfast dataset.
Br-Prompt achieves state-of-the-art on multiple benchmarks. Code is available
at https://github.com/ttlmh/Bridge-Prompt
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Registration for Gaussian Process 3D shape modelling in the presence of extensive missing data. (arXiv:2203.14113v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14113">
<div class="article-summary-box-inner">
<span><p>Gaussian Processes are a powerful tool for shape modelling. While the
existing methods on this area prove to work well for the general case of the
human head, when looking at more detailed and deformed data, with a high
prevalence of missing data, such as the ears, the results are not satisfactory.
In order to overcome this, we formulate the shape fitting problem as a
multi-annotator Gaussian Process Regression and establish a parallel with the
standard probabilistic registration. The achieved method GPReg shows better
performance when dealing with extensive areas of missing data when compared to
a state-of-the-art registration method and the current approach for
registration with GP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Selective Transformer for Semantic Image Segmentation. (arXiv:2203.14124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14124">
<div class="article-summary-box-inner">
<span><p>Recently, it has attracted more and more attentions to fuse multi-scale
features for semantic image segmentation. Various works were proposed to employ
progressive local or global fusion, but the feature fusions are not rich enough
for modeling multi-scale context features. In this work, we focus on fusing
multi-scale features from Transformer-based backbones for semantic
segmentation, and propose a Feature Selective Transformer (FeSeFormer), which
aggregates features from all scales (or levels) for each query feature.
Specifically, we first propose a Scale-level Feature Selection (SFS) module,
which can choose an informative subset from the whole multi-scale feature set
for each scale, where those features that are important for the current scale
(or level) are selected and the redundant are discarded. Furthermore, we
propose a Full-scale Feature Fusion (FFF) module, which can adaptively fuse
features of all scales for queries. Based on the proposed SFS and FFF modules,
we develop a Feature Selective Transformer (FeSeFormer), and evaluate our
FeSeFormer on four challenging semantic segmentation benchmarks, including
PASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14128">
<div class="article-summary-box-inner">
<span><p>In the last two years, millions of lives have been lost due to COVID-19.
Despite the vaccination programmes for a year, hospitalization rates and deaths
are still high due to the new variants of COVID-19. Stringent guidelines and
COVID-19 screening measures such as temperature check and mask check at all
public places are helping reduce the spread of COVID-19. Visual inspections to
ensure these screening measures can be taxing and erroneous. Automated
inspection ensures an effective and accurate screening. Traditional approaches
involve identification of faces and masks from visual camera images followed by
extraction of temperature values from thermal imaging cameras. Use of visual
imaging as a primary modality limits these applications only for good-lighting
conditions. The use of thermal imaging alone for these screening measures makes
the system invariant to illumination. However, lack of open source datasets is
an issue to develop such systems. In this paper, we discuss our work on using
machine learning over thermal video streams for face and mask detection and
subsequent temperature screening in a passive non-invasive way that enables an
effective automated COVID-19 screening method in public places. We open source
our NTIC dataset that was used for training our models and was collected at 8
different locations. Our results show that the use of thermal imaging is as
effective as visual imaging in the presence of high illumination. This
performance stays the same for thermal images even under low-lighting
conditions, whereas the performance with visual trained classifiers show more
than 50% degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGBD Object Tracking: An In-depth Review. (arXiv:2203.14134v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14134">
<div class="article-summary-box-inner">
<span><p>RGBD object tracking is gaining momentum in computer vision research thanks
to the development of depth sensors. Although numerous RGBD trackers have been
proposed with promising performance, an in-depth review for comprehensive
understanding of this area is lacking. In this paper, we firstly review RGBD
object trackers from different perspectives, including RGBD fusion, depth
usage, and tracking framework. Then, we summarize the existing datasets and the
evaluation metrics. We benchmark a representative set of RGBD trackers, and
give detailed analyses based on their performances. Particularly, we are the
first to provide depth quality evaluation and analysis of tracking results in
depth-friendly scenarios in RGBD tracking. For long-term settings in most RGBD
tracking videos, we give an analysis of trackers' performance on handling
target disappearance. To enable better understanding of RGBD trackers, we
propose robustness evaluation against input perturbations. Finally, we
summarize the challenges and provide open directions for this community. All
resources are publicly available at
https://github.com/memoryunreal/RGBD-tracking-review.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reverse Engineering of Imperceptible Adversarial Image Perturbations. (arXiv:2203.14145v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14145">
<div class="article-summary-box-inner">
<span><p>It has been well recognized that neural network based image classifiers are
easily fooled by images with tiny perturbations crafted by an adversary. There
has been a vast volume of research to generate and defend such adversarial
attacks. However, the following problem is left unexplored: How to
reverse-engineer adversarial perturbations from an adversarial image? This
leads to a new adversarial learning paradigm--Reverse Engineering of Deceptions
(RED). If successful, RED allows us to estimate adversarial perturbations and
recover the original images. However, carefully crafted, tiny adversarial
perturbations are difficult to recover by optimizing a unilateral RED
objective. For example, the pure image denoising method may overfit to
minimizing the reconstruction error but hardly preserve the classification
properties of the true adversarial perturbations. To tackle this challenge, we
formalize the RED problem and identify a set of principles crucial to the RED
approach design. Particularly, we find that prediction alignment and proper
data augmentation (in terms of spatial transformations) are two criteria to
achieve a generalizable RED approach. By integrating these RED principles with
image denoising, we propose a new Class-Discriminative Denoising based RED
framework, termed CDD-RED. Extensive experiments demonstrate the effectiveness
of CDD-RED under different evaluation metrics (ranging from the pixel-level,
prediction-level to the attribution-level alignment) and a variety of attack
generation methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate 3-DoF Camera Geo-Localization via Ground-to-Satellite Image Matching. (arXiv:2203.14148v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14148">
<div class="article-summary-box-inner">
<span><p>We address the problem of ground-to-satellite image geo-localization, that
is, estimating the camera latitude, longitude and orientation (azimuth angle)
by matching a query image captured at the ground level against a large-scale
database with geotagged satellite images. Our prior arts treat the above task
as pure image retrieval by selecting the most similar satellite reference image
matching the ground-level query image. However, such an approach often produces
coarse location estimates because the geotag of the retrieved satellite image
only corresponds to the image center while the ground camera can be located at
any point within the image. To further consolidate our prior research findings,
we present a novel geometry-aware geo-localization method. Our new method is
able to achieve the fine-grained location of a query image, up to pixel size
precision of the satellite image, once its coarse location and orientation have
been determined. Moreover, we propose a new geometry-aware image retrieval
pipeline to improve the coarse localization accuracy. Apart from a polar
transform in our conference work, this new pipeline also maps satellite image
pixels to the ground-level plane in the ground-view via a geometry-constrained
projective transform to emphasize informative regions, such as road structures,
for cross-view geo-localization. Extensive quantitative and qualitative
experiments demonstrate the effectiveness of our newly proposed framework. We
also significantly improve the performance of coarse localization results
compared to the state-of-the-art in terms of location recalls.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Optimization Method for Label Noisy Datasets Based on Adaptive Threshold: Adaptive-k. (arXiv:2203.14165v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14165">
<div class="article-summary-box-inner">
<span><p>SGD does not produce robust results on datasets with label noise. Because the
gradients calculated according to the losses of the noisy samples cause the
optimization process to go in the wrong direction. In this paper, as an
alternative to SGD, we recommend using samples with loss less than a threshold
value determined during the optimization process, instead of using all samples
in the mini-batch. Our proposed method, Adaptive-k, aims to exclude label noise
samples from the optimization process and make the process robust. On noisy
datasets, we found that using a threshold-based approach, such as Adaptive-k,
produces better results than using all samples or a fixed number of low-loss
samples in the mini-batch. Based on our theoretical analysis and experimental
results, we show that the Adaptive-k method is closest to the performance of
the oracle, in which noisy samples are entirely removed from the dataset.
Adaptive-k is a simple but effective method. It does not require prior
knowledge of the noise ratio of the dataset, does not require additional model
training, and does not increase training time significantly. The code for
Adaptive-k is available at https://github.com/enesdedeoglu-TR/Adaptive-k
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ThunderNet: Towards Real-time Generic Object Detection. (arXiv:1903.11752v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1903.11752">
<div class="article-summary-box-inner">
<span><p>Real-time generic object detection on mobile platforms is a crucial but
challenging computer vision task. However, previous CNN-based detectors suffer
from enormous computational cost, which hinders them from real-time inference
in computation-constrained scenarios. In this paper, we investigate the
effectiveness of two-stage detectors in real-time generic detection and propose
a lightweight two-stage detector named ThunderNet. In the backbone part, we
analyze the drawbacks in previous lightweight backbones and present a
lightweight backbone designed for object detection. In the detection part, we
exploit an extremely efficient RPN and detection head design. To generate more
discriminative feature representation, we design two efficient architecture
blocks, Context Enhancement Module and Spatial Attention Module. At last, we
investigate the balance between the input resolution, the backbone, and the
detection head. Compared with lightweight one-stage detectors, ThunderNet
achieves superior performance with only 40% of the computational cost on PASCAL
VOC and COCO benchmarks. Without bells and whistles, our model runs at 24.1 fps
on an ARM-based device. To the best of our knowledge, this is the first
real-time detector reported on ARM platforms. Our code and models are available
at \url{https://github.com/qinzheng93/ThunderNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Low-light Image Enhancement with Decoupled Networks. (arXiv:2005.02818v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02818">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the problem of enhancing real-world low-light images
with significant noise in an unsupervised fashion. Conventional unsupervised
learning-based approaches usually tackle the low-light image enhancement
problem using an image-to-image translation model. They focus primarily on
illumination or contrast enhancement but fail to suppress the noise that
ubiquitously exists in images taken under real-world low-light conditions. To
address this issue, we explicitly decouple this task into two sub-tasks:
illumination enhancement and noise suppression. We propose to learn a two-stage
GAN-based framework to enhance the real-world low-light images in a fully
unsupervised fashion. To facilitate the unsupervised training of our model, we
construct samples with pseudo labels. Furthermore, we propose an adaptive
content loss to suppress real image noise in different regions based on
illumination intensity. In addition to conventional benchmark datasets, a new
unpaired low-light image enhancement dataset is built and used to thoroughly
evaluate the performance of our model. Extensive experiments show that our
proposed method outperforms the state-of-the-art unsupervised image enhancement
methods in terms of both illumination enhancement and noise reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection. (arXiv:2008.06254v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.06254">
<div class="article-summary-box-inner">
<span><p>We consider the problem of Human-Object Interaction (HOI) Detection, which
aims to locate and recognize HOI instances in the form of &lt;human, action,
object&gt; in images. Most existing works treat HOIs as individual interaction
categories, thus can not handle the problem of long-tail distribution and
polysemy of action labels. We argue that multi-level consistencies among
objects, actions and interactions are strong cues for generating semantic
representations of rare or previously unseen HOIs. Leveraging the compositional
and relational peculiarities of HOI labels, we propose ConsNet, a
knowledge-aware framework that explicitly encodes the relations among objects,
actions and interactions into an undirected graph called consistency graph, and
exploits Graph Attention Networks (GATs) to propagate knowledge among HOI
categories as well as their constituents. Our model takes visual features of
candidate human-object pairs and word embeddings of HOI labels as inputs, maps
them into visual-semantic joint embedding space and obtains detection results
by measuring their similarities. We extensively evaluate our model on the
challenging V-COCO and HICO-DET datasets, and results validate that our
approach outperforms state-of-the-arts under both fully-supervised and
zero-shot settings. Code is available at https://github.com/yeliudev/ConsNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Compress Videos without Computing Motion. (arXiv:2009.14110v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14110">
<div class="article-summary-box-inner">
<span><p>With the development of higher resolution contents and displays, its
significant volume poses significant challenges to the goals of acquiring,
transmitting, compressing, and displaying high-quality video content. In this
paper, we propose a new deep learning video compression architecture that does
not require motion estimation, which is the most expensive element of modern
hybrid video compression codecs like H.264 and HEVC. Our framework exploits the
regularities inherent to video motion, which we capture by using displaced
frame differences as video representations to train the neural network. In
addition, we propose a new space-time reconstruction network based on both an
LSTM model and a UNet model, which we call LSTM-UNet. The new video compression
framework has three components: a Displacement Calculation Unit (DCU), a
Displacement Compression Network (DCN), and a Frame Reconstruction Network
(FRN). The DCU removes the need for motion estimation found in hybrid codecs
and is less expensive. In the DCN, an RNN-based network is utilized to compress
displaced frame differences as well as retain temporal information between
frames. The LSTM-UNet is used in the FRN to learn space-time differential
representations of videos. Our experimental results show that our compression
model, which we call the MOtionless VIdeo Codec (MOVI-Codec), learns how to
efficiently compress videos without computing motion. Our experiments show that
MOVI-Codec outperforms the Low-Delay P veryfast setting of the video coding
standard H.264 and exceeds the performance of the modern global standard HEVC
codec, using the same setting, as measured by MS-SSIM, especially on higher
resolution videos. In addition, our network outperforms the latest H.266 (VVC)
codec at higher bitrates, when assessed using MS-SSIM, on high-resolution
videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Representations of Positive Functions via First and Second-Order Pseudo-Mirror Descent. (arXiv:2011.07142v3 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07142">
<div class="article-summary-box-inner">
<span><p>We consider expected risk minimization problems when the range of the
estimator is required to be nonnegative, motivated by the settings of maximum
likelihood estimation (MLE) and trajectory optimization. To facilitate
nonlinear interpolation, we hypothesize that the search space is a Reproducing
Kernel Hilbert Space (RKHS). We develop first and second-order variants of
stochastic mirror descent employing (i) \emph{pseudo-gradients} and (ii)
complexity-reducing projections. Compressive projection in the first-order
scheme is executed via kernel orthogonal matching pursuit (KOMP), which
overcomes the fact that the vanilla RKHS parameterization grows unbounded with
the iteration index in the stochastic setting. Moreover, pseudo-gradients are
needed when gradient estimates for cost are only computable up to some
numerical error, which arise in, e.g., integral approximations. Under constant
step-size and compression budget, we establish tradeoffs between the radius of
convergence of the expected sub-optimality and the projection budget parameter,
as well as non-asymptotic bounds on the model complexity. To refine the
solution's precision, we develop a second-order extension which employs
recursively averaged pseudo-gradient outer-products to approximate the Hessian
inverse, whose convergence in mean is established under an additional
eigenvalue decay condition on the Hessian of the optimal RKHS element, which is
unique to this work. Experiments demonstrate favorable performance on
inhomogeneous Poisson Process intensity estimation in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Segmentation for Terracotta Warrior Point Cloud (SRG-Net). (arXiv:2012.00433v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00433">
<div class="article-summary-box-inner">
<span><p>The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum
Site Museum is handcrafted by experts, and the increasing amounts of unearthed
pieces of terracotta warriors make the archaeologists too challenging to
conduct the restoration of terracotta warriors efficiently. We hope to segment
the 3D point cloud data of the terracotta warriors automatically and store the
fragment data in the database to assist the archaeologists in matching the
actual fragments with the ones in the database, which could result in higher
repairing efficiency of terracotta warriors. Moreover, the existing 3D neural
network research is mainly focusing on supervised classification, clustering,
unsupervised representation, and reconstruction. There are few pieces of
researches concentrating on unsupervised point cloud part segmentation. In this
paper, we present SRG-Net for 3D point clouds of terracotta warriors to address
these problems. Firstly, we adopt a customized seed-region-growing algorithm to
segment the point cloud coarsely. Then we present a supervised segmentation and
unsupervised reconstruction networks to learn the characteristics of 3D point
clouds. Finally, we combine the SRG algorithm with our improved CNN(convolution
neural network) using a refinement method. This pipeline is called SRG-Net,
which aims at conducting segmentation tasks on the terracotta warriors. Our
proposed SRG-Net is evaluated on the terracotta warrior data and ShapeNet
dataset by measuring the accuracy and the latency. The experimental results
show that our SRG-Net outperforms the state-of-the-art methods. Our code is
available at https://github.com/hyoau/SRG-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01699">
<div class="article-summary-box-inner">
<span><p>Preprocessing defenses such as pixel discretization are appealing to remove
adversarial attacks due to their simplicity. However, they have been shown to
be ineffective except on simple datasets such as MNIST. We hypothesize that
existing discretization approaches failed because using a fixed codebook for
the entire dataset limits their ability to balance image representation and
codeword separability. We propose a per-image adaptive preprocessing defense
called Essential Features, which first applies adaptive blurring to push
perturbed pixel values back to their original value and then discretizes the
image to an image-adaptive codebook to reduce the color space. Essential
Features thus constrains the attack space by forcing the adversary to perturb
large regions both locally and color-wise for its effects to survive the
preprocessing. Against adaptive attacks, we find that our approach increases
the $L_2$ and $L_\infty$ robustness on higher resolution datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are DNNs fooled by extremely unrecognizable images?. (arXiv:2012.03843v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03843">
<div class="article-summary-box-inner">
<span><p>Fooling images are a potential threat to deep neural networks (DNNs). These
images are not recognizable to humans as natural objects, such as dogs and
cats, but are misclassified by DNNs as natural-object classes with high
confidence scores. Despite their original design concept, existing fooling
images retain some features that are characteristic of the target objects if
looked into closely. Hence, DNNs can react to these features. In this paper, we
address the question of whether there can be fooling images with no
characteristic pattern of natural objects locally or globally. As a minimal
case, we introduce single-color images with a few pixels altered, called sparse
fooling images (SFIs). We first prove that SFIs always exist under mild
conditions for linear and nonlinear models and reveal that complex models are
more likely to be vulnerable to SFI attacks. With two SFI generation methods,
we demonstrate that in deeper layers, SFIs end up with similar features to
those of natural images, and consequently, fool DNNs successfully. Among other
layers, we discovered that the max pooling layer causes the vulnerability
against SFIs. The defense against SFIs and transferability are also discussed.
This study highlights the new vulnerability of DNNs by introducing a novel
class of images that distributes extremely far from natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04631">
<div class="article-summary-box-inner">
<span><p>Machine translation between many languages at once is highly challenging,
since training with ground truth requires supervision between all language
pairs, which is difficult to obtain. Our key insight is that, while languages
may vary drastically, the underlying visual appearance of the world remains
consistent. We introduce a method that uses visual observations to bridge the
gap between languages, rather than relying on parallel corpora or topological
properties of the representations. We train a model that aligns segments of
text from different languages if and only if the images associated with them
are similar and each image in turn is well-aligned with its textual
description. We train our model from scratch on a new dataset of text in over
fifty languages with accompanying images. Experiments show that our method
outperforms previous work on unsupervised word and sentence translation using
retrieval. Code, models and data are available on globetrotter.cs.columbia.edu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Handheld to Unconstrained Object Detection: a Weakly-supervised On-line Learning Approach. (arXiv:2012.14345v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14345">
<div class="article-summary-box-inner">
<span><p>Deep Learning (DL) based methods for object detection achieve remarkable
performance at the cost of computationally expensive training and extensive
data labeling. Robots embodiment can be exploited to mitigate this burden by
acquiring automatically annotated training data via a natural interaction with
a human showing the object of interest, handheld. However, learning solely from
this data may introduce biases (the so-called domain shift), and prevents
adaptation to novel tasks. While Weakly-supervised Learning (WSL) offers a
well-established set of techniques to cope with these problems in
general-purpose Computer Vision, its adoption in challenging robotic domains is
still at a preliminary stage. In this work, we target the scenario of a robot
trained in a teacher-learner setting to detect handheld objects. The aim is to
improve detection performance in different settings by letting the robot
explore the environment with a limited human labeling budget. We compare
several techniques for WSL in detection pipelines to reduce model re-training
costs without compromising accuracy, proposing solutions which target the
considered robotic scenario. We show that the robot can improve adaptation to
novel domains, either by interacting with a human teacher (Active Learning) or
with an autonomous supervision (Semi-supervised Learning). We integrate our
strategies into an on-line detection method, achieving efficient model update
capabilities with few labels. We experimentally benchmark our method on
challenging robotic object detection tasks under domain shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05260">
<div class="article-summary-box-inner">
<span><p>In cases of serious crime, including sexual abuse, often the only available
information with demonstrated potential for identification is images of the
hands. Since this evidence is captured in uncontrolled situations, it is
difficult to analyse. As global approaches to feature comparison are limited in
this case, it is important to extend to consider local information. In this
work, we propose hand-based person identification by learning both global and
local deep feature representations. Our proposed method, Global and Part-Aware
Network (GPA-Net), creates global and local branches on the conv-layer for
learning robust discriminative global and part-level features. For learning the
local (part-level) features, we perform uniform partitioning on the conv-layer
in both horizontal and vertical directions. We retrieve the parts by conducting
a soft partition without explicitly partitioning the images or requiring
external cues such as pose estimation. We make extensive evaluations on two
large multi-ethnic and publicly available hand datasets, demonstrating that our
proposed method significantly outperforms competing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery. (arXiv:2103.01623v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01623">
<div class="article-summary-box-inner">
<span><p>This paper presents a new approach for synthesizing a novel street-view
panorama given an overhead satellite image. Taking a small satellite image
patch as input, our method generates a Google's omnidirectional street-view
type panorama, as if it is captured from the same geographical location as the
center of the satellite patch. Existing works tackle this task as an image
generation problem which adopts generative adversarial networks to implicitly
learn the cross-view transformations, while ignoring the domain relevance. In
this paper, we propose to explicitly establish the geometric correspondences
between the two-view images so as to facilitate the cross-view transformation
learning. Specifically, we observe that when a 3D point in the real world is
visible in both views, there is a deterministic mapping between the projected
points in the two-view images given the height information of this 3D point.
Motivated by this, we develop a novel Satellite to Street-view image Projection
(S2SP) module which explicitly establishes such geometric correspondences and
projects the satellite images to the street viewpoint. With these projected
satellite images as network input, we next employ a generator to synthesize
realistic street-view panoramas that are geometrically consistent with the
satellite images. Our S2SP module is differentiable and the whole framework is
trained in an end-to-end manner. Extensive experimental results on two
cross-view benchmark datasets demonstrate that our method generates images that
better respect the scene geometry than existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. (arXiv:2103.04922v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04922">
<div class="article-summary-box-inner">
<span><p>Deep generative models are a class of techniques that train deep neural
networks to model the distribution of training samples. Research has fragmented
into various interconnected approaches, each of which make trade-offs including
run-time, diversity, and architectural restrictions. In particular, this
compendium covers energy-based models, variational autoencoders, generative
adversarial networks, autoregressive models, normalizing flows, in addition to
numerous hybrid approaches. These techniques are compared and contrasted,
explaining the premises behind each and how they are interrelated, while
reviewing current state-of-the-art advances and implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stepwise Goal-Driven Networks for Trajectory Prediction. (arXiv:2103.14107v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14107">
<div class="article-summary-box-inner">
<span><p>We propose to predict the future trajectories of observed agents (e.g.,
pedestrians or vehicles) by estimating and using their goals at multiple time
scales. We argue that the goal of a moving agent may change over time, and
modeling goals continuously provides more accurate and detailed information for
future trajectory estimation. To this end, we present a recurrent network for
trajectory prediction, called Stepwise Goal-Driven Network (SGNet). Unlike
prior work that models only a single, long-term goal, SGNet estimates and uses
goals at multiple temporal scales. In particular, it incorporates an encoder
that captures historical information, a stepwise goal estimator that predicts
successive goals into the future, and a decoder that predicts future
trajectory. We evaluate our model on three first-person traffic datasets
(HEV-I, JAAD, and PIE) as well as on three bird's eye view datasets (NuScenes,
ETH, and UCY), and show that our model achieves state-of-the-art results on all
datasets. Code has been made available at:
https://github.com/ChuhuaW/SGNet.pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlignMixup: Improving Representations By Interpolating Aligned Features. (arXiv:2103.15375v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15375">
<div class="article-summary-box-inner">
<span><p>Mixup is a powerful data augmentation method that interpolates between two or
more examples in the input or feature space and between the corresponding
target labels. Many recent mixup methods focus on cutting and pasting two or
more objects into one image, which is more about efficient processing than
interpolation. However, how to best interpolate images is not well defined. In
this sense, mixup has been connected to autoencoders, because often
autoencoders "interpolate well", for instance generating an image that
continuously deforms into another.
</p>
<p>In this work, we revisit mixup from the interpolation perspective and
introduce AlignMix, where we geometrically align two images in the feature
space. The correspondences allow us to interpolate between two sets of
features, while keeping the locations of one set. Interestingly, this gives
rise to a situation where mixup retains mostly the geometry or pose of one
image and the texture of the other, connecting it to style transfer. More than
that, we show that an autoencoder can still improve representation learning
under mixup, without the classifier ever seeing decoded images. AlignMix
outperforms state-of-the-art mixup methods on five different benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation. (arXiv:2103.17235v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17235">
<div class="article-summary-box-inner">
<span><p>The increase of available large clinical and experimental datasets has
contributed to a substantial amount of important contributions in the area of
biomedical image analysis. Image segmentation, which is crucial for any
quantitative analysis, has especially attracted attention. Recent hardware
advancement has led to the success of deep learning approaches. However,
although deep learning models are being trained on large datasets, existing
methods do not use the information from different learning epochs effectively.
In this work, we leverage the information of each training epoch to prune the
prediction maps of the subsequent epochs. We propose a novel architecture
called feedback attention network (FANet) that unifies the previous epoch mask
with the feature map of the current training epoch. The previous epoch mask is
then used to provide a hard attention to the learned feature maps at different
convolutional layers. The network also allows to rectify the predictions in an
iterative fashion during the test time. We show that our proposed
\textit{feedback attention} model provides a substantial improvement on most
segmentation metrics tested on seven publicly available biomedical imaging
datasets demonstrating the effectiveness of FANet. The source code is available
at \url{https://github.com/nikhilroxtomar/FANet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes. (arXiv:2104.07300v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07300">
<div class="article-summary-box-inner">
<span><p>We consider the problem of recovering a single person's 3D human mesh from
in-the-wild crowded scenes. While much progress has been in 3D human mesh
estimation, existing methods struggle when test input has crowded scenes. The
first reason for the failure is a domain gap between training and testing data.
A motion capture dataset, which provides accurate 3D labels for training, lacks
crowd data and impedes a network from learning crowded scene-robust image
features of a target person. The second reason is a feature processing that
spatially averages the feature map of a localized bounding box containing
multiple people. Averaging the whole feature map makes a target person's
feature indistinguishable from others. We present 3DCrowdNet that firstly
explicitly targets in-the-wild crowded scenes and estimates a robust 3D human
mesh by addressing the above issues. First, we leverage 2D human pose
estimation that does not require a motion capture dataset with 3D labels for
training and does not suffer from the domain gap. Second, we propose a
joint-based regressor that distinguishes a target person's feature from others.
Our joint-based regressor preserves the spatial activation of a target by
sampling features from the target's joint locations and regresses human model
parameters. As a result, 3DCrowdNet learns target-focused features and
effectively excludes the irrelevant features of nearby persons. We conduct
experiments on various benchmarks and prove the robustness of 3DCrowdNet to the
in-the-wild crowded scenes both quantitatively and qualitatively. Codes are
available in https://github.com/hongsukchoi/3DCrowdNet_RELEASE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion. (arXiv:2104.07661v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07661">
<div class="article-summary-box-inner">
<span><p>This paper studies the problem of StyleGAN inversion, which plays an
essential role in enabling the pretrained StyleGAN to be used for real image
editing tasks. The goal of StyleGAN inversion is to find the exact latent code
of the given image in the latent space of StyleGAN. This problem has a high
demand for quality and efficiency. Existing optimization-based methods can
produce high-quality results, but the optimization often takes a long time. On
the contrary, forward-based methods are usually faster but the quality of their
results is inferior. In this paper, we present a new feed-forward network
"E2Style" for StyleGAN inversion, with significant improvement in terms of
efficiency and effectiveness. In our inversion network, we introduce: 1) a
shallower backbone with multiple efficient heads across scales; 2) multi-layer
identity loss and multi-layer face parsing loss to the loss function; and 3)
multi-stage refinement. Combining these designs together forms an effective and
efficient method that exploits all benefits of optimization-based and
forward-based methods. Quantitative and qualitative results show that our
E2Style performs better than existing forward-based methods and comparably to
state-of-the-art optimization-based methods while maintaining the high
efficiency as well as forward-based methods. Moreover, a number of real image
editing applications demonstrate the efficacy of our E2Style. Our code is
available at \url{https://github.com/wty-ustc/e2style}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09008">
<div class="article-summary-box-inner">
<span><p>In the paper of ExquisiteNetV1, the ability of classification of
ExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and
better model ExquisiteNetV2. We conduct many experiments to evaluate its
performance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known
models on 15 credible datasets under the same condition. According to the
experimental results, ExquisiteNetV2 gets the highest classification accuracy
over half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts
of parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing
speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smaller Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning. (arXiv:2105.14106v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14106">
<div class="article-summary-box-inner">
<span><p>The design of machines and algorithms capable of learning in a dynamically
changing environment has become an increasingly topical problem with the
increase of the size and heterogeneity of data available to learning systems.
As a consequence, the key issue of Continual Learning has become that of
addressing the stability-plasticity dilemma of connectionist systems, as they
need to adapt their model without forgetting previously acquired knowledge.
Within this context, rehearsal-based methods i.e., solutions in where the
learner exploits memory to revisit past data, has proven to be very effective,
leading to performance at the state-of-the-art. In our study, we propose an
analysis of the memory quantity/quality trade-off adopting various data
reduction approaches to increase the number of instances storable in memory. In
particular, we investigate complex instance compression techniques such as deep
encoders, but also trivial approaches such as image resizing and linear
dimensionality reduction. Our findings suggest that the optimal trade-off is
severely skewed toward instance quantity, where rehearsal approaches with
several heavily compressed instances easily outperform state-of-the-art
approaches with the same amount of memory at their disposal. Further, in high
memory configurations, deep approaches extracting spatial structure combined
with extreme resizing (of the order of $8\times8$ images) yield the best
results, while in memory-constrained configurations where deep approaches
cannot be used due to their memory requirement in training, Extreme Learning
Machines (ELM) offer a clear advantage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Multi-View Object Recognition and Grasping in Open-Ended Domains. (arXiv:2106.01866v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01866">
<div class="article-summary-box-inner">
<span><p>A robot working in human-centric environments needs to know which kind of
objects exist in the scene, where they are, and how to grasp and manipulate
various objects in different situations to help humans in everyday tasks.
Therefore, object recognition and grasping are two key functionalities for such
robots. Most state-of-the-art tackles object recognition and grasping as two
separate problems while both use visual input. Furthermore, the knowledge of
the robot is fixed after the training phase. In such cases, if the robot faces
new object categories, it must retrain from scratch to incorporate new
information without catastrophic interference. To address this problem, we
propose a deep learning architecture with augmented memory capacities to handle
open-ended object recognition and grasping simultaneously. In particular, our
approach takes multi-views of an object as input and jointly estimates
pixel-wise grasp configuration as well as a deep scale- and rotation-invariant
representation as outputs. The obtained representation is then used for
open-ended object recognition through a meta-active learning technique. We
demonstrate the ability of our approach to grasp never-seen-before objects and
to rapidly learn new object categories using very few examples on-site in both
simulation and real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Reconstruction and Rendering. (arXiv:2106.03798v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03798">
<div class="article-summary-box-inner">
<span><p>We introduce DoubleField, a novel framework combining the merits of both
surface field and radiance field for high-fidelity human reconstruction and
rendering. Within DoubleField, the surface field and radiance field are
associated together by a shared feature embedding and a surface-guided sampling
strategy. Moreover, a view-to-view transformer is introduced to fuse multi-view
features and learn view-dependent features directly from high-resolution
inputs. With the modeling power of DoubleField and the view-to-view
transformer, our method significantly improves the reconstruction quality of
both geometry and appearance, while supporting direct inference, scene-specific
high-resolution finetuning, and fast rendering. The efficacy of DoubleField is
validated by the quantitative evaluations on several datasets and the
qualitative results in a real-world sparse multi-view system, showing its
superior capability for high-quality human model reconstruction and
photo-realistic free-viewpoint human rendering. Data and source code will be
made public for the research purpose. Please refer to our project page:
<a href="http://www.liuyebin.com/dbfield/dbfield.html.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Sparse R-CNN for Direct Scene Graph Generation. (arXiv:2106.10815v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10815">
<div class="article-summary-box-inner">
<span><p>Scene graph generation (SGG) is to detect object pairs with their relations
in an image. Existing SGG approaches often use multi-stage pipelines to
decompose this task into object detection, relation graph construction, and
dense or dense-to-sparse relation prediction. Instead, from a perspective on
SGG as a direct set prediction, this paper presents a simple, sparse, and
unified framework, termed as Structured Sparse R-CNN. The key to our method is
a set of learnable triplet queries and a structured triplet detector which
could be jointly optimized from the training set in an end-to-end manner.
Specifically, the triplet queries encode the general prior for object pairs
with their relations, and provide an initial guess of scene graphs for
subsequent refinement. The triplet detector presents a cascaded architecture to
progressively refine the detected scene graphs with the customized dynamic
heads. In addition, to relieve the training difficulty of our method, we
propose a relaxed and enhanced training strategy based on knowledge
distillation from a Siamese Sparse R-CNN. We perform experiments on several
datasets: Visual Genome and Open Images V4/V6, and the results demonstrate that
our method achieves the state-of-the-art performance. In addition, we also
perform in-depth ablation studies to provide insights on our structured
modeling in triplet detector design and training strategies. The code and
models are made available at https://github.com/MCG-NJU/Structured-Sparse-RCNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12226">
<div class="article-summary-box-inner">
<span><p>Cloud removal is a relevant topic in Remote Sensing as it fosters the
usability of high-resolution optical images for Earth monitoring and study.
Related techniques have been analyzed for years with a progressively clearer
view of the appropriate methods to adopt, from multi-spectral to inpainting
methods. Recent applications of deep generative models and
sequence-to-sequence-based models have proved their capability to advance the
field significantly. Nevertheless, there are still some gaps, mostly related to
the amount of cloud coverage, the density and thickness of clouds, and the
occurred temporal landscape changes. In this work, we fill some of these gaps
by introducing a novel multi-modal method that uses different sources of
information, both spatial and temporal, to restore the whole optical scene of
interest. The proposed method introduces an innovative deep model, using the
outcomes of both temporal-sequence blending and direct translation from
Synthetic Aperture Radar (SAR) to optical images to obtain a pixel-wise
restoration of the whole scene. The advantage of our approach is demonstrated
across a variety of atmospheric conditions tested on a dataset we have
generated and made available. Quantitative and qualitative results prove that
the proposed method obtains cloud-free images, preserving scene details without
resorting to a huge portion of a clean image and coping with landscape changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation. (arXiv:2107.02718v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02718">
<div class="article-summary-box-inner">
<span><p>Hand segmentation is a crucial task in first-person vision. Since
first-person images exhibit strong bias in appearance among different
environments, adapting a pre-trained segmentation model to a new domain is
required in hand segmentation. Here, we focus on appearance gaps for hand
regions and backgrounds separately. We propose (i) foreground-aware image
stylization and (ii) consensus pseudo-labeling for domain adaptation of hand
segmentation. We stylize source images independently for the foreground and
background using target images as style. To resolve the domain shift that the
stylization has not addressed, we apply careful pseudo-labeling by taking a
consensus between the models trained on the source and stylized source images.
We validated our method on domain adaptation of hand segmentation from real and
simulation images. Our method achieved state-of-the-art performance in both
settings. We also demonstrated promising results in challenging multi-target
domain adaptation and domain generalization settings. Code is available at
https://github.com/ut-vision/FgSty-CPL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers. (arXiv:2107.11472v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11472">
<div class="article-summary-box-inner">
<span><p>Hyperbolic space can embed hierarchical structures continuously. Hyperbolic
Neural Networks (HNNs) exploit such representational power by lifting Euclidean
features into hyperbolic space for classification, outperforming Euclidean
neural networks (ENNs) on datasets with known hierarchical structures. However,
HNNs underperform ENNs on standard benchmarks with unclear hierarchies, greatly
restricting HNNs' practical applicability.
</p>
<p>Our key insight is that HNNs' poorer general classification performance
results from vanishing gradients during backpropagation, caused by their hybrid
architecture connecting Euclidean features to a hyperbolic classifier. We
propose an effective solution by simply clipping the Euclidean feature
magnitude while training HNNs.
</p>
<p>Our experimental results demonstrate that clipped HNNs become
super-hyperbolic classifiers: They are not only consistently better than HNNs
which already outperform ENNs on hierarchical data, but also on-par with ENNs
on MNIST, CIFAR10, CIFAR100 and ImageNet benchmarks, with better adversarial
robustness and out-of-distribution detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Rays for Occlusion-aware Image-based Rendering. (arXiv:2107.13421v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13421">
<div class="article-summary-box-inner">
<span><p>We present a new neural representation, called Neural Ray (NeuRay), for the
novel view synthesis task. Recent works construct radiance fields from image
features of input views to render novel view images, which enables the
generalization to new scenes. However, due to occlusions, a 3D point may be
invisible to some input views. On such a 3D point, these generalization methods
will include inconsistent image features from invisible views, which interfere
with the radiance field construction. To solve this problem, we predict the
visibility of 3D points to input views within our NeuRay representation. This
visibility enables the radiance field construction to focus on visible image
features, which significantly improves its rendering quality. Meanwhile, a
novel consistency loss is proposed to refine the visibility in NeuRay when
finetuning on a specific scene. Experiments demonstrate that our approach
achieves state-of-the-art performance on the novel view synthesis task when
generalizing to unseen scenes and outperforms per-scene optimization methods
after finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DarkLighter: Light Up the Darkness for UAV Tracking. (arXiv:2107.14389v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14389">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the fast evolution and promising performance of
the convolutional neural network (CNN)-based trackers, which aim at imitating
biological visual systems. However, current CNN-based trackers can hardly
generalize well to low-light scenes that are commonly lacked in the existing
training set. In indistinguishable night scenarios frequently encountered in
unmanned aerial vehicle (UAV) tracking-based applications, the robustness of
the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial
tracking in the dark through a general fashion, this work proposes a low-light
image enhancer namely DarkLighter, which dedicates to alleviate the impact of
poor illumination and noise iteratively. A lightweight map estimation network,
i.e., ME-Net, is trained to efficiently estimate illumination maps and noise
maps jointly. Experiments are conducted with several SOTA trackers on numerous
UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability
and universality of DarkLighter, with high efficiency. Moreover, DarkLighter
has further been implemented on a typical UAV system. Real-world tests at night
scenes have verified its practicability and dependability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical Dependency Guided Contrastive Learning for Multiple Labeling in Prenatal Ultrasound. (arXiv:2108.05055v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05055">
<div class="article-summary-box-inner">
<span><p>Standard plane recognition plays an important role in prenatal ultrasound
(US) screening. Automatically recognizing the standard plane along with the
corresponding anatomical structures in US image can not only facilitate US
image interpretation but also improve diagnostic efficiency. In this study, we
build a novel multi-label learning (MLL) scheme to identify multiple standard
planes and corresponding anatomical structures of fetus simultaneously. Our
contribution is three-fold. First, we represent the class correlation by word
embeddings to capture the fine-grained semantic and latent statistical
concurrency. Second, we equip the MLL with a graph convolutional network to
explore the inner and outer relationship among categories. Third, we propose a
novel cluster relabel-based contrastive learning algorithm to encourage the
divergence among ambiguous classes. Extensive validation was performed on our
large in-house dataset. Our approach reports the highest accuracy as 90.25% for
standard planes labeling, 85.59% for planes and structures labeling and mAP as
94.63%. The proposed MLL scheme provides a novel perspective for standard plane
recognition and can be easily extended to other medical image classification
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11250">
<div class="article-summary-box-inner">
<span><p>A panoptic driving perception system is an essential part of autonomous
driving. A high-precision and real-time perception system can assist the
vehicle in making the reasonable decision while driving. We present a panoptic
driving perception network (YOLOP) to perform traffic object detection,
drivable area segmentation and lane detection simultaneously. It is composed of
one encoder for feature extraction and three decoders to handle the specific
tasks. Our model performs extremely well on the challenging BDD100K dataset,
achieving state-of-the-art on all three tasks in terms of accuracy and speed.
Besides, we verify the effectiveness of our multi-task learning model for joint
training via ablative studies. To our best knowledge, this is the first work
that can process these three visual perception tasks simultaneously in
real-time on an embedded device Jetson TX2(23 FPS) and maintain excellent
accuracy. To facilitate further research, the source codes and pre-trained
models are released at https://github.com/hustvl/YOLOP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Filter Adaptive Network for Single Image Defocus Deblurring. (arXiv:2108.13610v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13610">
<div class="article-summary-box-inner">
<span><p>We propose a novel end-to-end learning-based approach for single image
defocus deblurring. The proposed approach is equipped with a novel Iterative
Filter Adaptive Network (IFAN) that is specifically designed to handle
spatially-varying and large defocus blur. For adaptively handling
spatially-varying blur, IFAN predicts pixel-wise deblurring filters, which are
applied to defocused features of an input image to generate deblurred features.
For effectively managing large blur, IFAN models deblurring filters as stacks
of small-sized separable filters. Predicted separable deblurring filters are
applied to defocused features using a novel Iterative Adaptive Convolution
(IAC) layer. We also propose a training scheme based on defocus disparity
estimation and reblurring, which significantly boosts the deblurring quality.
We demonstrate that our method achieves state-of-the-art performance both
quantitatively and qualitatively on real-world images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00590">
<div class="article-summary-box-inner">
<span><p>Scaling Visual Question Answering (VQA) to the open-domain and multi-hop
nature of web searches, requires fundamental advances in visual representation
learning, knowledge aggregation, and language generation. In this work, we
introduce WebQA, a challenging new benchmark that proves difficult for
large-scale state-of-the-art models which lack language groundable visual
representations for novel objects and the ability to reason, yet trivial for
humans. WebQA mirrors the way humans use the web: 1) Ask a question, 2) Choose
sources to aggregate, and 3) Produce a fluent language response. This is the
behavior we should be expecting from IoT devices and digital assistants.
Existing work prefers to assume that a model can either reason about knowledge
in images or in text. WebQA includes a secondary text-only QA task to ensure
improved visual performance does not come at the cost of language
understanding. Our challenge for the community is to create unified multimodal
reasoning models that answer questions regardless of the source modality,
moving us closer to digital assistants that not only query language knowledge,
but also the richer visual online world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10852">
<div class="article-summary-box-inner">
<span><p>We present Pix2Seq, a simple and generic framework for object detection.
Unlike existing approaches that explicitly integrate prior knowledge about the
task, we cast object detection as a language modeling task conditioned on the
observed pixel inputs. Object descriptions (e.g., bounding boxes and class
labels) are expressed as sequences of discrete tokens, and we train a neural
network to perceive the image and generate the desired sequence. Our approach
is based mainly on the intuition that if a neural network knows about where and
what the objects are, we just need to teach it how to read them out. Beyond the
use of task-specific data augmentations, our approach makes minimal assumptions
about the task, yet it achieves competitive results on the challenging COCO
dataset, compared to highly specialized and well optimized detection
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Attribute and Structure Subspace Clustering Network. (arXiv:2109.13742v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13742">
<div class="article-summary-box-inner">
<span><p>Deep self-expressiveness-based subspace clustering methods have demonstrated
effectiveness. However, existing works only consider the attribute information
to conduct the self-expressiveness, which may limit the clustering performance.
In this paper, we propose a novel adaptive attribute and structure subspace
clustering network (AASSC-Net) to simultaneously consider the attribute and
structure information in an adaptive graph fusion manner. Specifically, we
first exploit an auto-encoder to represent input data samples with latent
features for the construction of an attribute matrix. We also construct a mixed
signed and symmetric structure matrix to capture the local geometric structure
underlying data samples. Then, we perform self-expressiveness on the
constructed attribute and structure matrices to learn their affinity graphs
separately. Finally, we design a novel attention-based fusion module to
adaptively leverage these two affinity graphs to construct a more
discriminative affinity graph. Extensive experimental results on commonly used
benchmark datasets demonstrate that our AASSC-Net significantly outperforms
state-of-the-art methods. In addition, we conduct comprehensive ablation
studies to discuss the effectiveness of the designed modules. The code will be
publicly available at https://github.com/ZhihaoPENG-CityU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02711">
<div class="article-summary-box-inner">
<span><p>Recently, GAN inversion methods combined with Contrastive Language-Image
Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts.
However, their applications to diverse real images are still difficult due to
the limited GAN inversion capability. Specifically, these approaches often have
difficulties in reconstructing images with novel poses, views, and highly
variable contents compared to the training data, altering object identity, or
producing unwanted image artifacts. To mitigate these problems and enable
faithful manipulation of real images, we propose a novel method, dubbed
DiffusionCLIP, that performs text-driven image manipulation using diffusion
models. Based on full inversion capability and high-quality image generation
power of recent diffusion models, our method performs zero-shot image
manipulation successfully even between unseen domains and takes another step
towards general application by manipulating images from a widely varying
ImageNet dataset. Furthermore, we propose a novel noise combination method that
allows straightforward multi-attribute manipulation. Extensive experiments and
human evaluation confirmed robust and superior manipulation performance of our
methods compared to the existing baselines. Code is available at
https://github.com/gwang-kim/DiffusionCLIP.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Higher-Order Dynamics in Video-Based Cardiac Measurement. (arXiv:2110.03690v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03690">
<div class="article-summary-box-inner">
<span><p>Computer vision methods typically optimize for first-order dynamics (e.g.,
optical flow). However, in many cases the properties of interest are subtle
variations in higher-order changes, such as acceleration. This is true in the
cardiac pulse, where the second derivative can be used as an indicator of blood
pressure and arterial disease. Recent developments in camera-based vital sign
measurement have shown that cardiac measurements can be recovered with
impressive accuracy from videos; however, most of the research has focused on
extracting summary statistics such as heart rate. Less emphasis has been put on
the accuracy of waveform morphology that is necessary for many clinically
meaningful assessments. In this work, we provide evidence that higher-order
dynamics are better estimated by neural models when explicitly optimized for in
the loss function. Furthermore, adding second-derivative inputs also improves
performance when estimating second-order dynamics. We illustrate this, by
showing that incorporating the second derivative of both the input frames and
the target vital sign signals into the training procedure, models are better
able to estimate left ventricle ejection time (LVET) intervals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient ImageNet: How to discover spurious features in Deep Learning?. (arXiv:2110.04301v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04301">
<div class="article-summary-box-inner">
<span><p>Deep neural networks can be unreliable in the real world especially when they
heavily use {\it spurious} features for their predictions. Focusing on image
classifications, we define {\it core features} as the set of visual features
that are always a part of the object definition while {\it spurious features}
are the ones that are likely to {\it co-occur} with the object but not a part
of it (e.g., attribute "fingers" for class "band aid"). Traditional methods for
discovering spurious features either require extensive human annotations (thus,
not scalable), or are useful on specific models. In this work, we introduce a
{\it general} framework to discover a subset of spurious and core visual
features used in inferences of a general model and localize them on a large
number of images with minimal human supervision. Our methodology is based on
this key idea: to identify spurious or core \textit{visual features} used in
model predictions, we identify spurious or core \textit{neural features}
(penultimate layer neurons of a robust model) via limited human supervision
(e.g., using top 5 activating images per feature). We then show that these
neural feature annotations {\it generalize} extremely well to many more images
{\it without} any human supervision. We use the activation maps for these
neural features as the soft masks to highlight spurious or core visual
features. Using this methodology, we introduce the {\it Salient Imagenet}
dataset containing core and spurious masks for a large set of samples from
Imagenet. Using this dataset, we show that several popular Imagenet models rely
heavily on various spurious features in their predictions, indicating the
standard accuracy alone is not sufficient to fully assess model performance.
Code and dataset for reproducing all experiments in the paper is available at
\url{https://github.com/singlasahil14/salient_imagenet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05319">
<div class="article-summary-box-inner">
<span><p>Data-driven fault detection has been regarded as a 3D image segmentation
task. The models trained from synthetic data are difficult to generalize in
some surveys. Recently, training 3D fault segmentation using sparse manual 2D
slices is thought to yield promising results, but manual labeling has many
false negative labels (abnormal annotations), which is detrimental to training
and consequently to detection performance. Motivated to train 3D fault
segmentation networks under sparse 2D labels while suppressing false negative
labels, we analyze the training process gradient and propose the Mask Dice (MD)
loss. Moreover, the fault is an edge feature, and current encoderdecoder
architectures widely used for fault detection (e.g., Ushape network) are not
conducive to edge representation and have redundant parameters. Consequently,
Fault-Net is proposed, which is designed for the characteristics of faults,
employs high-resolution propagation features, and embeds Multi-Scale
Compression Fusion module to fuse multi-scale information, which allows the
edge information to be fully preserved during propagation and fusion, thus
enabling advanced performance via few computational resources. Experimental
demonstrates that MD loss supports the inclusion of human experience in
training and suppresses false negative labels therein, allowing the baseline
model to generalize to more surveys. The Fault-Net parameter is only 0.42MB,
support up to 5283 (FP32) and 6403 (FP16) size cuboid inference on 16GB RAM,
its inference speed is significantly faster than other models. Our approach
employs fewer computational resources while providing more reliable and clearer
interpretations of seismic faults.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TDACNN: Target-domain-free Domain Adaptation Convolutional Neural Network for Drift Compensation in Gas Sensors. (arXiv:2110.07509v3 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07509">
<div class="article-summary-box-inner">
<span><p>Sensor drift is a long-existing unpredictable problem that deteriorates the
performance of gaseous substance recognition, calling for an antidrift domain
adaptation algorithm. However, the prerequisite for traditional methods to
achieve fine results is to have data from both nondrift distributions (source
domain) and drift distributions (target domain) for domain alignment, which is
usually unrealistic and unachievable in real-life scenarios. To compensate for
this, in this paper, deep learning based on a target-domain-free domain
adaptation convolutional neural network (TDACNN) is proposed. The main concept
is that CNNs extract not only the domain-specific features of samples but also
the domain-invariant features underlying both the source and target domains.
Making full use of these various levels of embedding features can lead to
comprehensive utilization of different levels of characteristics, thus
achieving drift compensation by the extracted intermediate features between two
domains. In the TDACNN, a flexible multibranch backbone with a multiclassifier
structure is proposed under the guidance of bionics, which utilizes multiple
embedding features comprehensively without involving target domain data during
training. A classifier ensemble method based on maximum mean discrepancy (MMD)
is proposed to evaluate all the classifiers jointly based on the credibility of
the pseudolabel. To optimize network training, an additive angular margin
softmax loss with parameter dynamic adjustment is utilized. Experiments on two
drift datasets under different settings demonstrate the superiority of TDACNN
compared with several state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A unifying framework for $n$-dimensional quasi-conformal mappings. (arXiv:2110.10437v2 [cs.CG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10437">
<div class="article-summary-box-inner">
<span><p>With the advancement of computer technology, there is a surge of interest in
effective mapping methods for objects in higher-dimensional spaces. To
establish a one-to-one correspondence between objects, higher-dimensional
quasi-conformal theory can be utilized for ensuring the bijectivity of the
mappings. In addition, it is often desirable for the mappings to satisfy
certain prescribed geometric constraints and possess low distortion in
conformality or volume. In this work, we develop a unifying framework for
computing $n$-dimensional quasi-conformal mappings. More specifically, we
propose a variational model that integrates quasi-conformal distortion,
volumetric distortion, landmark correspondence, intensity mismatch and volume
prior information to handle a large variety of deformation problems. We further
prove the existence of a minimizer for the proposed model and devise efficient
numerical methods to solve the optimization problem. We demonstrate the
effectiveness of the proposed framework using various experiments in two- and
three-dimensions, with applications to medical image registration, adaptive
remeshing and shape modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11524">
<div class="article-summary-box-inner">
<span><p>A key component of understanding hand-object interactions is the ability to
identify the active object -- the object that is being manipulated by the human
hand. In order to accurately localize the active object, any method must reason
using information encoded by each image pixel, such as whether it belongs to
the hand, the object, or the background. To leverage each pixel as evidence to
determine the bounding box of the active object, we propose a pixel-wise voting
function. Our pixel-wise voting function takes an initial bounding box as input
and produces an improved bounding box of the active object as output. The
voting function is designed so that each pixel inside of the input bounding box
votes for an improved bounding box, and the box with the majority vote is
selected as the output. We call the collection of bounding boxes generated
inside of the voting function, the Relational Box Field, as it characterizes a
field of bounding boxes defined in relationship to the current bounding box.
While our voting function is able to improve the bounding box of the active
object, one round of voting is typically not enough to accurately localize the
active object. Therefore, we repeatedly apply the voting function to
sequentially improve the location of the bounding box. However, since it is
known that repeatedly applying a one-step predictor (i.e., auto-regressive
processing with our voting function) can cause a data distribution shift, we
mitigate this issue using reinforcement learning (RL). We adopt standard RL to
learn the voting function parameters and show that it provides a meaningful
improvement over a standard supervised learning approach. We perform
experiments on two large-scale datasets: 100DOH and MECCANO, improving AP50
performance by 8% and 30%, respectively, over the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Co-segmentation by Segment Swapping for Retrieval and Discovery. (arXiv:2110.15904v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15904">
<div class="article-summary-box-inner">
<span><p>The goal of this work is to efficiently identify visually similar patterns in
images, e.g. identifying an artwork detail copied between an engraving and an
oil painting, or recognizing parts of a night-time photograph visible in its
daytime counterpart. Lack of training data is a key challenge for this
co-segmentation task. We present a simple yet surprisingly effective approach
to overcome this difficulty: we generate synthetic training pairs by selecting
segments in an image and copy-pasting them into another image. We then learn to
predict the repeated region masks. We find that it is crucial to predict the
correspondences as an auxiliary task and to use Poisson blending and style
transfer on the training pairs to generalize on real data. We analyse results
with two deep architectures relevant to our joint image analysis task: a
transformer-based architecture and Sparse Nc-Net, a recent network designed to
predict coarse correspondences using 4D convolutions. We show our approach
provides clear improvements for artwork details retrieval on the Brueghel
dataset and achieves competitive performance on two place recognition
benchmarks, Tokyo247 and Pitts30K. We also demonstrate the potential of our
approach for unsupervised image collection analysis by introducing a spectral
graph clustering approach to object discovery and demonstrating it on the
object discovery dataset of \cite{rubinstein2013unsupervised} and the Brueghel
dataset. Our code and data are available at
<a href="http://imagine.enpc.fr/~shenx/SegSwap/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images. (arXiv:2111.11057v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11057">
<div class="article-summary-box-inner">
<span><p>The task of instance segmentation in remote sensing images, aiming at
performing per-pixel labeling of objects at instance level, is of great
importance for various civil applications. Despite previous successes, most
existing instance segmentation methods designed for natural images encounter
sharp performance degradations when they are directly applied to top-view
remote sensing images. Through careful analysis, we observe that the challenges
mainly come from the lack of discriminative object features due to severe scale
variations, low contrasts, and clustered distributions. In order to address
these problems, a novel context aggregation network (CATNet) is proposed to
improve the feature extraction process. The proposed model exploits three
lightweight plug-and-play modules, namely dense feature pyramid network
(DenseFPN), spatial context pyramid (SCP), and hierarchical region of interest
extractor (HRoIE), to aggregate global visual context at feature, spatial, and
instance domains, respectively. DenseFPN is a multi-scale feature propagation
module that establishes more flexible information flows by adopting inter-level
residual connections, cross-level dense connections, and feature re-weighting
strategy. Leveraging the attention mechanism, SCP further augments the features
by aggregating global spatial context into local regions. For each instance,
HRoIE adaptively generates RoI features for different downstream tasks. We
carry out extensive evaluations of the proposed scheme on the challenging
iSAID, DIOR, NWPU VHR-10, and HRSID datasets. The evaluation results
demonstrate that the proposed approach outperforms state-of-the-arts under
similar computational costs. Source code and pre-trained models are available
at https://github.com/yeliudev/CATNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion. (arXiv:2111.11326v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11326">
<div class="article-summary-box-inner">
<span><p>Deep network architectures struggle to continually learn new tasks without
forgetting the previous tasks. A recent trend indicates that dynamic
architectures based on an expansion of the parameters can reduce catastrophic
forgetting efficiently in continual learning. However, existing approaches
often require a task identifier at test-time, need complex tuning to balance
the growing number of parameters, and barely share any information across
tasks. As a result, they struggle to scale to a large number of tasks without
significant overhead. In this paper, we propose a transformer architecture
based on a dedicated encoder/decoder framework. Critically, the encoder and
decoder are shared among all tasks. Through a dynamic expansion of special
tokens, we specialize each forward of our decoder network on a task
distribution. Our strategy scales to a large number of tasks while having
negligible memory and time overheads due to strict control of the parameters
expansion. Moreover, this efficient strategy doesn't need any hyperparameter
tuning to control the network's expansion. Our model reaches excellent results
on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100
and ImageNet1000 while having less parameters than concurrent dynamic
frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. (arXiv:2111.12077v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12077">
<div class="article-summary-box-inner">
<span><p>Though neural radiance fields (NeRF) have demonstrated impressive view
synthesis results on objects and small bounded regions of space, they struggle
on "unbounded" scenes, where the camera may point in any direction and content
may exist at any distance. In this setting, existing NeRF-like models often
produce blurry or low-resolution renderings (due to the unbalanced detail and
scale of nearby and distant objects), are slow to train, and may exhibit
artifacts due to the inherent ambiguity of the task of reconstructing a large
scene from a small set of images. We present an extension of mip-NeRF (a NeRF
variant that addresses sampling and aliasing) that uses a non-linear scene
parameterization, online distillation, and a novel distortion-based regularizer
to overcome the challenges presented by unbounded scenes. Our model, which we
dub "mip-NeRF 360" as we target scenes in which the camera rotates 360 degrees
around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is
able to produce realistic synthesized views and detailed depth maps for highly
intricate, unbounded real-world scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Up Vision-Language Pre-training for Image Captioning. (arXiv:2111.12233v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12233">
<div class="article-summary-box-inner">
<span><p>In recent years, we have witnessed significant performance boost in the image
captioning task based on vision-language pre-training (VLP). Scale is believed
to be an important factor for this advance. However, most existing work only
focuses on pre-training transformers with moderate sizes (e.g., 12 or 24
layers) on roughly 4 million images. In this paper, we present LEMON, a
LargE-scale iMage captiONer, and provide the first empirical study on the
scaling behavior of VLP for image captioning. We use the state-of-the-art VinVL
model as our reference model, which consists of an image feature extractor and
a transformer model, and scale the transformer both up and down, with model
sizes ranging from 13 to 675 million parameters. In terms of data, we conduct
experiments with up to 200 million image-text pairs which are automatically
collected from web based on the alt attribute of the image (dubbed as ALT200M).
Extensive analysis helps to characterize the performance trend as the model
size and the pre-training data size increase. We also compare different
training recipes, especially for training on large-scale noisy data. As a
result, LEMON achieves new state of the arts on several major image captioning
benchmarks, including COCO Caption, nocaps, and Conceptual Captions. We also
show LEMON can generate captions with long-tail visual concepts when used in a
zero-shot manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. (arXiv:2111.12707v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12707">
<div class="article-summary-box-inner">
<span><p>Estimating 3D human poses from monocular videos is a challenging task due to
depth ambiguity and self-occlusion. Most existing works attempt to solve both
issues by exploiting spatial and temporal relationships. However, those works
ignore the fact that it is an inverse problem where multiple feasible solutions
(i.e., hypotheses) exist. To relieve this limitation, we propose a
Multi-Hypothesis Transformer (MHFormer) that learns spatio-temporal
representations of multiple plausible pose hypotheses. In order to effectively
model multi-hypothesis dependencies and build strong relationships across
hypothesis features, the task is decomposed into three stages: (i) Generate
multiple initial hypothesis representations; (ii) Model self-hypothesis
communication, merge multiple hypotheses into a single converged representation
and then partition it into several diverged hypotheses; (iii) Learn
cross-hypothesis communication and aggregate the multi-hypothesis features to
synthesize the final 3D pose. Through the above processes, the final
representation is enhanced and the synthesized pose is much more accurate.
Extensive experiments show that MHFormer achieves state-of-the-art results on
two challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and
whistles, its performance surpasses the previous best result by a large margin
of 3% on Human3.6M. Code and models are available at
\url{https://github.com/Vegetebird/MHFormer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation. (arXiv:2111.12903v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12903">
<div class="article-summary-box-inner">
<span><p>Consistency learning using input image, feature, or network perturbations has
shown remarkable results in semi-supervised semantic segmentation, but this
approach can be seriously affected by inaccurate predictions of unlabelled
training images. There are two consequences of these inaccurate predictions: 1)
the training based on the "strict" cross-entropy (CE) loss can easily overfit
prediction mistakes, leading to confirmation bias; and 2) the perturbations
applied to these inaccurate predictions will use potentially erroneous
predictions as training signals, degrading consistency learning. In this paper,
we address the prediction accuracy problem of consistency learning methods with
novel extensions of the mean-teacher (MT) model, which include a new auxiliary
teacher, and the replacement of MT's mean square error (MSE) by a stricter
confidence-weighted cross-entropy (Conf-CE) loss. The accurate prediction by
this model allows us to use a challenging combination of network, input data
and feature perturbations to improve the consistency learning generalisation,
where the feature perturbations consist of a new adversarial perturbation.
Results on public benchmarks show that our approach achieves remarkable
improvements over the previous SOTA methods in the field. Our code is available
at https://github.com/yyliu01/PS-MT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation. (arXiv:2111.12940v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12940">
<div class="article-summary-box-inner">
<span><p>Self-training has greatly facilitated domain adaptive semantic segmentation,
which iteratively generates pseudo labels on unlabeled target data and retrains
the network. However, realistic segmentation datasets are highly imbalanced,
pseudo labels are typically biased to the majority classes and basically noisy,
leading to an error-prone and suboptimal model. In this paper, we propose a
simple region-based active learning approach for semantic segmentation under a
domain shift, aiming to automatically query a small partition of image regions
to be labeled while maximizing segmentation performance. Our algorithm, Region
Impurity and Prediction Uncertainty (RIPU), introduces a new acquisition
strategy characterizing the spatial adjacency of image regions along with the
prediction confidence. We show that the proposed region-based selection
strategy makes more efficient use of a limited budget than image-based or
point-based counterparts. Further, we enforce local prediction consistency
between a pixel and its nearest neighbors on a source image. Alongside, we
develop a negative learning loss to make the features more discriminative.
Extensive experiments demonstrate that our method only requires very few
annotations to almost reach the supervised performance and substantially
outperforms state-of-the-art methods. The code is available at
https://github.com/BIT-DA/RIPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13196">
<div class="article-summary-box-inner">
<span><p>The canonical approach to video captioning dictates a caption generation
model to learn from offline-extracted dense video features. These feature
extractors usually operate on video frames sampled at a fixed frame rate and
are often trained on image/video understanding tasks, without adaption to video
captioning data. In this work, we present SwinBERT, an end-to-end
transformer-based model for video captioning, which takes video frame patches
directly as inputs, and outputs a natural language description. Instead of
leveraging multiple 2D/3D feature extractors, our method adopts a video
transformer to encode spatial-temporal representations that can adapt to
variable lengths of video input without dedicated design for different frame
rates. Based on this model architecture, we show that video captioning can
benefit significantly from more densely sampled video frames as opposed to
previous successes with sparsely sampled video frames for video-and-language
understanding tasks (e.g., video question answering). Moreover, to avoid the
inherent redundancy in consecutive video frames, we propose adaptively learning
a sparse attention mask and optimizing it for task-specific performance
improvement through better long-range video sequence modeling. Through
extensive experiments on 5 video captioning datasets, we show that SwinBERT
achieves across-the-board performance improvements over previous methods, often
by a large margin. The learned sparse attention masks in addition push the
limit to new state of the arts, and can be transferred between different video
lengths and between different datasets. Code is available at
https://github.com/microsoft/SwinBERT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers. (arXiv:2111.13587v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13587">
<div class="article-summary-box-inner">
<span><p>Vision transformers have delivered tremendous success in representation
learning. This is primarily due to effective token mixing through self
attention. However, this scales quadratically with the number of pixels, which
becomes infeasible for high-resolution inputs. To cope with this challenge, we
propose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer
that learns to mix in the Fourier domain. AFNO is based on a principled
foundation of operator learning which allows us to frame token mixing as a
continuous global convolution without any dependence on the input resolution.
This principle was previously used to design FNO, which solves global
convolution efficiently in the Fourier domain and has shown promise in learning
challenging PDEs. To handle challenges in visual representation learning such
as discontinuities in images and high resolution inputs, we propose principled
architectural modifications to FNO which results in memory and computational
efficiency. This includes imposing a block-diagonal structure on the channel
mixing weights, adaptively sharing weights across tokens, and sparsifying the
frequency modes via soft-thresholding and shrinkage. The resulting model is
highly parallel with a quasi-linear complexity and has linear memory in the
sequence size. AFNO outperforms self-attention mechanisms for few-shot
segmentation in terms of both efficiency and accuracy. For Cityscapes
segmentation with the Segformer-B3 backbone, AFNO can handle a sequence size of
65k and outperforms other efficient self-attention mechanisms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GMFlow: Learning Optical Flow via Global Matching. (arXiv:2111.13680v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13680">
<div class="article-summary-box-inner">
<span><p>Learning-based optical flow estimation has been dominated with the pipeline
of cost volume with convolutions for flow regression, which is inherently
limited to local correlations and thus is hard to address the long-standing
challenge of large displacements. To alleviate this, the state-of-the-art
framework RAFT gradually improves its prediction quality by using a large
number of iterative refinements, achieving remarkable performance but
introducing linearly increasing inference time. To enable both high accuracy
and efficiency, we completely revamp the dominant flow regression pipeline by
reformulating optical flow as a global matching problem, which identifies the
correspondences by directly comparing feature similarities. Specifically, we
propose a GMFlow framework, which consists of three main components: a
customized Transformer for feature enhancement, a correlation and softmax layer
for global feature matching, and a self-attention layer for flow propagation.
We further introduce a refinement step that reuses GMFlow at higher feature
resolution for residual flow prediction. Our new framework outperforms
31-refinements RAFT on the challenging Sintel benchmark, while using only one
refinement and running faster, suggesting a new paradigm for accurate and
efficient optical flow estimation. Code is available at
https://github.com/haofeixu/gmflow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Frame Interpolation Transformer. (arXiv:2111.13817v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13817">
<div class="article-summary-box-inner">
<span><p>Existing methods for video interpolation heavily rely on deep convolution
neural networks, and thus suffer from their intrinsic limitations, such as
content-agnostic kernel weights and restricted receptive field. To address
these issues, we propose a Transformer-based video interpolation framework that
allows content-aware aggregation weights and considers long-range dependencies
with the self-attention operations. To avoid the high computational cost of
global self-attention, we introduce the concept of local attention into video
interpolation and extend it to the spatial-temporal domain. Furthermore, we
propose a space-time separation strategy to save memory usage, which also
improves performance. In addition, we develop a multi-scale frame synthesis
scheme to fully realize the potential of Transformers. Extensive experiments
demonstrate the proposed model performs favorably against the state-of-the-art
methods both quantitatively and qualitatively on a variety of benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deblur-NeRF: Neural Radiance Fields from Blurry Images. (arXiv:2111.14292v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14292">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Field (NeRF) has gained considerable attention recently for
3D scene reconstruction and novel view synthesis due to its remarkable
synthesis quality. However, image blurriness caused by defocus or motion, which
often occurs when capturing scenes in the wild, significantly degrades its
reconstruction quality. To address this problem, We propose Deblur-NeRF, the
first method that can recover a sharp NeRF from blurry input. We adopt an
analysis-by-synthesis approach that reconstructs blurry views by simulating the
blurring process, thus making NeRF robust to blurry inputs. The core of this
simulation is a novel Deformable Sparse Kernel (DSK) module that models
spatially-varying blur kernels by deforming a canonical sparse kernel at each
spatial location. The ray origin of each kernel point is jointly optimized,
inspired by the physical blurring process. This module is parameterized as an
MLP that has the ability to be generalized to various blur types. Jointly
optimizing the NeRF and the DSK module allows us to restore a sharp NeRF. We
demonstrate that our method can be used on both camera motion blur and defocus
blur: the two most common types of blur in real scenes. Evaluation results on
both synthetic and real-world data show that our method outperforms several
baselines. The synthetic and real datasets along with the source code is
publicly available at https://limacv.github.io/deblurnerf/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-instance Point Cloud Registration by Efficient Correspondence Clustering. (arXiv:2111.14582v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14582">
<div class="article-summary-box-inner">
<span><p>We address the problem of estimating the poses of multiple instances of the
source point cloud within a target point cloud. Existing solutions require
sampling a lot of hypotheses to detect possible instances and reject the
outliers, whose robustness and efficiency degrade notably when the number of
instances and outliers increase. We propose to directly group the set of noisy
correspondences into different clusters based on a distance invariance matrix.
The instances and outliers are automatically identified through clustering. Our
method is robust and fast. We evaluated our method on both synthetic and
real-world datasets. The results show that our approach can correctly register
up to 20 instances with an F1 score of 90.46% in the presence of 70% outliers,
which performs significantly better and at least 10x faster than existing
methods
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis. (arXiv:2111.14791v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14791">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT)s have shown great performance in self-supervised
learning of global and local representations that can be transferred to
downstream applications. Inspired by these results, we introduce a novel
self-supervised learning framework with tailored proxy tasks for medical image
analysis. Specifically, we propose: (i) a new 3D transformer-based model,
dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for
self-supervised pre-training; (ii) tailored proxy tasks for learning the
underlying pattern of human anatomy. We demonstrate successful pre-training of
the proposed model on 5,050 publicly available computed tomography (CT) images
from various body organs. The effectiveness of our approach is validated by
fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV)
Segmentation Challenge with 13 abdominal organs and segmentation tasks from the
Medical Segmentation Decathlon (MSD) dataset. Our model is currently the
state-of-the-art (i.e. ranked 1st) on the public test leaderboards of both MSD
and BTCV datasets. Code: https://monai.io/research/swin-unetr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blended Diffusion for Text-driven Editing of Natural Images. (arXiv:2111.14818v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14818">
<div class="article-summary-box-inner">
<span><p>Natural language offers a highly intuitive interface for image editing. In
this paper, we introduce the first solution for performing local (region-based)
edits in generic natural images, based on a natural language description along
with an ROI mask. We achieve our goal by leveraging and combining a pretrained
language-image model (CLIP), to steer the edit towards a user-provided text
prompt, with a denoising diffusion probabilistic model (DDPM) to generate
natural-looking results. To seamlessly fuse the edited region with the
unchanged parts of the image, we spatially blend noised versions of the input
image with the local text-guided diffusion latent at a progression of noise
levels. In addition, we show that adding augmentations to the diffusion process
mitigates adversarial results. We compare against several baselines and related
methods, both qualitatively and quantitatively, and show that our method
outperforms these solutions in terms of overall realism, ability to preserve
the background and matching the text. Finally, we show several text-driven
editing applications, including adding a new object to an image,
removing/replacing/altering existing objects, background replacement, and image
extrapolation. Code is available at:
https://omriavrahami.com/blended-diffusion-page/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-domain Integrative Swin Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v6 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14831">
<div class="article-summary-box-inner">
<span><p>Decreasing projection views to lower X-ray radiation dose usually leads to
severe streak artifacts. To improve image quality from sparse-view data, a
Multi-domain Integrative Swin Transformer network (MIST-net) was developed in
this article. First, MIST-net incorporated lavish domain features from data,
residual-data, image, and residual-image using flexible network architectures,
where residual-data and residual-image sub-network was considered as data
consistency module to eliminate interpolation and reconstruction errors.
Second, a trainable edge enhancement filter was incorporated to detect and
protect image edges. Third, a high-quality reconstruction Swin transformer
(i.e., Recformer) was designed to capture image global features. The experiment
results on numerical and real cardiac clinical datasets with 48-views
demonstrated that our proposed MIST-net provided better image quality with more
small features and sharp edges than other competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZZ-Net: A Universal Rotation Equivariant Architecture for 2D Point Clouds. (arXiv:2111.15341v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15341">
<div class="article-summary-box-inner">
<span><p>In this paper, we are concerned with rotation equivariance on 2D point cloud
data. We describe a particular set of functions able to approximate any
continuous rotation equivariant and permutation invariant function. Based on
this result, we propose a novel neural network architecture for processing 2D
point clouds and we prove its universality for approximating functions
exhibiting these symmetries.
</p>
<p>We also show how to extend the architecture to accept a set of 2D-2D
correspondences as indata, while maintaining similar equivariance properties.
Experiments are presented on the estimation of essential matrices in stereo
vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">360MonoDepth: High-Resolution 360{\deg} Monocular Depth Estimation. (arXiv:2111.15669v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15669">
<div class="article-summary-box-inner">
<span><p>360{\deg} cameras can capture complete environments in a single shot, which
makes 360{\deg} imagery alluring in many computer vision tasks. However,
monocular depth estimation remains a challenge for 360{\deg} data, particularly
for high resolutions like 2K (2048x1024) and beyond that are important for
novel-view synthesis and virtual reality applications. Current CNN-based
methods do not support such high resolutions due to limited GPU memory. In this
work, we propose a flexible framework for monocular depth estimation from
high-resolution 360{\deg} images using tangent images. We project the 360{\deg}
input image onto a set of tangent planes that produce perspective views, which
are suitable for the latest, most accurate state-of-the-art perspective
monocular depth estimators. To achieve globally consistent disparity estimates,
we recombine the individual depth estimates using deformable multi-scale
alignment followed by gradient-domain blending. The result is a dense,
high-resolution 360{\deg} depth map with a high level of detail, also for
outdoor scenes which are not supported by existing methods. Our source code and
data are available at https://manurare.github.io/360monodepth/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-tailed Classification. (arXiv:2112.00412v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00412">
<div class="article-summary-box-inner">
<span><p>The problem of class imbalanced data is that the generalization performance
of the classifier deteriorates due to the lack of data from minority classes.
In this paper, we propose a novel minority over-sampling method to augment
diversified minority samples by leveraging the rich context of the majority
classes as background images. To diversify the minority samples, our key idea
is to paste an image from a minority class onto rich-context images from a
majority class, using them as background images. Our method is simple and can
be easily combined with the existing long-tailed recognition methods. We
empirically prove the effectiveness of the proposed oversampling method through
extensive experiments and ablation studies. Without any architectural changes
or complex algorithms, our method achieves state-of-the-art performance on
various long-tailed classification benchmarks. Our code is made available at
https://github.com/naver-ai/cmo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions. (arXiv:2112.00431v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00431">
<div class="article-summary-box-inner">
<span><p>The recent and increasing interest in video-language research has driven the
development of large-scale datasets that enable data-intensive machine learning
techniques. In comparison, limited effort has been made at assessing the
fitness of these datasets for the video-language grounding task. Recent works
have begun to discover significant limitations in these datasets, suggesting
that state-of-the-art techniques commonly overfit to hidden dataset biases. In
this work, we present MAD (Movie Audio Descriptions), a novel benchmark that
departs from the paradigm of augmenting existing video datasets with text
annotations and focuses on crawling and aligning available audio descriptions
of mainstream movies. MAD contains over 384,000 natural language sentences
grounded in over 1,200 hours of videos and exhibits a significant reduction in
the currently diagnosed biases for video-language grounding datasets. MAD's
collection strategy enables a novel and more challenging version of
video-language grounding, where short temporal moments (typically seconds long)
must be accurately grounded in diverse long-form videos that can last up to
three hours. We have released MAD's data and baselines code at
https://github.com/Soldelli/MAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Transferability of Supervised Pretraining: an MLP Perspective. (arXiv:2112.00496v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00496">
<div class="article-summary-box-inner">
<span><p>The pretrain-finetune paradigm is a classical pipeline in visual learning.
Recent progress on unsupervised pretraining methods shows superior transfer
performance to their supervised counterparts. This paper revisits this
phenomenon and sheds new light on understanding the transferability gap between
unsupervised and supervised pretraining from a multilayer perceptron (MLP)
perspective. While previous works focus on the effectiveness of MLP on
unsupervised image classification where pretraining and evaluation are
conducted on the same dataset, we reveal that the MLP projector is also the key
factor to better transferability of unsupervised pretraining methods than
supervised pretraining methods. Based on this observation, we attempt to close
the transferability gap between supervised and unsupervised pretraining by
adding an MLP projector before the classifier in supervised pretraining. Our
analysis indicates that the MLP projector can help retain intra-class variation
of visual features, decrease the feature distribution distance between
pretraining and evaluation datasets, and reduce feature redundancy. Extensive
experiments on public benchmarks demonstrate that the added MLP projector
significantly boosts the transferability of supervised pretraining, e.g. +7.2%
top-1 accuracy on the concept generalization task, +5.8% top-1 accuracy for
linear evaluation on 12-domain classification tasks, and +0.8% AP on COCO
object detection task, making supervised pretraining comparable or even better
than unsupervised pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N-ImageNet: Towards Robust, Fine-Grained Object Recognition with Event Cameras. (arXiv:2112.01041v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01041">
<div class="article-summary-box-inner">
<span><p>We introduce N-ImageNet, a large-scale dataset targeted for robust,
fine-grained object recognition with event cameras. The dataset is collected
using programmable hardware in which an event camera consistently moves around
a monitor displaying images from ImageNet. N-ImageNet serves as a challenging
benchmark for event-based object recognition, due to its large number of
classes and samples. We empirically show that pretraining on N-ImageNet
improves the performance of event-based classifiers and helps them learn with
few labeled data. In addition, we present several variants of N-ImageNet to
test the robustness of event-based classifiers under diverse camera
trajectories and severe lighting conditions, and propose a novel event
representation to alleviate the performance degradation. To the best of our
knowledge, we are the first to quantitatively investigate the consequences
caused by various environmental conditions on event-based object recognition
algorithms. N-ImageNet and its variants are expected to guide practical
implementations for deploying event-based object recognition algorithms in the
real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Head Avatars from Monocular RGB Videos. (arXiv:2112.01554v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01554">
<div class="article-summary-box-inner">
<span><p>We present Neural Head Avatars, a novel neural representation that explicitly
models the surface geometry and appearance of an animatable human avatar that
can be used for teleconferencing in AR/VR or other applications in the movie or
games industry that rely on a digital human. Our representation can be learned
from a monocular RGB portrait video that features a range of different
expressions and views. Specifically, we propose a hybrid representation
consisting of a morphable model for the coarse shape and expressions of the
face, and two feed-forward networks, predicting vertex offsets of the
underlying mesh as well as a view- and expression-dependent texture. We
demonstrate that this representation is able to accurately extrapolate to
unseen poses and view points, and generates natural expressions while providing
sharp texture details. Compared to previous works on head avatars, our method
provides a disentangled shape and appearance model of the complete human head
(including hair) that is compatible with the standard graphics pipeline.
Moreover, it quantitatively and qualitatively outperforms current state of the
art in terms of reconstruction quality and novel-view synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Two-Stage Detection of Human-Object Interactions with a Novel Unary-Pairwise Transformer. (arXiv:2112.01838v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01838">
<div class="article-summary-box-inner">
<span><p>Recent developments in transformer models for visual data have led to
significant improvements in recognition and detection tasks. In particular,
using learnable queries in place of region proposals has given rise to a new
class of one-stage detection models, spearheaded by the Detection Transformer
(DETR). Variations on this one-stage approach have since dominated human-object
interaction (HOI) detection. However, the success of such one-stage HOI
detectors can largely be attributed to the representation power of
transformers. We discovered that when equipped with the same transformer, their
two-stage counterparts can be more performant and memory-efficient, while
taking a fraction of the time to train. In this work, we propose the
Unary-Pairwise Transformer, a two-stage detector that exploits unary and
pairwise representations for HOIs. We observe that the unary and pairwise parts
of our transformer network specialise, with the former preferentially
increasing the scores of positive examples and the latter decreasing the scores
of negative examples. We evaluate our method on the HICO-DET and V-COCO
datasets, and significantly outperform state-of-the-art approaches. At
inference time, our model with ResNet50 approaches real-time performance on a
single GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training. (arXiv:2112.03552v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03552">
<div class="article-summary-box-inner">
<span><p>Recently, vision Transformers (ViTs) are developing rapidly and starting to
challenge the domination of convolutional neural networks (CNNs) in the realm
of computer vision (CV). With the general-purpose Transformer architecture
replacing the hard-coded inductive biases of convolution, ViTs have surpassed
CNNs, especially in data-sufficient circumstances. However, ViTs are prone to
over-fit on small datasets and thus rely on large-scale pre-training, which
expends enormous time. In this paper, we strive to liberate ViTs from
pre-training by introducing CNNs' inductive biases back to ViTs while
preserving their network architectures for higher upper bound and setting up
more suitable optimization objectives. To begin with, an agent CNN is designed
based on the given ViT with inductive biases. Then a bootstrapping training
algorithm is proposed to jointly optimize the agent and ViT with weight
sharing, during which the ViT learns inductive biases from the intermediate
features of the agent. Extensive experiments on CIFAR-10/100 and ImageNet-1k
with limited training data have shown encouraging results that the inductive
biases help ViTs converge significantly faster and outperform conventional CNNs
with even fewer parameters. Our code is publicly available at
https://github.com/zhfeing/Bootstrapping-ViTs-pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BACON: Band-limited Coordinate Networks for Multiscale Scene Representation. (arXiv:2112.04645v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04645">
<div class="article-summary-box-inner">
<span><p>Coordinate-based networks have emerged as a powerful tool for 3D
representation and scene reconstruction. These networks are trained to map
continuous input coordinates to the value of a signal at each point. Still,
current architectures are black boxes: their spectral characteristics cannot be
easily analyzed, and their behavior at unsupervised points is difficult to
predict. Moreover, these networks are typically trained to represent a signal
at a single scale, so naive downsampling or upsampling results in artifacts. We
introduce band-limited coordinate networks (BACON), a network architecture with
an analytical Fourier spectrum. BACON has constrained behavior at unsupervised
points, can be designed based on the spectral characteristics of the
represented signal, and can represent signals at multiple scales without
per-scale supervision. We demonstrate BACON for multiscale neural
representation of images, radiance fields, and 3D scenes using signed distance
functions and show that it outperforms conventional single-scale coordinate
networks in terms of interpretability and quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic Prior. (arXiv:2112.05077v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05077">
<div class="article-summary-box-inner">
<span><p>Evaluating and improving planning for autonomous vehicles requires scalable
generation of long-tail traffic scenarios. To be useful, these scenarios must
be realistic and challenging, but not impossible to drive through safely. In
this work, we introduce STRIVE, a method to automatically generate challenging
scenarios that cause a given planner to produce undesirable behavior, like
collisions. To maintain scenario plausibility, the key idea is to leverage a
learned model of traffic motion in the form of a graph-based conditional VAE.
Scenario generation is formulated as an optimization in the latent space of
this traffic model, perturbing an initial real-world scene to produce
trajectories that collide with a given planner. A subsequent optimization is
used to find a "solution" to the scenario, ensuring it is useful to improve the
given planner. Further analysis clusters generated scenarios based on collision
type. We attack two planners and show that STRIVE successfully generates
realistic, challenging scenarios in both cases. We additionally "close the
loop" and use these scenarios to optimize hyperparameters of a rule-based
planner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06116">
<div class="article-summary-box-inner">
<span><p>We study the effect of adversarial perturbations of images on deep stereo
matching networks for the disparity estimation task. We present a method to
craft a single set of perturbations that, when added to any stereo image pair
in a dataset, can fool a stereo network to significantly alter the perceived
scene geometry. Our perturbation images are "universal" in that they not only
corrupt estimates of the network on the dataset they are optimized for, but
also generalize to different architectures trained on different datasets. We
evaluate our approach on multiple benchmark datasets where our perturbations
can increase the D1-error (akin to fooling rate) of state-of-the-art stereo
networks from 1% to as much as 87%. We investigate the effect of perturbations
on the estimated scene geometry and identify object classes that are most
vulnerable. Our analysis on the activations of registered points between left
and right images led us to find architectural components that can increase
robustness against adversaries. By simply designing networks with such
components, one can reduce the effect of adversaries by up to 60.5%, which
rivals the robustness of networks fine-tuned with costly adversarial data
augmentation. Our design principle also improves their robustness against
common image corruptions by an average of 70%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active learning with MaskAL reduces annotation effort for training Mask R-CNN. (arXiv:2112.06586v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06586">
<div class="article-summary-box-inner">
<span><p>The generalisation performance of a convolutional neural network (CNN) is
influenced by the quantity, quality, and variety of the training images.
Training images must be annotated, and this is time consuming and expensive.
The goal of our work was to reduce the number of annotated images needed to
train a CNN while maintaining its performance. We hypothesised that the
performance of a CNN can be improved faster by ensuring that the set of
training images contains a large fraction of hard-to-classify images. The
objective of our study was to test this hypothesis with an active learning
method that can automatically select the hard-to-classify images. We developed
an active learning method for Mask Region-based CNN (Mask R-CNN) and named this
method MaskAL. MaskAL involved the iterative training of Mask R-CNN, after
which the trained model was used to select a set of unlabelled images about
which the model was most uncertain. The selected images were then annotated and
used to retrain Mask R-CNN, and this was repeated for a number of sampling
iterations. In our study, MaskAL was compared to a random sampling method on a
broccoli dataset with five visually similar classes. MaskAL performed
significantly better than the random sampling. In addition, MaskAL had the same
performance after sampling 900 images as the random sampling had after 2300
images. Compared to a Mask R-CNN model that was trained on the entire training
set (14,000 images), MaskAL achieved 93.9% of that model's performance with
17.9% of its training data. The random sampling achieved 81.9% of that model's
performance with 16.4% of its training data. We conclude that by using MaskAL,
the annotation effort can be reduced for training Mask R-CNN on a broccoli
dataset with visually similar classes. Our software is available on
https://github.com/pieterblok/maskal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logically at Factify 2022: Multimodal Fact Verification. (arXiv:2112.09253v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09253">
<div class="article-summary-box-inner">
<span><p>This paper describes our participant system for the multi-modal fact
verification (Factify) challenge at AAAI 2022. Despite the recent advance in
text based verification techniques and large pre-trained multimodal models
cross vision and language, very limited work has been done in applying
multimodal techniques to automate fact checking process, particularly
considering the increasing prevalence of claims and fake news about images and
videos on social media. In our work, the challenge is treated as multimodal
entailment task and framed as multi-class classification. Two baseline
approaches are proposed and explored including an ensemble model (combining two
uni-modal models) and a multi-modal attention network (modeling the interaction
between image and text pair from claim and evidence document). We conduct
several experiments investigating and benchmarking different SoTA pre-trained
transformers and vision models in this work. Our best model is ranked first in
leaderboard which obtains a weighted average F-measure of 0.77 on both
validation and test set. Exploratory analysis of dataset is also carried out on
the Factify data set and uncovers salient patterns and issues (e.g., word
overlapping, visual entailment correlation, source bias) that motivates our
hypothesis. Finally, we highlight challenges of the task and multimodal dataset
for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10325">
<div class="article-summary-box-inner">
<span><p>Due to the constraints of the imaging device and high cost in operation time,
computer tomography (CT) scans are usually acquired with low intra-slice
resolution. Improving the intra-slice resolution is beneficial to the disease
diagnosis for both human experts and computer-aided systems. To this end, this
paper builds a novel medical slice synthesis to increase the between-slice
resolution. Considering that the ground-truth intermediate medical slices are
always absent in clinical practice, we introduce the incremental cross-view
mutual distillation strategy to accomplish this task in the self-supervised
learning manner. Specifically, we model this problem from three different
views: slice-wise interpolation from axial view and pixel-wise interpolation
from coronal and sagittal views. Under this circumstance, the models learned
from different views can distill valuable knowledge to guide the learning
processes of each other. We can repeat this process to make the models
synthesize intermediate slice data with increasing inter-slice resolution. To
demonstrate the effectiveness of the proposed approach, we conduct
comprehensive experiments on a large-scale CT dataset. Quantitative and
qualitative comparison results show that our method outperforms
state-of-the-art algorithms by clear margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization. (arXiv:2112.11177v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11177">
<div class="article-summary-box-inner">
<span><p>For medical image segmentation, imagine if a model was only trained using MR
images in source domain, how about its performance to directly segment CT
images in target domain? This setting, namely generalizable cross-modality
segmentation, owning its clinical potential, is much more challenging than
other related settings, e.g., domain adaptation. To achieve this goal, we in
this paper propose a novel dual-normalization model by leveraging the augmented
source-similar and source-dissimilar images during our generalizable
segmentation. To be specific, given a single source domain, aiming to simulate
the possible appearance change in unseen target domains, we first utilize a
nonlinear transformation to augment source-similar and source-dissimilar
images. Then, to sufficiently exploit these two types of augmentations, our
proposed dual-normalization based model employs a shared backbone yet
independent batch normalization layer for separate normalization. Afterward, we
put forward a style-based selection scheme to automatically choose the
appropriate path in the test stage. Extensive experiments on three publicly
available datasets, i.e., BraTS, Cross-Modality Cardiac, and Abdominal
Multi-Organ datasets, have demonstrated that our method outperforms other
state-of-the-art domain generalization methods. Code is available at
https://github.com/zzzqzhou/Dual-Normalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recur, Attend or Convolve? Frame Dependency Modeling Matters for Cross-Domain Robustness in Action Recognition. (arXiv:2112.12175v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12175">
<div class="article-summary-box-inner">
<span><p>Most action recognition models today are highly parameterized, and evaluated
on datasets with predominantly spatially distinct classes. Previous results for
single images have shown that 2D Convolutional Neural Networks (CNNs) tend to
be biased toward texture rather than shape for various computer vision tasks
(Geirhos et al., 2019), reducing generalization. Taken together, this raises
suspicion that large video models learn spurious correlations rather than to
track relevant shapes over time and infer generalizable semantics from their
movement. A natural way to avoid parameter explosion when learning visual
patterns over time is to make use of recurrence across the time-axis. In this
article, we empirically study the cross-domain robustness of models with
different frame dependency modeling (recurrent, attention-based or 3D
convolutional). In order to enable a light-weight and systematic assessment of
the ability to capture temporal structure, not revealed from single frames, we
provide the Temporal Shape dataset. We find that when controlling for
performance and layer structure, convolutional-recurrent models show better
out-of-domain generalization ability on the Temporal Shape dataset than 3D
convolution- and attention-based models. Moreover, our experiments indicate
that convolution- and attention-based models exhibit more texture bias on
Diving48 than convolutional-recurrent models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01763">
<div class="article-summary-box-inner">
<span><p>Audio-based automatic speech recognition (ASR) degrades significantly in
noisy environments and is particularly vulnerable to interfering speech, as the
model cannot determine which speaker to transcribe. Audio-visual speech
recognition (AVSR) systems improve robustness by complementing the audio stream
with the visual information that is invariant to noise and helps the model
focus on the desired speaker. However, previous AVSR work focused solely on the
supervised learning setup; hence the progress was hindered by the amount of
labeled data available. In this work, we present a self-supervised AVSR
framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art
audio-visual speech representation learning model. On the largest available
AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by
~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in
the presence of babble noise, while reducing the WER of an audio-based model by
over 75% (25.8% vs. 5.8%) on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupling Makes Weakly Supervised Local Feature Better. (arXiv:2201.02861v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02861">
<div class="article-summary-box-inner">
<span><p>Weakly supervised learning can help local feature methods to overcome the
obstacle of acquiring a large-scale dataset with densely labeled
correspondences. However, since weak supervision cannot distinguish the losses
caused by the detection and description steps, directly conducting weakly
supervised learning within a joint describe-then-detect pipeline suffers
limited performance. In this paper, we propose a decoupled describe-then-detect
pipeline tailored for weakly supervised local feature learning. Within our
pipeline, the detection step is decoupled from the description step and
postponed until discriminative and robust descriptors are learned. In addition,
we introduce a line-to-window search strategy to explicitly use the camera pose
information for better descriptor learning. Extensive experiments show that our
method, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous
fully and weakly supervised methods and achieves state-of-the-art performance
on a wide range of downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A statistical shape model for radiation-free assessment and classification of craniosynostosis. (arXiv:2201.03288v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03288">
<div class="article-summary-box-inner">
<span><p>The assessment of craniofacial deformities requires patient data which is
sparsely available. Statistical shape models provide realistic and synthetic
data enabling comparisons of existing methods on a common dataset.
</p>
<p>We build the first publicly available statistical 3D head model of
craniosynostosis patients and the first model focusing on infants younger than
1.5 years. We further present a shape-model-based classification pipeline to
distinguish between three different classes of craniosynostosis and a control
group on photogrammetric surface scans. To the best of our knowledge, our study
uses the largest dataset of craniosynostosis patients in a classification study
for craniosynostosis and statistical shape modeling to date.
</p>
<p>We demonstrate that our shape model performs similar to other statistical
shape models of the human head. Craniosynostosis-specific pathologies are
represented in the first eigenmodes of the model. Regarding the automatic
classification of craniosynostis, our classification approach yields an
accuracy of 97.8%, comparable to other state-of-the-art methods using both
computed tomography scans and stereophotogrammetry.
</p>
<p>Our publicly available, craniosynostosis-specific statistical shape model
enables the assessment of craniosynostosis on realistic and synthetic data. We
further present a state-of-the-art shape-model-based classification approach
for a radiation-free diagnosis of craniosynostosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency Constrained Arbitrary Image Style Transfer using SIFT and DCNN. (arXiv:2201.05346v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05346">
<div class="article-summary-box-inner">
<span><p>This paper develops a new image synthesis approach to transfer an example
image (style image) to other images (content images) by using Deep
Convolutional Neural Networks (DCNN) model. When common neural style transfer
methods are used, the textures and colors in the style image are usually
transferred imperfectly to the content image, or some visible errors are
generated. This paper proposes a novel saliency constrained method to reduce or
avoid such effects. It first evaluates some existing saliency detection methods
to select the most suitable one for use in our method. The selected saliency
detection method is used to detect the object in the style image, corresponding
to the object of the content image with the same saliency. In addition, aim to
solve the problem that the size or resolution is different in the style image
and content, the scale-invariant feature transform is used to generate a series
of style images and content images which can be used to generate more feature
maps for patches matching. It then proposes a new loss function combining the
saliency loss, style loss and content loss, adding gradient of saliency
constraint into style transfer in iterations. Finally the source images and
saliency detection results are utilized as multichannel input to an improved
deep CNN framework for style transfer. The experiments show that the saliency
maps of source images can help find the correct matching and avoid artifacts.
Experimental results on different kind of images demonstrate that our method
outperforms nine representative methods from recent publications and has good
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STURE: Spatial-Temporal Mutual Representation Learning for Robust Data Association in Online Multi-Object Tracking. (arXiv:2201.06824v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06824">
<div class="article-summary-box-inner">
<span><p>Online multi-object tracking (MOT) is a longstanding task for computer vision
and intelligent vehicle platform. At present, the main paradigm is
tracking-by-detection, and the main difficulty of this paradigm is how to
associate current candidate detections with historical tracklets. However, in
the MOT scenarios, each historical tracklet is composed of an object sequence,
while each candidate detection is just a flat image, which lacks temporal
features of the object sequence. The feature difference between current
candidate detections and historical tracklets makes the object association much
harder. Therefore, we propose a Spatial-Temporal Mutual Representation Learning
(STURE) approach which learns spatial-temporal representations between current
candidate detections and historical sequences in a mutual representation space.
For historical trackelets, the detection learning network is forced to match
the representations of sequence learning network in a mutual representation
space. The proposed approach is capable of extracting more distinguishing
detection and sequence representations by using various designed losses in
object association. As a result, spatial-temporal feature is learned mutually
to reinforce the current detection features, and the feature difference can be
relieved. To prove the robustness of the STURE, it is applied to the public MOT
challenge benchmarks and performs well compared with various state-of-the-art
online MOT trackers based on identity-preserving metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CP-Net: Contour-Perturbed Reconstruction Network for Self-Supervised Point Cloud Learning. (arXiv:2201.08215v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08215">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has not been fully explored for point cloud
analysis. Current frameworks are mainly based on point cloud reconstruction.
Given only 3D coordinates, such approaches tend to learn local geometric
structures and contours, while failing in understanding high level semantic
content. Consequently, they achieve unsatisfactory performance in downstream
tasks such as classification, segmentation, etc. To fill this gap, we propose a
generic Contour-Perturbed Reconstruction Network (CP-Net), which can
effectively guide self-supervised reconstruction to learn semantic content in
the point cloud, and thus promote discriminative power of point cloud
representation. First, we introduce a concise contour-perturbed augmentation
module for point cloud reconstruction. With guidance of geometry disentangling,
we divide point cloud into contour and content components. Subsequently, we
perturb the contour components and preserve the content components on the point
cloud. As a result, self supervisor can effectively focus on semantic content,
by reconstructing the original point cloud from such perturbed one. Second, we
use this perturbed reconstruction as an assistant branch, to guide the learning
of basic reconstruction branch via a distinct dual-branch consistency loss. In
this case, our CP-Net not only captures structural contour but also learn
semantic content for discriminative downstream tasks. Finally, we perform
extensive experiments on a number of point cloud benchmarks. Part segmentation
results demonstrate that our CP-Net (81.5% of mIoU) outperforms the previous
self-supervised models, and narrows the gap with the fully-supervised methods.
For classification, we get a competitive result with the fully-supervised
methods on ModelNet40 (92.5% accuracy) and ScanObjectNN (87.9% accuracy). The
codes and models will be released afterwards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Global Diversity and Local Context for Video Summarization. (arXiv:2201.11345v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11345">
<div class="article-summary-box-inner">
<span><p>Video summarization aims to automatically generate a diverse and concise
summary which is useful in large-scale video processing. Most of the methods
tend to adopt self-attention mechanism across video frames, which fails to
model the diversity of video frames. To alleviate this problem, we revisit the
pairwise similarity measurement in self-attention mechanism and find that the
existing inner-product affinity leads to discriminative features rather than
diversified features. In light of this phenomenon, we propose global diverse
attention which uses the squared Euclidean distance instead to compute the
affinities. Moreover, we model the local contextual information by novel local
contextual attention to remove the redundancy in the video. By combining these
two attention mechanisms, a video SUMmarization model with Diversified
Contextual Attention scheme is developed, namely SUM-DCA. Extensive experiments
are conducted on benchmark data sets to verify the effectiveness and the
superiority of SUM-DCA in terms of F-score and rank-based evaluation without
any bells and whistles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures. (arXiv:2201.12763v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12763">
<div class="article-summary-box-inner">
<span><p>We introduce RIM-Net, a neural network which learns recursive implicit fields
for unsupervised inference of hierarchical shape structures. Our network
recursively decomposes an input 3D shape into two parts, resulting in a binary
tree hierarchy. Each level of the tree corresponds to an assembly of shape
parts, represented as implicit functions, to reconstruct the input shape. At
each node of the tree, simultaneous feature decoding and shape decomposition
are carried out by their respective feature and part decoders, with weight
sharing across the same hierarchy level. As an implicit field decoder, the part
decoder is designed to decompose a sub-shape, via a two-way branched
reconstruction, where each branch predicts a set of parameters defining a
Gaussian to serve as a local point distribution for shape reconstruction. With
reconstruction losses accounted for at each hierarchy level and a decomposition
loss at each node, our network training does not require any ground-truth
segmentations, let alone hierarchies. Through extensive experiments and
comparisons to state-of-the-art alternatives, we demonstrate the quality,
consistency, and interpretability of hierarchical structural inference by
RIM-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label Fusion. (arXiv:2202.02000v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02000">
<div class="article-summary-box-inner">
<span><p>Multi-atlas segmentation (MAS) is a promising framework for medical image
segmentation. Generally, MAS methods register multiple atlases, i.e., medical
images with corresponding labels, to a target image; and the transformed atlas
labels can be combined to generate target segmentation via label fusion
schemes. Many conventional MAS methods employed the atlases from the same
modality as the target image. However, the number of atlases with the same
modality may be limited or even missing in many clinical applications. Besides,
conventional MAS methods suffer from the computational burden of registration
or label fusion procedures. In this work, we design a novel cross-modality MAS
framework, which uses available atlases from a certain modality to segment a
target image from another modality. To boost the computational efficiency of
the framework, both the image registration and label fusion are achieved by
well-designed deep neural networks. For the atlas-to-target image registration,
we propose a bi-directional registration network (BiRegNet), which can
efficiently align images from different modalities. For the label fusion, we
design a similarity estimation network (SimNet), which estimates the fusion
weight of each atlas by measuring its similarity to the target image. SimNet
can learn multi-scale information for similarity estimation to improve the
performance of label fusion. The proposed framework was evaluated by the left
ventricle and liver segmentation tasks on the MM-WHS and CHAOS datasets,
respectively. Results have shown that the framework is effective for
cross-modality MAS in both registration and label fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Message Passing Neural PDE Solvers. (arXiv:2202.03376v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03376">
<div class="article-summary-box-inner">
<span><p>The numerical solution of partial differential equations (PDEs) is difficult,
having led to a century of research so far. Recently, there have been pushes to
build neural--numerical hybrid solvers, which piggy-backs the modern trend
towards fully end-to-end learned systems. Most works so far can only generalize
over a subset of properties to which a generic solver would be faced,
including: resolution, topology, geometry, boundary conditions, domain
discretization regularity, dimensionality, etc. In this work, we build a
solver, satisfying these properties, where all the components are based on
neural message passing, replacing all heuristically designed components in the
computation graph with backprop-optimized neural function approximators. We
show that neural message passing solvers representationally contain some
classical methods, such as finite differences, finite volumes, and WENO
schemes. In order to encourage stability in training autoregressive models, we
put forward a method that is based on the principle of zero-stability, posing
stability as a domain adaptation problem. We validate our method on various
fluid-like flow problems, demonstrating fast, stable, and accurate performance
across different domain topologies, discretization, etc. in 1D and 2D. Our
model outperforms state-of-the-art numerical solvers in the low resolution
regime in terms of speed and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07028">
<div class="article-summary-box-inner">
<span><p>We study the problem of developing autonomous agents that can follow human
instructions to infer and perform a sequence of actions to complete the
underlying task. Significant progress has been made in recent years, especially
for tasks with short horizons. However, when it comes to long-horizon tasks
with extended sequences of actions, an agent can easily ignore some
instructions or get stuck in the middle of the long instructions and eventually
fail the task. To address this challenge, we propose a model-agnostic
milestone-based task tracker (M-TRACK) to guide the agent and monitor its
progress. Specifically, we propose a milestone builder that tags the
instructions with navigation and interaction milestones which the agent needs
to complete step by step, and a milestone checker that systemically checks the
agent's progress in its current milestone and determines when to proceed to the
next. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and
52% relative improvement in unseen success rate over two competitive base
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering. (arXiv:2202.09277v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09277">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal scene-graph approaches to video-based reasoning tasks, such
as video question-answering (QA), typically construct such graphs for every
video frame. These approaches often ignore the fact that videos are essentially
sequences of 2D "views" of events happening in a 3D space, and that the
semantics of the 3D scene can thus be carried over from frame to frame.
Leveraging this insight, we propose a (2.5+1)D scene graph representation to
better capture the spatio-temporal information flows inside the videos.
Specifically, we first create a 2.5D (pseudo-3D) scene graph by transforming
every 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D
transformation module, following which we register the video frames into a
shared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it.
Such a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic
sub-graph, corresponding to whether the objects within them usually move in the
world. The nodes in the dynamic graph are enriched with motion features
capturing their interactions with other graph nodes. Next, for the video QA
task, we present a novel transformer-based reasoning pipeline that embeds the
(2.5+1)D graph into a spatio-temporal hierarchical latent space, where the
sub-graphs and their interactions are captured at varied granularity. To
demonstrate the effectiveness of our approach, we present experiments on the
NExT-QA and AVSD-QA datasets. Our results show that our proposed (2.5+1)D
representation leads to faster training and inference, while our hierarchical
model showcases superior performance on the video QA task versus the state of
the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSSNet: Multi-Scale-Stage Network for Single Image Deblurring. (arXiv:2202.09652v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09652">
<div class="article-summary-box-inner">
<span><p>Most of traditional single image deblurring methods before deep learning
adopt a coarse-to-fine scheme that estimates a sharp image at a coarse scale
and progressively refines it at finer scales. While this scheme has also been
adopted to several deep learning-based approaches, recently a number of
single-scale approaches have been introduced showing superior performance to
previous coarse-to-fine approaches both in quality and computation time. In
this paper, we revisit the coarse-to-fine scheme, and analyze defects of
previous coarse-to-fine approaches that degrade their performance. Based on the
analysis, we propose Multi-Scale-Stage Network (MSSNet), a novel deep
learning-based approach to single image deblurring that adopts our remedies to
the defects. Specifically, MSSNet adopts three novel technical components:
stage configuration reflecting blur scales, an inter-scale information
propagation scheme, and a pixel-shuffle-based multi-scale scheme. Our
experiments show that MSSNet achieves the state-of-the-art performance in terms
of quality, network size, and computation time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Self-Supervised Descriptor for Image Copy Detection. (arXiv:2202.10261v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10261">
<div class="article-summary-box-inner">
<span><p>Image copy detection is an important task for content moderation. We
introduce SSCD, a model that builds on a recent self-supervised contrastive
training objective. We adapt this method to the copy detection task by changing
the architecture and training objective, including a pooling operator from the
instance matching literature, and adapting contrastive learning to
augmentations that combine images.
</p>
<p>Our approach relies on an entropy regularization term, promoting consistent
separation between descriptor vectors, and we demonstrate that this
significantly improves copy detection accuracy. Our method produces a compact
descriptor vector, suitable for real-world web scale applications. Statistical
information from a background image distribution can be incorporated into the
descriptor.
</p>
<p>On the recent DISC2021 benchmark, SSCD is shown to outperform both baseline
copy detection models and self-supervised architectures designed for image
classification by huge margins, in all settings. For example, SSCD out-performs
SimCLR descriptors by 48% absolute. Code is available at
https://github.com/facebookresearch/sscd-copy-detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Bulk Motion Artifact Removal in Optical Coherence Tomography Angiography. (arXiv:2202.10360v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10360">
<div class="article-summary-box-inner">
<span><p>Optical coherence tomography angiography (OCTA) is an important imaging
modality in many bioengineering tasks. The image quality of OCTA, however, is
often degraded by Bulk Motion Artifacts (BMA), which are due to micromotion of
subjects and typically appear as bright stripes surrounded by blurred areas.
State-of-the-art methods usually treat BMA removal as a learning-based image
inpainting problem, but require numerous training samples with nontrivial
annotation. In addition, these methods discard the rich structural and
appearance information carried in the BMA stripe region. To address these
issues, in this paper we propose a self-supervised content-aware BMA removal
model. First, the gradient-based structural information and appearance feature
are extracted from the BMA area and injected into the model to capture more
connectivity. Second, with easily collected defective masks, the model is
trained in a self-supervised manner, in which only the clear areas are used for
training while the BMA areas for inference. With the structural information and
appearance feature from noisy image as references, our model can remove larger
BMA and produce better visualizing result. In addition, only 2D images with
defective masks are involved, hence improving the efficiency of our method.
Experiments on OCTA of mouse cortex demonstrate that our model can remove most
BMA with extremely large sizes and inconsistent intensities while previous
methods fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Pre-Training with Triple Contrastive Learning. (arXiv:2202.10401v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10401">
<div class="article-summary-box-inner">
<span><p>Vision-language representation learning largely benefits from image-text
alignment through contrastive losses (e.g., InfoNCE loss). The success of this
alignment strategy is attributed to its capability in maximizing the mutual
information (MI) between an image and its matched text. However, simply
performing cross-modal alignment (CMA) ignores data potential within each
modality, which may result in degraded representations. For instance, although
CMA-based models are able to map image-text pairs close together in the
embedding space, they fail to ensure that similar inputs from the same modality
stay close by. This problem can get even worse when the pre-training data is
noisy. In this paper, we propose triple contrastive learning (TCL) for
vision-language pre-training by leveraging both cross-modal and intra-modal
self-supervision. Besides CMA, TCL introduces an intra-modal contrastive
objective to provide complementary benefits in representation learning. To take
advantage of localized and structural information from image and text input,
TCL further maximizes the average MI between local regions of image/text and
their global summary. To the best of our knowledge, ours is the first work that
takes into account local structure information for multi-modality
representation learning. Experimental evaluations show that our approach is
competitive and achieves the new state of the art on various common down-stream
vision-language tasks such as image-text retrieval and visual question
answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction. (arXiv:2202.11884v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11884">
<div class="article-summary-box-inner">
<span><p>Predicting future motions of road participants is an important task for
driving autonomously in urban scenes. Existing models excel at predicting
marginal trajectories for single agents, yet it remains an open question to
jointly predict scene compliant trajectories over multiple agents. The
challenge is due to exponentially increasing prediction space as a function of
the number of agents. In this work, we exploit the underlying relations between
interacting agents and decouple the joint prediction problem into marginal
prediction problems. Our proposed approach M2I first classifies interacting
agents as pairs of influencers and reactors, and then leverages a marginal
prediction model and a conditional prediction model to predict trajectories for
the influencers and reactors, respectively. The predictions from interacting
agents are combined and selected according to their joint likelihoods.
Experiments show that our simple but effective approach achieves
state-of-the-art performance on the Waymo Open Motion Dataset interactive
prediction benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralHOFusion: Neural Volumetric Rendering under Human-object Interactions. (arXiv:2202.12825v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12825">
<div class="article-summary-box-inner">
<span><p>4D modeling of human-object interactions is critical for numerous
applications. However, efficient volumetric capture and rendering of complex
interaction scenarios, especially from sparse inputs, remain challenging. In
this paper, we propose NeuralHOFusion, a neural approach for volumetric
human-object capture and rendering using sparse consumer RGBD sensors. It
marries traditional non-rigid fusion with recent neural implicit modeling and
blending advances, where the captured humans and objects are layerwise
disentangled. For geometry modeling, we propose a neural implicit inference
scheme with non-rigid key-volume fusion, as well as a template-aid robust
object tracking pipeline. Our scheme enables detailed and complete geometry
generation under complex interactions and occlusions. Moreover, we introduce a
layer-wise human-object texture rendering scheme, which combines volumetric and
image-based rendering in both spatial and temporal domains to obtain
photo-realistic results. Extensive experiments demonstrate the effectiveness
and efficiency of our approach in synthesizing photo-realistic free-view
results under complex human-object interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Deep Multi-View Photometric Stereo. (arXiv:2202.13071v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13071">
<div class="article-summary-box-inner">
<span><p>This paper presents a simple and effective solution to the longstanding
classical multi-view photometric stereo (MVPS) problem. It is well-known that
photometric stereo (PS) is excellent at recovering high-frequency surface
details, whereas multi-view stereo (MVS) can help remove the low-frequency
distortion due to PS and retain the global geometry of the shape. This paper
proposes an approach that can effectively utilize such complementary strengths
of PS and MVS. Our key idea is to combine them suitably while considering the
per-pixel uncertainty of their estimates. To this end, we estimate per-pixel
surface normals and depth using an uncertainty-aware deep-PS network and
deep-MVS network, respectively. Uncertainty modeling helps select reliable
surface normal and depth estimates at each pixel which then act as a true
representative of the dense surface geometry. At each pixel, our approach
either selects or discards deep-PS and deep-MVS network prediction depending on
the prediction uncertainty measure. For dense, detailed, and precise inference
of the object's surface profile, we propose to learn the implicit neural shape
representation via a multilayer perceptron (MLP). Our approach encourages the
MLP to converge to a natural zero-level set surface using the confident
prediction from deep-PS and deep-MVS networks, providing superior dense surface
reconstruction. Extensive experiments on the DiLiGenT-MV benchmark dataset show
that our method provides high-quality shape recovery with a much lower memory
footprint while outperforming almost all of the existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Alignment using Representation Codebook. (arXiv:2203.00048v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00048">
<div class="article-summary-box-inner">
<span><p>Aligning signals from different modalities is an important step in
vision-language representation learning as it affects the performance of later
stages such as cross-modality fusion. Since image and text typically reside in
different regions of the feature space, directly aligning them at instance
level is challenging especially when features are still evolving during
training. In this paper, we propose to align at a higher and more stable level
using cluster representation. Specifically, we treat image and text as two
"views" of the same entity, and encode them into a joint vision-language coding
space spanned by a dictionary of cluster centers (codebook). We contrast
positive and negative samples via their cluster assignments while
simultaneously optimizing the cluster centers. To further smooth out the
learning process, we adopt a teacher-student distillation paradigm, where the
momentum teacher of one view guides the student learning of the other. We
evaluated our approach on common vision language benchmarks and obtain new SoTA
on zero-shot cross modality retrieval while being competitive on various other
transfer tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Person Re-Identification via Self-Supervised Batch Norm Test-Time Adaption. (arXiv:2203.00672v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00672">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the generalization problem of person
re-identification (re-id), whose major challenge is the distribution shift on
an unseen domain. As an important tool of regularizing the distribution, batch
normalization (BN) has been widely used in existing methods. However, they
neglect that BN is severely biased to the training domain and inevitably
suffers the performance drop if directly generalized without being updated. To
tackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel
re-id framework that applies the self-supervised strategy to update BN
parameters adaptively. Specifically, BNTA quickly explores the domain-aware
information within unlabeled target data before inference, and accordingly
modulates the feature distribution normalized by BN to adapt to the target
domain. This is accomplished by two designed self-supervised auxiliary tasks,
namely part positioning and part nearest neighbor matching, which help the
model mine the domain-aware information with respect to the structure and
identity of body parts, respectively. To demonstrate the effectiveness of our
method, we conduct extensive experiments on three re-id datasets and confirm
the superior performance to the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">There is a Time and Place for Reasoning Beyond the Image. (arXiv:2203.00758v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00758">
<div class="article-summary-box-inner">
<span><p>Images are often more significant than only the pixels to human eyes, as we
can infer, associate, and reason with contextual information from other sources
to establish a more complete picture. For example, in Figure 1, we can find a
way to identify the news articles related to the picture through segment-wise
understandings of the signs, the buildings, the crowds, and more. This
reasoning could provide the time and place the image was taken, which will help
us in subsequent tasks, such as automatic storyline construction, correction of
image source in intended effect photographs, and upper-stream processing such
as image clustering for certain location or time.
</p>
<p>In this work, we formulate this problem and introduce TARA: a dataset with
16k images with their associated news, time, and location, automatically
extracted from New York Times, and an additional 61k examples as distant
supervision from WIT. On top of the extractions, we present a crowdsourced
subset in which we believe it is possible to find the images' spatio-temporal
information for evaluation purpose. We show that there exists a $70\%$ gap
between a state-of-the-art joint model and human performance, which is slightly
filled by our proposed model that uses segment-wise reasoning, motivating
higher-level vision-language joint models that can conduct open-ended reasoning
with world knowledge. The data and code are publicly available at
https://github.com/zeyofu/TARA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00859">
<div class="article-summary-box-inner">
<span><p>Recent transformer-based solutions have been introduced to estimate 3D human
pose from 2D keypoint sequence by considering body joints among all frames
globally to learn spatio-temporal correlation. We observe that the motions of
different joints differ significantly. However, the previous methods cannot
efficiently model the solid inter-frame correspondence of each joint, leading
to insufficient learning of spatial-temporal correlation. We propose MixSTE
(Mixed Spatio-Temporal Encoder), which has a temporal transformer block to
separately model the temporal motion of each joint and a spatial transformer
block to learn inter-joint spatial correlation. These two blocks are utilized
alternately to obtain better spatio-temporal feature encoding. In addition, the
network output is extended from the central frame to entire frames of the input
video, thereby improving the coherence between the input and output sequences.
Extensive experiments are conducted on three benchmarks (Human3.6M,
MPI-INF-3DHP, and HumanEva). The results show that our model outperforms the
state-of-the-art approach by 10.9% P-MPJPE and 7.6% MPJPE. The code is
available at https://github.com/JinluZhang1126/MixSTE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAFE: Learning to Condense Dataset by Aligning Features. (arXiv:2203.01531v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01531">
<div class="article-summary-box-inner">
<span><p>Dataset condensation aims at reducing the network training effort through
condensing a cumbersome training set into a compact synthetic one.
State-of-the-art approaches largely rely on learning the synthetic data by
matching the gradients between the real and synthetic data batches. Despite the
intuitive motivation and promising results, such gradient-based methods, by
nature, easily overfit to a biased set of samples that produce dominant
gradients, and thus lack global supervision of data distribution. In this
paper, we propose a novel scheme to Condense dataset by Aligning FEatures
(CAFE), which explicitly attempts to preserve the real-feature distribution as
well as the discriminant power of the resulting synthetic set, lending itself
to strong generalization capability to various architectures. At the heart of
our approach is an effective strategy to align features from the real and
synthetic data across various scales, while accounting for the classification
of real samples. Our scheme is further backed up by a novel dynamic bi-level
optimization, which adaptively adjusts parameter updates to prevent
over-/under-fitting. We validate the proposed CAFE across various datasets, and
demonstrate that it generally outperforms the state of the art: on the SVHN
dataset, for example, the performance gain is up to 11%. Extensive experiments
and analyses verify the effectiveness and necessity of proposed designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntax-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2203.01601v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01601">
<div class="article-summary-box-inner">
<span><p>Handwritten mathematical expression recognition (HMER) is a challenging task
that has many potential applications. Recent methods for HMER have achieved
outstanding performance with an encoder-decoder architecture. However, these
methods adhere to the paradigm that the prediction is made "from one character
to another", which inevitably yields prediction errors due to the complicated
structures of mathematical expressions or crabbed handwritings. In this paper,
we propose a simple and efficient method for HMER, which is the first to
incorporate syntax information into an encoder-decoder network. Specifically,
we present a set of grammar rules for converting the LaTeX markup sequence of
each expression into a parsing tree; then, we model the markup sequence
prediction as a tree traverse process with a deep neural network. In this way,
the proposed method can effectively describe the syntax context of expressions,
alleviating the structure prediction errors of HMER. Experiments on three
benchmark datasets demonstrate that our method achieves better recognition
performance than prior arts. To further validate the effectiveness of our
method, we create a large-scale dataset consisting of 100k handwritten
mathematical expression images acquired from ten thousand writers. The source
code, new dataset, and pre-trained models of this work will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TCTrack: Temporal Contexts for Aerial Tracking. (arXiv:2203.01885v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01885">
<div class="article-summary-box-inner">
<span><p>Temporal contexts among consecutive frames are far from being fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Image Synthesis with Panoptic Layout Generation. (arXiv:2203.02104v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02104">
<div class="article-summary-box-inner">
<span><p>Interactive image synthesis from user-guided input is a challenging task when
users wish to control the scene structure of a generated image with
ease.Although remarkable progress has been made on layout-based image synthesis
approaches, in order to get realistic fake image in interactive scene, existing
methods require high-precision inputs, which probably need adjustment several
times and are unfriendly to novice users. When placement of bounding boxes is
subject to perturbation, layout-based models suffer from "missing regions" in
the constructed semantic layouts and hence undesirable artifacts in the
generated images. In this work, we propose Panoptic Layout Generative
Adversarial Networks (PLGAN) to address this challenge. The PLGAN employs
panoptic theory which distinguishes object categories between "stuff" with
amorphous boundaries and "things" with well-defined shapes, such that stuff and
instance layouts are constructed through separate branches and later fused into
panoptic layouts. In particular, the stuff layouts can take amorphous shapes
and fill up the missing regions left out by the instance layouts. We
experimentally compare our PLGAN with state-of-the-art layout-based models on
the COCO-Stuff, Visual Genome, and Landscape datasets. The advantages of PLGAN
are not only visually demonstrated but quantitatively verified in terms of
inception score, Fr\'echet inception distance, classification accuracy score,
and coverage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02503">
<div class="article-summary-box-inner">
<span><p>Pansharpening aims to fuse a registered high-resolution panchromatic image
(PAN) with a low-resolution hyperspectral image (LR-HSI) to generate an
enhanced HSI with high spectral and spatial resolution. Existing pansharpening
approaches neglect using an attention mechanism to transfer HR texture features
from PAN to LR-HSI features, resulting in spatial and spectral distortions. In
this paper, we present a novel attention mechanism for pansharpening called
HyperTransformer, in which features of LR-HSI and PAN are formulated as queries
and keys in a transformer, respectively. HyperTransformer consists of three
main modules, namely two separate feature extractors for PAN and HSI, a
multi-head feature soft attention module, and a spatial-spectral feature fusion
module. Such a network improves both spatial and spectral quality measures of
the pansharpened HSI by learning cross-feature space dependencies and
long-range details of PAN and LR-HSI. Furthermore, HyperTransformer can be
utilized across multiple spatial scales at the backbone for obtaining improved
performance. Extensive experiments conducted on three widely used datasets
demonstrate that HyperTransformer achieves significant improvement over the
state-of-the-art methods on both spatial and spectral quality measures.
Implementation code and pre-trained weights can be accessed at
https://github.com/wgcban/HyperTransformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Applications in Lung Cancer Diagnosis, Treatment and Prognosis. (arXiv:2203.02794v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02794">
<div class="article-summary-box-inner">
<span><p>The recent development of imaging and sequencing technologies enables
systematic advances in the clinical study of lung cancer. Meanwhile, the human
mind is limited in effectively handling and fully utilizing the accumulation of
such enormous amounts of data. Machine learning-based approaches play a
critical role in integrating and analyzing these large and complex datasets,
which have extensively characterized lung cancer through the use of different
perspectives from these accrued data. In this article, we provide an overview
of machine learning-based approaches that strengthen the varying aspects of
lung cancer diagnosis and therapy, including early detection, auxiliary
diagnosis, prognosis prediction and immunotherapy practice. Moreover, we
highlight the challenges and opportunities for future applications of machine
learning in lung cancer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos. (arXiv:2203.03014v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03014">
<div class="article-summary-box-inner">
<span><p>With the assumption that a video dataset is multimodality annotated in which
auditory and visual modalities both are labeled or class-relevant, current
multimodal methods apply modality fusion or cross-modality attention. However,
effectively leveraging the audio modality in vision-specific annotated videos
for action recognition is of particular challenge. To tackle this challenge, we
propose a novel audio-visual framework that effectively leverages the audio
modality in any solely vision-specific annotated dataset. We adopt the language
models (e.g., BERT) to build a semantic audio-video label dictionary (SAVLD)
that maps each video label to its most K-relevant audio labels in which SAVLD
serves as a bridge between audio and video datasets. Then, SAVLD along with a
pretrained audio multi-label model are used to estimate the audio-visual
modality relevance during the training phase. Accordingly, a novel learnable
irrelevant modality dropout (IMD) is proposed to completely drop out the
irrelevant audio modality and fuse only the relevant modalities. Moreover, we
present a new two-stream video Transformer for efficiently modeling the visual
modalities. Results on several vision-specific annotated datasets including
Kinetics400 and UCF-101 validated our framework as it outperforms most relevant
action recognition methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPPF: Towards Robust Category-Level 9D Pose Estimation in the Wild. (arXiv:2203.03089v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03089">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the problem of category-level 9D pose estimation in
the wild, given a single RGB-D frame. Using supervised data of real-world 9D
poses is tedious and erroneous, and also fails to generalize to unseen
scenarios. Besides, category-level pose estimation requires a method to be able
to generalize to unseen objects at test time, which is also challenging.
Drawing inspirations from traditional point pair features (PPFs), in this
paper, we design a novel Category-level PPF (CPPF) voting method to achieve
accurate, robust and generalizable 9D pose estimation in the wild. To obtain
robust pose estimation, we sample numerous point pairs on an object, and for
each pair our model predicts necessary SE(3)-invariant voting statistics on
object centers, orientations and scales. A novel coarse-to-fine voting
algorithm is proposed to eliminate noisy point pair samples and generate final
predictions from the population. To get rid of false positives in the
orientation voting process, an auxiliary binary disambiguating classification
task is introduced for each sampled point pair. In order to detect objects in
the wild, we carefully design our sim-to-real pipeline by training on synthetic
point clouds only, unless objects have ambiguous poses in geometry. Under this
circumstance, color information is leveraged to disambiguate these poses.
Results on standard benchmarks show that our method is on par with current
state of the arts with real-world training data. Extensive experiments further
show that our method is robust to noise and gives promising results under
extremely challenging scenarios. Our code is available on
https://github.com/qq456cvb/CPPF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer. (arXiv:2203.03121v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03121">
<div class="article-summary-box-inner">
<span><p>While deep face recognition (FR) systems have shown amazing performance in
identification and verification, they also arouse privacy concerns for their
excessive surveillance on users, especially for public face images widely
spread on social networks. Recently, some studies adopt adversarial examples to
protect photos from being identified by unauthorized face recognition systems.
However, existing methods of generating adversarial face images suffer from
many limitations, such as awkward visual, white-box setting, weak
transferability, making them difficult to be applied to protect face privacy in
reality. In this paper, we propose adversarial makeup transfer GAN (AMT-GAN), a
novel face protection method aiming at constructing adversarial face images
that preserve stronger black-box transferability and better visual quality
simultaneously. AMT-GAN leverages generative adversarial networks (GAN) to
synthesize adversarial face images with makeup transferred from reference
images. In particular, we introduce a new regularization module along with a
joint training strategy to reconcile the conflicts between the adversarial
noises and the cycle consistence loss in makeup transfer, achieving a desirable
balance between the attack strength and visual changes. Extensive experiments
verify that compared with state of the arts, AMT-GAN can not only preserve a
comfortable visual quality, but also achieve a higher attack success rate over
commercial FR APIs, including Face++, Aliyun, and Microsoft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Semi-Supervised Learning for Video Action Detection. (arXiv:2203.04251v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04251">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on semi-supervised learning for video action detection
which utilizes both labeled as well as unlabeled data. We propose a simple
end-to-end consistency based approach which effectively utilizes the unlabeled
data. Video action detection requires both, action class prediction as well as
a spatio-temporal localization of actions. Therefore, we investigate two types
of constraints, classification consistency, and spatio-temporal consistency.
The presence of predominant background and static regions in a video makes it
challenging to utilize spatio-temporal consistency for action detection. To
address this, we propose two novel regularization constraints for
spatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness.
Both these aspects exploit the temporal continuity of action in videos and are
found to be effective for utilizing unlabeled videos for action detection. We
demonstrate the effectiveness of the proposed approach on two different action
detection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show
the effectiveness of the proposed approach for video object segmentation on the
Youtube-VOS which demonstrates its generalization capability The proposed
approach achieves competitive performance by using merely 20% of annotations on
UCF101-24 when compared with recent fully supervised methods. On UCF101-24, it
improves the score by +8.9% and +11% at 0.5 f-mAP and v-mAP respectively,
compared to supervised approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Trajectory Prediction via Transferable GNN. (arXiv:2203.05046v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05046">
<div class="article-summary-box-inner">
<span><p>Pedestrian trajectory prediction is an essential component in a wide range of
AI applications such as autonomous driving and robotics. Existing methods
usually assume the training and testing motions follow the same pattern while
ignoring the potential distribution differences (e.g., shopping mall and
street). This issue results in inevitable performance decrease. To address this
issue, we propose a novel Transferable Graph Neural Network (T-GNN) framework,
which jointly conducts trajectory prediction as well as domain alignment in a
unified framework. Specifically, a domain-invariant GNN is proposed to explore
the structural motion knowledge where the domain-specific knowledge is reduced.
Moreover, an attention-based adaptive knowledge learning module is further
proposed to explore fine-grained individual-level feature representations for
knowledge transfer. By this way, disparities across different trajectory
domains will be better alleviated. More challenging while practical trajectory
prediction experiments are designed, and the experimental results verify the
superior performance of our proposed model. To the best of our knowledge, our
work is the pioneer which fills the gap in benchmarks and techniques for
practical pedestrian trajectory prediction across different domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (arXiv:2203.05154v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05154">
<div class="article-summary-box-inner">
<span><p>Defense models against adversarial attacks have grown significantly, but the
lack of practical evaluation methods has hindered progress. Evaluation can be
defined as looking for defense models' lower bound of robustness given a budget
number of iterations and a test dataset. A practical evaluation method should
be convenient (i.e., parameter-free), efficient (i.e., fewer iterations) and
reliable (i.e., approaching the lower bound of robustness). Towards this
target, we propose a parameter-free Adaptive Auto Attack (A$^3$) evaluation
method which addresses the efficiency and reliability in a test-time-training
fashion. Specifically, by observing that adversarial examples to a specific
defense model follow some regularities in their starting points, we design an
Adaptive Direction Initialization strategy to speed up the evaluation.
Furthermore, to approach the lower bound of robustness under the budget number
of iterations, we propose an online statistics-based discarding strategy that
automatically identifies and abandons hard-to-attack images. Extensive
experiments demonstrate the effectiveness of our A$^3$. Particularly, we apply
A$^3$ to nearly 50 widely-used defense models. By consuming much fewer
iterations than existing methods, i.e., $1/10$ on average (10$\times$ speed
up), we achieve lower robust accuracy in all cases. Notably, we won
$\textbf{first place}$ out of 1681 teams in CVPR 2021 White-box Adversarial
Attacks on Defense Models competitions with this method. Code is available at:
$\href{https://github.com/liuye6666/adaptive_auto_attack}{https://github.com/liuye6666/adaptive\_auto\_attack}$
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability. (arXiv:2203.05180v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05180">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-training has been proven to be crucial for various computer
vision tasks. However, with the increase of pre-training data amount, model
architecture amount, and the private/inaccessible data, it is not very
efficient or possible to pre-train all the model architectures on large-scale
datasets. In this work, we investigate an alternative strategy for
pre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP),
aiming to efficiently transfer the learned feature representation from existing
pre-trained models to new student models for future downstream tasks. We
observe that existing Knowledge Distillation (KD) methods are unsuitable
towards pre-training since they normally distill the logits that are going to
be discarded when transferred to downstream tasks. To resolve this problem, we
propose a feature-based KD method with non-parametric feature dimension
aligning. Notably, our method performs comparably with supervised pre-training
counterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less
data and 5x less pre-training time. Code is available at
https://github.com/CVMI-Lab/KDEP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement. (arXiv:2203.05238v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05238">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a weakly-supervised approach for 3D object
detection, which makes it possible to train a strong 3D detector with
position-level annotations (i.e. annotations of object centers). In order to
remedy the information loss from box annotations to centers, our method, namely
Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak
labels into fully-annotated virtual scenes as stronger supervision, and in turn
utilizes the perfect virtual labels to complement and refine the real labels.
Specifically, we first assemble 3D shapes into physically reasonable virtual
scenes according to the coarse scene layout extracted from position-level
annotations. Then we go back to reality by applying a virtual-to-real domain
adaptation method, which refine the weak labels and additionally supervise the
training of detector with the virtual scenes. Furthermore, we propose a more
challenging benckmark for indoor 3D object detection with more diversity in
object sizes to better show the potential of BR. With less than 5% of the
labeling labor, we achieve comparable detection performance with some popular
fully-supervised approaches on the widely used ScanNet dataset. Code is
available at: https://github.com/wyf-ACCEPT/BackToReality
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Font Shape-to-Impression Translation. (arXiv:2203.05808v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05808">
<div class="article-summary-box-inner">
<span><p>Different fonts have different impressions, such as elegant, scary, and cool.
This paper tackles part-based shape-impression analysis based on the
Transformer architecture, which is able to handle the correlation among local
parts by its self-attention mechanism. This ability will reveal how
combinations of local parts realize a specific impression of a font. The
versatility of Transformer allows us to realize two very different approaches
for the analysis, i.e., multi-label classification and translation. A
quantitative evaluation shows that our Transformer-based approaches estimate
the font impressions from a set of local parts more accurately than other
approaches. A qualitative evaluation then indicates the important local parts
for a specific impression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection. (arXiv:2203.06398v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06398">
<div class="article-summary-box-inner">
<span><p>Domain Adaptive Object Detection (DAOD) leverages a labeled domain to learn
an object detector generalizing to a novel domain free of annotations. Recent
advances align class-conditional distributions by narrowing down cross-domain
prototypes (class centers). Though great success,they ignore the significant
within-class variance and the domain-mismatched semantics within the training
batch, leading to a sub-optimal adaptation. To overcome these challenges, we
propose a novel SemantIc-complete Graph MAtching (SIGMA) framework for DAOD,
which completes mismatched semantics and reformulates the adaptation with graph
matching. Specifically, we design a Graph-embedded Semantic Completion module
(GSC) that completes mismatched semantics through generating hallucination
graph nodes in missing categories. Then, we establish cross-image graphs to
model class-conditional distributions and learn a graph-guided memory bank for
better semantic completion in turn. After representing the source and target
data as graphs, we reformulate the adaptation as a graph matching problem,
i.e., finding well-matched node pairs across graphs to reduce the domain gap,
which is solved with a novel Bipartite Graph Matching adaptor (BGM). In a
nutshell, we utilize graph nodes to establish semantic-aware node affinity and
leverage graph edges as quadratic constraints in a structure-aware matching
loss, achieving fine-grained adaptation with a node-to-node graph matching.
Extensive experiments verify that SIGMA outperforms existing works
significantly. Our code is available at
https://github.com/CityU-AIM-Group/SIGMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning. (arXiv:2203.06541v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06541">
<div class="article-summary-box-inner">
<span><p>Heatmap regression methods have dominated face alignment area in recent years
while they ignore the inherent relation between different landmarks. In this
paper, we propose a Sparse Local Patch Transformer (SLPT) for learning the
inherent relation. The SLPT generates the representation of each single
landmark from a local patch and aggregates them by an adaptive inherent
relation based on the attention mechanism. The subpixel coordinate of each
landmark is predicted independently based on the aggregated feature. Moreover,
a coarse-to-fine framework is further introduced to incorporate with the SLPT,
which enables the initial landmarks to gradually converge to the target facial
landmarks using fine-grained features from dynamically resized local patches.
Extensive experiments carried out on three popular benchmarks, including WFLW,
300W and COFW, demonstrate that the proposed method works at the
state-of-the-art level with much less computational complexity by learning the
inherent relation between facial landmarks. The code is available at the
project website.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Autoencoders for Point Cloud Self-supervised Learning. (arXiv:2203.06604v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06604">
<div class="article-summary-box-inner">
<span><p>As a promising scheme of self-supervised learning, masked autoencoding has
significantly advanced natural language processing and computer vision.
Inspired by this, we propose a neat scheme of masked autoencoders for point
cloud self-supervised learning, addressing the challenges posed by point
cloud's properties, including leakage of location information and uneven
information density. Concretely, we divide the input point cloud into irregular
point patches and randomly mask them at a high ratio. Then, a standard
Transformer based autoencoder, with an asymmetric design and a shifting mask
tokens operation, learns high-level latent features from unmasked point
patches, aiming to reconstruct the masked point patches. Extensive experiments
show that our approach is efficient during pre-training and generalizes well on
various downstream tasks. Specifically, our pre-trained models achieve 85.18%
accuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all
the other self-supervised learning methods. We show with our scheme, a simple
architecture entirely based on standard Transformers can surpass dedicated
Transformer models from supervised learning. Our approach also advances
state-of-the-art accuracies by 1.5%-2.3% in the few-shot object classification.
Furthermore, our work inspires the feasibility of applying unified
architectures from languages and images to the point cloud.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
derived from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps of the semantic features
between the textual question and visual answer, existing methods adopting
visual span predictor perform poorly in the TAGV task. To bridge these gaps, we
propose a visual-prompt text span localizing (VPTSL) method, which introduces
the timestamped subtitles as a passage to perform the text span localization
for the input text question, and prompts the visual highlight features into the
pre-trained language model (PLM) for enhancing the joint semantic
representations. Specifically, the context query attention is utilized to
perform cross-modal interaction between the extracted textual and visual
features. Then, the highlight features are obtained through the video-text
highlighting for the visual prompt. To alleviate semantic differences between
textual and visual features, we design the text span predictor by encoding the
question, the subtitles, and the prompted visual highlight features with the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the visual answer. Extensive experiments on the medical instructional
dataset, namely MedVidQA, show that the proposed VPTSL outperforms the
state-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,
which demonstrates the effectiveness of the proposed visual prompt and the text
span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07996">
<div class="article-summary-box-inner">
<span><p>Training Transformer-based models demands a large amount of data, while
obtaining aligned and labelled data in multimodality is rather cost-demanding,
especially for audio-visual speech recognition (AVSR). Thus it makes a lot of
sense to make use of unlabelled unimodal data. On the other side, although the
effectiveness of large-scale self-supervised learning is well established in
both audio and visual modalities, how to integrate those pre-trained models
into a multimodal scenario remains underexplored. In this work, we successfully
leverage unimodal self-supervised learning to promote the multimodal AVSR. In
particular, audio and visual front-ends are trained on large-scale unimodal
datasets, then we integrate components of both front-ends into a larger
multimodal framework which learns to recognize parallel audio-visual data into
characters through a combination of CTC and seq2seq decoding. We show that both
components inherited from unimodal self-supervised learning cooperate well,
resulting in that the multimodal framework yields competitive results through
fine-tuning. Our model is experimentally validated on both word-level and
sentence-level tasks. Especially, even without an external language model, our
proposed model raises the state-of-the-art performances on the widely accepted
Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative
improvement of 30%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnseenNet: Fast Training Detector for Any Unseen Concept. (arXiv:2203.08759v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08759">
<div class="article-summary-box-inner">
<span><p>Training of object detection models using less data is currently the focus of
existing N-shot learning models in computer vision. Such methods use
object-level labels and takes hours to train on unseen classes. There are many
cases where we have large amount of image-level labels available for training
but cannot be utilized by few shot object detection models for training. There
is a need for a machine learning framework that can be used for training any
unseen class and can become useful in real-time situations. In this paper, we
proposed an "Unseen Class Detector" that can be trained within a very short
time for any possible unseen class without bounding boxes with competitive
accuracy. We build our approach on "Strong" and "Weak" baseline detectors,
which we trained on existing object detection and image classification
datasets, respectively. Unseen concepts are fine-tuned on the strong baseline
detector using only image-level labels and further adapted by transferring the
classifier-detector knowledge between baselines. We use semantic as well as
visual similarities to identify the source class (i.e. Sheep) for the
fine-tuning and adaptation of unseen class (i.e. Goat). Our model (UnseenNet)
is trained on the ImageNet classification dataset for unseen classes and tested
on an object detection dataset (OpenImages). UnseenNet improves the mean
average precision (mAP) by 10% to 30% over existing baselines (semi-supervised
and few-shot) of object detection on different unseen class splits. Moreover,
training time of our model is &lt;10 min for each unseen class. Qualitative
results demonstrate that UnseenNet is suitable not only for few classes of
Pascal VOC but for unseen classes of any dataset or web. Code is available at
https://github.com/Asra-Aslam/UnseenNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizing Global Explanations of Point Cloud DNNs. (arXiv:2203.09505v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09505">
<div class="article-summary-box-inner">
<span><p>In the field of autonomous driving and robotics, point clouds are showing
their excellent real-time performance as raw data from most of the mainstream
3D sensors. Therefore, point cloud neural networks have become a popular
research direction in recent years. So far, however, there has been little
discussion about the explainability of deep neural networks for point clouds.
In this paper, we propose a point cloud-applicable explainability approach
based on a local surrogate model-based method to show which components
contribute to the classification. Moreover, we propose quantitative fidelity
validations for generated explanations that enhance the persuasive power of
explainability and compare the plausibility of different existing point
cloud-applicable explainability methods. Our new explainability approach
provides a fairly accurate, more semantically coherent and widely applicable
explanation for point cloud classification tasks. Our code is available at
https://github.com/Explain3D/LIME-3D
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance. (arXiv:2203.09887v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09887">
<div class="article-summary-box-inner">
<span><p>Transformers have gained much attention by outperforming convolutional neural
networks in many 2D vision tasks. However, they are known to have
generalization problems and rely on massive-scale pre-training and
sophisticated training techniques. When applying to 3D tasks, the irregular
data structure and limited data scale add to the difficulty of transformer's
application. We propose CodedVTR (Codebook-based Voxel TRansformer), which
improves data efficiency and generalization ability for 3D sparse voxel
transformers. On the one hand, we propose the codebook-based attention that
projects an attention space into its subspace represented by the combination of
"prototypes" in a learnable codebook. It regularizes attention learning and
improves generalization. On the other hand, we propose geometry-aware
self-attention that utilizes geometric information (geometric pattern, density)
to guide attention learning. CodedVTR could be embedded into existing sparse
convolution-based methods, and bring consistent performance improvements for
indoor and outdoor 3D semantic segmentation tasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Enhanced Belief Propagation for Data Association in Multiobject Tracking. (arXiv:2203.09948v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09948">
<div class="article-summary-box-inner">
<span><p>Situation-aware technologies enabled by multiobject tracking (MOT) methods
will create new services and applications in fields such as autonomous
navigation and applied ocean sciences. Belief propagation (BP) is a
state-of-the-art method for Bayesian MOT but fully relies on a statistical
model and preprocessed sensor measurements. In this paper, we establish a
hybrid method for model-based and data-driven MOT. The proposed neural enhanced
belief propagation (NEBP) approach complements BP by information learned from
raw sensor data with the goal to improve data association and to reject false
alarm measurements. We evaluate the performance of our NEBP approach for MOT on
the nuScenes autonomous driving dataset and demonstrate that it can outperform
state-of-the-art reference methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion and Volume Maximization-Based Clustering of Highly Mixed Hyperspectral Images. (arXiv:2203.09992v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09992">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images of a scene or object are a rich data source, often
encoding a hundred or more spectral bands of reflectance at each pixel. Despite
being very high-dimensional, these images typically encode latent
low-dimensional structure that can be exploited for material discrimination.
However, due to an inherent trade-off between spectral and spatial resolution,
many hyperspectral images are generated at a coarse spatial scale, and single
pixels may correspond to spatial regions containing multiple materials. This
article introduces the Diffusion and Volume maximization-based Image Clustering
(D-VIC) algorithm for unsupervised material discrimination. D-VIC locates
cluster modes - high-density, high-purity pixels in the hyperspectral image
that are far in diffusion distance (a data-dependent distance metric) from
other high-density, high-purity pixels - and assigns these pixels unique
labels, as these points are meant to exemplify underlying material structure.
Non-modal pixels are labeled according to their diffusion distance nearest
neighbor of higher density and purity that is already labeled. By directly
incorporating pixel purity into its modal and non-modal labeling, D-VIC
upweights pixels that correspond to a spatial region containing just a single
material, yielding more interpretable clusterings. D-VIC is shown to outperform
baseline and comparable state-of-the-art methods in extensive numerical
experiments on a range of hyperspectral images, implying that it is
well-equipped for material discrimination and clustering of these data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unbiased Subclass Regularization for Semi-Supervised Semantic Segmentation. (arXiv:2203.10026v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10026">
<div class="article-summary-box-inner">
<span><p>Semi-supervised semantic segmentation learns from small amounts of labelled
images and large amounts of unlabelled images, which has witnessed impressive
progress with the recent advance of deep neural networks. However, it often
suffers from severe class-bias problem while exploring the unlabelled images,
largely due to the clear pixel-wise class imbalance in the labelled images.
This paper presents an unbiased subclass regularization network (USRN) that
alleviates the class imbalance issue by learning class-unbiased segmentation
from balanced subclass distributions. We build the balanced subclass
distributions by clustering pixels of each original class into multiple
subclasses of similar sizes, which provide class-balanced pseudo supervision to
regularize the class-biased segmentation. In addition, we design an
entropy-based gate mechanism to coordinate learning between the original
classes and the clustered subclasses which facilitates subclass regularization
effectively by suppressing unconfident subclass predictions. Extensive
experiments over multiple public benchmarks show that USRN achieves superior
performance as compared with the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TO-FLOW: Efficient Continuous Normalizing Flows with Temporal Optimization adjoint with Moving Speed. (arXiv:2203.10335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10335">
<div class="article-summary-box-inner">
<span><p>Continuous normalizing flows (CNFs) construct invertible mappings between an
arbitrary complex distribution and an isotropic Gaussian distribution using
Neural Ordinary Differential Equations (neural ODEs). It has not been tractable
on large datasets due to the incremental complexity of the neural ODE training.
Optimal Transport theory has been applied to regularize the dynamics of the ODE
to speed up training in recent works. In this paper, a temporal optimization is
proposed by optimizing the evolutionary time for forward propagation of the
neural ODE training. In this appoach, we optimize the network weights of the
CNF alternately with evolutionary time by coordinate descent. Further with
temporal regularization, stability of the evolution is ensured. This approach
can be used in conjunction with the original regularization approach. We have
experimentally demonstrated that the proposed approach can significantly
accelerate training without sacrifying performance over baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attri-VAE: attribute-based, disentangled and interpretable representations of medical images with variational autoencoders. (arXiv:2203.10417v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10417">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) methods where interpretability is intrinsically considered
as part of the model are required to better understand the relationship of
clinical and imaging-based attributes with DL outcomes, thus facilitating their
use in reasoning medical decisions. Latent space representations built with
variational autoencoders (VAE) do not ensure individual control of data
attributes. Attribute-based methods enforcing attribute disentanglement have
been proposed in the literature for classical computer vision tasks in
benchmark data. In this paper, we propose a VAE approach, the Attri-VAE, that
includes an attribute regularization term to associate clinical and medical
imaging attributes with different regularized dimensions in the generated
latent space, enabling a better disentangled interpretation of the attributes.
Furthermore, the generated attention maps explained the attribute encoding in
the regularized latent space dimensions. The Attri-VAE approach analyzed
healthy and myocardial infarction patients with clinical, cardiac morphology,
and radiomics attributes. The proposed model provided an excellent trade-off
between reconstruction fidelity, disentanglement, and interpretability,
outperforming state-of-the-art VAE approaches according to several quantitative
metrics. The resulting latent space allowed the generation of realistic
synthetic data in the trajectory between two distinct input samples or along a
specific attribute dimension to better interpret changes between different
cardiac conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breast Cancer Induced Bone Osteolysis Prediction Using Temporal Variational Auto-Encoders. (arXiv:2203.10645v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10645">
<div class="article-summary-box-inner">
<span><p>Objective and Impact Statement. We adopt a deep learning model for bone
osteolysis prediction on computed tomography (CT) images of murine breast
cancer bone metastases. Given the bone CT scans at previous time steps, the
model incorporates the bone-cancer interactions learned from the sequential
images and generates future CT images. Its ability of predicting the
development of bone lesions in cancer-invading bones can assist in assessing
the risk of impending fractures and choosing proper treatments in breast cancer
bone metastasis. Introduction. Breast cancer often metastasizes to bone, causes
osteolytic lesions, and results in skeletal related events (SREs) including
severe pain and even fatal fractures. Although current imaging techniques can
detect macroscopic bone lesions, predicting the occurrence and progression of
bone lesions remains a challenge. Methods. We adopt a temporal variational
auto-encoder (T-VAE) model that utilizes a combination of variational
auto-encoders and long short-term memory networks to predict bone lesion
emergence on our micro-CT dataset containing sequential images of murine
tibiae. Given the CT scans of murine tibiae at early weeks, our model can learn
the distribution of their future states from data. Results. We test our model
against other deep learning-based prediction models on the bone lesion
progression prediction task. Our model produces much more accurate predictions
than existing models under various evaluation metrics. Conclusion. We develop a
deep learning framework that can accurately predict and visualize the
progression of osteolytic bone lesions. It will assist in planning and
evaluating treatment strategies to prevent SREs in breast cancer patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boost Test-Time Performance with Closed-Loop Inference. (arXiv:2203.10853v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10853">
<div class="article-summary-box-inner">
<span><p>Conventional deep models predict a test sample with a single forward
propagation, which, however, may not be sufficient for predicting
hard-classified samples. On the contrary, we human beings may need to carefully
check the sample many times before making a final decision. During the recheck
process, one may refine/adjust the prediction by referring to related samples.
Motivated by this, we propose to predict those hard-classified test samples in
a looped manner to boost the model performance. However, this idea may pose a
critical challenge: how to construct looped inference, so that the original
erroneous predictions on these hard test samples can be corrected with little
additional effort. To address this, we propose a general Closed-Loop Inference
(CLI) method. Specifically, we first devise a filtering criterion to identify
those hard-classified test samples that need additional inference loops. For
each hard sample, we construct an additional auxiliary learning task based on
its original top-$K$ predictions to calibrate the model, and then use the
calibrated model to obtain the final prediction. Promising results on ImageNet
(in-distribution test samples) and ImageNet-C (out-of-distribution test
samples) demonstrate the effectiveness of CLI in improving the performance of
any pre-trained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer. (arXiv:2203.10981v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10981">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection is an important yet challenging task in
autonomous driving. Some existing methods leverage depth information from an
off-the-shelf depth estimator to assist 3D detection, but suffer from the
additional computational burden and achieve limited performance caused by
inaccurate depth priors. To alleviate this, we propose MonoDTR, a novel
end-to-end depth-aware transformer network for monocular 3D object detection.
It mainly consists of two components: (1) the Depth-Aware Feature Enhancement
(DFE) module that implicitly learns depth-aware features with auxiliary
supervision without requiring extra computation, and (2) the Depth-Aware
Transformer (DTR) module that globally integrates context- and depth-aware
features. Moreover, different from conventional pixel-wise positional
encodings, we introduce a novel depth positional encoding (DPE) to inject depth
positional hints into transformers. Our proposed depth-aware modules can be
easily plugged into existing image-only monocular 3D object detectors to
improve the performance. Extensive experiments on the KITTI dataset demonstrate
that our approach outperforms previous state-of-the-art monocular-based methods
and achieves real-time detection. Code is available at
https://github.com/kuanchihhuang/MonoDTR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stereo Neural Vernier Caliper. (arXiv:2203.11018v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11018">
<div class="article-summary-box-inner">
<span><p>We propose a new object-centric framework for learning-based stereo 3D object
detection. Previous studies build scene-centric representations that do not
consider the significant variation among outdoor instances and thus lack the
flexibility and functionalities that an instance-level model can offer. We
build such an instance-level model by formulating and tackling a local update
problem, i.e., how to predict a refined update given an initial 3D cuboid
guess. We demonstrate how solving this problem can complement scene-centric
approaches in (i) building a coarse-to-fine multi-resolution system, (ii)
performing model-agnostic object location refinement, and (iii) conducting
stereo 3D tracking-by-detection. Extensive experiments demonstrate the
effectiveness of our approach, which achieves state-of-the-art performance on
the KITTI benchmark. Code and pre-trained models are available at
https://github.com/Nicholasli1995/SNVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11442">
<div class="article-summary-box-inner">
<span><p>This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computation resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
jointly and collaboratively. In detail, AOT employs an identification mechanism
to associate multiple targets into the same high-dimensional embedding space.
Thus, we can simultaneously process multiple objects' matching and segmentation
decoding as efficiently as processing a single object. To sufficiently model
multi-object association, a Long Short-Term Transformer (LSTT) is devised to
construct hierarchical matching and propagation. Based on AOT, we further
propose a more flexible and robust framework, Associating Objects with Scalable
Transformers (AOST), in which a scalable version of LSTT is designed to enable
run-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces
a better layer-wise manner to couple identification and vision embeddings. We
conduct extensive experiments on multi-object and single-object benchmarks to
examine AOT series frameworks. Compared to the state-of-the-art competitors,
our methods can maintain times of run-time efficiency with superior
performance. Notably, we achieve new state-of-the-art performance on three
popular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test
(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:
https://github.com/z-x-yang/AOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Broad Study of Pre-training for Domain Generalization and Adaptation. (arXiv:2203.11819v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11819">
<div class="article-summary-box-inner">
<span><p>Deep models must learn robust and transferable representations in order to
perform well on new domains. While domain transfer methods (e.g., domain
adaptation, domain generalization) have been proposed to learn transferable
representations across domains, they are typically applied to ResNet backbones
pre-trained on ImageNet. Thus, existing works pay little attention to the
effects of pre-training on domain transfer tasks. In this paper, we provide a
broad study and in-depth analysis of pre-training for domain adaptation and
generalization, namely: network architectures, size, pre-training loss, and
datasets. We observe that simply using a state-of-the-art backbone outperforms
existing state-of-the-art domain adaptation baselines and set new baselines on
Office-Home and DomainNet improving by 10.7\% and 5.5\%. We hope that this work
can provide more insights for future domain transfer research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11894">
<div class="article-summary-box-inner">
<span><p>In this work we demonstrate the vulnerability of vision transformers (ViTs)
to gradient-based inversion attacks. During this attack, the original data
batch is reconstructed given model weights and the corresponding gradients. We
introduce a method, named GradViT, that optimizes random noise into naturally
looking images via an iterative process. The optimization objective consists of
(i) a loss on matching the gradients, (ii) image prior in the form of distance
to batch-normalization statistics of a pretrained CNN model, and (iii) a total
variation regularization on patches to guide correct recovery locations. We
propose a unique loss scheduling function to overcome local minima during
optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and
observe unprecedentedly high fidelity and closeness to the original (hidden)
data. During the analysis we find that vision transformers are significantly
more vulnerable than previously studied CNNs due to the presence of the
attention mechanism. Our method demonstrates new state-of-the-art results for
gradient inversion in both qualitative and quantitative metrics. Project page
at https://gradvit.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-Equivalent Distillation for Semi-Supervised Object Detection. (arXiv:2203.12244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12244">
<div class="article-summary-box-inner">
<span><p>Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on
self-training, i.e., generating hard pseudo-labels by a teacher model on
unlabeled data as supervisory signals. Although they achieved certain success,
the limited labeled data in semi-supervised learning scales up the challenges
of object detection. We analyze the challenges these methods meet with the
empirical experiment results. We find that the massive False Negative samples
and inferior localization precision lack consideration. Besides, the large
variance of object sizes and class imbalance (i.e., the extreme ratio between
background and object) hinder the performance of prior arts. Further, we
overcome these challenges by introducing a novel approach, Scale-Equivalent
Distillation (SED), which is a simple yet effective end-to-end knowledge
distillation framework robust to large object size variance and class
imbalance. SED has several appealing benefits compared to the previous works.
(1) SED imposes a consistency regularization to handle the large scale variance
problem. (2) SED alleviates the noise problem from the False Negative samples
and inferior localization precision. (3) A re-weighting strategy can implicitly
screen the potential foreground regions of the unlabeled data to reduce the
effect of class imbalance. Extensive experiments show that SED consistently
outperforms the recent state-of-the-art methods on different datasets with
significant margins. For example, it surpasses the supervised counterpart by
more than 10 mAP when using 5% and 10% labeled data on MS-COCO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ev-TTA: Test-Time Adaptation for Event-Based Object Recognition. (arXiv:2203.12247v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12247">
<div class="article-summary-box-inner">
<span><p>We introduce Ev-TTA, a simple, effective test-time adaptation algorithm for
event-based object recognition. While event cameras are proposed to provide
measurements of scenes with fast motions or drastic illumination changes, many
existing event-based recognition algorithms suffer from performance
deterioration under extreme conditions due to significant domain shifts. Ev-TTA
mitigates the severe domain gaps by fine-tuning the pre-trained classifiers
during the test phase using loss functions inspired by the spatio-temporal
characteristics of events. Since the event data is a temporal stream of
measurements, our loss function enforces similar predictions for adjacent
events to quickly adapt to the changed environment online. Also, we utilize the
spatial correlations between two polarities of events to handle noise under
extreme illumination, where different polarities of events exhibit distinctive
noise distributions. Ev-TTA demonstrates a large amount of performance gain on
a wide range of event-based object recognition tasks without extensive
additional training. Our formulation can be successfully applied regardless of
input representations and further extended into regression tasks. We expect
Ev-TTA to provide the key technique to deploy event-based vision algorithms in
challenging real-world applications where significant domain shift is
inevitable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DR.VIC: Decomposition and Reasoning for Video Individual Counting. (arXiv:2203.12335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12335">
<div class="article-summary-box-inner">
<span><p>Pedestrian counting is a fundamental tool for understanding pedestrian
patterns and crowd flow analysis. Existing works (e.g., image-level pedestrian
counting, crossline crowd counting et al.) either only focus on the image-level
counting or are constrained to the manual annotation of lines. In this work, we
propose to conduct the pedestrian counting from a new perspective - Video
Individual Counting (VIC), which counts the total number of individual
pedestrians in the given video (a person is only counted once). Instead of
relying on the Multiple Object Tracking (MOT) techniques, we propose to solve
the problem by decomposing all pedestrians into the initial pedestrians who
existed in the first frame and the new pedestrians with separate identities in
each following frame. Then, an end-to-end Decomposition and Reasoning Network
(DRNet) is designed to predict the initial pedestrian count with the density
estimation method and reason the new pedestrian's count of each frame with the
differentiable optimal transport. Extensive experiments are conducted on two
datasets with congested pedestrians and diverse scenes, demonstrating the
effectiveness of our method over baselines with great superiority in counting
the individual pedestrians. Code: https://github.com/taohan10200/DRNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-label Transformer for Action Unit Detection. (arXiv:2203.12531v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12531">
<div class="article-summary-box-inner">
<span><p>Action Unit (AU) Detection is the branch of affective computing that aims at
recognizing unitary facial muscular movements. It is key to unlock unbiased
computational face representations and has therefore aroused great interest in
the past few years. One of the main obstacles toward building efficient deep
learning based AU detection system is the lack of wide facial image databases
annotated by AU experts. In that extent the ABAW challenge paves the way toward
better AU detection as it involves a 2M frames AU annotated dataset. In this
paper, we present our submission to the ABAW3 challenge. In a nutshell, we
applied a multi-label detection transformer that leverage multi-head attention
to learn which part of the face image is the most relevant to predict each AU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StructToken : Rethinking Semantic Segmentation with Structural Prior. (arXiv:2203.12612v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12612">
<div class="article-summary-box-inner">
<span><p>In this paper, we present structure token (StructToken), a new paradigm for
semantic segmentation. From a perspective on semantic segmentation as per-pixel
classification, the previous deep learning-based methods learn the per-pixel
representation first through an encoder and a decoder head and then classify
each pixel representation to a specific category to obtain the semantic masks.
Differently, we propose a structure-aware algorithm that takes structural
information as prior to predict semantic masks directly without per-pixel
classification. Specifically, given an input image, the learnable structure
token interacts with the image representations to reason the final semantic
masks. Three interaction approaches are explored and the results not only
outperform the state-of-the-art methods but also contain more structural
information. Experiments are conducted on three widely used datasets including
ADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could
serve as an alternative for semantic segmentation and inspire future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to generate line drawings that convey geometry and semantics. (arXiv:2203.12691v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12691">
<div class="article-summary-box-inner">
<span><p>This paper presents an unpaired method for creating line drawings from
photographs. Current methods often rely on high quality paired datasets to
generate line drawings. However, these datasets often have limitations due to
the subjects of the drawings belonging to a specific domain, or in the amount
of data collected. Although recent work in unsupervised image-to-image
translation has shown much progress, the latest methods still struggle to
generate compelling line drawings. We observe that line drawings are encodings
of scene information and seek to convey 3D shape and semantic meaning. We build
these observations into a set of objectives and train an image translation to
map photographs into line drawings. We introduce a geometry loss which predicts
depth information from the image features of a line drawing, and a semantic
loss which matches the CLIP features of a line drawing with its corresponding
photograph. Our approach outperforms state-of-the-art unpaired image
translation and line drawing generation methods on creating line drawings from
arbitrary photographs. For code and demo visit our webpage
carolineec.github.io/informative_drawings
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Challenges of Continuous Self-Supervised Learning. (arXiv:2203.12710v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12710">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks
in representation learning - the need for human annotations. As a result, SSL
holds the promise to learn representations from data in-the-wild, i.e., without
the need for finite and static datasets. Instead, true SSL algorithms should be
able to exploit the continuous stream of data being generated on the internet
or by agents exploring their environments. But do traditional self-supervised
learning approaches work in this setup? In this work, we investigate this
question by conducting experiments on the continuous self-supervised learning
problem. While learning in the wild, we expect to see a continuous (infinite)
non-IID data stream that follows a non-stationary distribution of visual
concepts. The goal is to learn a representation that can be robust, adaptive
yet not forgetful of concepts seen in the past. We show that a direct
application of current methods to such continuous setup is 1) inefficient both
computationally and in the amount of data required, 2) leads to inferior
representations due to temporal correlations (non-IID data) in some sources of
streaming data and 3) exhibits signs of catastrophic forgetting when trained on
sources with non-stationary data distributions. We propose the use of replay
buffers as an approach to alleviate the issues of inefficiency and temporal
correlations. We further propose a novel method to enhance the replay buffer by
maintaining the least redundant samples. Minimum redundancy (MinRed) buffers
allow us to learn effective representations even in the most challenging
streaming scenarios composed of sequential visual data obtained from a single
embodied agent, and alleviates the problem of catastrophic forgetting when
learning from data with non-stationary semantic distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection. (arXiv:2203.12745v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12745">
<div class="article-summary-box-inner">
<span><p>Finding relevant moments and highlights in videos according to natural
language queries is a natural and highly valuable common need in the current
video content explosion era. Nevertheless, jointly conducting moment retrieval
and highlight detection is an emerging research topic, even though its
component problems and some related tasks have already been studied for a
while. In this paper, we present the first unified framework, named Unified
Multi-modal Transformers (UMT), capable of realizing such joint optimization
while can also be easily degenerated for solving individual problems. As far as
we are aware, this is the first scheme to integrate multi-modal (visual-audio)
learning for either joint optimization or the individual moment retrieval task,
and tackles moment retrieval as a keypoint detection problem using a novel
query generator and query decoder. Extensive comparisons with existing methods
and ablation studies on QVHighlights, Charades-STA, YouTube Highlights, and
TVSum datasets demonstrate the effectiveness, superiority, and flexibility of
the proposed method under various settings. Source code and pre-trained models
are available at https://github.com/TencentARC/UMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12861">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNN) have demonstrated outstanding Compressed
Sensing (CS) performance compared to traditional, hand-crafted methods.
However, they are broadly limited in terms of generalisability, inductive bias
and difficulty to model long distance relationships. Transformer neural
networks (TNN) overcome such issues by implementing an attention mechanism
designed to capture dependencies between inputs. However, high-resolution tasks
typically require vision Transformers (ViT) to decompose an image into
patch-based tokens, limiting inputs to inherently local contexts. We propose a
novel image decomposition that naturally embeds images into low-resolution
inputs. These Kaleidoscope tokens (KD) provide a mechanism for global
attention, at the same computational cost as a patch-based approach. To
showcase this development, we replace CNN components in a well-known CS-MRI
neural network with TNN blocks and demonstrate the improvements afforded by KD.
We also propose an ensemble of image tokens, which enhance overall image
quality and reduces model size. Supplementary material is available:
https://github.com/uqmarlonbran/TCS.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks. (arXiv:2203.12915v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12915">
<div class="article-summary-box-inner">
<span><p>Deep learning has recently been widely applied to many applications across
different domains, e.g., image classification and audio recognition. However,
the quality of Deep Neural Networks (DNNs) still raises concerns in the
practical operational environment, which calls for systematic testing,
especially in safety-critical scenarios. Inspired by software testing, a number
of structural coverage criteria are designed and proposed to measure the test
adequacy of DNNs. However, due to the blackbox nature of DNN, the existing
structural coverage criteria are difficult to interpret, making it hard to
understand the underlying principles of these criteria. The relationship
between the structural coverage and the decision logic of DNNs is unknown.
Moreover, recent studies have further revealed the non-existence of correlation
between the structural coverage and DNN defect detection, which further posts
concerns on what a suitable DNN testing criterion should be.
</p>
<p>In this paper, we propose the interpretable coverage criteria through
constructing the decision structure of a DNN. Mirroring the control flow graph
of the traditional program, we first extract a decision graph from a DNN based
on its interpretation, where a path of the decision graph represents a decision
logic of the DNN. Based on the control flow and data flow of the decision
graph, we propose two variants of path coverage to measure the adequacy of the
test cases in exercising the decision logic. The higher the path coverage, the
more diverse decision logic the DNN is expected to be explored. Our large-scale
evaluation results demonstrate that: the path in the decision graph is
effective in characterizing the decision of the DNN, and the proposed coverage
criteria are also sensitive with errors including natural errors and
adversarial examples, and strongly correlated with the output impartiality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image. (arXiv:2203.13009v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13009">
<div class="article-summary-box-inner">
<span><p>Recently, significant progress has been made on image denoising with strong
supervision from large-scale datasets. However, obtaining well-aligned
noisy-clean training image pairs for each specific scenario is complicated and
costly in practice. Consequently, applying a conventional supervised denoising
network on in-the-wild noisy inputs is not straightforward. Although several
studies have challenged this problem without strong supervision, they rely on
less practical assumptions and cannot be applied to practical situations
directly. To address the aforementioned challenges, we propose a novel and
powerful self-supervised denoising method called CVF-SID based on a Cyclic
multi-Variate Function (CVF) module and a self-supervised image disentangling
(SID) framework. The CVF module can output multiple decomposed variables of the
input and take a combination of the outputs back as an input in a cyclic
manner. Our CVF-SID can disentangle a clean image and noise maps from the input
by leveraging various self-supervised loss terms. Unlike several methods that
only consider the signal-independent noise models, we also deal with
signal-dependent noise components for real-world applications. Furthermore, we
do not rely on any prior assumptions about the underlying noise distribution,
making CVF-SID more generalizable toward realistic noise. Extensive experiments
on real-world datasets show that CVF-SID achieves state-of-the-art
self-supervised image denoising performance and is comparable to other existing
approaches. The code is publicly available from
https://github.com/Reyhanehne/CVF-SID_PyTorch .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13032">
<div class="article-summary-box-inner">
<span><p>In this paper, we briefly introduce our submission to the Valence-Arousal
Estimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)
competition. Our method utilizes the multi-modal information, i.e., the visual
and audio information, and employs a temporal encoder to model the temporal
context in the videos. Besides, a smooth processor is applied to get more
reasonable predictions, and a model ensemble strategy is used to improve the
performance of our proposed method. The experiment results show that our method
achieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation
set of the Aff-Wild2 dataset, which prove the effectiveness of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning. (arXiv:2203.13049v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13049">
<div class="article-summary-box-inner">
<span><p>Temporal grounding in videos aims to localize one target video segment that
semantically corresponds to a given query sentence. Thanks to the semantic
diversity of natural language descriptions, temporal grounding allows activity
grounding beyond pre-defined classes and has received increasing attention in
recent years. The semantic diversity is rooted in the principle of
compositionality in linguistics, where novel semantics can be systematically
described by combining known words in novel ways (compositional
generalization). However, current temporal grounding datasets do not
specifically test for the compositional generalizability. To systematically
measure the compositional generalizability of temporal grounding models, we
introduce a new Compositional Temporal Grounding task and construct two new
dataset splits, i.e., Charades-CG and ActivityNet-CG. Evaluating the
state-of-the-art methods on our new dataset splits, we empirically find that
they fail to generalize to queries with novel combinations of seen words. To
tackle this challenge, we propose a variational cross-graph reasoning framework
that explicitly decomposes video and language into multiple structured
hierarchies and learns fine-grained semantic correspondence among them.
Experiments illustrate the superior compositional generalizability of our
approach. The repository of this work is at https://github.com/YYJMJC/
Compositional-Temporal-Grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13052">
<div class="article-summary-box-inner">
<span><p>Facial expression recognition plays an important role in human-computer
interaction. In this paper, we propose the Coarse-to-Fine Cascaded network with
Smooth Predicting (CFC-SP) to improve the performance of facial expression
recognition. CFC-SP contains two core components, namely Coarse-to-Fine
Cascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups
several similar emotions to form a rough category, and then employs a network
to conduct a coarse but accurate classification. Later, an additional network
for these grouped emotions is further used to obtain fine-grained predictions.
For SP, it improves the recognition capability of the model by capturing both
universal and unique expression features. To be specific, the universal
features denote the general characteristic of facial emotions within a period
and the unique features denote the specific characteristic at this moment.
Experiments on Aff-Wild2 show the effectiveness of the proposed CFSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13167">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the continual learning of Vision Transformers
(ViT) for the challenging exemplar-free scenario, with special focus on how to
efficiently distill the knowledge of its crucial self-attention mechanism
(SAM). Our work takes an initial step towards a surgical investigation of SAM
for designing coherent continual learning methods in ViTs. We first carry out
an evaluation of established continual learning regularization techniques. We
then examine the effect of regularization when applied to two key enablers of
SAM: (a) the contextualized embedding layers, for their ability to capture
well-scaled representations with respect to the values, and (b) the prescaled
attention maps, for carrying value-independent global contextual information.
We depict the perks of each distilling strategy on two image recognition
benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall
accuracy, (b) helps enhance the rigidity by maintaining competitive
performances. Furthermore, we identify the limitation imposed by the symmetric
nature of regularization losses. To alleviate this, we propose an asymmetric
variant and apply it to the pooled output distillation (POD) loss adapted for
ViTs. Our experiments confirm that introducing asymmetry to POD boosts its
plasticity while retaining stability across (a) and (b). Moreover, we
acknowledge low forgetting measures for all the compared methods, indicating
that ViTs might be naturally inclined continual learner
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Multi-label Facial Action Unit Detection with Transformer. (arXiv:2203.13301v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13301">
<div class="article-summary-box-inner">
<span><p>Facial Action Coding System is an important approach of facial expression
analysis.This paper describes our submission to the third Affective Behavior
Analysis (ABAW) 2022 competition. We proposed a transfomer based model to
detect facial action unit (FAU) in video. To be specific, we firstly trained a
multi-modal model to extract both audio and visual feature. After that, we
proposed a action units correlation module to learn relationships between each
action unit labels and refine action unit detection result. Experimental
results on validation dataset shows that our method achieves better performance
than baseline model, which verifies that the effectiveness of proposed network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13310">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection has long been a challenging task in autonomous
driving, which requires to decode 3D predictions solely from a single 2D image.
Most existing methods follow conventional 2D object detectors to first localize
objects by their centers, and then predict 3D attributes using
center-neighboring local features. However, such center-based pipeline views 3D
prediction as a subordinate task and lacks inter-object depth interactions with
global spatial clues. In this paper, we introduce a simple framework for
Monocular DEtection with depth-aware TRansformer, named MonoDETR. We enable the
vanilla transformer to be depth-aware and enforce the whole detection process
guided by depth. Specifically, we represent 3D object candidates as a set of
queries and produce non-local depth embeddings of the input image by a
lightweight depth predictor and an attention-based depth encoder. Then, we
propose a depth-aware decoder to conduct both inter-query and query-scene depth
feature communication. In this way, each object estimates its 3D attributes
adaptively from the depth-informative regions on the image, not limited by
center-around features. With minimal handcrafted designs, MonoDETR is an
end-to-end framework without additional data, anchors or NMS and achieves
competitive performance on KITTI benchmark among state-of-the-art center-based
networks. Extensive ablation studies demonstrate the effectiveness of our
approach and its potential to serve as a transformer baseline for future
monocular research. Code is available at
https://github.com/ZrrSkywalker/MonoDETR.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration Framework. (arXiv:2003.06513v2 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06513">
<div class="article-summary-box-inner">
<span><p>Weight pruning of deep neural networks (DNNs) has been proposed to satisfy
the limited storage and computing capability of mobile edge devices. However,
previous pruning methods mainly focus on reducing the model size and/or
improving performance without considering the privacy of user data. To mitigate
this concern, we propose a privacy-preserving-oriented pruning and mobile
acceleration framework that does not require the private training dataset. At
the algorithm level of the proposed framework, a systematic weight pruning
technique based on the alternating direction method of multipliers (ADMM) is
designed to iteratively solve the pattern-based pruning problem for each layer
with randomly generated synthetic data. In addition, corresponding
optimizations at the compiler level are leveraged for inference accelerations
on devices. With the proposed framework, users could avoid the time-consuming
pruning process for non-experts and directly benefit from compressed models.
Experimental results show that the proposed framework outperforms three
state-of-art end-to-end DNN frameworks, i.e., TensorFlow-Lite, TVM, and MNN,
with speedup up to 4.2X, 2.5X, and 2.0X, respectively, with almost no accuracy
loss, while preserving data privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search. (arXiv:2108.08910v1 [eess.IV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08910">
<div class="article-summary-box-inner">
<span><p>Though recent years have witnessed remarkable progress in single image
super-resolution (SISR) tasks with the prosperous development of deep neural
networks (DNNs), the deep learning methods are confronted with the computation
and memory consumption issues in practice, especially for resource-limited
platforms such as mobile devices. To overcome the challenge and facilitate the
real-time deployment of SISR tasks on mobile, we combine neural architecture
search with pruning search and propose an automatic search framework that
derives sparse super-resolution (SR) models with high image quality while
satisfying the real-time inference requirement. To decrease the search cost, we
leverage the weight sharing strategy by introducing a supernet and decouple the
search problem into three stages, including supernet construction,
compiler-aware architecture and pruning search, and compiler-aware pruning
ratio search. With the proposed framework, we are the first to achieve
real-time SR inference (with only tens of milliseconds per frame) for
implementing 720p resolution with competitive image quality (in terms of PSNR
and SSIM) on mobile platforms (Samsung Galaxy S20).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time Mobile Acceleration. (arXiv:2111.11581v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11581">
<div class="article-summary-box-inner">
<span><p>Weight pruning is an effective model compression technique to tackle the
challenges of achieving real-time deep neural network (DNN) inference on mobile
devices. However, prior pruning schemes have limited application scenarios due
to accuracy degradation, difficulty in leveraging hardware acceleration, and/or
restriction on certain types of DNN layers. In this paper, we propose a
general, fine-grained structured pruning scheme and corresponding compiler
optimizations that are applicable to any type of DNN layer while achieving high
accuracy and hardware inference performance. With the flexibility of applying
different pruning schemes to different layers enabled by our compiler
optimizations, we further probe into the new problem of determining the
best-suited pruning scheme considering the different acceleration and accuracy
performance of various pruning schemes. Two pruning scheme mapping methods, one
is search-based and the other is rule-based, are proposed to automatically
derive the best-suited pruning regularity and block size for each layer of any
given DNN. Experimental results demonstrate that our pruning scheme mapping
methods, together with the general fine-grained structured pruning scheme,
outperform the state-of-the-art DNN optimization framework with up to
2.48$\times$ and 1.73$\times$ DNN inference acceleration on CIFAR-10 and
ImageNet dataset without accuracy loss.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-29 23:08:26.882474515 UTC">2022-03-29 23:08:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>