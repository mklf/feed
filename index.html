<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-18T01:30:00Z">03-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?. (arXiv:2203.08850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08850">
<div class="article-summary-box-inner">
<span><p>What can pre-trained multilingual sequence-to-sequence models like mBART
contribute to translating low-resource languages? We conduct a thorough
empirical experiment in 10 languages to ascertain this, considering five
factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning
data, (3) the amount of pre-training data in the model, (4) the impact of
domain mismatch, and (5) language typology. In addition to yielding several
heuristics, the experiments form a framework for evaluating the data
sensitivities of machine translation systems. While mBART is robust to domain
differences, its translations for unseen and typologically distant languages
remain below 3.0 BLEU. In answer to our title's question, mBART is not a
low-resource panacea; we therefore encourage shifting the emphasis from new
models to new data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Learning on Graphs for Disease Relation Extraction. (arXiv:2203.08893v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08893">
<div class="article-summary-box-inner">
<span><p>Objective: Disease knowledge graphs are a way to connect, organize, and
access disparate information about diseases with numerous benefits for
artificial intelligence (AI). To create knowledge graphs, it is necessary to
extract knowledge from multimodal datasets in the form of relationships between
disease concepts and normalize both concepts and relationship types. Methods:
We introduce REMAP, a multimodal approach for disease relation extraction and
classification. The REMAP machine learning approach jointly embeds a partial,
incomplete knowledge graph and a medical language dataset into a compact latent
vector space, followed by aligning the multimodal embeddings for optimal
disease relation extraction. Results: We apply REMAP approach to a disease
knowledge graph with 96,913 relations and a text dataset of 1.24 million
sentences. On a dataset annotated by human experts, REMAP improves text-based
disease relation extraction by 10.0% (accuracy) and 17.2% (F1-score) by fusing
disease knowledge graphs with text information. Further, REMAP leverages text
information to recommend new relationships in the knowledge graph,
outperforming graph-based methods by 8.4% (accuracy) and 10.4% (F1-score).
Discussion: Systematized knowledge is becoming the backbone of AI, creating
opportunities to inject semantics into AI and fully integrate it into machine
learning algorithms. While prior semantic knowledge can assist in extracting
disease relationships from text, existing methods can not fully leverage
multimodal datasets. Conclusion: REMAP is a multimodal approach for extracting
and classifying disease relationships by fusing structured knowledge and text
information. REMAP provides a flexible neural architecture to easily find,
access, and validate AI-driven relationships between disease concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morphological Processing of Low-Resource Languages: Where We Are and What's Next. (arXiv:2203.08909v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08909">
<div class="article-summary-box-inner">
<span><p>Automatic morphological processing can aid downstream natural language
processing applications, especially for low-resource languages, and assist
language documentation efforts for endangered languages. Having long been
multilingual, the field of computational morphology is increasingly moving
towards approaches suitable for languages with minimal or no annotated
resources. First, we survey recent developments in computational morphology
with a focus on low-resource languages. Second, we argue that the field is
ready to tackle the logical next challenge: understanding a language's
morphology from raw text alone. We perform an empirical study on a truly
unsupervised version of the paradigm completion task and show that, while
existing state-of-the-art models bridged by two newly proposed models we devise
perform reasonably, there is still much room for improvement. The stakes are
high: solving this task will increase the language coverage of morphological
resources by a number of magnitudes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memorizing Transformers. (arXiv:2203.08913v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08913">
<div class="article-summary-box-inner">
<span><p>Language models typically need to be trained or finetuned in order to acquire
new knowledge, which involves updating their weights. We instead envision
language models that can simply read and memorize new data at inference time,
thus acquiring new knowledge immediately. In this work, we extend language
models with the ability to memorize the internal representations of past
inputs. We demonstrate that an approximate kNN lookup into a non-differentiable
memory of recent (key, value) pairs improves language modeling across various
benchmarks and tasks, including generic webtext (C4), math papers (arXiv),
books (PG-19), code (Github), as well as formal theorems (Isabelle). We show
that the performance steadily improves when we increase the size of memory up
to 262K tokens. On benchmarks including code and mathematics, we find that the
model is capable of making use of newly defined functions and theorems during
test time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Question Value Estimation for Domain Adaptation of Question Answering. (arXiv:2203.08926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08926">
<div class="article-summary-box-inner">
<span><p>Synthesizing QA pairs with a question generator (QG) on the target domain has
become a popular approach for domain adaptation of question answering (QA)
models. Since synthetic questions are often noisy in practice, existing work
adapts scores from a pretrained QA (or QG) model as criteria to select
high-quality questions. However, these scores do not directly serve the
ultimate goal of improving QA performance on the target domain. In this paper,
we introduce a novel idea of training a question value estimator (QVE) that
directly estimates the usefulness of synthetic questions for improving the
target-domain QA performance. By conducting comprehensive experiments, we show
that the synthetic questions selected by QVE can help achieve better
target-domain QA performance, in comparison with existing techniques. We
additionally show that by using such questions and only around 15% of the human
annotations on the target domain, we can achieve comparable performance to the
fully-supervised baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References. (arXiv:2203.08928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08928">
<div class="article-summary-box-inner">
<span><p>We consider the problem of pretraining a two-stage open-domain question
answering (QA) system (retriever + reader) with strong transfer capabilities.
The key challenge is how to construct a large amount of high-quality
question-answer-context triplets without task-specific annotations.
Specifically, the triplets should align well with downstream tasks by: (i)
covering a wide range of domains (for open-domain applications), (ii) linking a
question to its semantically relevant context with supporting evidence (for
training the retriever), and (iii) identifying the correct answer in the
context (for training the reader). Previous pretraining approaches generally
fall short of one or more of these requirements. In this work, we automatically
construct a large-scale corpus that meets all three criteria by consulting
millions of references cited within Wikipedia. The well-aligned pretraining
signals benefit both the retriever and the reader significantly. Our pretrained
retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our
pretrained reader, the entire system improves by up to 4% in exact match.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis of Negation in Natural Language Understanding Corpora. (arXiv:2203.08929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08929">
<div class="article-summary-box-inner">
<span><p>This paper analyzes negation in eight popular corpora spanning six natural
language understanding tasks. We show that these corpora have few negations
compared to general-purpose English, and that the few negations in them are
often unimportant. Indeed, one can often ignore negations and still make the
right predictions. Additionally, experimental results show that
state-of-the-art transformers trained with these corpora obtain substantially
worse results with instances that contain negation, especially if the negations
are important. We conclude that new corpora accounting for negation are needed
to solve natural language understanding tasks when negation is present.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creating Multimedia Summaries Using Tweets and Videos. (arXiv:2203.08931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08931">
<div class="article-summary-box-inner">
<span><p>While popular televised events such as presidential debates or TV shows are
airing, people provide commentary on them in real-time. In this paper, we
propose a simple yet effective approach to combine social media commentary and
videos to create a multimedia summary of televised events. Our approach
identifies scenes from these events based on spikes of mentions of people
involved in the event and automatically selects tweets and frames from the
videos that occur during the time period of the spike that talk about and show
the people being discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BPE vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages. (arXiv:2203.08954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08954">
<div class="article-summary-box-inner">
<span><p>Morphologically-rich polysynthetic languages present a challenge for NLP
systems due to data sparsity, and a common strategy to handle this issue is to
apply subword segmentation. We investigate a wide variety of supervised and
unsupervised morphological segmentation methods for four polysynthetic
languages: Nahuatl, Raramuri, Shipibo-Konibo, and Wixarika. Then, we compare
the morphologically inspired segmentation methods against Byte-Pair Encodings
(BPEs) as inputs for machine translation (MT) when translating to and from
Spanish. We show that for all language pairs except for Nahuatl, an
unsupervised morphological segmentation algorithm outperforms BPEs consistently
and that, although supervised methods achieve better segmentation scores, they
under-perform in MT challenges. Finally, we contribute two new morphological
segmentation datasets for Raramuri and Shipibo-Konibo, and a parallel corpus
for Raramuri--Spanish.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker Information Can Guide Models to Better Inductive Biases: A Case Study On Predicting Code-Switching. (arXiv:2203.08979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08979">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) models trained on people-generated data can
be unreliable because, without any constraints, they can learn from spurious
correlations that are not relevant to the task. We hypothesize that enriching
models with speaker information in a controlled, educated way can guide them to
pick up on relevant inductive biases. For the speaker-driven task of predicting
code-switching points in English--Spanish bilingual dialogues, we show that
adding sociolinguistically-grounded speaker features as prepended prompts
significantly improves accuracy. We find that by adding influential phrases to
the input, speaker-informed models learn useful and explainable linguistic
information. To our knowledge, we are the first to incorporate speaker
characteristics in a neural model for code-switching, and more generally, take
a step towards developing transparent, personalized models that use speaker
information in a controlled way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Semantics for Few Shot Named Entity Recognition. (arXiv:2203.08985v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08985">
<div class="article-summary-box-inner">
<span><p>We study the problem of few shot learning for named entity recognition.
Specifically, we leverage the semantic information in the names of the labels
as a way of giving the model additional signal and enriched priors. We propose
a neural architecture that consists of two BERT encoders, one to encode the
document and its tokens and another one to encode each of the labels in natural
language format. Our model learns to match the representations of named
entities computed by the first encoder with label representations computed by
the second encoder. The label semantics signal is shown to support improved
state-of-the-art results in multiple few shot NER benchmarks and on-par
performance in standard benchmarks. Our model is especially effective in low
resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdapLeR: Speeding up Inference by Adaptive Length Reduction. (arXiv:2203.08991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08991">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have shown stellar performance in various
downstream tasks. But, this usually comes at the cost of high latency and
computation, hindering their usage in resource-limited settings. In this work,
we propose a novel approach for reducing the computational cost of BERT with
minimal loss in downstream performance. Our method dynamically eliminates less
contributing tokens through layers, resulting in shorter lengths and
consequently lower computational cost. To determine the importance of each
token representation, we train a Contribution Predictor for each layer using a
gradient-based saliency method. Our experiments on several diverse
classification tasks show speedups up to 22x during inference time without much
sacrifice in performance. We also validate the quality of the selected tokens
in our method using human annotations in the ERASER benchmark. In comparison to
other widely used strategies for selecting important tokens, such as saliency
and attention, our proposed method has a significantly lower false positive
rate in generating rationales. Our code is freely available at
https://github.com/amodaresi/AdapLeR .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension. (arXiv:2203.08992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08992">
<div class="article-summary-box-inner">
<span><p>Recent machine reading comprehension datasets such as ReClor and LogiQA
require performing logical reasoning over text. Conventional neural models are
insufficient for logical reasoning, while symbolic reasoners cannot directly
apply to text. To meet the challenge, we present a neural-symbolic approach
which, to predict an answer, passes messages over a graph representing logical
relations between text units. It incorporates an adaptive logic graph network
(AdaLoGN) which adaptively infers logical relations to extend the graph and,
essentially, realizes mutual and iterative reinforcement between neural and
symbolic reasoning. We also implement a novel subgraph-to-node message passing
mechanism to enhance context-option interaction for answering multiple-choice
questions. Our approach shows promising results on ReClor and LogiQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Autonomy: Self-Initiation, Adaptation and Continual Learning. (arXiv:2203.08994v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08994">
<div class="article-summary-box-inner">
<span><p>As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can (1) learn by
themselves continually in a self-motivated and self-initiated manner rather
than being retrained offline periodically on the initiation of human engineers
and (2) accommodate or adapt to unexpected or novel circumstances. As the
real-world is an open environment that is full of unknowns or novelties,
detecting novelties, characterizing them, accommodating or adapting to them,
and gathering ground-truth training data and incrementally learning the
unknowns/novelties are critical to making the AI agent more and more
knowledgeable and powerful over time. The key challenge is how to automate the
process so that it is carried out continually on the agent's own initiative and
through its own interactions with humans, other agents and the environment just
like human on-the-job learning. This paper proposes a framework (called SOLA)
for this learning paradigm to promote the research of building autonomous and
continual learning enabled AI agents. To show feasibility, an implemented agent
is also described.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Communication with a Teachable Agent. (arXiv:2203.09016v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09016">
<div class="article-summary-box-inner">
<span><p>Conversational teachable agents offer a promising platform to support
learning, both in the classroom and in remote settings. In this context, the
agent takes the role of the novice, while the student takes on the role of
teacher. This framing is significant for its ability to elicit the Prot\'eg\'e
effect in the student-teacher, a pedagogical phenomenon known to increase
engagement in the teaching task, and also improve cognitive outcomes. In prior
work, teachable agents often take a passive role in the learning interaction,
and there are few studies in which the agent and student engage in natural
language dialogue during the teaching task. This work investigates the effect
of teaching modality when interacting with a virtual agent, via the web-based
teaching platform, the Curiosity Notebook. A method of teaching the agent by
selecting sentences from source material is compared to a method paraphrasing
the source material and typing text input to teach. A user study has been
conducted to measure the effect teaching modality on the learning outcomes and
engagement of the participants. The results indicate that teaching via
paraphrasing and text input has a positive effect on learning outcomes for the
material covered, and also on aspects of affective engagement. Furthermore,
increased paraphrasing effort, as measured by the similarity between the source
material and the material the teacher conveyed to the robot, improves learning
outcomes for participants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Triangular Transfer: Freezing the Pivot for Triangular Machine Translation. (arXiv:2203.09027v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09027">
<div class="article-summary-box-inner">
<span><p>Triangular machine translation is a special case of low-resource machine
translation where the language pair of interest has limited parallel data, but
both languages have abundant parallel data with a pivot language. Naturally,
the key to triangular machine translation is the successful exploitation of
such auxiliary data. In this work, we propose a transfer-learning-based
approach that utilizes all types of auxiliary data. As we train auxiliary
source-pivot and pivot-target translation models, we initialize some parameters
of the pivot side with a pre-trained language model and freeze them to
encourage both translation models to work in the same pivot language space, so
that they can be smoothly transferred to the source-target translation model.
Experiments show that our approach can outperform previous ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training. (arXiv:2203.09052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09052">
<div class="article-summary-box-inner">
<span><p>Due to the limitations of the model structure and pre-training objectives,
existing vision-and-language generation models cannot utilize pair-wise images
and text through bi-directional generation. In this paper, we propose DU-VLG, a
framework which unifies vision-and-language generation as sequence generation
problems. DU-VLG is trained with novel dual pre-training tasks: multi-modal
denoising autoencoder tasks and modality translation tasks. To bridge the gap
between image understanding and generation, we further design a novel
commitment loss. We compare pre-training objectives on image captioning and
text-to-image generation datasets. Results show that DU-VLG yields better
performance than variants trained with uni-directional generation objectives or
the variant without the commitment loss. We also obtain higher scores compared
to previous state-of-the-art systems on three vision-and-language generation
tasks. In addition, human judges further confirm that our model generates real
and relevant images as well as faithful and informative captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework. (arXiv:2203.09053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09053">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) starts translating while receiving
the streaming source inputs, and hence the source sentence is always incomplete
during translating. Different from the full-sentence MT using the conventional
seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,
which forces each target word to only align with a partial source prefix to
adapt to the incomplete source in streaming inputs. However, the source words
in the front positions are always illusoryly considered more important since
they appear in more prefixes, resulting in position bias, which makes the model
pay more attention on the front source positions in testing. In this paper, we
first analyze the phenomenon of position bias in SiMT, and develop a
Length-Aware Framework to reduce the position bias by bridging the structural
gap between SiMT and full-sentence MT. Specifically, given the streaming
inputs, we first predict the full-sentence length and then fill the future
source position with positional encoding, thereby turning the streaming inputs
into a pseudo full-sentence. The proposed framework can be integrated into most
existing SiMT methods to further improve performance. Experiments on two
representative SiMT methods, including the state-of-the-art adaptive policy,
show that our method successfully reduces the position bias and achieves better
SiMT performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT. (arXiv:2203.09055v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09055">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models, such as BERT, have shown extraordinary
success in achieving state-of-the-art results in many natural language
processing applications. However, deploying these models can be prohibitively
costly, as the standard self-attention mechanism of the Transformer suffers
from quadratic computational cost in the input sequence length. To confront
this, we propose FCA, a fine- and coarse-granularity hybrid self-attention that
reduces the computation cost through progressively shortening the computational
sequence length in self-attention. Specifically, FCA conducts an
attention-based scoring strategy to determine the informativeness of tokens at
each layer. Then, the informative tokens serve as the fine-granularity
computing units in self-attention and the uninformative tokens are replaced
with one or several clusters as the coarse-granularity computing units in
self-attention. Experiments on GLUE and RACE datasets show that BERT with FCA
achieves 2x reduction in FLOPs over original BERT with &lt;1% loss in accuracy. We
show that FCA offers a significantly better trade-off between accuracy and
FLOPs compared to prior methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIMO-2: End-to-End Unified Vision-Language Grounded Learning. (arXiv:2203.09067v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09067">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pre-training (VLP) has achieved impressive performance on
various cross-modal downstream tasks. However, most existing methods can only
learn from aligned image-caption data and rely heavily on expensive regional
features, which greatly limits their scalability and performance. In this
paper, we propose an end-to-end unified-modal pre-training framework, namely
UNIMO-2, for joint learning on both aligned image-caption data and unaligned
image-only and text-only corpus. We build a unified Transformer model to
jointly learn visual representations, textual representations and semantic
alignment between images and texts. In particular, we propose to conduct
grounded learning on both images and texts via a sharing grounded space, which
helps bridge unaligned images and texts, and align the visual and textual
semantic spaces on different types of corpora. The experiments show that our
grounded learning method can improve textual and visual semantic alignment for
improving performance on various cross-modal tasks. Moreover, benefiting from
effective joint modeling of different types of corpora, our model also achieves
impressive performance on single-modal visual and textual tasks. Our code and
models are public at the UNIMO project page https://unimo-ptm.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaussian Multi-head Attention for Simultaneous Machine Translation. (arXiv:2203.09072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09072">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) outputs translation while receiving
the streaming source inputs, and hence needs a policy to determine where to
start translating. The alignment between target and source words often implies
the most informative source word for each target word, and hence provides the
unified control over translation quality and latency, but unfortunately the
existing SiMT methods do not explicitly model the alignment to perform the
control. In this paper, we propose Gaussian Multi-head Attention (GMA) to
develop a new SiMT policy by modeling alignment and translation in a unified
manner. For SiMT policy, GMA models the aligned source position of each target
word, and accordingly waits until its aligned position to start translating. To
integrate the learning of alignment into the translation model, a Gaussian
distribution centered on predicted aligned position is introduced as an
alignment-related prior, which cooperates with translation-related soft
attention to determine the final attention. Experiments on En-Vi and De-En
tasks show that our method outperforms strong baselines on the trade-off
between translation and latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ask to Understand: Question Generation for Multi-hop Question Answering. (arXiv:2203.09073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09073">
<div class="article-summary-box-inner">
<span><p>Multi-hop Question Answering (QA) requires the machine to answer complex
questions by finding scattering clues and reasoning from multiple documents.
Graph Network (GN) and Question Decomposition (QD) are two common approaches at
present. The former uses the "black-box" reasoning process to capture the
potential relationship between entities and sentences, thus achieving good
performance. At the same time, the latter provides a clear reasoning logical
route by decomposing multi-hop questions into simple single-hop sub-questions.
In this paper, we propose a novel method to complete multi-hop QA from the
perspective of Question Generation (QG). Specifically, we carefully design an
end-to-end QG module on the basis of a classical QA module, which could help
the model understand the context by asking inherently logical sub-questions,
thus inheriting interpretability from the QD-based method and showing superior
performance. Experiments on the HotpotQA dataset demonstrate that the
effectiveness of our proposed QG module, human evaluation further clarifies its
interpretability quantitatively, and thorough analysis shows that the QG module
could generate better sub-questions than QD methods in terms of fluency,
consistency, and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLANET: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation. (arXiv:2203.09100v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09100">
<div class="article-summary-box-inner">
<span><p>Despite recent progress of pre-trained language models on generating fluent
text, existing methods still suffer from incoherence problems in long-form text
generation tasks that require proper content control and planning to form a
coherent high-level logical flow. In this work, we propose PLANET, a novel
generation framework leveraging autoregressive self-attention mechanism to
conduct content planning and surface realization dynamically. To guide the
generation of output sentences, our framework enriches the Transformer decoder
with latent representations to maintain sentence-level semantic plans grounded
by bag-of-words. Moreover, we introduce a new coherence-based contrastive
learning objective to further improve the coherence of output. Extensive
experiments are conducted on two challenging long-form text generation tasks
including counterargument generation and opinion article generation. Both
automatic and human evaluations show that our method significantly outperforms
strong baselines and generates more coherent texts with richer contents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction. (arXiv:2203.09101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09101">
<div class="article-summary-box-inner">
<span><p>Despite the importance of relation extraction in building and representing
knowledge, less research is focused on generalizing to unseen relations types.
We introduce the task setting of Zero-Shot Relation Triplet Extraction
(ZeroRTE) to encourage further research in low-resource relation extraction
methods. Given an input sentence, each extracted triplet consists of the head
entity, relation label, and tail entity where the relation label is not seen at
the training stage. To solve ZeroRTE, we propose to synthesize relation
examples by prompting language models to generate structured texts. Concretely,
we unify language model prompts and structured text approaches to design a
structured prompt template for generating synthetic relation samples when
conditioning on relation label prompts (RelationPrompt). To overcome the
limitation for extracting multiple relation triplets in a sentence, we design a
novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL
datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot
relation classification. Our code and data are available at
github.com/declare-lab/RelationPrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph-Enabled Text-Based Automatic Personality Prediction. (arXiv:2203.09103v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09103">
<div class="article-summary-box-inner">
<span><p>How people think, feel, and behave, primarily is a representation of their
personality characteristics. By being conscious of personality characteristics
of individuals whom we are dealing with or decided to deal with, one can
competently ameliorate the relationship, regardless of its type. With the rise
of Internet-based communication infrastructures (social networks, forums,
etc.), a considerable amount of human communications take place there. The most
prominent tool in such communications, is the language in written and spoken
form that adroitly encodes all those essential personality characteristics of
individuals. Text-based Automatic Personality Prediction (APP) is the automated
forecasting of the personality of individuals based on the generated/exchanged
text contents. This paper presents a novel knowledge graph-enabled approach to
text-based APP that relies on the Big Five personality traits. To this end,
given a text a knowledge graph which is a set of interlinked descriptions of
concepts, was built through matching the input text's concepts with DBpedia
knowledge base entries. Then, due to achieving more powerful representation the
graph was enriched with the DBpedia ontology, NRC Emotion Intensity Lexicon,
and MRC psycholinguistic database information. Afterwards, the knowledge graph
which is now a knowledgeable alternative for the input text was embedded to
yield an embedding matrix. Finally, to perform personality predictions the
resulting embedding matrix was fed to four suggested deep learning models
independently, which are based on convolutional neural network (CNN), simple
recurrent neural network (RNN), long short term memory (LSTM) and bidirectional
long short term memory (BiLSTM). The results indicated a considerable
improvements in prediction accuracies in all of the suggested classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Community-Driven Comprehensive Scientific Paper Summarization: Insight from cvpaper.challenge. (arXiv:2203.09109v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09109">
<div class="article-summary-box-inner">
<span><p>The present paper introduces a group activity involving writing summaries of
conference proceedings by volunteer participants. The rapid increase in
scientific papers is a heavy burden for researchers, especially non-native
speakers, who need to survey scientific literature. To alleviate this problem,
we organized a group of non-native English speakers to write summaries of
papers presented at a computer vision conference to share the knowledge of the
papers read by the group. We summarized a total of 2,000 papers presented at
the Conference on Computer Vision and Pattern Recognition, a top-tier
conference on computer vision, in 2019 and 2020. We quantitatively analyzed
participants' selection regarding which papers they read among the many
available papers. The experimental results suggest that we can summarize a wide
range of papers without asking participants to read papers unrelated to their
interests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time and the Value of Data. (arXiv:2203.09118v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09118">
<div class="article-summary-box-inner">
<span><p>Managers often believe that collecting more data will continually improve the
accuracy of their machine learning models. However, we argue in this paper that
when data lose relevance over time, it may be optimal to collect a limited
amount of recent data instead of keeping around an infinite supply of older
(less relevant) data. In addition, we argue that increasing the stock of data
by including older datasets may, in fact, damage the model's accuracy.
Expectedly, the model's accuracy improves by increasing the flow of data
(defined as data collection rate); however, it requires other tradeoffs in
terms of refreshing or retraining machine learning models more frequently.
</p>
<p>Using these results, we investigate how the business value created by machine
learning models scales with data and when the stock of data establishes a
sustainable competitive advantage. We argue that data's time-dependency weakens
the barrier to entry that the stock of data creates. As a result, a competing
firm equipped with a limited (yet sufficient) amount of recent data can develop
more accurate models. This result, coupled with the fact that older datasets
may deteriorate models' accuracy, suggests that created business value doesn't
scale with the stock of available data unless the firm offloads less relevant
data from its data repository. Consequently, a firm's growth policy should
incorporate a balance between the stock of historical data and the flow of new
data.
</p>
<p>We complement our theoretical results with an experiment. In the experiment,
we empirically measure the loss in the accuracy of a next word prediction model
trained on datasets from various time periods. Our empirical measurements
confirm the economic significance of the value decline over time. For example,
100MB of text data, after seven years, becomes as valuable as 50MB of current
data for the next word prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POLARIS: A Geographic Pre-trained Model and its Applications in Baidu Maps. (arXiv:2203.09127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09127">
<div class="article-summary-box-inner">
<span><p>Pre-trained models (PTMs) have become a fundamental backbone for downstream
tasks in natural language processing and computer vision. Despite initial gains
that were obtained by applying generic PTMs to geo-related tasks at Baidu Maps,
a clear performance plateau over time was observed. One of the main reasons for
this plateau is the lack of readily available geographic knowledge in generic
PTMs. To address this problem, in this paper, we present POLARIS, which is a
geographic pre-trained model designed and developed for improving the
geo-related tasks at Baidu Maps. POLARIS is elaborately designed to learn a
universal representation of geography-language by pre-training on large-scale
data generated from a heterogeneous graph that contains abundant geographic
knowledge. Extensive quantitative and qualitative experiments conducted on
large-scale real-world datasets demonstrate the superiority and effectiveness
of POLARIS. POLARIS has already been deployed in production at Baidu Maps since
April 2021, which significantly benefits the performance of a wide range of
downstream tasks. This demonstrates that POLARIS can serve as a fundamental
backbone for geo-related tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Dependency, Data Flow, and Competitive Advantage. (arXiv:2203.09128v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09128">
<div class="article-summary-box-inner">
<span><p>Data is fundamental to machine learning-based products and services and is
considered strategic due to its externalities for businesses, governments,
non-profits, and more generally for society. It is renowned that the value of
organizations (businesses, government agencies and programs, and even
industries) scales with the volume of available data. What is often less
appreciated is that the data value in making useful organizational predictions
will range widely and is prominently a function of data characteristics and
underlying algorithms.
</p>
<p>In this research, our goal is to study how the value of data changes over
time and how this change varies across contexts and business areas (e.g. next
word prediction in the context of history, sports, politics). We focus on data
from Reddit.com and compare the value's time-dependency across various Reddit
topics (Subreddits). We make this comparison by measuring the rate at which
user-generated text data loses its relevance to the algorithmic prediction of
conversations. We show that different subreddits have different rates of
relevance decline over time.
</p>
<p>Relating the text topics to various business areas of interest, we argue that
competing in a business area in which data value decays rapidly alters
strategies to acquire competitive advantage. When data value decays rapidly,
access to a continuous flow of data will be more valuable than access to a
fixed stock of data. In this kind of setting, improving user engagement and
increasing user-base help creating and maintaining a competitive advantage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Type-Driven Multi-Turn Corrections for Grammatical Error Correction. (arXiv:2203.09136v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09136">
<div class="article-summary-box-inner">
<span><p>Grammatical Error Correction (GEC) aims to automatically detect and correct
grammatical errors. In this aspect, dominant models are trained by
one-iteration learning while performing multiple iterations of corrections
during inference. Previous studies mainly focus on the data augmentation
approach to combat the exposure bias, which suffers from two drawbacks. First,
they simply mix additionally-constructed training instances and original ones
to train models, which fails to help models be explicitly aware of the
procedure of gradual corrections. Second, they ignore the interdependence
between different types of corrections. In this paper, we propose a Type-Driven
Multi-Turn Corrections approach for GEC. Using this approach, from each
training instance, we additionally construct multiple training instances, each
of which involves the correction of a specific type of errors. Then, we use
these additionally-constructed training instances and the original one to train
the model in turn. Experimental results and in-depth analysis show that our
approach significantly benefits the model training. Particularly, our enhanced
model achieves state-of-the-art single-model performance on English GEC
benchmarks. We release our code at Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of speech intelligibility with DNN-based performance measures. (arXiv:2203.09148v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09148">
<div class="article-summary-box-inner">
<span><p>This paper presents a speech intelligibility model based on automatic speech
recognition (ASR), combining phoneme probabilities from deep neural networks
(DNN) and a performance measure that estimates the word error rate from these
probabilities. This model does not require the clean speech reference nor the
word labels during testing as the ASR decoding step, which finds the most
likely sequence of words given phoneme posterior probabilities, is omitted. The
model is evaluated via the root-mean-squared error between the predicted and
observed speech reception thresholds from eight normal-hearing listeners. The
recognition task consists of identifying noisy words from a German matrix
sentence test. The speech material was mixed with eight noise maskers covering
different modulation types, from speech-shaped stationary noise to a
single-talker masker. The prediction performance is compared to five
established models and an ASR-model using word labels. Two combinations of
features and networks were tested. Both include temporal information either at
the feature level (amplitude modulation filterbanks and a feed-forward network)
or captured by the architecture (mel-spectrograms and a time-delay deep neural
network, TDNN). The TDNN model is on par with the DNN while reducing the number
of parameters by a factor of 37; this optimization allows parallel streams on
dedicated hearing aid hardware as a forward-pass can be computed within the
10ms of each frame. The proposed model performs almost as well as the
label-based model and produces more accurate predictions than the baseline
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09161">
<div class="article-summary-box-inner">
<span><p>Recently introduced instruction-paradigm empowers non-expert users to
leverage NLP resources by defining a new task in natural language.
Instruction-tuned models have significantly outperformed multitask learning
models (without instruction); however they are far from state of the art task
specific models. Conventional approaches to improve model performance via
creating large datasets with lots of task instances or architectural/training
changes in model may not be feasible for non-expert users. However, they can
write alternate instructions to represent an instruction task. Is
Instruction-augumentation helpful? We augment a subset of tasks in NATURAL
INSTRUCTIONS with additional instructions and find that these significantly
improve model performance (upto 35%) specially in low-data regime. Our results
indicate that an additional instruction can be equivalent to ~40 instances on
average across our evaluation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Dual Read/Write Paths for Simultaneous Machine Translation. (arXiv:2203.09163v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09163">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) outputs the translation while reading
the source sentence and hence requires a policy to determine whether to wait
for the next source word (READ) or generate a target word (WRITE), the actions
of which form a read/write path. Although the read/write path is essential to
SiMT performance, there is no direct supervision given to the path in the
existing methods. In this paper, we propose a method of Dual Path SiMT which
introduces duality constraints to guide the read/write path. According to
duality constraints, the read/write paths in source-to-target and
target-to-source SiMT models can be mapped to each other. Therefore, the SiMT
models in two directions are jointly optimized by forcing their read/write
paths to satisfy the mapping relation. Experiments on En-Vi and De-En SiMT
tasks show that our method can outperform strong baselines under all latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Vision Features in Multimodal Machine Translation. (arXiv:2203.09173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09173">
<div class="article-summary-box-inner">
<span><p>Previous work on multimodal machine translation (MMT) has focused on the way
of incorporating vision features into translation but little attention is on
the quality of vision models. In this work, we investigate the impact of vision
models on MMT. Given the fact that Transformer is becoming popular in computer
vision, we experiment with various strong models (such as Vision Transformer)
and enhanced features (such as object-detection and image captioning). We
develop a selective attention model to study the patch-level contribution of an
image in MMT. On detailed probing tasks, we find that stronger vision models
are helpful for learning translation from the visual modality. Our results also
suggest the need of carefully examining MMT models, especially when current
benchmarks are small-scale and biased. Our code could be found at
\url{https://github.com/libeineu/fairseq_mmt}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation. (arXiv:2203.09176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09176">
<div class="article-summary-box-inner">
<span><p>Residual networks are an Euler discretization of solutions to Ordinary
Differential Equations (ODE). This paper explores a deeper relationship between
Transformer and numerical ODE methods. We first show that a residual block of
layers in Transformer can be described as a higher-order solution to ODE.
Inspired by this, we design a new architecture, {\it ODE Transformer}, which is
analogous to the Runge-Kutta method that is well motivated in ODE. As a natural
extension to Transformer, ODE Transformer is easy to implement and efficient to
use. Experimental results on the large-scale machine translation, abstractive
summarization, and grammar error correction tasks demonstrate the high
genericity of ODE Transformer. It can gain large improvements in model
performance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the
WMT'14 English-German and English-French benchmarks) at a slight cost in
inference efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Detection of Personal Employment Status on Twitter. (arXiv:2203.09178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09178">
<div class="article-summary-box-inner">
<span><p>Detecting disclosures of individuals' employment status on social media can
provide valuable information to match job seekers with suitable vacancies,
offer social protection, or measure labor market flows. However, identifying
such personal disclosures is a challenging task due to their rarity in a sea of
social media content and the variety of linguistic forms used to describe them.
Here, we examine three Active Learning (AL) strategies in real-world settings
of extreme class imbalance, and identify five types of disclosures about
individuals' employment status (e.g. job loss) in three languages using
BERT-based classification models. Our findings show that, even under extreme
imbalance settings, a small number of AL iterations is sufficient to obtain
large and significant gains in precision, recall, and diversity of results
compared to a supervised baseline with the same number of labels. We also find
that no AL strategy consistently outperforms the rest. Qualitative analysis
suggests that AL helps focus the attention mechanism of BERT on core terms and
adjust the boundaries of semantic expansion, highlighting the importance of
interpretable models to provide greater control and visibility into this
dynamic learning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoMe: A Robust Metric for Evaluating Natural Language Generation. (arXiv:2203.09183v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09183">
<div class="article-summary-box-inner">
<span><p>Evaluating Natural Language Generation (NLG) systems is a challenging task.
Firstly, the metric should ensure that the generated hypothesis reflects the
reference's semantics. Secondly, it should consider the grammatical quality of
the generated sentence. Thirdly, it should be robust enough to handle various
surface forms of the generated sentence. Thus, an effective evaluation metric
has to be multifaceted. In this paper, we propose an automatic evaluation
metric incorporating several core aspects of natural language understanding
(language competence, syntactic and semantic variation). Our proposed metric,
RoMe, is trained on language features such as semantic similarity combined with
tree edit distance and grammatical acceptability, using a self-supervised
neural network to assess the overall quality of the generated sentence.
Moreover, we perform an extensive robustness analysis of the state-of-the-art
methods and RoMe. Empirical results suggest that RoMe has a stronger
correlation to human judgment over state-of-the-art metrics in evaluating
system-generated sentences across several NLG tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstract Interpretation on E-Graphs. (arXiv:2203.09191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09191">
<div class="article-summary-box-inner">
<span><p>Recent e-graph applications have typically considered concrete semantics of
expressions, where the notion of equivalence stems from concrete interpretation
of expressions. However, equivalences that hold over one interpretation may not
hold in an alternative interpretation. Such an observation can be exploited. We
consider the application of abstract interpretation to e-graphs, and show that
within an e-graph, the lattice meet operation associated with the abstract
domain has a natural interpretation for an e-class, leading to improved
precision in over-approximation. In this extended abstract, we use Interval
Arithmetic (IA) to illustrate this point.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists. (arXiv:2203.09192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09192">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) models risk overfitting to specific terms
in the training data, thereby reducing their performance, fairness, and
generalizability. E.g., neural hate speech detection models are strongly
influenced by identity terms like gay, or women, resulting in false positives,
severe unintended bias, and lower performance. Most mitigation techniques use
lists of identity terms or samples from the target domain during training.
However, this approach requires a-priori knowledge and introduces further bias
if important terms are neglected. Instead, we propose a knowledge-free
Entropy-based Attention Regularization (EAR) to discourage overfitting to
training-specific terms. An additional objective function penalizes tokens with
low self-attention entropy. We fine-tune BERT via EAR: the resulting model
matches or exceeds state-of-the-art performance for hate speech classification
and bias metrics on three benchmark corpora in English and Italian. EAR also
reveals overfitting terms, i.e., terms most likely to induce bias, to help
identify their effect on the model, task, and predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Conditional Masked Language Pre-training for Neural Machine Translation. (arXiv:2203.09210v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09210">
<div class="article-summary-box-inner">
<span><p>Pre-trained sequence-to-sequence models have significantly improved Neural
Machine Translation (NMT). Different from prior works where pre-trained models
usually adopt an unidirectional decoder, this paper demonstrates that
pre-training a sequence-to-sequence model but with a bidirectional decoder can
produce notable performance gains for both Autoregressive and
Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked
language model pre-trained on large-scale bilingual and monolingual corpora in
many languages. We also introduce two simple but effective methods to enhance
the CeMAT, aligned code-switching &amp; masking and dynamic dual-masking. We
conduct extensive experiments and show that our CeMAT can achieve significant
performance improvement for all scenarios from low to extremely high resource,
i.e., up to 14.4 BLEU on low resource and 7.9 BLEU improvements on average for
Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also
produce consistent performance gains, i.e., up to 5.3 BLEU. As far as we know,
this is the first work to pre-train a unified model for fine-tuning on both NMT
tasks. Code, data, and pre-trained models are available at
https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence Calibration for Intent Detection via Hyperspherical Space and Rebalanced Accuracy-Uncertainty Loss. (arXiv:2203.09278v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09278">
<div class="article-summary-box-inner">
<span><p>Data-driven methods have achieved notable performance on intent detection,
which is a task to comprehend user queries. Nonetheless, they are controversial
for over-confident predictions. In some scenarios, users do not only care about
the accuracy but also the confidence of model. Unfortunately, mainstream neural
networks are poorly calibrated, with a large gap between accuracy and
confidence. To handle this problem defined as confidence calibration, we
propose a model using the hyperspherical space and rebalanced
accuracy-uncertainty loss. Specifically, we project the label vector onto
hyperspherical space uniformly to generate a dense label representation matrix,
which mitigates over-confident predictions due to overfitting sparce one-hot
label matrix. Besides, we rebalance samples of different accuracy and
uncertainty to better guide model training. Experiments on the open datasets
verify that our model outperforms the existing calibration methods and achieves
a significant improvement on the calibration metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Structural Knowledge in Multimodal-BERT. (arXiv:2203.09306v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09306">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate the knowledge learned in the embeddings of
multimodal-BERT models. More specifically, we probe their capabilities of
storing the grammatical structure of linguistic data and the structure learned
over objects in visual data. To reach that goal, we first make the inherent
structure of language and visuals explicit by a dependency parse of the
sentences that describe the image and by the dependencies between the object
regions in the image, respectively. We call this explicit visual structure the
\textit{scene tree}, that is based on the dependency tree of the language
description. Extensive probing experiments show that the multimodal-BERT models
do not encode these scene trees.Code available at
\url{https://github.com/VSJMilewski/multimodal-probes}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training. (arXiv:2203.09313v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09313">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-training has shown remarkable performance in building
open-domain dialogue systems. However, previous works mainly focus on showing
and evaluating the conversational performance of the released dialogue model,
ignoring the discussion of some key factors towards a powerful human-like
chatbot, especially in Chinese scenarios. In this paper, we conduct extensive
experiments to investigate these under-explored factors, including data quality
control, model architecture designs, training approaches, and decoding
strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese
dialogue model with 2.8 billion parameters, and make our models and code
publicly available. To our knowledge, EVA2.0 is the largest open-source Chinese
dialogue model. Automatic and human evaluations show that our model
significantly outperforms other open-source counterparts. We also discuss the
limitations of this work by presenting some failure cases and pose some future
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Static and Contextualised Multilingual Embeddings. (arXiv:2203.09326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09326">
<div class="article-summary-box-inner">
<span><p>Static and contextual multilingual embeddings have complementary strengths.
Static embeddings, while less expressive than contextual language models, can
be more straightforwardly aligned across multiple languages. We combine the
strengths of static and contextual models to improve multilingual
representations. We extract static embeddings for 40 languages from XLM-R,
validate those embeddings with cross-lingual word retrieval, and then align
them using VecMap. This results in high-quality, highly multilingual static
embeddings. Then we apply a novel continued pre-training approach to XLM-R,
leveraging the high quality alignment of our static embeddings to better align
the representation space of XLM-R. We show positive results for multiple
complex semantic tasks. We release the static embeddings and the continued
pre-training code. Unlike most previous work, our continued pre-training
approach does not require parallel text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation. (arXiv:2203.09391v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09391">
<div class="article-summary-box-inner">
<span><p>Data Augmentation (DA) is known to improve the generalizability of deep
neural networks. Most existing DA techniques naively add a certain number of
augmented samples without considering the quality and the added computational
cost of these samples. To tackle this problem, a common strategy, adopted by
several state-of-the-art DA methods, is to adaptively generate or re-weight
augmented samples with respect to the task objective during training. However,
these adaptive DA methods: (1) are computationally expensive and not
sample-efficient, and (2) are designed merely for a specific setting. In this
work, we present a universal DA technique, called Glitter, to overcome both
issues. Glitter can be plugged into any DA method, making training
sample-efficient without sacrificing performance. From a pre-generated pool of
augmented samples, Glitter adaptively selects a subset of worst-case samples
with maximal loss, analogous to adversarial DA. Without altering the training
strategy, the task objective can be optimized on the selected subset. Our
thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three
widely used training setups including consistency training, self-distillation
and knowledge distillation reveal that Glitter is substantially faster to train
and achieves a competitive performance, compared to strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models. (arXiv:2203.09397v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09397">
<div class="article-summary-box-inner">
<span><p>Relations between words are governed by hierarchical structure rather than
linear ordering. Sequence-to-sequence (seq2seq) models, despite their success
in downstream NLP applications, often fail to generalize in a
hierarchy-sensitive manner when performing syntactic transformations - for
example, transforming declarative sentences into questions. However, syntactic
evaluations of seq2seq models have only observed models that were not
pre-trained on natural language data before being trained to perform syntactic
transformations, in spite of the fact that pre-training has been found to
induce hierarchical linguistic generalizations in language models; in other
words, the syntactic capabilities of seq2seq models may have been greatly
understated. We address this gap using the pre-trained seq2seq models T5 and
BART, as well as their multilingual variants mT5 and mBART. We evaluate whether
they generalize hierarchically on two transformations in two languages:
question formation and passivization in English and German. We find that
pre-trained seq2seq models generalize hierarchically when performing syntactic
transformations, whereas models trained from scratch on syntactic
transformations do not. This result presents evidence for the learnability of
hierarchical syntactic information from non-annotated natural language text
while also demonstrating that seq2seq models are capable of syntactic
generalization, though only after exposure to much more language data than
human learners receive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">elBERto: Self-supervised Commonsense Learning for Question Answering. (arXiv:2203.09424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09424">
<div class="article-summary-box-inner">
<span><p>Commonsense question answering requires reasoning about everyday situations
and causes and effects implicit in context. Typically, existing approaches
first retrieve external evidence and then perform commonsense reasoning using
these evidence. In this paper, we propose a Self-supervised Bidirectional
Encoder Representation Learning of Commonsense (elBERto) framework, which is
compatible with off-the-shelf QA model architectures. The framework comprises
five self-supervised tasks to force the model to fully exploit the additional
training signals from contexts containing rich commonsense. The tasks include a
novel Contrastive Relation Learning task to encourage the model to distinguish
between logically contrastive contexts, a new Jigsaw Puzzle task that requires
the model to infer logical chains in long contexts, and three classic SSL tasks
to maintain pre-trained models language encoding ability. On the representative
WIQA, CosmosQA, and ReClor datasets, elBERto outperforms all other methods,
including those utilizing explicit graph reasoning and external knowledge
retrieval. Moreover, elBERto achieves substantial improvements on
out-of-paragraph and no-effect questions where simple lexical similarity
comparison does not help, indicating that it successfully learns commonsense
and is able to leverage it when given dynamic context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation. (arXiv:2203.09435v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09435">
<div class="article-summary-box-inner">
<span><p>The performance of multilingual pretrained models is highly dependent on the
availability of monolingual or parallel text present in a target language.
Thus, the majority of the world's languages cannot benefit from recent progress
in NLP as they have no or limited textual data. To expand possibilities of
using NLP technology in these under-represented languages, we systematically
study strategies that relax the reliance on conventional language resources
through the use of bilingual lexicons, an alternative resource with much better
language coverage. We analyze different strategies to synthesize textual or
labeled data using lexicons, and how this data can be combined with monolingual
or parallel text when available. For 19 under-represented languages across 3
tasks, our methods lead to consistent improvements of up to 5 and 15 points
with and without extra monolingual text respectively. Overall, our study
highlights how NLP methods can be adapted to thousands more languages that are
under-served by current technology
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models. (arXiv:2203.09486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09486">
<div class="article-summary-box-inner">
<span><p>We propose a framework for training non-autoregressive sequence-to-sequence
models for editing tasks, where the original input sequence is iteratively
edited to produce the output. We show that the imitation learning algorithms
designed to train such models for machine translation introduces mismatches
between training and inference that lead to undertraining and poor
generalization in editing scenarios. We address this issue with two
complementary strategies: 1) a roll-in policy that exposes the model to
intermediate training sequences that it is more likely to encounter during
inference, 2) a curriculum that presents easy-to-learn edit operations first,
gradually increasing the difficulty of training samples as the model becomes
competent. We show the efficacy of these strategies on two challenging English
editing tasks: controllable text simplification and abstractive summarization.
Our approach significantly improves output quality on both tasks and controls
output complexity better on the simplification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents. (arXiv:2203.09498v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09498">
<div class="article-summary-box-inner">
<span><p>Learned communication between agents is a powerful tool when approaching
decision-making problems that are hard to overcome by any single agent in
isolation. However, continual coordination and communication learning between
machine agents or human-machine partnerships remains a challenging open
problem. As a stepping stone toward solving the continual communication
learning problem, in this paper we contribute a multi-faceted study into what
we term Pavlovian signalling -- a process by which learned, temporally extended
predictions made by one agent inform decision-making by another agent with
different perceptual access to their shared environment. We seek to establish
how different temporal processes and representational choices impact Pavlovian
signalling between learning agents. To do so, we introduce a partially
observable decision-making domain we call the Frost Hollow. In this domain a
prediction learning agent and a reinforcement learning agent are coupled into a
two-part decision-making system that seeks to acquire sparse reward while
avoiding time-conditional hazards. We evaluate two domain variations: 1)
machine prediction and control learning in a linear walk, and 2) a prediction
learning machine interacting with a human participant in a virtual reality
environment. Our results showcase the speed of learning for Pavlovian
signalling, the impact that different temporal representations do (and do not)
have on agent-agent coordination, and how temporal aliasing impacts agent-agent
and human-agent interactions differently. As a main contribution, we establish
Pavlovian signalling as a natural bridge between fixed signalling paradigms and
fully adaptive communication learning. Our results therefore point to an
actionable, constructivist path towards continual communication learning
between reinforcement learning agents, with potential impact in a range of
real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. (arXiv:2203.09509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09509">
<div class="article-summary-box-inner">
<span><p>Toxic language detection systems often falsely flag text that contains
minority group mentions as toxic, as those groups are often the targets of
online hate. Such over-reliance on spurious correlations also causes systems to
struggle with detecting implicitly toxic language. To help mitigate these
issues, we create ToxiGen, a new large-scale and machine-generated dataset of
274k toxic and benign statements about 13 minority groups. We develop a
demonstration-based prompting framework and an adversarial
classifier-in-the-loop decoding method to generate subtly toxic and benign text
with a massive pretrained language model. Controlling machine generation in
this way allows ToxiGen to cover implicitly toxic text at a larger scale, and
about more demographic groups, than previous resources of human-written text.
We conduct a human evaluation on a challenging subset of ToxiGen and find that
annotators struggle to distinguish machine-generated text from human-written
language. We also find that 94.5% of toxic examples are labeled as hate speech
by human annotators. Using three publicly-available datasets, we show that
finetuning a toxicity classifier on our data improves its performance on
human-written data substantially. We also demonstrate that ToxiGen can be used
to fight machine-generated toxicity as finetuning improves the classifier
significantly on our evaluation subset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KART: Parameterization of Privacy Leakage Scenarios from Pre-trained Language Models. (arXiv:2101.00036v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00036">
<div class="article-summary-box-inner">
<span><p>For the safe sharing pre-trained language models, no guidelines exist at
present owing to the difficulty in estimating the upper bound of the risk of
privacy leakage. One problem is that previous studies have assessed the risk
for different real-world privacy leakage scenarios and attack methods, which
reduces the portability of the findings. To tackle this problem, we represent
complex real-world privacy leakage scenarios under a universal
parameterization, \textit{Knowledge, Anonymization, Resource, and Target}
(KART). KART parameterization has two merits: (i) it clarifies the definition
of privacy leakage in each experiment and (ii) it improves the comparability of
the findings of risk assessments. We show that previous studies can be simply
reviewed by parameterizing the scenarios with KART. We also demonstrate privacy
risk assessments in different scenarios under the same attack method, which
suggests that KART helps approximate the upper bound of risk under a specific
attack or scenario. We believe that KART helps integrate past and future
findings on privacy risk and will contribute to a standard for sharing language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLM: General Language Model Pretraining with Autoregressive Blank Infilling. (arXiv:2103.10360v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10360">
<div class="article-summary-box-inner">
<span><p>There have been various types of pretraining architectures including
autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and
encoder-decoder models (e.g., T5). However, none of the pretraining frameworks
performs the best for all tasks of three main categories including natural
language understanding (NLU), unconditional generation, and conditional
generation. We propose a General Language Model (GLM) based on autoregressive
blank infilling to address this challenge. GLM improves blank filling
pretraining by adding 2D positional encodings and allowing an arbitrary order
to predict spans, which results in performance gains over BERT and T5 on NLU
tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying
the number and lengths of blanks. On a wide range of tasks across NLU,
conditional and unconditional generation, GLM outperforms BERT, T5, and GPT
given the same model sizes and data, and achieves the best performance from a
single pretrained model with 1.25x parameters of BERT Large , demonstrating its
generalizability to different downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">First the worst: Finding better gender translations during beam search. (arXiv:2104.07429v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07429">
<div class="article-summary-box-inner">
<span><p>Neural machine translation inference procedures like beam search generate the
most likely output under the model. This can exacerbate any demographic biases
exhibited by the model. We focus on gender bias resulting from systematic
errors in grammatical gender translation, which can lead to human referents
being misrepresented or misgendered.
</p>
<p>Most approaches to this problem adjust the training data or the model. By
contrast, we experiment with simply adjusting the inference procedure. We
experiment with reranking nbest lists using gender features obtained
automatically from the source sentence, and applying gender constraints while
decoding to improve nbest list gender diversity. We find that a combination of
these techniques allows large gains in WinoMT accuracy without requiring
additional bilingual data or an additional NMT model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Idioms: Conventionality and Contingency. (arXiv:2104.08664v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08664">
<div class="article-summary-box-inner">
<span><p>Idioms are unlike other phrases in two important ways. First, the words in an
idiom have unconventional meanings. Second, the unconventional meaning of words
in an idiom are contingent on the presence of the other words in the idiom.
Linguistic theories disagree about whether these two properties depend on one
another, as well as whether special theoretical machinery is needed to
accommodate idioms. We define two measures that correspond to these two
properties, and we show that idioms fall at the expected intersection of the
two dimensions, but that the dimensions themselves are not correlated. Our
results suggest that idioms are no more anomalous than other types of phrases,
and that introducing special machinery to handle idioms may not be warranted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages. (arXiv:2104.08726v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08726">
<div class="article-summary-box-inner">
<span><p>Pretrained multilingual models are able to perform cross-lingual transfer in
a zero-shot setting, even for languages unseen during pretraining. However,
prior work evaluating performance on unseen languages has largely been limited
to low-level, syntactic tasks, and it remains unclear if zero-shot learning of
high-level, semantic tasks is possible for unseen languages. To explore this
question, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018)
to 10 indigenous languages of the Americas. We conduct experiments with XLM-R,
testing multiple zero-shot and translation-based approaches. Additionally, we
explore model adaptation via continued pretraining and provide an analysis of
the dataset by considering hypothesis-only models. We find that XLM-R's
zero-shot performance is poor for all 10 languages, with an average performance
of 38.62%. Continued pretraining offers improvements, with an average accuracy
of 44.05%. Surprisingly, training on poorly translated data by far outperforms
all other methods with an accuracy of 48.72%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Multi-Branch Layers for On-Device Neural Machine Translation. (arXiv:2105.06679v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06679">
<div class="article-summary-box-inner">
<span><p>With the rapid development of artificial intelligence (AI), there is a trend
in moving AI applications, such as neural machine translation (NMT), from cloud
to mobile devices. Constrained by limited hardware resources and battery, the
performance of on-device NMT systems is far from satisfactory. Inspired by
conditional computation, we propose to improve the performance of on-device NMT
systems with dynamic multi-branch layers. Specifically, we design a layer-wise
dynamic multi-branch network with only one branch activated during training and
inference. As not all branches are activated during training, we propose
shared-private reparameterization to ensure sufficient training for each
branch. At almost the same computational cost, our method achieves improvements
of up to 1.7 BLEU points on the WMT14 English-German translation task and 1.8
BLEU points on the WMT20 Chinese-English translation task over the Transformer
model, respectively. Compared with a strong baseline that also uses multiple
branches, the proposed method is up to 1.5 times faster with the same number of
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues. (arXiv:2105.07122v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07122">
<div class="article-summary-box-inner">
<span><p>It is a common practice for recent works in vision language cross-modal
reasoning to adopt a binary or multi-choice classification formulation taking
as input a set of source image(s) and textual query. In this work, we take a
sober look at such an unconditional formulation in the sense that no prior
knowledge is specified with respect to the source image(s). Inspired by the
designs of both visual commonsense reasoning and natural language inference
tasks, we propose a new task termed Premise-based Multi-modal Reasoning(PMR)
where a textual premise is the background presumption on each source image. The
PMR dataset contains 15,360 manually annotated samples which are created by a
multi-phase crowd-sourcing process. With selected high-quality movie
screenshots and human-curated premise templates from 6 pre-defined categories,
we ask crowd-source workers to write one true hypothesis and three distractors
(4 choices) given the premise and image through a cross-check procedure.
Besides, we generate adversarial samples to alleviate the annotation artifacts
and double the size of PMR. We benchmark various state-of-the-art (pretrained)
multi-modal inference models on PMR and conduct comprehensive experimental
analyses to showcase the utility of our dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10199">
<div class="article-summary-box-inner">
<span><p>We introduce BitFit, a sparse-finetuning method where only the bias-terms of
the model (or a subset of them) are being modified. We show that with
small-to-medium training data, applying BitFit on pre-trained BERT models is
competitive with (and sometimes better than) fine-tuning the entire model. For
larger data, the method is competitive with other sparse fine-tuning methods.
Besides their practical utility, these findings are relevant for the question
of understanding the commonly-used process of finetuning: they support the
hypothesis that finetuning is mainly about exposing knowledge induced by
language-modeling training, rather than learning new task-specific linguistic
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03158">
<div class="article-summary-box-inner">
<span><p>Data augmentation, the artificial creation of training data for machine
learning by transformations, is a widely studied research field across machine
learning disciplines. While it is useful for increasing a model's
generalization capabilities, it can also address many other challenges and
problems, from overcoming a limited amount of training data, to regularizing
the objective, to limiting the amount data used to protect privacy. Based on a
precise description of the goals and applications of data augmentation and a
taxonomy for existing works, this survey is concerned with data augmentation
methods for textual classification and aims to provide a concise and
comprehensive overview for researchers and practitioners. Derived from the
taxonomy, we divide more than 100 methods into 12 different groupings and give
state-of-the-art references expounding which methods are highly promising by
relating them to each other. Finally, research perspectives that may constitute
a building block for future work are provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10314">
<div class="article-summary-box-inner">
<span><p>We present small-text, a simple and modular active learning library, which
offers pool-based active learning for single- and multi-label text
classification in Python. It comes with various pre-implemented
state-of-the-art query strategies, including some that can leverage the GPU.
Clearly defined interfaces allow the combination of a multitude of classifiers,
query strategies, and stopping criteria, thereby facilitating a quick mix and
match, and enabling a rapid development of both active learning experiments and
applications. To make various classifiers accessible in a consistent way, it
integrates several well-known existing machine learning libraries, namely,
scikit-learn, PyTorch, and huggingface transformers, where the latter
integrations are available as optionally installable extensions, making the
availability of a GPU competely optional. The library is available under the
MIT License at https://github.com/webis-de/small-text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10904">
<div class="article-summary-box-inner">
<span><p>With recent progress in joint modeling of visual and textual representations,
Vision-Language Pretraining (VLP) has achieved impressive performance on many
multimodal downstream tasks. However, the requirement for expensive annotations
including clean image captions and regional labels limits the scalability of
existing approaches, and complicates the pretraining procedure with the
introduction of multiple dataset-specific objectives. In this work, we relax
these constraints and present a minimalist pretraining framework, named Simple
Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training
complexity by exploiting large-scale weak supervision, and is trained
end-to-end with a single prefix language modeling objective. Without utilizing
extra data or task-specific customization, the resulting model significantly
outperforms previous pretraining methods and achieves new state-of-the-art
results on a wide range of discriminative and generative vision-language
benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE
(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).
Furthermore, we demonstrate that SimVLM acquires strong generalization and
transfer ability, enabling zero-shot behavior including open-ended visual
question answering and cross-modality transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chronic Pain and Language: A Topic Modelling Approach to Personal Pain Descriptions. (arXiv:2109.00402v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00402">
<div class="article-summary-box-inner">
<span><p>Chronic pain is recognized as a major health problem, with impacts not only
at the economic, but also at the social, and individual levels. Being a private
and subjective experience, it is impossible to externally and impartially
experience, describe, and interpret chronic pain as a purely noxious stimulus
that would directly point to a causal agent and facilitate its mitigation,
contrary to acute pain, the assessment of which is usually straightforward.
Verbal communication is, thus, key to convey relevant information to health
professionals that would otherwise not be accessible to external entities,
namely, intrinsic qualities about the painful experience and the patient. We
propose and discuss a topic modelling approach to recognize patterns in verbal
descriptions of chronic pain, and use these patterns to quantify and qualify
experiences of pain. Our approaches allow for the extraction of novel insights
on chronic pain experiences from the obtained topic models and latent spaces.
We argue that our results are clinically relevant for the assessment and
management of chronic pain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">xGQA: Cross-Lingual Visual Question Answering. (arXiv:2109.06082v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06082">
<div class="article-summary-box-inner">
<span><p>Recent advances in multimodal vision and language modeling have predominantly
focused on the English language, mostly due to the lack of multilingual
multimodal datasets to steer modeling efforts. In this work, we address this
gap and provide xGQA, a new multilingual evaluation benchmark for the visual
question answering task. We extend the established English GQA dataset to 7
typologically diverse languages, enabling us to detect and explore crucial
challenges in cross-lingual visual question answering. We further propose new
adapter-based approaches to adapt multimodal transformer-based models to become
multilingual, and -- vice versa -- multilingual models to become multimodal.
Our proposed methods outperform current state-of-the-art multilingual
multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the
accuracy remains low across the board; a performance drop of around 38 accuracy
points in target languages showcases the difficulty of zero-shot cross-lingual
transfer for this task. Our results suggest that simple cross-lingual transfer
of multimodal models yields latent multilingual multimodal misalignment,
calling for more sophisticated methods for vision and multilingual language
modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Isotropy Analysis in the Multilingual BERT Embedding Space. (arXiv:2110.04504v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04504">
<div class="article-summary-box-inner">
<span><p>Several studies have explored various advantages of multilingual pre-trained
models (such as multilingual BERT) in capturing shared linguistic knowledge.
However, less attention has been paid to their limitations. In this paper, we
investigate the multilingual BERT for two known issues of the monolingual
models: anisotropic embedding space and outlier dimensions. We show that,
unlike its monolingual counterpart, the multilingual BERT model exhibits no
outlier dimension in its representations while it has a highly anisotropic
space. There are a few dimensions in the monolingual BERT with high
contributions to the anisotropic distribution. However, we observe no such
dimensions in the multilingual BERT. Furthermore, our experimental results
demonstrate that increasing the isotropy of multilingual space can
significantly improve its representation power and performance, similarly to
what had been observed for monolingual CWRs on semantic similarity tasks. Our
analysis indicates that, despite having different degenerated directions, the
embedding spaces in various languages tend to be partially similar with respect
to their structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained style control in Transformer-based Text-to-speech Synthesis. (arXiv:2110.06306v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06306">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel architecture to realize fine-grained style
control on the transformer-based text-to-speech synthesis (TransformerTTS).
Specifically, we model the speaking style by extracting a time sequence of
local style tokens (LST) from the reference speech. The existing content
encoder in TransformerTTS is then replaced by our designed cross-attention
blocks for fusion and alignment between content and style. As the fusion is
performed along with the skip connection, our cross-attention block provides a
good inductive bias to gradually infuse the phoneme representation with a given
style. Additionally, we prevent the style embedding from encoding linguistic
content by randomly truncating LST during training and using wav2vec 2.0
features. Experiments show that with fine-grained style control, our system
performs better in terms of naturalness, intelligibility, and style
transferability. Our code and samples are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators. (arXiv:2110.06609v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06609">
<div class="article-summary-box-inner">
<span><p>Prompting has recently been shown as a promising approach for applying
pre-trained language models to perform downstream tasks. We present Multi-Stage
Prompting (MSP), a simple and automatic approach for leveraging pre-trained
language models to translation tasks. To better mitigate the discrepancy
between pre-training and translation, MSP divides the translation process via
pre-trained language models into multiple separate stages: the encoding stage,
the re-encoding stage, and the decoding stage. During each stage, we
independently apply different continuous prompts for allowing pre-trained
language models better shift to translation tasks. We conduct extensive
experiments on three translation tasks. Experiments show that our method can
significantly improve the translation performance of pre-trained language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FILM: Following Instructions in Language with Modular Methods. (arXiv:2110.07342v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07342">
<div class="article-summary-box-inner">
<span><p>Recent methods for embodied instruction following are typically trained
end-to-end using imitation learning. This often requires the use of expert
trajectories and low-level language instructions. Such approaches assume that
neural states will integrate multimodal semantics to perform state tracking,
building spatial memory, exploration, and long-term planning. In contrast, we
propose a modular method with structured representations that (1) builds a
semantic map of the scene and (2) performs exploration with a semantic search
policy, to achieve the natural language goal. Our modular method achieves SOTA
performance (24.46 %) with a substantial (8.17 % absolute) gap from previous
work while using less data by eschewing both expert trajectories and low-level
instructions. Leveraging low-level language, however, can further increase our
performance (26.49 %). Our findings suggest that an explicit spatial memory and
a semantic search policy can provide a stronger and more general representation
for state-tracking and guidance, even in the absence of expert trajectories or
low-level instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer. (arXiv:2110.07904v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07904">
<div class="article-summary-box-inner">
<span><p>There has been growing interest in parameter-efficient methods to apply
pre-trained language models to downstream tasks. Building on the Prompt Tuning
approach of Lester et al. (2021), which learns task-specific soft prompts to
condition a frozen pre-trained model to perform different tasks, we propose a
novel prompt-based transfer learning approach called SPoT: Soft Prompt
Transfer. SPoT first learns a prompt on one or more source tasks and then uses
it to initialize the prompt for a target task. We show that SPoT significantly
boosts the performance of Prompt Tuning across many tasks. More remarkably,
across all model sizes, SPoT matches or outperforms standard Model Tuning
(which fine-tunes all model parameters) on the SuperGLUE benchmark, while using
up to 27,000x fewer task-specific parameters. To understand where SPoT is most
effective, we conduct a large-scale study on task transferability with 26 NLP
tasks in 160 combinations, and demonstrate that many tasks can benefit each
other via prompt transfer. Finally, we propose an efficient retrieval approach
that interprets task prompts as task embeddings to identify similar tasks and
predict the most transferable source tasks for a novel target task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask Prompted Training Enables Zero-Shot Task Generalization. (arXiv:2110.08207v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08207">
<div class="article-summary-box-inner">
<span><p>Large language models have recently been shown to attain reasonable zero-shot
generalization on a diverse set of tasks (Brown et al., 2020). It has been
hypothesized that this is a consequence of implicit multitask learning in
language models' pretraining (Radford et al., 2019). Can zero-shot
generalization instead be directly induced by explicit multitask learning? To
test this question at scale, we develop a system for easily mapping any natural
language tasks into a human-readable prompted form. We convert a large set of
supervised datasets, each with multiple prompts with diverse wording. These
prompted datasets allow for benchmarking the ability of a model to perform
completely held-out tasks. We fine-tune a pretrained encoder-decoder model
(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a
wide variety of tasks. The model attains strong zero-shot performance on
several standard datasets, often outperforming models up to 16x its size.
Further, our approach attains strong performance on a subset of tasks from the
BIG-bench benchmark, outperforming models up to 6x its size. All trained models
are available at https://github.com/bigscience-workshop/t-zero and all prompts
are available at https://github.com/bigscience-workshop/promptsource.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generated Knowledge Prompting for Commonsense Reasoning. (arXiv:2110.08387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08387">
<div class="article-summary-box-inner">
<span><p>It remains an open question whether incorporating external knowledge benefits
commonsense reasoning while maintaining the flexibility of pretrained sequence
models. To investigate this question, we develop generated knowledge prompting,
which consists of generating knowledge from a language model, then providing
the knowledge as additional input when answering a question. Our method does
not require task-specific supervision for knowledge integration, or access to a
structured knowledge base, yet it improves performance of large-scale,
state-of-the-art models on four commonsense reasoning tasks, achieving
state-of-the-art results on numerical commonsense (NumerSense), general
commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks.
Generated knowledge prompting highlights large-scale language models as
flexible sources of external knowledge for improving commonsense reasoning. Our
code is available at \url{github.com/liujch1998/GKP}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08486">
<div class="article-summary-box-inner">
<span><p>The ability to sequence unordered events is an essential skill to comprehend
and reason about real world task procedures, which often requires thorough
understanding of temporal common sense and multimodal information, as these
procedures are often communicated through a combination of texts and images.
Such capability is essential for applications such as sequential task planning
and multi-source instruction summarization. While humans are capable of
reasoning about and sequencing unordered multimodal procedural instructions,
whether current machine learning models have such essential capability is still
an open question. In this work, we benchmark models' capability of reasoning
over and sequencing unordered multimodal instructions by curating datasets from
popular online instructional manuals and collecting comprehensive human
annotations. We find models not only perform significantly worse than humans
but also seem incapable of efficiently utilizing the multimodal information. To
improve machines' performance on multimodal event sequencing, we propose
sequentiality-aware pretraining techniques that exploit the sequential
alignment properties of both texts and images, resulting in &gt; 5% significant
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization. (arXiv:2110.08499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08499">
<div class="article-summary-box-inner">
<span><p>We introduce PRIMERA, a pre-trained model for multi-document representation
with a focus on summarization that reduces the need for dataset-specific
architectures and large amounts of fine-tuning labeled data. PRIMERA uses our
newly proposed pre-training objective designed to teach the model to connect
and aggregate information across documents. It also uses efficient
encoder-decoder transformers to simplify the processing of concatenated input
documents. With extensive experiments on 6 multi-document summarization
datasets from 3 different domains on zero-shot, few-shot and full-supervised
settings, PRIMERA outperforms current state-of-the-art dataset-specific and
pre-trained models on most of these settings with large margins. The code and
pre-trained models can be found at \url{https://github.com/allenai/PRIMER}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking. (arXiv:2112.01206v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01206">
<div class="article-summary-box-inner">
<span><p>The goal of local citation recommendation is to recommend a missing reference
from the local citation context and optionally also from the global context. To
balance the tradeoff between speed and accuracy of citation recommendation in
the context of a large-scale paper database, a viable approach is to first
prefetch a limited number of relevant documents using efficient ranking methods
and then to perform a fine-grained reranking using more sophisticated models.
In that vein, BM25 has been found to be a tough-to-beat approach to
prefetching, which is why recent work has focused mainly on the reranking step.
Even so, we explore prefetching with nearest neighbor search among text
embeddings constructed by a hierarchical attention network. When coupled with a
SciBERT reranker fine-tuned on local citation recommendation tasks, our
hierarchical Attention encoder (HAtten) achieves high prefetch recall for a
given number of candidates to be reranked. Consequently, our reranker requires
fewer prefetch candidates to rerank, yet still achieves state-of-the-art
performance on various local citation recommendation datasets such as ACL-200,
FullTextPeerRead, RefSeer, and arXiv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TiltedBERT: Resource Adjustable Version of BERT. (arXiv:2201.03327v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03327">
<div class="article-summary-box-inner">
<span><p>In this paper, a novel adjustable fine-tuning method is proposed that
improves the training and inference time of the BERT model on downstream tasks.
In the proposed method, first, the more important word vectors are detected in
each layer by the proposed Attention Context Contribution (ACC) metric. Second,
the less important ones are eliminated with the proposed strategy. In the
TiltedBERT method, the word vector elimination rate in each layer is controlled
by the Tilt-Rate hyper-parameter, and the model learns to work with a
considerably lower number of Floating Point Operations (FLOPs) than the
original BERTbase model. The proposed method does not need any extra training
steps, and also it can be generalized to other transformer-based models. The
extensive experiments show that the word vectors in higher layers have less
contribution that can be eliminated and improve the training and inference
time. Experimental results on extensive sentiment analysis, classification and
regression datasets, and benchmarks like IMDB and GLUE showed that the
TiltedBERT is effective in various datasets. TiltedBERT improves the inference
time of BERTbase up to 5.3 times with less than 0.85% accuracy drop on average.
After the fine-tuning by the offline-tuning property, the inference time of the
model can be adjusted for a wide range of Tilt-Rate selection. Also, A
mathematical speedup analysis is proposed to estimate the TiltedBERT methods
speedup accurately. With the help of this analysis, the proper Tilt-Rate value
can be selected before finetuning and during offline-tuning phases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02113">
<div class="article-summary-box-inner">
<span><p>Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKGC/tree/main/GenKGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks. (arXiv:2202.12499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12499">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the Data Augmentation for low-resource Natural Language
Understanding (NLU) tasks. We propose Prompt-based D}ata Augmentation model
(PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable
vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human
effort in collecting unlabeled in-domain data and maintains the quality of
generated synthetic data. In addition, PromDA generates synthetic data via two
different views and filters out the low-quality data using NLU models.
Experiments on four benchmarks show that synthetic data produced by PromDA
successfully boost up the performance of NLU models which consistently
outperform several competitive baseline models, including a state-of-the-art
semi-supervised model using unlabeled in-domain data. The synthetic data from
PromDA are also complementary with unlabeled in-domain data. The NLU models can
be further improved when they are combined for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation. (arXiv:2202.13663v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13663">
<div class="article-summary-box-inner">
<span><p>Most dominant neural machine translation (NMT) models are restricted to make
predictions only according to the local context of preceding words in a
left-to-right manner. Although many previous studies try to incorporate global
information into NMT models, there still exist limitations on how to
effectively exploit bidirectional global context. In this paper, we propose a
Confidence Based Bidirectional Global Context Aware (CBBGCA) training framework
for NMT, where the NMT model is jointly trained with an auxiliary conditional
masked language model (CMLM). The training consists of two stages: (1)
multi-task joint training; (2) confidence based knowledge distillation. At the
first stage, by sharing encoder parameters, the NMT model is additionally
supervised by the signal from the CMLM decoder that contains bidirectional
global contexts. Moreover, at the second stage, using the CMLM as teacher, we
further pertinently incorporate bidirectional global context to the NMT model
on its unconfidently-predicted target words via knowledge distillation.
Experimental results show that our proposed CBBGCA training framework
significantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on
three large-scale translation datasets, namely WMT'14 English-to-German, WMT'19
Chinese-to-English and WMT'14 English-to-French, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OneRel:Joint Entity and Relation Extraction with One Module in One Step. (arXiv:2203.05412v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05412">
<div class="article-summary-box-inner">
<span><p>Joint entity and relation extraction is an essential task in natural language
processing and knowledge graph construction. Existing approaches usually
decompose the joint extraction task into several basic modules or processing
steps to make it easy to conduct. However, such a paradigm ignores the fact
that the three elements of a triple are interdependent and indivisible.
Therefore, previous joint methods suffer from the problems of cascading errors
and redundant information. To address these issues, in this paper, we propose a
novel joint entity and relation extraction model, named OneRel, which casts
joint extraction as a fine-grained triple classification problem. Specifically,
our model consists of a scoring-based classifier and a relation-specific horns
tagging strategy. The former evaluates whether a token pair and a relation
belong to a factual triple. The latter ensures a simple but effective decoding
process. Extensive experimental results on two widely used datasets demonstrate
that the proposed method performs better than the state-of-the-art baselines,
and delivers consistent performance gain on complex scenarios of various
overlapping patterns and multiple triples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Language Modeling with Sparse all-MLP. (arXiv:2203.06850v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06850">
<div class="article-summary-box-inner">
<span><p>All-MLP architectures have attracted increasing interest as an alternative to
attention-based models. In NLP, recent work like gMLP shows that all-MLPs can
match Transformers in language modeling, but still lag behind in downstream
tasks. In this work, we analyze the limitations of MLPs in expressiveness, and
propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature
and input (token) dimensions. Such sparse all-MLPs significantly increase model
capacity and expressiveness while keeping the compute constant. We address
critical challenges in incorporating conditional computation with two routing
strategies. The proposed sparse all-MLP improves language modeling perplexity
and obtains up to 2$\times$ improvement in training efficiency compared to both
Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH
Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its
zero-shot in-context learning performance on six downstream tasks, and find
that it surpasses Transformer-based MoEs and dense Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting associations and meanings of objects depicted in artworks through bi-modal deep networks. (arXiv:2203.07026v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07026">
<div class="article-summary-box-inner">
<span><p>We present a novel bi-modal system based on deep networks to address the
problem of learning associations and simple meanings of objects depicted in
"authored" images, such as fine art paintings and drawings. Our overall system
processes both the images and associated texts in order to learn associations
between images of individual objects, their identities and the abstract
meanings they signify. Unlike past deep nets that describe depicted objects and
infer predicates, our system identifies meaning-bearing objects ("signifiers")
and their associations ("signifieds") as well as basic overall meanings for
target artworks. Our system had precision of 48% and recall of 78% with an F1
metric of 0.6 on a curated set of Dutch vanitas paintings, a genre celebrated
for its concentration on conveying a meaning of great import at the time of
their execution. We developed and tested our system on fine art paintings but
our general methods can be applied to other authored images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data. (arXiv:2203.07264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07264">
<div class="article-summary-box-inner">
<span><p>Procedures are inherently hierarchical. To "make videos", one may need to
"purchase a camera", which in turn may require one to "set a budget". While
such hierarchical knowledge is critical for reasoning about complex procedures,
most existing work has treated procedures as shallow structures without
modeling the parent-child relation. In this work, we attempt to construct an
open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a
website containing more than 110k instructional articles, each documenting the
steps to carry out a complex procedure. To this end, we develop a simple and
efficient method that links steps (e.g., "purchase a camera") in an article to
other articles with similar goals (e.g., "how to choose a camera"), recursively
constructing the KB. Our method significantly outperforms several strong
baselines according to automatic evaluation, human judgment, and application to
downstream tasks such as instructional video retrieval.
</p>
<p>A demo with partial data can be found at https://wikihow-hierarchy.github.io.
The code and the data are at https://github.com/shuyanzhou/wikihow_hierarchy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer. (arXiv:2203.07519v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07519">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models are still far from human performance in tasks
that need understanding of properties (e.g. appearance, measurable quantity)
and affordances of everyday objects in the real world since the text lacks such
information due to reporting bias. In this work, we study whether integrating
visual knowledge into a language model can fill the gap. We investigate two
types of knowledge transfer: (1) text knowledge transfer using image captions
that may contain enriched visual knowledge and (2) cross-modal knowledge
transfer using both images and captions with vision-language training
objectives. On 5 downstream tasks that may need visual knowledge to solve the
problem, we perform extensive empirical comparisons over the presented
objectives. Our experiments show that visual knowledge transfer can improve
performance in both low-resource and fully supervised settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation. (arXiv:2203.08394v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08394">
<div class="article-summary-box-inner">
<span><p>Back-translation is a critical component of Unsupervised Neural Machine
Translation (UNMT), which generates pseudo parallel data from target
monolingual data. A UNMT model is trained on the pseudo parallel data with
translated source, and translates natural source sentences in inference. The
source discrepancy between training and inference hinders the translation
performance of UNMT models. By carefully designing experiments, we identify two
representative characteristics of the data gap in source: (1) style gap (i.e.,
translated vs. natural text style) that leads to poor generalization
capability; (2) content gap that induces the model to produce hallucination
content biased towards the target language. To narrow the data gap, we propose
an online self-training approach, which simultaneously uses the pseudo parallel
data {natural source, translated target} to mimic the inference scenario.
Experimental results on several widely-used language pairs show that our
approach outperforms two strong baselines (XLM and MASS) by remedying the style
and content gaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KinyaBERT: a Morphology-aware Kinyarwanda Language Model. (arXiv:2203.08459v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08459">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models such as BERT have been successful at tackling
many natural language processing tasks. However, the unsupervised sub-word
tokenization methods commonly used in these models (e.g., byte-pair encoding -
BPE) are sub-optimal at handling morphologically rich languages. Even given a
morphological analyzer, naive sequencing of morphemes into a standard BERT
architecture is inefficient at capturing morphological compositionality and
expressing word-relative syntactic regularities. We address these challenges by
proposing a simple yet effective two-tier BERT architecture that leverages a
morphological analyzer and explicitly represents morphological
compositionality. Despite the success of BERT, most of its evaluations have
been conducted on high-resource languages, obscuring its applicability on
low-resource languages. We evaluate our proposed method on the low-resource
morphologically rich Kinyarwanda language, naming the proposed model
architecture KinyaBERT. A robust set of experimental results reveal that
KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity
recognition task and by 4.3% in average score of a machine-translated GLUE
benchmark. KinyaBERT fine-tuning has better convergence and achieves more
robust results on multiple tasks even in the presence of translation noise.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Disparities in Dermatology AI Performance on a Diverse, Curated Clinical Image Set. (arXiv:2203.08807v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08807">
<div class="article-summary-box-inner">
<span><p>Access to dermatological care is a major issue, with an estimated 3 billion
people lacking access to care globally. Artificial intelligence (AI) may aid in
triaging skin diseases. However, most AI models have not been rigorously
assessed on images of diverse skin tones or uncommon diseases. To ascertain
potential biases in algorithm performance in this context, we curated the
Diverse Dermatology Images (DDI) dataset-the first publicly available, expertly
curated, and pathologically confirmed image dataset with diverse skin tones.
Using this dataset of 656 images, we show that state-of-the-art dermatology AI
models perform substantially worse on DDI, with receiver operator curve area
under the curve (ROC-AUC) dropping by 27-36 percent compared to the models'
original test results. All the models performed worse on dark skin tones and
uncommon diseases, which are represented in the DDI dataset. Additionally, we
find that dermatologists, who typically provide visual labels for AI training
and test datasets, also perform worse on images of dark skin tones and uncommon
diseases compared to ground truth biopsy annotations. Finally, fine-tuning AI
models on the well-characterized and diverse DDI images closed the performance
gap between light and dark skin tones. Moreover, algorithms fine-tuned on
diverse skin tones outperformed dermatologists on identifying malignancy on
images of dark skin tones. Our findings identify important weaknesses and
biases in dermatology AI that need to be addressed to ensure reliable
application to diverse patients and diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Deep Learning to Enhance Breast Cancer Detection on Screening Mammography. (arXiv:2203.08812v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08812">
<div class="article-summary-box-inner">
<span><p>A major limitation in applying deep learning to artificial intelligence (AI)
systems is the scarcity of high-quality curated datasets. We investigate strong
augmentation based self-supervised learning (SSL) techniques to address this
problem. Using breast cancer detection as an example, we first identify a
mammogram-specific transformation paradigm and then systematically compare four
recent SSL methods representing a diversity of approaches. We develop a method
to convert a pretrained model from making predictions on uniformly tiled
patches to whole images, and an attention-based pooling method that improves
the classification performance. We found that the best SSL model substantially
outperformed the baseline supervised model. The best SSL model also improved
the data efficiency of sample labeling by nearly 4-fold and was highly
transferrable from one dataset to another. SSL represents a major breakthrough
in computer vision and may help the AI for medical imaging field to shift away
from supervised learning and dependency on scarce labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Example Perplexity. (arXiv:2203.08813v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08813">
<div class="article-summary-box-inner">
<span><p>Some examples are easier for humans to classify than others. The same should
be true for deep neural networks (DNNs). We use the term example perplexity to
refer to the level of difficulty of classifying an example. In this paper, we
propose a method to measure the perplexity of an example and investigate what
factors contribute to high example perplexity. The related codes and resources
are available at https://github.com/vaynexie/Example-Perplexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DePS: An improved deep learning model for de novo peptide sequencing. (arXiv:2203.08820v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08820">
<div class="article-summary-box-inner">
<span><p>De novo peptide sequencing from mass spectrometry data is an important method
for protein identification. Recently, various deep learning approaches were
applied for de novo peptide sequencing and DeepNovoV2 is one of the
represetative models. In this study, we proposed an enhanced model, DePS, which
can improve the accuracy of de novo peptide sequencing even with missing signal
peaks or large number of noisy peaks in tandem mass spectrometry data. It is
showed that, for the same test set of DeepNovoV2, the DePS model achieved
excellent results of 74.22%, 74.21% and 41.68% for amino acid recall, amino
acid precision and peptide recall respectively. Furthermore, the results
suggested that DePS outperforms DeepNovoV2 on the cross species dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding robustness and generalization of artificial neural networks through Fourier masks. (arXiv:2203.08822v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08822">
<div class="article-summary-box-inner">
<span><p>Despite the enormous success of artificial neural networks (ANNs) in many
disciplines, the characterization of their computations and the origin of key
properties such as generalization and robustness remain open questions. Recent
literature suggests that robust networks with good generalization properties
tend to be biased towards processing low frequencies in images. To explore the
frequency bias hypothesis further, we develop an algorithm that allows us to
learn modulatory masks highlighting the essential input frequencies needed for
preserving a trained network's performance. We achieve this by imposing
invariance in the loss with respect to such modulations in the input
frequencies. We first use our method to test the low-frequency preference
hypothesis of adversarially trained or data-augmented networks. Our results
suggest that adversarially robust networks indeed exhibit a low-frequency bias
but we find this bias is also dependent on directions in frequency space.
However, this is not necessarily true for other types of data augmentation. Our
results also indicate that the essential frequencies in question are
effectively the ones used to achieve generalization in the first place.
Surprisingly, images seen through these modulatory masks are not recognizable
and resemble texture-like patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real-Time Region Tracking Algorithm Tailored to Endoscopic Video with Open-Source Implementation. (arXiv:2203.08858v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08858">
<div class="article-summary-box-inner">
<span><p>With a video data source, such as multispectral video acquired during
administration of fluorescent tracers, extraction of time-resolved data
typically requires the compensation of motion. While this can be done manually,
which is arduous, or using off-the-shelf object tracking software, which often
yields unsatisfactory performance, we present an algorithm which is simple and
performant. Most importantly, we provide an open-source implementation, with an
easy-to-use interface for researchers not inclined to write their own code, as
well as Python modules that can be used programmatically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SC2: Supervised Compression for Split Computing. (arXiv:2203.08875v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08875">
<div class="article-summary-box-inner">
<span><p>Split computing distributes the execution of a neural network (e.g., for a
classification task) between a mobile device and a more powerful edge server. A
simple alternative to splitting the network is to carry out the supervised task
purely on the edge server while compressing and transmitting the full data, and
most approaches have barely outperformed this baseline. This paper proposes a
new approach for discretizing and entropy-coding intermediate feature
activations to efficiently transmit them from the mobile device to the edge
server. We show that a efficient splittable network architecture results from a
three-way tradeoff between (a) minimizing the computation on the mobile device,
(b) minimizing the size of the data to be transmitted, and (c) maximizing the
model's prediction performance. We propose an architecture based on this
tradeoff and train the splittable network and entropy model in a knowledge
distillation framework. In an extensive set of experiments involving three
vision tasks, three datasets, nine baselines, and more than 180 trained models,
we show that our approach improves supervised rate-distortion tradeoffs while
maintaining a considerably smaller encoder size. We also release sc2bench, an
installable Python package, to encourage and facilitate future studies on
supervised compression for split computing (SC2).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layer Ensembles: A Single-Pass Uncertainty Estimation in Deep Learning for Segmentation. (arXiv:2203.08878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08878">
<div class="article-summary-box-inner">
<span><p>Uncertainty estimation in deep learning has become a leading research field
in medical image analysis due to the need for safe utilisation of AI algorithms
in clinical practice. Most approaches for uncertainty estimation require
sampling the network weights multiple times during testing or training multiple
networks. This leads to higher training and testing costs in terms of time and
computational resources. In this paper, we propose Layer Ensembles, a novel
uncertainty estimation method that uses a single network and requires only a
single pass to estimate predictive uncertainty of a network. Moreover, we
introduce an image-level uncertainty metric, which is more beneficial for
segmentation tasks compared to the commonly used pixel-wise metrics such as
entropy and variance. We evaluate our approach on 2D and 3D, binary and
multi-class medical image segmentation tasks. Our method shows competitive
results with state-of-the-art Deep Ensembles, requiring only a single network
and a single pass.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperbolic Uncertainty Aware Semantic Segmentation. (arXiv:2203.08881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08881">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation (SS) aims to classify each pixel into one of the
pre-defined classes. This task plays an important role in self-driving cars and
autonomous drones. In SS, many works have shown that most misclassified pixels
are commonly near object boundaries with high uncertainties. However, existing
SS loss functions are not tailored to handle these uncertain pixels during
training, as these pixels are usually treated equally as confidently classified
pixels and cannot be embedded with arbitrary low distortion in Euclidean space,
thereby degenerating the performance of SS. To overcome this problem, this
paper designs a "Hyperbolic Uncertainty Loss" (HyperUL), which dynamically
highlights the misclassified and high-uncertainty pixels in Hyperbolic space
during training via the hyperbolic distances. The proposed HyperUL is model
agnostic and can be easily applied to various neural architectures. After
employing HyperUL to three recent SS models, the experimental results on
Cityscapes and UAVid datasets reveal that the segmentation performance of
existing SS models can be consistently improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient Objects and Shadow Modeling Using RPC Cameras. (arXiv:2203.08896v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08896">
<div class="article-summary-box-inner">
<span><p>We introduce the Satellite Neural Radiance Field (Sat-NeRF), a new end-to-end
model for learning multi-view satellite photogrammetry in the wild. Sat-NeRF
combines some of the latest trends in neural rendering with native satellite
camera models, represented by rational polynomial coefficient (RPC) functions.
The proposed method renders new views and infers surface models of similar
quality to those obtained with traditional state-of-the-art stereo pipelines.
Multi-date images exhibit significant changes in appearance, mainly due to
varying shadows and transient objects (cars, vegetation). Robustness to these
challenges is achieved by a shadow-aware irradiance model and uncertainty
weighting to deal with transient phenomena that cannot be explained by the
position of the sun. We evaluate Sat-NeRF using WorldView-3 images from
different locations and stress the advantages of applying a bundle adjustment
to the satellite camera models prior to training. This boosts the network
performance and can optionally be used to extract additional cues for depth
supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gate-Shift-Fuse for Video Action Recognition. (arXiv:2203.08897v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08897">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks are the de facto models for image recognition.
However 3D CNNs, the straight forward extension of 2D CNNs for video
recognition, have not achieved the same success on standard action recognition
benchmarks. One of the main reasons for this reduced performance of 3D CNNs is
the increased computational complexity requiring large scale annotated datasets
to train them in scale. 3D kernel factorization approaches have been proposed
to reduce the complexity of 3D CNNs. Existing kernel factorization approaches
follow hand-designed and hard-wired techniques. In this paper we propose
Gate-Shift-Fuse (GSF), a novel spatio-temporal feature extraction module which
controls interactions in spatio-temporal decomposition and learns to adaptively
route features through time and combine them in a data dependent manner. GSF
leverages grouped spatial gating to decompose input tensor and channel
weighting to fuse the decomposed tensors. GSF can be inserted into existing 2D
CNNs to convert them into an efficient and high performing spatio-temporal
feature extractor, with negligible parameter and compute overhead. We perform
an extensive analysis of GSF using two popular 2D CNN families and achieve
state-of-the-art or competitive performance on five standard action recognition
benchmarks. Code and models will be made publicly available at
https://github.com/swathikirans/GSF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Grading of Radiographic Knee Osteoarthritis Severity Combined with Joint Space Narrowing. (arXiv:2203.08914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08914">
<div class="article-summary-box-inner">
<span><p>The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical "black box" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
</p>
<p>The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Pixel-Unshuffled Network for Lightweight Image Super-Resolution. (arXiv:2203.08921v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08921">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has achieved great success on image
super-resolution (SR). However, most deep CNN-based SR models take massive
computations to obtain high performance. Downsampling features for
multi-resolution fusion is an efficient and effective way to improve the
performance of visual recognition. Still, it is counter-intuitive in the SR
task, which needs to project a low-resolution input to high-resolution. In this
paper, we propose a novel Hybrid Pixel-Unshuffled Network (HPUN) by introducing
an efficient and effective downsampling module into the SR task. The network
contains pixel-unshuffled downsampling and Self-Residual Depthwise Separable
Convolutions. Specifically, we utilize pixel-unshuffle operation to downsample
the input features and use grouped convolution to reduce the channels. Besides,
we enhance the depthwise convolution's performance by adding the input feature
to its output. Experiments on benchmark datasets show that our HPUN achieves
and surpasses the state-of-the-art reconstruction performance with fewer
parameters and computation costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards True Detail Restoration for Super-Resolution: A Benchmark and a Quality Metric. (arXiv:2203.08923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08923">
<div class="article-summary-box-inner">
<span><p>Super-resolution (SR) has become a widely researched topic in recent years.
SR methods can improve overall image and video quality and create new
possibilities for further content analysis. But the SR mainstream focuses
primarily on increasing the naturalness of the resulting image despite
potentially losing context accuracy. Such methods may produce an incorrect
digit, character, face, or other structural object even though they otherwise
yield good visual quality. Incorrect detail restoration can cause errors when
detecting and identifying objects both manually and automatically. To analyze
the detail-restoration capabilities of image and video SR models, we developed
a benchmark based on our own video dataset, which contains complex patterns
that SR models generally fail to correctly restore. We assessed 32 recent SR
models using our benchmark and compared their ability to preserve scene
context. We also conducted a crowd-sourced comparison of restored details and
developed an objective assessment metric that outperforms other quality metrics
by correlation with subjective scores for this task. In conclusion, we provide
a deep analysis of benchmark results that yields insights for future SR-based
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creating Multimedia Summaries Using Tweets and Videos. (arXiv:2203.08931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08931">
<div class="article-summary-box-inner">
<span><p>While popular televised events such as presidential debates or TV shows are
airing, people provide commentary on them in real-time. In this paper, we
propose a simple yet effective approach to combine social media commentary and
videos to create a multimedia summary of televised events. Our approach
identifies scenes from these events based on spikes of mentions of people
involved in the event and automatically selects tweets and frames from the
videos that occur during the time period of the spike that talk about and show
the people being discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ABN: Agent-Aware Boundary Networks for Temporal Action Proposal Generation. (arXiv:2203.08942v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08942">
<div class="article-summary-box-inner">
<span><p>Temporal action proposal generation (TAPG) aims to estimate temporal
intervals of actions in untrimmed videos, which is a challenging yet plays an
important role in many tasks of video analysis and understanding. Despite the
great achievement in TAPG, most existing works ignore the human perception of
interaction between agents and the surrounding environment by applying a deep
learning model as a black-box to the untrimmed videos to extract video visual
representation. Therefore, it is beneficial and potentially improve the
performance of TAPG if we can capture these interactions between agents and the
environment. In this paper, we propose a novel framework named Agent-Aware
Boundary Network (ABN), which consists of two sub-networks (i) an Agent-Aware
Representation Network to obtain both agent-agent and agents-environment
relationships in the video representation, and (ii) a Boundary Generation
Network to estimate the confidence score of temporal intervals. In the
Agent-Aware Representation Network, the interactions between agents are
expressed through local pathway, which operates at a local level to focus on
the motions of agents whereas the overall perception of the surroundings are
expressed through global pathway, which operates at a global level to perceive
the effects of agents-environment. Comprehensive evaluations on 20-action
THUMOS-14 and 200-action ActivityNet-1.3 datasets with different backbone
networks (i.e C3D, SlowFast and Two-Stream) show that our proposed ABN robustly
outperforms state-of-the-art methods regardless of the employed backbone
network on TAPG. We further examine the proposal quality by leveraging
proposals generated by our method onto temporal action detection (TAD)
frameworks and evaluate their detection performances. The source code can be
found in this URL https://github.com/vhvkhoa/TAPG-AgentEnvNetwork.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CapsNet for Medical Image Segmentation. (arXiv:2203.08948v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08948">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) have been successful in solving tasks in
computer vision including medical image segmentation due to their ability to
automatically extract features from unstructured data. However, CNNs are
sensitive to rotation and affine transformation and their success relies on
huge-scale labeled datasets capturing various input variations. This network
paradigm has posed challenges at scale because acquiring annotated data for
medical segmentation is expensive, and strict privacy regulations. Furthermore,
visual representation learning with CNNs has its own flaws, e.g., it is
arguable that the pooling layer in traditional CNNs tends to discard positional
information and CNNs tend to fail on input images that differ in orientations
and sizes. Capsule network (CapsNet) is a recent new architecture that has
achieved better robustness in representation learning by replacing pooling
layers with dynamic routing and convolutional strides, which has shown
potential results on popular tasks such as classification, recognition,
segmentation, and natural language processing. Different from CNNs, which
result in scalar outputs, CapsNet returns vector outputs, which aim to preserve
the part-whole relationships. In this work, we first introduce the limitations
of CNNs and fundamentals of CapsNet. We then provide recent developments of
CapsNet for the task of medical image segmentation. We finally discuss various
effective network architectures to implement a CapsNet for both 2D images and
3D volumetric medical image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Learning of NAS for Few-shot Learning in Medical Image Applications. (arXiv:2203.08951v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08951">
<div class="article-summary-box-inner">
<span><p>Deep learning methods have been successful in solving tasks in machine
learning and have made breakthroughs in many sectors owing to their ability to
automatically extract features from unstructured data. However, their
performance relies on manual trial-and-error processes for selecting an
appropriate network architecture, hyperparameters for training, and
pre-/post-procedures. Even though it has been shown that network architecture
plays a critical role in learning feature representation feature from data and
the final performance, searching for the best network architecture is
computationally intensive and heavily relies on researchers' experience.
Automated machine learning (AutoML) and its advanced techniques i.e. Neural
Architecture Search (NAS) have been promoted to address those limitations. Not
only in general computer vision tasks, but NAS has also motivated various
applications in multiple areas including medical imaging. In medical imaging,
NAS has significant progress in improving the accuracy of image classification,
segmentation, reconstruction, and more. However, NAS requires the availability
of large annotated data, considerable computation resources, and pre-defined
tasks. To address such limitations, meta-learning has been adopted in the
scenarios of few-shot learning and multiple tasks. In this book chapter, we
first present a brief review of NAS by discussing well-known approaches in
search space, search strategy, and evaluation strategy. We then introduce
various NAS approaches in medical imaging with different applications such as
classification, segmentation, detection, reconstruction, etc. Meta-learning in
NAS for few-shot learning and multiple tasks is then explained. Finally, we
describe several open problems in NAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training. (arXiv:2203.08959v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08959">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel neural network training framework that
increases model's adversarial robustness to adversarial attacks while
maintaining high clean accuracy by combining contrastive learning (CL) with
adversarial training (AT). We propose to improve model robustness to
adversarial attacks by learning feature representations that are consistent
under both data augmentations and adversarial perturbations. We leverage
contrastive learning to improve adversarial robustness by considering an
adversarial example as another positive example, and aim to maximize the
similarity between random augmentations of data samples and their adversarial
example, while constantly updating the classification head in order to avoid a
cognitive dissociation between the classification head and the embedding space.
This dissociation is caused by the fact that CL updates the network up to the
embedding space, while freezing the classification head which is used to
generate new positive adversarial examples. We validate our method, Contrastive
Learning with Adversarial Features(CLAF), on the CIFAR-10 dataset on which it
outperforms both robust accuracy and clean accuracy over alternative supervised
and self-supervised adversarial learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-Unet: A Context-aware Point-based Neural Network for Volumetric Segmentation. (arXiv:2203.08964v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08964">
<div class="article-summary-box-inner">
<span><p>Medical image analysis using deep learning has recently been prevalent,
showing great performance for various downstream tasks including medical image
segmentation and its sibling, volumetric image segmentation. Particularly, a
typical volumetric segmentation network strongly relies on a voxel grid
representation which treats volumetric data as a stack of individual voxel
`slices', which allows learning to segment a voxel grid to be as
straightforward as extending existing image-based segmentation networks to the
3D domain. However, using a voxel grid representation requires a large memory
footprint, expensive test-time and limiting the scalability of the solutions.
In this paper, we propose Point-Unet, a novel method that incorporates the
efficiency of deep learning with 3D point clouds into volumetric segmentation.
Our key idea is to first predict the regions of interest in the volume by
learning an attentional probability map, which is then used for sampling the
volume into a sparse point cloud that is subsequently segmented using a
point-based neural network. We have conducted the experiments on the medical
volumetric segmentation task with both a small-scale dataset Pancreas and
large-scale datasets BraTS18, BraTS19, and BraTS20 challenges. A comprehensive
benchmark on different metrics has shown that our context-aware Point-Unet
robustly outperforms the SOTA voxel-based networks at both accuracies, memory
usage during training, and time consumption during testing. Our code is
available at https://github.com/VinAIResearch/Point-Unet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-UCaps: 3D Capsules Unet for Volumetric Image Segmentation. (arXiv:2203.08965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08965">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation has been so far achieving promising results with
Convolutional Neural Networks (CNNs). However, it is arguable that in
traditional CNNs, its pooling layer tends to discard important information such
as positions. Moreover, CNNs are sensitive to rotation and affine
transformation. Capsule network is a data-efficient network design proposed to
overcome such limitations by replacing pooling layers with dynamic routing and
convolutional strides, which aims to preserve the part-whole relationships.
Capsule network has shown a great performance in image recognition and natural
language processing, but applications for medical image segmentation,
particularly volumetric image segmentation, has been limited. In this work, we
propose 3D-UCaps, a 3D voxel-based Capsule network for medical volumetric image
segmentation. We build the concept of capsules into a CNN by designing a
network with two pathways: the first pathway is encoded by 3D Capsule blocks,
whereas the second pathway is decoded by 3D CNNs blocks. 3D-UCaps, therefore
inherits the merits from both Capsule network to preserve the spatial
relationship and CNNs to learn visual representation. We conducted experiments
on various datasets to demonstrate the robustness of 3D-UCaps including
iSeg-2017, LUNA16, Hippocampus, and Cardiac, where our method outperforms
previous Capsule networks and 3D-Unets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extensive Threat Analysis of Vein Attack Databases and Attack Detection by Fusion of Comparison Scores. (arXiv:2203.08972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08972">
<div class="article-summary-box-inner">
<span><p>The last decade has brought forward many great contributions regarding
presentation attack detection for the domain of finger and hand vein
biometrics. Among those contributions, one is able to find a variety of
different attack databases that are either private or made publicly available
to the research community. However, it is not always shown whether the used
attack samples hold the capability to actually deceive a realistic vein
recognition system. Inspired by previous works, this study provides a
systematic threat evaluation including three publicly available finger vein
attack databases and one private dorsal hand vein database. To do so, 14
distinct vein recognition schemes are confronted with attack samples and the
percentage of wrongly accepted attack samples is then reported as the Impostor
Attack Presentation Match Rate. As a second step, comparison scores from
different recognition schemes are combined using score level fusion with the
goal of performing presentation attack detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-diversity transfer network for generalized zero-shot learning via inner disagreement based OOD detector. (arXiv:2203.09017v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09017">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) aims to recognize objects from unseen classes, where
the kernel problem is to transfer knowledge from seen classes to unseen classes
by establishing appropriate mappings between visual and semantic features. The
knowledge transfer in many existing works is limited mainly due to the facts
that 1) the widely used visual features are global ones but not totally
consistent with semantic attributes; 2) only one mapping is learned in existing
works, which is not able to effectively model diverse visual-semantic
relations; 3) the bias problem in the generalized ZSL (GZSL) could not be
effectively handled. In this paper, we propose two techniques to alleviate
these limitations. Firstly, we propose a Semantic-diversity transfer Network
(SetNet) addressing the first two limitations, where 1) a multiple-attention
architecture and a diversity regularizer are proposed to learn multiple local
visual features that are more consistent with semantic attributes and 2) a
projector ensemble that geometrically takes diverse local features as inputs is
proposed to model visual-semantic relations from diverse local perspectives.
Secondly, we propose an inner disagreement based domain detection module (ID3M)
for GZSL to alleviate the third limitation, which picks out unseen-class data
before class-level classification. Due to the absence of unseen-class data in
training stage, ID3M employs a novel self-contained training scheme and detects
out unseen-class data based on a designed inner disagreement criterion.
Experimental results on three public datasets demonstrate that the proposed
SetNet with the explored ID3M achieves a significant improvement against $30$
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HybridNets: End-to-End Perception Network. (arXiv:2203.09035v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09035">
<div class="article-summary-box-inner">
<span><p>End-to-end Network has become increasingly important in multi-tasking. One
prominent example of this is the growing significance of a driving perception
system in autonomous driving. This paper systematically studies an end-to-end
perception network for multi-tasking and proposes several key optimizations to
improve accuracy. First, the paper proposes efficient segmentation head and
box/class prediction networks based on weighted bidirectional feature network.
Second, the paper proposes automatically customized anchor for each level in
the weighted bidirectional feature network. Third, the paper proposes an
efficient training loss function and training strategy to balance and optimize
network. Based on these optimizations, we have developed an end-to-end
perception network to perform multi-tasking, including traffic object
detection, drivable area segmentation and lane detection simultaneously, called
HybridNets, which achieves better accuracy than prior art. In particular,
HybridNets achieves 77.3 mean Average Precision on Berkeley DeepDrive Dataset,
outperforms lane detection with 31.6 mean Intersection Over Union with 12.83
million parameters and 15.6 billion floating-point operations. In addition, it
can perform visual perception tasks in real-time and thus is a practical and
accurate solution to the multi-tasking problem. Code is available at
https://github.com/datvuthanh/HybridNets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Active Contour Model with Local Variance Force Term and Its Efficient Minimization Solver for Multi-phase Image Segmentation. (arXiv:2203.09036v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09036">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an active contour model with a local variance force
(LVF) term that can be applied to multi-phase image segmentation problems. With
the LVF, the proposed model is very effective in the segmentation of images
with noise. To solve this model efficiently, we represent the regularization
term by characteristic functions and then design a minimization algorithm based
on a modification of the iterative convolution-thresholding method (ICTM),
namely ICTM-LVF. This minimization algorithm enjoys the energy-decaying
property under some conditions and has highly efficient performance in the
segmentation. To overcome the initialization issue of active contour models, we
generalize the inhomogeneous graph Laplacian initialization method (IGLIM) to
the multi-phase case and then apply it to give the initial contour of the
ICTM-LVF solver. Numerical experiments are conducted on synthetic images and
real images to demonstrate the capability of our initialization method, and the
effectiveness of the local variance force for noise robustness in the
multi-phase image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DATA: Domain-Aware and Task-Aware Pre-training. (arXiv:2203.09041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09041">
<div class="article-summary-box-inner">
<span><p>The paradigm of training models on massive data without label through
self-supervised learning (SSL) and finetuning on many downstream tasks has
become a trend recently. However, due to the high training costs and the
unconsciousness of downstream usages, most self-supervised learning methods
lack the capability to correspond to the diversities of downstream scenarios,
as there are various data domains, different vision tasks and latency
constraints on models. Neural architecture search (NAS) is one universally
acknowledged fashion to conquer the issues above, but applying NAS on SSL seems
impossible as there is no label or metric provided for judging model selection.
In this paper, we present DATA, a simple yet effective NAS approach specialized
for SSL that provides Domain-Aware and Task-Aware pre-training. Specifically,
we (i) train a supernet which could be deemed as a set of millions of networks
covering a wide range of model scales without any label, (ii) propose a
flexible searching mechanism compatible with SSL that enables finding networks
of different computation costs, for various downstream vision tasks and data
domains without explicit metric provided. Instantiated With MoCo v2, our method
achieves promising results across a wide range of computation costs on
downstream tasks, including image classification, object detection and semantic
segmentation. DATA is orthogonal to most existing SSL methods and endows them
the ability of customization on downstream needs. Extensive experiments on
other SSL methods demonstrate the generalizability of the proposed method. Code
is released at https://github.com/GAIA-vision/GAIA-ssl
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Image Animator: Learning to Animate Images via Latent Space Navigation. (arXiv:2203.09043v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09043">
<div class="article-summary-box-inner">
<span><p>Due to the remarkable progress of deep generative models, animating images
has become increasingly efficient, whereas associated results have become
increasingly realistic. Current animation-approaches commonly exploit structure
representation extracted from driving videos. Such structure representation is
instrumental in transferring motion from driving videos to still images.
However, such approaches fail in case the source image and driving video
encompass large appearance variation. Moreover, the extraction of structure
information requires additional modules that endow the animation-model with
increased complexity. Deviating from such models, we here introduce the Latent
Image Animator (LIA), a self-supervised autoencoder that evades need for
structure representation. LIA is streamlined to animate images by linear
navigation in the latent space. Specifically, motion in generated video is
constructed by linear displacement of codes in the latent space. Towards this,
we learn a set of orthogonal motion directions simultaneously, and use their
linear combination, in order to represent any displacement in the latent space.
Extensive quantitative and qualitative analysis suggests that our model
systematically and significantly outperforms state-of-art methods on VoxCeleb,
Taichi and TED-talk datasets w.r.t. generated quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training. (arXiv:2203.09052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09052">
<div class="article-summary-box-inner">
<span><p>Due to the limitations of the model structure and pre-training objectives,
existing vision-and-language generation models cannot utilize pair-wise images
and text through bi-directional generation. In this paper, we propose DU-VLG, a
framework which unifies vision-and-language generation as sequence generation
problems. DU-VLG is trained with novel dual pre-training tasks: multi-modal
denoising autoencoder tasks and modality translation tasks. To bridge the gap
between image understanding and generation, we further design a novel
commitment loss. We compare pre-training objectives on image captioning and
text-to-image generation datasets. Results show that DU-VLG yields better
performance than variants trained with uni-directional generation objectives or
the variant without the commitment loss. We also obtain higher scores compared
to previous state-of-the-art systems on three vision-and-language generation
tasks. In addition, human judges further confirm that our model generates real
and relevant images as well as faithful and informative captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Table Detection and Structure Recognition from Heterogeneous Document Images. (arXiv:2203.09056v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09056">
<div class="article-summary-box-inner">
<span><p>We introduce a new table detection and structure recognition approach named
RobusTabNet to detect the boundaries of tables and reconstruct the cellular
structure of the table from heterogeneous document images. For table detection,
we propose to use CornerNet as a new region proposal network to generate higher
quality table proposals for Faster R-CNN, which has significantly improved the
localization accuracy of Faster R-CNN for table detection. Consequently, our
table detection approach achieves state-of-the-art performance on three public
table detection benchmarks, namely cTDaR TrackA, PubLayNet and IIIT-AR-13K, by
only using a lightweight ResNet-18 backbone network. Furthermore, we propose a
new split-and-merge based table structure recognition approach, in which a
novel spatial CNN based separation line prediction module is proposed to split
each detected table into a grid of cells, and a Grid CNN based cell merging
module is applied to recover the spanning cells. As the spatial CNN module can
effectively propagate contextual information across the whole table image, our
table structure recognizer can robustly recognize tables with large blank
spaces and geometrically distorted (even curved) tables. Thanks to these two
techniques, our table structure recognition approach achieves state-of-the-art
performance on three public benchmarks, including SciTSR, PubTabNet and cTDaR
TrackB. Moreover, we have further demonstrated the advantages of our approach
in recognizing tables with complex structures, large blank spaces, empty or
spanning cells as well as geometrically distorted or even curved tables on a
more challenging in-house dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-shot Learning. (arXiv:2203.09064v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09064">
<div class="article-summary-box-inner">
<span><p>This paper presents new hierarchically cascaded transformers that can improve
data efficiency through attribute surrogates learning and spectral tokens
pooling. Vision transformers have recently been thought of as a promising
alternative to convolutional neural networks for visual recognition. But when
there is no sufficient data, it gets stuck in overfitting and shows inferior
performance. To improve data efficiency, we propose hierarchically cascaded
transformers that exploit intrinsic image structures through spectral tokens
pooling and optimize the learnable parameters through latent attribute
surrogates. The intrinsic image structure is utilized to reduce the ambiguity
between foreground content and background noise by spectral tokens pooling. And
the attribute surrogate learning scheme is designed to benefit from the rich
visual information in image-label pairs instead of simple visual concepts
assigned by their labels. Our Hierarchically Cascaded Transformers, called
HCTransformers, is built upon a self-supervised learning framework DINO and is
tested on several popular few-shot learning benchmarks.
</p>
<p>In the inductive setting, HCTransformers surpass the DINO baseline by a large
margin of 9.7% 5-way 1-shot accuracy and 9.17% 5-way 5-shot accuracy on
miniImageNet, which demonstrates HCTransformers are efficient to extract
discriminative features. Also, HCTransformers show clear advantages over SOTA
few-shot classification methods in both 5-way 1-shot and 5-way 5-shot settings
on four popular benchmark datasets, including miniImageNet, tieredImageNet,
FC100, and CIFAR-FS. The trained weights and codes are available at
https://github.com/StomachCold/HCTransformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STPLS3D: A Large-Scale Synthetic and Real Aerial Photogrammetry 3D Point Cloud Dataset. (arXiv:2203.09065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09065">
<div class="article-summary-box-inner">
<span><p>Although various 3D datasets with different functions and scales have been
proposed recently, it remains challenging for individuals to complete the whole
pipeline of large-scale data collection, sanitization, and annotation.
Moreover, the created datasets usually suffer from extremely imbalanced class
distribution or partial low-quality data samples. Motivated by this, we explore
the procedurally synthetic 3D data generation paradigm to equip individuals
with the full capability of creating large-scale annotated photogrammetry point
clouds. Specifically, we introduce a synthetic aerial photogrammetry point
clouds generation pipeline that takes full advantage of open geospatial data
sources and off-the-shelf commercial packages. Unlike generating synthetic data
in virtual games, where the simulated data usually have limited gaming
environments created by artists, the proposed pipeline simulates the
reconstruction process of the real environment by following the same UAV flight
pattern on different synthetic terrain shapes and building densities, which
ensure similar quality, noise pattern, and diversity with real data. In
addition, the precise semantic and instance annotations can be generated fully
automatically, avoiding the expensive and time-consuming manual annotation.
Based on the proposed pipeline, we present a richly-annotated synthetic 3D
aerial photogrammetry point cloud dataset, termed STPLS3D, with more than 16
$km^2$ of landscapes and up to 18 fine-grained semantic categories. For
verification purposes, we also provide a parallel dataset collected from four
areas in the real environment. Extensive experiments conducted on our datasets
demonstrate the effectiveness and quality of the proposed synthetic dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIMO-2: End-to-End Unified Vision-Language Grounded Learning. (arXiv:2203.09067v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09067">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pre-training (VLP) has achieved impressive performance on
various cross-modal downstream tasks. However, most existing methods can only
learn from aligned image-caption data and rely heavily on expensive regional
features, which greatly limits their scalability and performance. In this
paper, we propose an end-to-end unified-modal pre-training framework, namely
UNIMO-2, for joint learning on both aligned image-caption data and unaligned
image-only and text-only corpus. We build a unified Transformer model to
jointly learn visual representations, textual representations and semantic
alignment between images and texts. In particular, we propose to conduct
grounded learning on both images and texts via a sharing grounded space, which
helps bridge unaligned images and texts, and align the visual and textual
semantic spaces on different types of corpora. The experiments show that our
grounded learning method can improve textual and visual semantic alignment for
improving performance on various cross-modal tasks. Moreover, benefiting from
effective joint modeling of different types of corpora, our model also achieves
impressive performance on single-modal visual and textual tasks. Our code and
models are public at the UNIMO project page https://unimo-ptm.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do We Really Need a Learnable Classifier at the End of Deep Neural Network?. (arXiv:2203.09081v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09081">
<div class="article-summary-box-inner">
<span><p>Modern deep neural networks for classification usually jointly learn a
backbone for representation and a linear classifier to output the logit of each
class. A recent study has shown a phenomenon called neural collapse that the
within-class means of features and the classifier vectors converge to the
vertices of a simplex equiangular tight frame (ETF) at the terminal phase of
training on a balanced dataset. Since the ETF geometric structure maximally
separates the pair-wise angles of all classes in the classifier, it is natural
to raise the question, why do we spend an effort to learn a classifier when we
know its optimal geometric structure? In this paper, we study the potential of
learning a neural network for classification with the classifier randomly
initialized as an ETF and fixed during training. Our analytical work based on
the layer-peeled model indicates that the feature learning with a fixed ETF
classifier naturally leads to the neural collapse state even when the dataset
is imbalanced among classes. We further show that in this case the cross
entropy (CE) loss is not necessary and can be replaced by a simple squared loss
that shares the same global optimality but enjoys a more accurate gradient and
better convergence property. Our experimental results show that our method is
able to achieve similar performances on image classification for balanced
datasets, and bring significant improvements in the long-tailed and
fine-grained classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Point Cloud Simplification for High-quality Surface Reconstruction. (arXiv:2203.09088v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09088">
<div class="article-summary-box-inner">
<span><p>The growing size of point clouds enlarges consumptions of storage,
transmission, and computation of 3D scenes. Raw data is redundant, noisy, and
non-uniform. Therefore, simplifying point clouds for achieving compact, clean,
and uniform points is becoming increasingly important for 3D vision and
graphics tasks. Previous learning based methods aim to generate fewer points
for scene understanding, regardless of the quality of surface reconstruction,
leading to results with low reconstruction accuracy and bad point distribution.
In this paper, we propose a novel point cloud simplification network (PCS-Net)
dedicated to high-quality surface mesh reconstruction while maintaining
geometric fidelity. We first learn a sampling matrix in a feature-aware
simplification module to reduce the number of points. Then we propose a novel
double-scale resampling module to refine the positions of the sampled points,
to achieve a uniform distribution. To further retain important shape features,
an adaptive sampling strategy with a novel saliency loss is designed. With our
PCS-Net, the input non-uniform and noisy point cloud can be simplified in a
feature-aware manner, i.e., points near salient features are consolidated but
still with uniform distribution locally. Experiments demonstrate the
effectiveness of our method and show that we outperform previous simplification
or reconstruction-oriented upsampling methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">deepNIR: Datasets for generating synthetic NIR images and improved fruit detection system using deep learning techniques. (arXiv:2203.09091v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09091">
<div class="article-summary-box-inner">
<span><p>This paper presents datasets utilised for synthetic near-infrared (NIR) image
generation and bounding-box level fruit detection systems. It is undeniable
that high-calibre machine learning frameworks such as Tensorflow or Pytorch,
and large-scale ImageNet or COCO datasets with the aid of accelerated GPU
hardware have pushed the limit of machine learning techniques for more than
decades. Among these breakthroughs, a high-quality dataset is one of the
essential building blocks that can lead to success in model generalisation and
the deployment of data-driven deep neural networks. In particular, synthetic
data generation tasks often require more training samples than other supervised
approaches. Therefore, in this paper, we share the NIR+RGB datasets that are
re-processed from two public datasets (i.e., nirscene and SEN12MS) and our
novel NIR+RGB sweet pepper(capsicum) dataset. We quantitatively and
qualitatively demonstrate that these NIR+RGB datasets are sufficient to be used
for synthetic NIR image generation. We achieved Frechet Inception Distance
(FID) of 11.36, 26.53, and 40.15 for nirscene1, SEN12MS, and sweet pepper
datasets respectively. In addition, we release manual annotations of 11 fruit
bounding boxes that can be exported as various formats using cloud service.
Four newly added fruits [blueberry, cherry, kiwi, and wheat] compound 11 novel
bounding box datasets on top of our previous work presented in the deepFruits
project [apple, avocado, capsicum, mango, orange, rockmelon, strawberry]. The
total number of bounding box instances of the dataset is 162k and it is ready
to use from cloud service. For the evaluation of the dataset, Yolov5 single
stage detector is exploited and reported impressive
mean-average-precision,mAP[0.5:0.95] results of[min:0.49, max:0.812]. We hope
these datasets are useful and serve as a baseline for the future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-aligned Fusion Transformer for One-shot Object Detection. (arXiv:2203.09093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09093">
<div class="article-summary-box-inner">
<span><p>One-shot object detection aims at detecting novel objects according to merely
one given instance. With extreme data scarcity, current approaches explore
various feature fusions to obtain directly transferable meta-knowledge. Yet,
their performances are often unsatisfactory. In this paper, we attribute this
to inappropriate correlation methods that misalign query-support semantics by
overlooking spatial structures and scale variances. Upon analysis, we leverage
the attention mechanism and propose a simple but effective architecture named
Semantic-aligned Fusion Transformer (SaFT) to resolve these issues.
Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale
semantic enhancement and a horizontal fusion module (HFM) for cross-sample
feature fusion. Together, they broaden the vision for each feature point from
the support to a whole augmented feature pyramid from the query, facilitating
semantic-aligned associations. Extensive experiments on multiple benchmarks
demonstrate the superiority of our framework. Without fine-tuning on novel
classes, it brings significant performance gains to one-stage baselines,
lifting state-of-the-art results to a higher level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Community-Driven Comprehensive Scientific Paper Summarization: Insight from cvpaper.challenge. (arXiv:2203.09109v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09109">
<div class="article-summary-box-inner">
<span><p>The present paper introduces a group activity involving writing summaries of
conference proceedings by volunteer participants. The rapid increase in
scientific papers is a heavy burden for researchers, especially non-native
speakers, who need to survey scientific literature. To alleviate this problem,
we organized a group of non-native English speakers to write summaries of
papers presented at a computer vision conference to share the knowledge of the
papers read by the group. We summarized a total of 2,000 papers presented at
the Conference on Computer Vision and Pattern Recognition, a top-tier
conference on computer vision, in 2019 and 2020. We quantitatively analyzed
participants' selection regarding which papers they read among the many
available papers. The experimental results suggest that we can summarize a wide
range of papers without asking participants to read papers unrelated to their
interests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MotionAug: Augmentation with Physical Correction for Human Motion Prediction. (arXiv:2203.09116v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09116">
<div class="article-summary-box-inner">
<span><p>This paper presents a motion data augmentation scheme incorporating motion
synthesis encouraging diversity and motion correction imposing physical
plausibility. This motion synthesis consists of our modified Variational
AutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed
sampling-near-samples method generates various valid motions even with
insufficient training motion data. Our IK-based motion synthesis method allows
us to generate a variety of motions semi-automatically. Since these two schemes
generate unrealistic artifacts in the synthesized motions, our motion
correction rectifies them. This motion correction scheme consists of imitation
learning with physics simulation and subsequent motion debiasing. For this
imitation learning, we propose the PD-residual force that significantly
accelerates the training process. Furthermore, our motion debiasing
successfully offsets the motion bias induced by imitation learning to maximize
the effect of augmentation. As a result, our method outperforms previous
noise-based motion augmentation methods by a large margin on both Recurrent
Neural Network-based and Graph Convolutional Network-based human motion
prediction models. The code is available at {\rm
\url{https://github.com/meaten/MotionAug}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRAG: Dynamic Region-Aware GCN for Privacy-Leaking Image Detection. (arXiv:2203.09121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09121">
<div class="article-summary-box-inner">
<span><p>The daily practice of sharing images on social media raises a severe issue
about privacy leakage. To address the issue, privacy-leaking image detection is
studied recently, with the goal to automatically identify images that may leak
privacy. Recent advance on this task benefits from focusing on crucial objects
via pretrained object detectors and modeling their correlation. However, these
methods have two limitations: 1) they neglect other important elements like
scenes, textures, and objects beyond the capacity of pretrained object
detectors; 2) the correlation among objects is fixed, but a fixed correlation
is not appropriate for all the images. To overcome the limitations, we propose
the Dynamic Region-Aware Graph Convolutional Network (DRAG) that dynamically
finds out crucial regions including objects and other important elements, and
models their correlation adaptively for each input image. To find out crucial
regions, we cluster spatially-correlated feature channels into several
region-aware feature maps. Further, we dynamically model the correlation with
the self-attention mechanism and explore the interaction among the regions with
a graph convolutional network. The DRAG achieved an accuracy of 87% on the
largest dataset for privacy-leaking image detection, which is 10 percentage
points higher than the state of the art. The further case study demonstrates
that it found out crucial regions containing not only objects but other
important elements like textures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input. (arXiv:2203.09123v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09123">
<div class="article-summary-box-inner">
<span><p>The transferability of adversarial examples allows the deception on black-box
models, and transfer-based targeted attacks have attracted a lot of interest
due to their practical applicability. To maximize the transfer success rate,
adversarial examples should avoid overfitting to the source model, and image
augmentation is one of the primary approaches for this. However, prior works
utilize simple image transformations such as resizing, which limits input
diversity. To tackle this limitation, we propose the object-based diverse input
(ODI) method that draws an adversarial image on a 3D object and induces the
rendered image to be classified as the target class. Our motivation comes from
the humans' superior perception of an image printed on a 3D object. If the
image is clear enough, humans can recognize the image content in a variety of
viewing conditions. Likewise, if an adversarial example looks like the target
class to the model, the model should also classify the rendered image of the 3D
object as the target class. The ODI method effectively diversifies the input by
leveraging an ensemble of multiple source objects and randomizing viewing
conditions. In our experimental results on the ImageNet-Compatible dataset,
this method boosts the average targeted attack success rate from 28.3% to 47.0%
compared to the state-of-the-art methods. We also demonstrate the applicability
of the ODI method to adversarial examples on the face verification task and its
superior performance improvement. Our code is available at
https://github.com/dreamflake/ODI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Vision Transformers Robust to Spurious Correlations?. (arXiv:2203.09125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09125">
<div class="article-summary-box-inner">
<span><p>Deep neural networks may be susceptible to learning spurious correlations
that hold on average but not in atypical test samples. As with the recent
emergence of vision transformer (ViT) models, it remains underexplored how
spurious correlations are manifested in such architectures. In this paper, we
systematically investigate the robustness of vision transformers to spurious
correlations on three challenging benchmark datasets and compare their
performance with popular CNNs. Our study reveals that when pre-trained on a
sufficiently large dataset, ViT models are more robust to spurious correlations
than CNNs. Key to their success is the ability to generalize better from the
examples where spurious correlations do not hold. Further, we perform extensive
ablations and experiments to understand the role of the self-attention
mechanism in providing robustness under spuriously correlated environments. We
hope that our work will inspire future research on further understanding the
robustness of ViT models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Generative Transformer Learning for Cross-view Geo-localization. (arXiv:2203.09135v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09135">
<div class="article-summary-box-inner">
<span><p>Cross-view geo-localization (CVGL), which aims to estimate the geographical
location of the ground-level camera by matching against enormous geo-tagged
aerial (e.g., satellite) images, remains extremely challenging due to the
drastic appearance differences across views. Existing methods mainly employ
Siamese-like CNNs to extract global descriptors without examining the mutual
benefits between the two modes. In this paper, we present a novel approach
using cross-modal knowledge generative tactics in combination with transformer,
namely mutual generative transformer learning (MGTL), for CVGL. Specifically,
MGTL develops two separate generative modules--one for aerial-like knowledge
generation from ground-level semantic information and vice versa--and fully
exploits their mutual benefits through the attention mechanism. Experiments on
challenging public benchmarks, CVACT and CVUSA, demonstrate the effectiveness
of the proposed method compared to the existing state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning. (arXiv:2203.09137v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09137">
<div class="article-summary-box-inner">
<span><p>Model-agnostic meta-learning (MAML) and its variants have become popular
approaches for few-shot learning. However, due to the non-convexity of deep
neural nets (DNNs) and the bi-level formulation of MAML, the theoretical
properties of MAML with DNNs remain largely unknown. In this paper, we first
prove that MAML with over-parameterized DNNs is guaranteed to converge to
global optima at a linear rate. Our convergence analysis indicates that MAML
with over-parameterized DNNs is equivalent to kernel regression with a novel
class of kernels, which we name as Meta Neural Tangent Kernels (MetaNTK). Then,
we propose MetaNTK-NAS, a new training-free neural architecture search (NAS)
method for few-shot learning that uses MetaNTK to rank and select
architectures. Empirically, we compare our MetaNTK-NAS with previous NAS
methods on two popular few-shot learning benchmarks, miniImageNet, and
tieredImageNet. We show that the performance of MetaNTK-NAS is comparable or
better than the state-of-the-art NAS method designed for few-shot learning
while enjoying more than 100x speedup. We believe the efficiency of MetaNTK-NAS
makes itself more practical for many real-world tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering. (arXiv:2203.09138v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09138">
<div class="article-summary-box-inner">
<span><p>Knowledge-based visual question answering requires the ability of associating
external knowledge for open-ended cross-modal scene understanding. One
limitation of existing solutions is that they capture relevant knowledge from
text-only knowledge bases, which merely contain facts expressed by first-order
predicates or language descriptions while lacking complex but indispensable
multimodal knowledge for visual understanding. How to construct vision-relevant
and explainable multimodal knowledge for the VQA scenario has been less
studied. In this paper, we propose MuKEA to represent multimodal knowledge by
an explicit triplet to correlate visual objects and fact answers with implicit
relations. To bridge the heterogeneous gap, we propose three objective losses
to learn the triplet representations from complementary views: embedding
structure, topological relation and semantic space. By adopting a pre-training
and fine-tuning learning strategy, both basic and domain-specific multimodal
knowledge are progressively accumulated for answer prediction. We outperform
the state-of-the-art by 3.35% and 6.08% respectively on two challenging
knowledge-required datasets: OK-VQA and KRVQA. Experimental results prove the
complementary benefits of the multimodal knowledge with existing knowledge
bases and the advantages of our end-to-end framework over the existing pipeline
methods. The code is available at https://github.com/AndersonStra/MuKEA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Visuo-Haptic Object Shape Completion. (arXiv:2203.09149v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09149">
<div class="article-summary-box-inner">
<span><p>Recent advancements in object shape completion have enabled impressive object
reconstructions using only visual input. However, due to self-occlusion, the
reconstructions have high uncertainty in the occluded object parts, which
negatively impacts the performance of downstream robotic tasks such as
grasping. In this work, we propose an active visuo-haptic shape completion
method called Act-VH that actively computes where to touch the objects based on
the reconstruction uncertainty. Act-VH reconstructs objects from point clouds
and calculates the reconstruction uncertainty using IGR, a recent
state-of-the-art implicit surface deep neural network. We experimentally
evaluate the reconstruction accuracy of Act-VH against five baselines in
simulation and in the real world. We also propose a new simulation environment
for this purpose. The results show that Act-VH outperforms all baselines and
that an uncertainty-driven haptic exploration policy leads to higher
reconstruction accuracy than a random policy and a policy driven by Gaussian
Process Implicit Surfaces. As a final experiment, we evaluate Act-VH and the
best reconstruction baseline on grasping 10 novel objects. The results show
that Act-VH reaches a significantly higher grasp success rate than the baseline
on all objects. Together, this work opens up the door for using active
visuo-haptic shape completion in more complex cluttered scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Rejection Function Meets Character Recognition Tasks. (arXiv:2203.09151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09151">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an optimal rejection method for rejecting ambiguous
samples by a rejection function. This rejection function is trained together
with a classification function under the framework of Learning-with-Rejection
(LwR). The highlights of LwR are: (1) the rejection strategy is not heuristic
but has a strong background from a machine learning theory, and (2) the
rejection function can be trained on an arbitrary feature space which is
different from the feature space for classification. The latter suggests we can
choose a feature space that is more suitable for rejection. Although the past
research on LwR focused only on its theoretical aspect, we propose to utilize
LwR for practical pattern classification tasks. Moreover, we propose to use
features from different CNN layers for classification and rejection. Our
extensive experiments of notMNIST classification and character/non-character
classification demonstrate that the proposed method achieves better performance
than traditional rejection strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biasing Like Human: A Cognitive Bias Framework for Scene Graph Generation. (arXiv:2203.09160v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09160">
<div class="article-summary-box-inner">
<span><p>Scene graph generation is a sophisticated task because there is no specific
recognition pattern (e.g., "looking at" and "near" have no conspicuous
difference concerning vision, whereas "near" could occur between entities with
different morphology). Thus some scene graph generation methods are trapped
into most frequent relation predictions caused by capricious visual features
and trivial dataset annotations. Therefore, recent works emphasized the
"unbiased" approaches to balance predictions for a more informative scene
graph. However, human's quick and accurate judgments over relations between
numerous objects should be attributed to "bias" (i.e., experience and
linguistic knowledge) rather than pure vision. To enhance the model capability,
inspired by the "cognitive bias" mechanism, we propose a novel 3-paradigms
framework that simulates how humans incorporate the label linguistic features
as guidance of vision-based representations to better mine hidden relation
patterns and alleviate noisy visual propagation. Our framework is
model-agnostic to any scene graph model. Comprehensive experiments prove our
framework outperforms baseline modules in several metrics with minimum
parameters increment and achieves new SOTA performance on Visual Genome
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09161">
<div class="article-summary-box-inner">
<span><p>Recently introduced instruction-paradigm empowers non-expert users to
leverage NLP resources by defining a new task in natural language.
Instruction-tuned models have significantly outperformed multitask learning
models (without instruction); however they are far from state of the art task
specific models. Conventional approaches to improve model performance via
creating large datasets with lots of task instances or architectural/training
changes in model may not be feasible for non-expert users. However, they can
write alternate instructions to represent an instruction task. Is
Instruction-augumentation helpful? We augment a subset of tasks in NATURAL
INSTRUCTIONS with additional instructions and find that these significantly
improve model performance (upto 35%) specially in low-data regime. Our results
indicate that an additional instruction can be equivalent to ~40 instances on
average across our evaluation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UWED: Unsigned Distance Field for Accurate 3D Scene Representation and Completion. (arXiv:2203.09167v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09167">
<div class="article-summary-box-inner">
<span><p>Scene Completion is the task of completing missing geometry from a partial
scan of a scene. The majority of previous methods compute an implicit
representation from range data using a Truncated Signed Distance Function
(TSDF) on a 3D grid as input to neural networks. The truncation limits but does
not remove the ambiguous cases introduced by the sign for non-closed surfaces.
As an alternative, we present an Unsigned Distance Function (UDF) called
Unsigned Weighted Euclidean Distance (UWED) as input to the scene completion
neural networks. UWED is simple and efficient as a surface representation, and
can be computed on any noisy point cloud without normals. To obtain the
explicit geometry, we present a method for extracting a point cloud from
discretized UDF values on a regular grid. We compare different SDFs and UDFs
for the scene completion task on indoor and outdoor point clouds collected from
RGB-D and LiDAR sensors and show improved completion using the proposed UWED
function.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Classification of Satellite Image Time Series with Thermal Positional Encoding. (arXiv:2203.09175v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09175">
<div class="article-summary-box-inner">
<span><p>Large-scale crop type classification is a task at the core of remote sensing
efforts with applications of both economic and ecological importance. Current
state-of-the-art deep learning methods are based on self-attention and use
satellite image time series (SITS) to discriminate crop types based on their
unique growth patterns. However, existing methods generalize poorly to regions
not seen during training mainly due to not being robust to temporal shifts of
the growing season caused by variations in climate. To this end, we propose
Thermal Positional Encoding (TPE) for attention-based crop classifiers. Unlike
previous positional encoding based on calendar time (e.g. day-of-year), TPE is
based on thermal time, which is obtained by accumulating daily average
temperatures over the growing season. Since crop growth is directly related to
thermal time, but not calendar time, TPE addresses the temporal shifts between
different regions to improve generalization. We propose multiple TPE
strategies, including learnable methods, to further improve results compared to
the common fixed positional encodings. We demonstrate our approach on a crop
classification task across four different European regions, where we obtain
state-of-the-art generalization results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel End-To-End Network for Reconstruction of Non-Regularly Sampled Image Data Using Locally Fully Connected Layers. (arXiv:2203.09180v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09180">
<div class="article-summary-box-inner">
<span><p>Quarter sampling and three-quarter sampling are novel sensor concepts that
enable the acquisition of higher resolution images without increasing the
number of pixels. This is achieved by non-regularly covering parts of each
pixel of a low-resolution sensor such that only one quadrant or three quadrants
of the sensor area of each pixel is sensitive to light. Combining a properly
designed mask and a high-quality reconstruction algorithm, a higher image
quality can be achieved than using a low-resolution sensor and subsequent
upsampling. For the latter case, the image quality can be further enhanced
using super resolution algorithms such as the very deep super resolution
network (VDSR). In this paper, we propose a novel end-to-end neural network to
reconstruct high resolution images from non-regularly sampled sensor data. The
network is a concatenation of a locally fully connected reconstruction network
(LFCR) and a standard VDSR network. Altogether, using a three-quarter sampling
sensor with our novel neural network layout, the image quality in terms of PSNR
for the Urban100 dataset can be increased by 2.96 dB compared to the
state-of-the-art approach. Compared to a low-resolution sensor with VDSR, a
gain of 1.11 dB is achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Interactive Explanatory AI System for Industrial Quality Control. (arXiv:2203.09181v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09181">
<div class="article-summary-box-inner">
<span><p>Machine learning based image classification algorithms, such as deep neural
network approaches, will be increasingly employed in critical settings such as
quality control in industry, where transparency and comprehensibility of
decisions are crucial. Therefore, we aim to extend the defect detection task
towards an interactive human-in-the-loop approach that allows us to integrate
rich background knowledge and the inference of complex relationships going
beyond traditional purely data-driven approaches. We propose an approach for an
interactive support system for classifications in an industrial quality control
setting that combines the advantages of both (explainable) knowledge-driven and
data-driven machine learning methods, in particular inductive logic programming
and convolutional neural networks, with human expertise and control. The
resulting system can assist domain experts with decisions, provide transparent
explanations for results, and integrate feedback from users; thus reducing
workload for humans while both respecting their expertise and without removing
their agency or accountability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Details or Artifacts: A Locally Discriminative Learning Approach to Realistic Image Super-Resolution. (arXiv:2203.09195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09195">
<div class="article-summary-box-inner">
<span><p>Single image super-resolution (SISR) with generative adversarial networks
(GAN) has recently attracted increasing attention due to its potentials to
generate rich details. However, the training of GAN is unstable, and it often
introduces many perceptually unpleasant artifacts along with the generated
details. In this paper, we demonstrate that it is possible to train a GAN-based
SISR model which can stably generate perceptually realistic details while
inhibiting visual artifacts. Based on the observation that the local statistics
(e.g., residual variance) of artifact areas are often different from the areas
of perceptually friendly details, we develop a framework to discriminate
between GAN-generated artifacts and realistic details, and consequently
generate an artifact map to regularize and stabilize the model training
process. Our proposed locally discriminative learning (LDL) method is simple
yet effective, which can be easily plugged in off-the-shelf SISR methods and
boost their performance. Experiments demonstrate that LDL outperforms the
state-of-the-art GAN based SISR methods, achieving not only higher
reconstruction accuracy but also superior perceptual quality on both synthetic
and real-world datasets. Codes and models are available at
https://github.com/csjliang/LDL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novel Consistency Check For Fast Recursive Reconstruction Of Non-Regularly Sampled Video Data. (arXiv:2203.09200v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09200">
<div class="article-summary-box-inner">
<span><p>Quarter sampling is a novel sensor design that allows for an acquisition of
higher resolution images without increasing the number of pixels. When being
used for video data, one out of four pixels is measured in each frame.
Effectively, this leads to a non-regular spatio-temporal sub-sampling. Compared
to purely spatial or temporal sub-sampling, this allows for an increased
reconstruction quality, as aliasing artifacts can be reduced. For the fast
reconstruction of such sensor data with a fixed mask, recursive variant of
frequency selective reconstruction (FSR) was proposed. Here, pixels measured in
previous frames are projected into the current frame to support its
reconstruction. In doing so, the motion between the frames is computed using
template matching. Since some of the motion vectors may be erroneous, it is
important to perform a proper consistency checking. In this paper, we propose
faster consistency checking methods as well as a novel recursive FSR that uses
the projected pixels different than in literature and can handle dynamic masks.
Altogether, we are able to significantly increase the reconstruction quality by
+ 1.01 dB compared to the state-of-the-art recursive reconstruction method
using a fixed mask. Compared to a single frame reconstruction, an average gain
of about + 1.52 dB is achieved for dynamic masks. At the same time, the
computational complexity of the consistency checks is reduced by a factor of 13
compared to the literature algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulation-Driven Training of Vision Transformers Enabling Metal Segmentation in X-Ray Images. (arXiv:2203.09207v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09207">
<div class="article-summary-box-inner">
<span><p>In several image acquisition and processing steps of X-ray radiography,
knowledge of the existence of metal implants and their exact position is highly
beneficial (e.g. dose regulation, image contrast adjustment). Another
application which would benefit from an accurate metal segmentation is cone
beam computed tomography (CBCT) which is based on 2D X-ray projections. Due to
the high attenuation of metals, severe artifacts occur in the 3D X-ray
acquisitions. The metal segmentation in CBCT projections usually serves as a
prerequisite for metal artifact avoidance and reduction algorithms. Since the
generation of high quality clinical training is a constant challenge, this
study proposes to generate simulated X-ray images based on CT data sets
combined with self-designed computer aided design (CAD) implants and make use
of convolutional neural network (CNN) and vision transformer (ViT) for metal
segmentation. Model test is performed on accurately labeled X-ray test datasets
obtained from specimen scans. The CNN encoder-based network like U-Net has
limited performance on cadaver test data with an average dice score below 0.30,
while the metal segmentation transformer with dual decoder (MST-DD) shows high
robustness and generalization on the segmentation task, with an average dice
score of 0.90. Our study indicates that the CAD model-based data generation has
high flexibility and could be a way to overcome the problem of shortage in
clinical data sampling and labelling. Furthermore, the MST-DD approach
generates a more reliable neural network in case of training on simulated data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Compression-Based Feature Learning for Video Restoration. (arXiv:2203.09208v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09208">
<div class="article-summary-box-inner">
<span><p>How to efficiently utilize the temporal features is crucial, yet challenging,
for video restoration. The temporal features usually contain various noisy and
uncorrelated information, and they may interfere with the restoration of the
current frame. This paper proposes learning noise-robust feature
representations to help video restoration. We are inspired by that the neural
codec is a natural denoiser. In neural codec, the noisy and uncorrelated
contents which are hard to predict but cost lots of bits are more inclined to
be discarded for bitrate saving. Therefore, we design a neural compression
module to filter the noise and keep the most useful information in features for
video restoration. To achieve robustness to noise, our compression module
adopts a spatial-channel-wise quantization mechanism to adaptively determine
the quantization step size for each position in the latent. Experiments show
that our method can significantly boost the performance on video denoising,
where we obtain 0.13 dB improvement over BasicVSR++ with only 0.23x FLOPs.
Meanwhile, our method also obtains SOTA results on video deraining and
dehazing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09215">
<div class="article-summary-box-inner">
<span><p>We propose Human-centered 4D Scene Capture (HSC4D) to accurately and
efficiently create a dynamic digital world, containing large-scale
indoor-outdoor scenes, diverse human motions, and rich interactions between
humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is
space-free without any external devices' constraints and map-free without
pre-built maps. Considering that IMUs can capture human poses but always drift
for long-period use, while LiDAR is stable for global localization but rough
for local positions and orientations, HSC4D makes both sensors complement each
other by a joint optimization and achieves promising results for long-term
capture. Relationships between humans and environments are also explored to
make their interaction more realistic. To facilitate many down-stream tasks,
like AR, VR, robots, autonomous driving, etc., we propose a dataset containing
three large scenes (1k-5k $m^2$) with accurate dynamic human motions and
locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)
and challenging human activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization ability of HSC4D.
The dataset and code is available at https://github.com/climbingdaily/HSC4D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surgical Workflow Recognition: from Analysis of Challenges to Architectural Study. (arXiv:2203.09230v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09230">
<div class="article-summary-box-inner">
<span><p>Algorithmic surgical workflow recognition is an ongoing research field and
can be divided into laparoscopic (Internal) and operating room (External)
analysis. So far many different works for the internal analysis have been
proposed with the combination of a frame-level and an additional temporal model
to address the temporal ambiguities between different workflow phases. For the
External recognition task, Clip-level methods are in the focus of researchers
targeting the local ambiguities present in the OR scene. In this work we
evaluate combinations of different model architectures for the task of surgical
workflow recognition to provide a fair comparison of the methods for both
Internal and External analysis. We show that methods designed for the Internal
analysis can be transferred to the external task with comparable performance
gains for different architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-aware Neural Style Transfer using Instance Normalization. (arXiv:2203.09242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09242">
<div class="article-summary-box-inner">
<span><p>Neural Style Transfer (NST) is concerned with the artistic stylization of
visual media. It can be described as the process of transferring the style of
an artistic image onto an ordinary photograph. Recently, a number of studies
have considered the enhancement of the depth-preserving capabilities of the NST
algorithms to address the undesired effects that occur when the input content
images include numerous objects at various depths. Our approach uses a deep
residual convolutional network with instance normalization layers that utilizes
an advanced depth prediction network to integrate depth preservation as an
additional loss function to content and style. We demonstrate results that are
effective in retaining the depth and global structure of content images. Three
different evaluation processes show that our system is capable of preserving
the structure of the stylized results while exhibiting style-capture
capabilities and aesthetic qualities comparable or superior to state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Properties of Adversarially-Trained CNNs. (arXiv:2203.09243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09243">
<div class="article-summary-box-inner">
<span><p>Adversarial Training has proved to be an effective training paradigm to
enforce robustness against adversarial examples in modern neural network
architectures. Despite many efforts, explanations of the foundational
principles underpinning the effectiveness of Adversarial Training are limited
and far from being widely accepted by the Deep Learning community. In this
paper, we describe surprising properties of adversarially-trained models,
shedding light on mechanisms through which robustness against adversarial
attacks is implemented. Moreover, we highlight limitations and failure modes
affecting these models that were not discussed by prior works. We conduct
extensive analyses on a wide range of architectures and datasets, performing a
deep comparison between robust and natural models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. (arXiv:2203.09249v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09249">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) is an emerging distributed learning paradigm under
privacy constraint. Data heterogeneity is one of the main challenges in FL,
which results in slow convergence and degraded performance. Most existing
approaches only tackle the heterogeneity challenge by restricting the local
model update in client, ignoring the performance drop caused by direct global
model aggregation. Instead, we propose a data-free knowledge distillation
method to fine-tune the global model in the server (FedFTG), which relieves the
issue of direct model aggregation. Concretely, FedFTG explores the input space
of local models through a generator, and uses it to transfer the knowledge from
local models to the global model. Besides, we propose a hard sample mining
scheme to achieve effective knowledge distillation throughout the training. In
addition, we develop customized label sampling and class-level ensemble to
derive maximum utilization of knowledge, which implicitly mitigates the
distribution discrepancy across clients. Extensive experiments show that our
FedFTG significantly outperforms the state-of-the-art (SOTA) FL algorithms and
can serve as a strong plugin for enhancing FedAvg, FedProx, FedDyn, and
SCAFFOLD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Cross-Domain Open World Recognition. (arXiv:2203.09257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09257">
<div class="article-summary-box-inner">
<span><p>The ability to evolve is fundamental for any valuable autonomous agent whose
knowledge cannot remain limited to that injected by the manufacturer. Consider
for example a home assistant robot: it should be able to incrementally learn
new object categories when requested, but also to recognize the same objects in
different environments (rooms) and poses (hand-held/on the floor/above
furniture), while rejecting unknown ones. Despite its importance, this scenario
has started to raise interest in the robotic community only recently and the
related research is still in its infancy, with existing experimental testbeds
but no tailored methods. With this work, we propose the first learning approach
that deals with all the previously mentioned challenges at once by exploiting a
single contrastive objective. We show how it learns a feature space perfectly
suitable to incrementally include new classes and is able to capture knowledge
which generalizes across a variety of visual domains. Our method is endowed
with a tailored effective stopping criterion for each learning episode and
exploits a novel self-paced thresholding strategy that provides the classifier
with a reliable rejection option. Both these contributions are based on the
observation of the data statistics and do not need manual tuning. An extensive
experimental analysis confirms the effectiveness of the proposed approach
establishing the new state-of-the-art. The code is available at
https://github.com/FrancescoCappio/Contrastive_Open_World.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09268">
<div class="article-summary-box-inner">
<span><p>We present PROSUB: PROgressive SUBsampling, a deep learning based, automated
methodology that subsamples an oversampled data set (e.g. multi-channeled 3D
images) with minimal loss of information. We build upon a recent dual-network
approach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI
measurement sampling-reconstruction challenge, but suffers from deep learning
training instability, by subsampling with a hard decision boundary. PROSUB uses
the paradigm of recursive feature elimination (RFE) and progressively
subsamples measurements during deep learning training, improving optimization
stability. PROSUB also integrates a neural architecture search (NAS) paradigm,
allowing the network architecture hyperparameters to respond to the subsampling
process. We show PROSUB outperforms the winner of the MUDI MICCAI challenge,
producing large improvements &gt;18% MSE on the MUDI challenge sub-tasks and
qualitative improvements on downstream processes useful for clinical
applications. We also show the benefits of incorporating NAS and analyze the
effect of PROSUB's components. As our method generalizes to other problems
beyond MRI measurement selection-reconstruction, our code is
https://github.com/sbb-gh/PROSUB
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ART-SS: An Adaptive Rejection Technique for Semi-Supervised restoration for adverse weather-affected images. (arXiv:2203.09275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09275">
<div class="article-summary-box-inner">
<span><p>In recent years, convolutional neural network-based single image adverse
weather removal methods have achieved significant performance improvements on
many benchmark datasets. However, these methods require large amounts of
clean-weather degraded image pairs for training, which is often difficult to
obtain in practice. Although various weather degradation synthesis methods
exist in the literature, the use of synthetically generated weather degraded
images often results in sub-optimal performance on the real weather degraded
images due to the domain gap between synthetic and real-world images. To deal
with this problem, various semi-supervised restoration (SSR) methods have been
proposed for deraining or dehazing which learn to restore the clean image using
synthetically generated datasets while generalizing better using unlabeled
real-world images. The performance of a semi-supervised method is essentially
based on the quality of the unlabeled data. In particular, if the unlabeled
data characteristics are very different from that of the labeled data, then the
performance of a semi-supervised method degrades significantly. We
theoretically study the effect of unlabeled data on the performance of an SSR
method and develop a technique that rejects the unlabeled images that degrade
the performance. Extensive experiments and ablation study show that the
proposed sample rejection method increases the performance of existing SSR
deraining and dehazing methods significantly. Code is available at
:https://github.com/rajeevyasarla/ART-SS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanoFormer: Panorama Transformer for Indoor 360{\deg} Depth Estimation. (arXiv:2203.09283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09283">
<div class="article-summary-box-inner">
<span><p>Existing panoramic depth estimation methods based on convolutional neural
networks (CNNs) focus on removing panoramic distortions, failing to perceive
panoramic structures efficiently due to the fixed receptive field in CNNs. This
paper proposes the panorama transformer (named PanoFormer) to estimate the
depth in panorama images, with tangent patches from spherical domain, learnable
token flows, and panorama specific metrics. In particular, we divide patches on
the spherical tangent domain into tokens to reduce the negative effect of
panoramic distortions. Since the geometric structures are essential for depth
estimation, a self-attention module is redesigned with an additional learnable
token flow. In addition, considering the characteristic of the spherical
domain, we present two panorama-specific metrics to comprehensively evaluate
the panoramic depth estimation models' performance. Extensive experiments
demonstrate that our approach significantly outperforms the state-of-the-art
(SOTA) methods. Furthermore, the proposed method can be effectively extended to
solve semantic panorama segmentation, a similar pixel2pixel task. Code will be
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HybridCap: Inertia-aid Monocular Capture of Challenging Human Motions. (arXiv:2203.09287v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09287">
<div class="article-summary-box-inner">
<span><p>Monocular 3D motion capture (mocap) is beneficial to many applications. The
use of a single camera, however, often fails to handle occlusions of different
body parts and hence it is limited to capture relatively simple movements. We
present a light-weight, hybrid mocap technique called HybridCap that augments
the camera with only 4 Inertial Measurement Units (IMUs) in a
learning-and-optimization framework. We first employ a weakly-supervised and
hierarchical motion inference module based on cooperative Gated Recurrent Unit
(GRU) blocks that serve as limb, body and root trackers as well as an inverse
kinematics solver. Our network effectively narrows the search space of
plausible motions via coarse-to-fine pose estimation and manages to tackle
challenging movements with high efficiency. We further develop a hybrid
optimization scheme that combines inertial feedback and visual cues to improve
tracking accuracy. Extensive experiments on various datasets demonstrate
HybridCap can robustly handle challenging movements ranging from fitness
actions to Latin dance. It also achieves real-time performance up to 60 fps
with state-of-the-art accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PreTR: Spatio-Temporal Non-Autoregressive Trajectory Prediction Transformer. (arXiv:2203.09293v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09293">
<div class="article-summary-box-inner">
<span><p>Nowadays, our mobility systems are evolving into the era of intelligent
vehicles that aim to improve road safety. Due to their vulnerability,
pedestrians are the users who will benefit the most from these developments.
However, predicting their trajectory is one of the most challenging concerns.
Indeed, accurate prediction requires a good understanding of multi-agent
interactions that can be complex. Learning the underlying spatial and temporal
patterns caused by these interactions is even more of a competitive and open
problem that many researchers are tackling. In this paper, we introduce a model
called PRediction Transformer (PReTR) that extracts features from the
multi-agent scenes by employing a factorized spatio-temporal attention module.
It shows less computational needs than previously studied models with
empirically better results. Besides, previous works in motion prediction suffer
from the exposure bias problem caused by generating future sequences
conditioned on model prediction samples rather than ground-truth samples. In
order to go beyond the proposed solutions, we leverage encoder-decoder
Transformer networks for parallel decoding a set of learned object queries.
This non-autoregressive solution avoids the need for iterative conditioning and
arguably decreases training and testing computational time. We evaluate our
model on the ETH/UCY datasets, a publicly available benchmark for pedestrian
trajectory prediction. Finally, we justify our usage of the parallel decoding
technique by showing that the trajectory prediction task can be better solved
as a non-autoregressive task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Differentiable Two-stage Alignment Scheme for Burst Image Reconstruction with Large Shift. (arXiv:2203.09294v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09294">
<div class="article-summary-box-inner">
<span><p>Denoising and demosaicking are two essential steps to reconstruct a clean
full-color image from the raw data. Recently, joint denoising and demosaicking
(JDD) for burst images, namely JDD-B, has attracted much attention by using
multiple raw images captured in a short time to reconstruct a single
high-quality image. One key challenge of JDD-B lies in the robust alignment of
image frames. State-of-the-art alignment methods in feature domain cannot
effectively utilize the temporal information of burst images, where large
shifts commonly exist due to camera and object motion. In addition, the higher
resolution (e.g., 4K) of modern imaging devices results in larger displacement
between frames. To address these challenges, we design a differentiable
two-stage alignment scheme sequentially in patch and pixel level for effective
JDD-B. The input burst images are firstly aligned in the patch level by using a
differentiable progressive block matching method, which can estimate the offset
between distant frames with small computational cost. Then we perform implicit
pixel-wise alignment in full-resolution feature domain to refine the alignment
results. The two stages are jointly trained in an end-to-end manner. Extensive
experiments demonstrate the significant improvement of our method over existing
JDD-B methods. Codes are available at https://github.com/GuoShi28/2StageAlign.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-Shot Adaptation of GAN in Just One CLIP. (arXiv:2203.09301v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09301">
<div class="article-summary-box-inner">
<span><p>There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Prediction at Multiple Scales with Hierarchical Recurrent Networks. (arXiv:2203.09303v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09303">
<div class="article-summary-box-inner">
<span><p>Autonomous systems not only need to understand their current environment, but
should also be able to predict future actions conditioned on past states, for
instance based on captured camera frames. For certain tasks, detailed
predictions such as future video frames are required in the near future,
whereas for others it is beneficial to also predict more abstract
representations for longer time horizons. However, existing video prediction
models mainly focus on forecasting detailed possible outcomes for short
time-horizons, hence being of limited use for robot perception and spatial
reasoning. We propose Multi-Scale Hierarchical Prediction (MSPred), a novel
video prediction model able to forecast future possible outcomes of different
levels of granularity at different time-scales simultaneously. By combining
spatial and temporal downsampling, MSPred is able to efficiently predict
abstract representations such as human poses or object locations over long time
horizons, while still maintaining a competitive performance for video frame
prediction. In our experiments, we demonstrate that our proposed model
accurately predicts future video frames as well as other representations (e.g.
keypoints or positions) on various scenarios, including bin-picking scenes or
action recognition datasets, consistently outperforming popular approaches for
video frame prediction. Furthermore, we conduct an ablation study to
investigate the importance of the different modules and design choices in
MSPred. In the spirit of reproducible research, we open-source VP-Suite, a
general framework for deep-learning-based video prediction, as well as
pretrained models to reproduce our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Structural Knowledge in Multimodal-BERT. (arXiv:2203.09306v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09306">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate the knowledge learned in the embeddings of
multimodal-BERT models. More specifically, we probe their capabilities of
storing the grammatical structure of linguistic data and the structure learned
over objects in visual data. To reach that goal, we first make the inherent
structure of language and visuals explicit by a dependency parse of the
sentences that describe the image and by the dependencies between the object
regions in the image, respectively. We call this explicit visual structure the
\textit{scene tree}, that is based on the dependency tree of the language
description. Extensive probing experiments show that the multimodal-BERT models
do not encode these scene trees.Code available at
\url{https://github.com/VSJMilewski/multimodal-probes}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localizing Visual Sounds the Easy Way. (arXiv:2203.09324v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09324">
<div class="article-summary-box-inner">
<span><p>Unsupervised audio-visual source localization aims at localizing visible
sound sources in a video without relying on ground-truth localization for
training. Previous works often seek high audio-visual similarities for likely
positive (sounding) regions and low similarities for likely negative regions.
However, accurately distinguishing between sounding and non-sounding regions is
challenging without manual annotations. In this work, we propose a simple yet
effective approach for Easy Visual Sound Localization, namely EZ-VSL, without
relying on the construction of positive and/or negative regions during
training. Instead, we align audio and visual spaces by seeking audio-visual
representations that are aligned in, at least, one location of the associated
image, while not matching other images, at any location. We also introduce a
novel object guided localization scheme at inference time for improved
precision. Our simple and effective framework achieves state-of-the-art
performance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In
particular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to
83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is
available at https://github.com/stoneMo/EZ-VSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modulated Contrast for Versatile Image Synthesis. (arXiv:2203.09333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09333">
<div class="article-summary-box-inner">
<span><p>Perceiving the similarity between images has been a long-standing and
fundamental problem underlying various visual generation tasks. Predominant
approaches measure the inter-image distance by computing pointwise absolute
deviations, which tends to estimate the median of instance distributions and
leads to blurs and artifacts in the generated images. This paper presents
MoNCE, a versatile metric that introduces image contrast to learn a calibrated
metric for the perception of multifaceted inter-image distances. Unlike vanilla
contrast which indiscriminately pushes negative samples from the anchor
regardless of their similarity, we propose to re-weight the pushing force of
negative samples adaptively according to their similarity to the anchor, which
facilitates the contrastive learning from informative negative samples. Since
multiple patch-level contrastive objectives are involved in image distance
measurement, we introduce optimal transport in MoNCE to modulate the pushing
force of negative samples collaboratively across multiple contrastive
objectives. Extensive experiments over multiple image translation tasks show
that the proposed MoNCE outperforms various prevailing metrics substantially.
The code is available at https://github.com/fnzhan/MoNCE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Localization under Single Coarse Point Supervision. (arXiv:2203.09338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09338">
<div class="article-summary-box-inner">
<span><p>Point-based object localization (POL), which pursues high-performance object
sensing under low-cost data annotation, has attracted increased attention.
However, the point annotation mode inevitably introduces semantic variance for
the inconsistency of annotated points. Existing POL methods heavily reply on
accurate key-point annotations which are difficult to define. In this study, we
propose a POL method using coarse point annotations, relaxing the supervision
signals from accurate key points to freely spotted points. To this end, we
propose a coarse point refinement (CPR) approach, which to our best knowledge
is the first attempt to alleviate semantic variance from the perspective of
algorithm. CPR constructs point bags, selects semantic-correlated points, and
produces semantic center points through multiple instance learning (MIL). In
this way, CPR defines a weakly supervised evolution procedure, which ensures
training high-performance object localizer under coarse point supervision.
Experimental results on COCO, DOTA and our proposed SeaPerson dataset validate
the effectiveness of the CPR approach. The dataset and code will be available
at https://github.com/ucas-vg/PointTinyBenchmark/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CYBORGS: Contrastively Bootstrapping Object Representations by Grounding in Segmentation. (arXiv:2203.09343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09343">
<div class="article-summary-box-inner">
<span><p>Many recent approaches in contrastive learning have worked to close the gap
between pretraining on iconic images like ImageNet and pretraining on complex
scenes like COCO. This gap exists largely because commonly used random crop
augmentations obtain semantically inconsistent content in crowded scene images
of diverse objects. Previous works use preprocessing pipelines to localize
salient objects for improved cropping, but an end-to-end solution is still
elusive. In this work, we propose a framework which accomplishes this goal via
joint learning of representations and segmentation. We leverage segmentation
masks to train a model with a mask-dependent contrastive loss, and use the
partially trained model to bootstrap better masks. By iterating between these
two components, we ground the contrastive updates in segmentation information,
and simultaneously improve segmentation throughout pretraining. Experiments
show our representations transfer robustly to downstream tasks in
classification, detection and segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POSTER: Diagnosis of COVID-19 through Transfer Learning Techniques on CT Scans: A Comparison of Deep Learning Models. (arXiv:2203.09348v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09348">
<div class="article-summary-box-inner">
<span><p>The novel coronavirus disease (COVID-19) constitutes a public health
emergency globally. It is a deadly disease which has infected more than 230
million people worldwide. Therefore, early and unswerving detection of COVID-19
is necessary. Evidence of this virus is most commonly being tested by RT-PCR
test. This test is not 100% reliable as it is known to give false positives and
false negatives. Other methods like X-Ray images or CT scans show the detailed
imaging of lungs and have been proven more reliable. This paper compares
different deep learning models used to detect COVID-19 through transfer
learning technique on CT scan dataset. VGG-16 outperforms all the other models
achieving an accuracy of 85.33% on the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine Detailed Texture Learning for 3D Meshes with Generative Models. (arXiv:2203.09362v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09362">
<div class="article-summary-box-inner">
<span><p>This paper presents a method to reconstruct high-quality textured 3D models
from both multi-view and single-view images. The reconstruction is posed as an
adaptation problem and is done progressively where in the first stage, we focus
on learning accurate geometry, whereas in the second stage, we focus on
learning the texture with a generative adversarial network. In the generative
learning pipeline, we propose two improvements. First, since the learned
textures should be spatially aligned, we propose an attention mechanism that
relies on the learnable positions of pixels. Secondly, since discriminator
receives aligned texture maps, we augment its input with a learnable embedding
which improves the feedback to the generator. We achieve significant
improvements on multi-view sequences from Tripod dataset as well as on
single-view image datasets, Pascal 3D+ and CUB. We demonstrate that our method
achieves superior 3D textured models compared to the previous works. Please
visit our web-page for 3D visuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interacting Attention Graph for Single Image Two-Hand Reconstruction. (arXiv:2203.09364v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09364">
<div class="article-summary-box-inner">
<span><p>Graph convolutional network (GCN) has achieved great success in single hand
reconstruction task, while interacting two-hand reconstruction by GCN remains
unexplored. In this paper, we present Interacting Attention Graph Hand
(IntagHand), the first graph convolution based network that reconstructs two
interacting hands from a single RGB image. To solve occlusion and interaction
challenges of two-hand reconstruction, we introduce two novel attention based
modules in each upsampling step of the original GCN. The first module is the
pyramid image feature attention (PIFA) module, which utilizes multiresolution
features to implicitly obtain vertex-to-image alignment. The second module is
the cross hand attention (CHA) module that encodes the coherence of interacting
hands by building dense cross-attention between two hand vertices. As a result,
our model outperforms all existing two-hand reconstruction methods by a large
margin on InterHand2.6M benchmark. Moreover, ablation studies verify the
effectiveness of both PIFA and CHA modules for improving the reconstruction
accuracy. Results on in-the-wild images further demonstrate the generalization
ability of our network. Our code is available at
https://github.com/Dw1010/IntagHand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming Gait: Video-Based Spatiotemporal Gait Analysis. (arXiv:2203.09371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09371">
<div class="article-summary-box-inner">
<span><p>Human pose estimation from monocular video is a rapidly advancing field that
offers great promise to human movement science and rehabilitation. This
potential is tempered by the smaller body of work ensuring the outputs are
clinically meaningful and properly calibrated. Gait analysis, typically
performed in a dedicated lab, produces precise measurements including
kinematics and step timing. Using over 7000 monocular video from an
instrumented gait analysis lab, we trained a neural network to map 3D joint
trajectories and the height of individuals onto interpretable biomechanical
outputs including gait cycle timing and sagittal plane joint kinematics and
spatiotemporal trajectories. This task specific layer produces accurate
estimates of the timing of foot contact and foot off events. After parsing the
kinematic outputs into individual gait cycles, it also enables accurate
cycle-by-cycle estimates of cadence, step time, double and single support time,
walking speed and step length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using the Order of Tomographic Slices as a Prior for Neural Networks Pre-Training. (arXiv:2203.09372v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09372">
<div class="article-summary-box-inner">
<span><p>The technical advances in Computed Tomography (CT) allow to obtain immense
amounts of 3D data. For such datasets it is very costly and time-consuming to
obtain the accurate 3D segmentation markup to train neural networks. The
annotation is typically done for a limited number of 2D slices, followed by an
interpolation. In this work, we propose a pre-training method SortingLoss. It
performs pre-training on slices instead of volumes, so that a model could be
fine-tuned on a sparse set of slices, without the interpolation step. Unlike
general methods (e.g. SimCLR or Barlow Twins), the task specific methods (e.g.
Transferable Visual Words) trade broad applicability for quality benefits by
imposing stronger assumptions on the input data. We propose a relatively mild
assumption -- if we take several slices along some axis of a volume, structure
of the sample presented on those slices, should give a strong clue to
reconstruct the correct order of those slices along the axis. Many biomedical
datasets fulfill this requirement due to the specific anatomy of a sample and
pre-defined alignment of the imaging setup. We examine the proposed method on
two datasets: medical CT of lungs affected by COVID-19 disease, and
high-resolution synchrotron-based full-body CT of model organisms (Medaka
fish). We show that the proposed method performs on par with SimCLR, while
working 2x faster and requiring 1.5x less memory. In addition, we present the
benefits in terms of practical scenarios, especially the applicability to the
pre-training of large models and the ability to localize samples within volumes
in an unsupervised setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Part Priors: Learning to Optimize Part-Based Object Completion in RGB-D Scans. (arXiv:2203.09375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09375">
<div class="article-summary-box-inner">
<span><p>3D object recognition has seen significant advances in recent years, showing
impressive performance on real-world 3D scan benchmarks, but lacking in object
part reasoning, which is fundamental to higher-level scene understanding such
as inter-object similarities or object functionality. Thus, we propose to
leverage large-scale synthetic datasets of 3D shapes annotated with part
information to learn Neural Part Priors (NPPs), optimizable spaces
characterizing geometric part priors. Crucially, we can optimize over the
learned part priors in order to fit to real-world scanned 3D scenes at test
time, enabling robust part decomposition of the real objects in these scenes
that also estimates the complete geometry of the object while fitting
accurately to the observed real geometry. Moreover, this enables global
optimization over geometrically similar detected objects in a scene, which
often share strong geometric commonalities, enabling scene-consistent part
decompositions. Experiments on the ScanNet dataset demonstrate that NPPs
significantly outperforms state of the art in part decomposition and object
completion in real-world scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-Stage Deep Edge Detection Based on Dense-Scale Feature Fusion and Pixel-Level Imbalance Learning. (arXiv:2203.09387v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09387">
<div class="article-summary-box-inner">
<span><p>Edge detection, a basic task in the field of computer vision, is an important
preprocessing operation for the recognition and understanding of a visual
scene. In conventional models, the edge image generated is ambiguous, and the
edge lines are also very thick, which typically necessitates the use of
non-maximum suppression (NMS) and morphological thinning operations to generate
clear and thin edge images. In this paper, we aim to propose a one-stage neural
network model that can generate high-quality edge images without
postprocessing. The proposed model adopts a classic encoder-decoder framework
in which a pre-trained neural model is used as the encoder and a
multi-feature-fusion mechanism that merges the features of each level with each
other functions as a learnable decoder. Further, we propose a new loss function
that addresses the pixel-level imbalance in the edge image by suppressing the
false positive (FP) edge information near the true positive (TP) edge and the
false negative (FN) non-edge. The results of experiments conducted on several
benchmark datasets indicate that the proposed method achieves state-of-the-art
results without using NMS and morphological thinning operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-resolution. (arXiv:2203.09388v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09388">
<div class="article-summary-box-inner">
<span><p>Scene text image super-resolution aims to increase the resolution and
readability of the text in low-resolution images. Though significant
improvement has been achieved by deep convolutional neural networks (CNNs), it
remains difficult to reconstruct high-resolution images for spatially deformed
texts, especially rotated and curve-shaped ones. This is because the current
CNN-based methods adopt locality-based operations, which are not effective to
deal with the variation caused by deformations. In this paper, we propose a CNN
based Text ATTention network (TATT) to address this problem. The semantics of
the text are firstly extracted by a text recognition module as text prior
information. Then we design a novel transformer-based module, which leverages
global attention mechanism, to exert the semantic guidance of text prior to the
text reconstruction process. In addition, we propose a text structure
consistency loss to refine the visual appearance by imposing structural
consistency on the reconstructions of regular and deformed texts. Experiments
on the benchmark TextZoom dataset show that the proposed TATT not only achieves
state-of-the-art performance in terms of PSNR/SSIM metrics, but also
significantly improves the recognition accuracy in the downstream text
recognition task, particularly for text instances with multi-orientation and
curved shapes. Code is available at https://github.com/mjq11302010044/TATT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medium Transmission Map Matters for Learning to Restore Real-World Underwater Images. (arXiv:2203.09414v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09414">
<div class="article-summary-box-inner">
<span><p>Underwater visual perception is essentially important for underwater
exploration, archeology, ecosystem and so on. The low illumination, light
reflections, scattering, absorption and suspended particles inevitably lead to
the critically degraded underwater image quality, which causes great challenges
on recognizing the objects from the underwater images. The existing underwater
enhancement methods that aim to promote the underwater visibility, heavily
suffer from the poor image restoration performance and generalization ability.
To reduce the difficulty of underwater image enhancement, we introduce the
media transmission map as guidance to assist in image enhancement. We formulate
the interaction between the underwater visual images and the transmission map
to obtain better enhancement results. Even with simple and lightweight network
configuration, the proposed method can achieve advanced results of 22.6 dB on
the challenging Test-R90 with an impressive 30 times faster than the existing
models. Comprehensive experimental results have demonstrated the superiority
and potential on underwater perception. Paper's code is privoded on:
https://github.com/GroupG-yk/MTUR-Net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-directional Object-context Prioritization Learning for Saliency Ranking. (arXiv:2203.09416v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09416">
<div class="article-summary-box-inner">
<span><p>The saliency ranking task is recently proposed to study the visual behavior
that humans would typically shift their attention over different objects of a
scene based on their degrees of saliency. Existing approaches focus on learning
either object-object or object-scene relations. Such a strategy follows the
idea of object-based attention in Psychology, but it tends to favor those
objects with strong semantics (e.g., humans), resulting in unrealistic saliency
ranking. We observe that spatial attention works concurrently with object-based
attention in the human visual recognition system. During the recognition
process, the human spatial attention mechanism would move, engage, and
disengage from region to region (i.e., context to context). This inspires us to
model the region-level interactions, in addition to the object-level reasoning,
for saliency ranking. To this end, we propose a novel bi-directional method to
unify spatial attention and object-based attention for saliency ranking. Our
model includes two novel modules: (1) a selective object saliency (SOS) module
that models objectbased attention via inferring the semantic representation of
the salient object, and (2) an object-context-object relation (OCOR) module
that allocates saliency ranks to objects by jointly modeling the object-context
and context-object interactions of the salient objects. Extensive experiments
show that our approach outperforms existing state-of-theart methods. Our code
and pretrained model are available at https://github.com/GrassBro/OCOR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation. (arXiv:2203.09418v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09418">
<div class="article-summary-box-inner">
<span><p>Establishing correspondences from image to 3D has been a key task of 6DoF
object pose estimation for a long time. To predict pose more accurately, deeply
learned dense maps replaced sparse templates. Dense methods also improved pose
estimation in the presence of occlusion. More recently researchers have shown
improvements by learning object fragments as segmentation. In this work, we
present a discrete descriptor, which can represent the object surface densely.
By incorporating a hierarchical binary grouping, we can encode the object
surface very efficiently. Moreover, we propose a coarse to fine training
strategy, which enables fine-grained correspondence prediction. Finally, by
matching predicted codes with object surface and using a PnP solver, we
estimate the 6DoF pose. Results on the public LM-O and YCB-V datasets show
major improvement over the state of the art w.r.t. ADD(-S) metric, even
surpassing RGB-D based methods in some cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Unsupervised Hashing with Latent Semantic Components. (arXiv:2203.09420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09420">
<div class="article-summary-box-inner">
<span><p>Deep unsupervised hashing has been appreciated in the regime of image
retrieval. However, most prior arts failed to detect the semantic components
and their relationships behind the images, which makes them lack discriminative
power. To make up the defect, we propose a novel Deep Semantic Components
Hashing (DSCH), which involves a common sense that an image normally contains a
bunch of semantic components with homology and co-occurrence relationships.
Based on this prior, DSCH regards the semantic components as latent variables
under the Expectation-Maximization framework and designs a two-step iterative
algorithm with the objective of maximum likelihood of training data. Firstly,
DSCH constructs a semantic component structure by uncovering the fine-grained
semantics components of images with a Gaussian Mixture Modal~(GMM), where an
image is represented as a mixture of multiple components, and the semantics
co-occurrence are exploited. Besides, coarse-grained semantics components, are
discovered by considering the homology relationships between fine-grained
components, and the hierarchy organization is then constructed. Secondly, DSCH
makes the images close to their semantic component centers at both fine-grained
and coarse-grained levels, and also makes the images share similar semantic
components close to each other. Extensive experiments on three benchmark
datasets demonstrate that the proposed hierarchical semantic components indeed
facilitate the hashing model to achieve superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Learning for Domain Adaptation: Self-distillation Image Dehazing Network with Sample-cycle. (arXiv:2203.09430v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09430">
<div class="article-summary-box-inner">
<span><p>Deep learning-based methods have made significant achievements for image
dehazing. However, most of existing dehazing networks are concentrated on
training models using simulated hazy images, resulting in generalization
performance degradation when applied on real-world hazy images because of
domain shift. In this paper, we propose a mutual learning dehazing framework
for domain adaption. Specifically, we first devise two siamese networks: a
teacher network in the synthetic domain and a student network in the real
domain, and then optimize them in a mutual learning manner by leveraging EMA
and joint loss. Moreover, we design a sample-cycle strategy based on density
augmentation (HDA) module to introduce pseudo real-world image pairs provided
by the student network into training for further improving the generalization
performance. Extensive experiments on both synthetic and real-world dataset
demonstrate that the propose mutual learning framework outperforms
state-of-the-art dehazing techniques in terms of subjective and objective
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes. (arXiv:2203.09440v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09440">
<div class="article-summary-box-inner">
<span><p>Many basic indoor activities such as eating or writing are always conducted
upon different tabletops (e.g., coffee tables, writing desks). It is
indispensable to understanding tabletop scenes in 3D indoor scene parsing
applications. Unfortunately, it is hard to meet this demand by directly
deploying data-driven algorithms, since 3D tabletop scenes are rarely available
in current datasets. To remedy this defect, we introduce TO-Scene, a
large-scale dataset focusing on tabletop scenes, which contains 20,740 scenes
with three variants. To acquire the data, we design an efficient and scalable
framework, where a crowdsourcing UI is developed to transfer CAD objects onto
tables from ScanNet. Then the output tabletop scenes are simulated into real
scans and annotated automatically.
</p>
<p>Further, we propose a tabletop-aware learning strategy for better perceiving
the small-sized tabletop instances. Notably, we also provide a real scanned
test set TO-Real to verify the practical value of TO-Scene. Experiments show
that the algorithms trained on TO-Scene indeed work on the realistic test data,
and our proposed tabletop-aware learning strategy greatly improves the
state-of-the-art results on both 3D semantic segmentation and object detection
tasks. TO-Scene and TO-Real, plus Web UI, will all be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Super-Resolution With Deep Variational Autoencoders. (arXiv:2203.09445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09445">
<div class="article-summary-box-inner">
<span><p>Image super-resolution (SR) techniques are used to generate a high-resolution
image from a low-resolution image. Until now, deep generative models such as
autoregressive models and Generative Adversarial Networks (GANs) have proven to
be effective at modelling high-resolution images. Models based on Variational
Autoencoders (VAEs) have often been criticized for their feeble generative
performance, but with new advancements such as VDVAE (very deep VAE), there is
now strong evidence that deep VAEs have the potential to outperform current
state-of-the-art models for high-resolution image generation. In this paper, we
introduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE
methodologies to improve upon image super-resolution using transfer learning on
pretrained VDVAEs. Through qualitative and quantitative evaluations, we show
that the proposed model is competitive with other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vox2Cortex: Fast Explicit Reconstruction of Cortical Surfaces from 3D MRI Scans with Geometric Deep Neural Networks. (arXiv:2203.09446v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09446">
<div class="article-summary-box-inner">
<span><p>The reconstruction of cortical surfaces from brain magnetic resonance imaging
(MRI) scans is essential for quantitative analyses of cortical thickness and
sulcal morphology. Although traditional and deep learning-based algorithmic
pipelines exist for this purpose, they have two major drawbacks: lengthy
runtimes of multiple hours (traditional) or intricate post-processing, such as
mesh extraction and topology correction (deep learning-based). In this work, we
address both of these issues and propose Vox2Cortex, a deep learning-based
algorithm that directly yields topologically correct, three-dimensional meshes
of the boundaries of the cortex. Vox2Cortex leverages convolutional and graph
convolutional neural networks to deform an initial template to the densely
folded geometry of the cortex represented by an input MRI scan. We show in
extensive experiments on three brain MRI datasets that our meshes are as
accurate as the ones reconstructed by state-of-the-art methods in the field,
without the need for time- and resource-intensive post-processing. To
accurately reconstruct the tightly folded cortex, we work with meshes
containing about 168,000 vertices at test time, scaling deep explicit
reconstruction methods to a new level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning Based on OOD Detection and Task Masking. (arXiv:2203.09450v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09450">
<div class="article-summary-box-inner">
<span><p>Existing continual learning techniques focus on either task incremental
learning (TIL) or class incremental learning (CIL) problem, but not both. CIL
and TIL differ mainly in that the task-id is provided for each test sample
during testing for TIL, but not provided for CIL. Continual learning methods
intended for one problem have limitations on the other problem. This paper
proposes a novel unified approach based on out-of-distribution (OOD) detection
and task masking, called CLOM, to solve both problems. The key novelty is that
each task is trained as an OOD detection model rather than a traditional
supervised learning model, and a task mask is trained to protect each task to
prevent forgetting. Our evaluation shows that CLOM outperforms existing
state-of-the-art baselines by large margins. The average TIL/CIL accuracy of
CLOM over six experiments is 87.6/67.9% while that of the best baselines is
only 82.4/55.0%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic-to-Real Domain Adaptation using Contrastive Unpaired Translation. (arXiv:2203.09454v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09454">
<div class="article-summary-box-inner">
<span><p>The usefulness of deep learning models in robotics is largely dependent on
the availability of training data. Manual annotation of training data is often
infeasible. Synthetic data is a viable alternative, but suffers from domain
gap. We propose a multi-step method to obtain training data without manual
annotation effort: From 3D object meshes, we generate images using a modern
synthesis pipeline. We utilize a state-of-the-art image-to-image translation
method to adapt the synthetic images to the real domain, minimizing the domain
gap in a learned manner. The translation network is trained from unpaired
images, i.e. just requires an un-annotated collection of real images. The
generated and refined images can then be used to train deep learning models for
a particular task. We also propose and evaluate extensions to the translation
method that further increase performance, such as patch-based training, which
shortens training time and increases global consistency. We evaluate our method
and demonstrate its effectiveness on two robotic datasets. We finally give
insight into the learned refinement operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image. (arXiv:2203.09457v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09457">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis from a single image has recently attracted a lot of
attention, and it has been primarily advanced by 3D deep learning and rendering
techniques. However, most work is still limited by synthesizing new views
within relatively small camera motions. In this paper, we propose a novel
approach to synthesize a consistent long-term video given a single scene image
and a trajectory of large camera motions. Our approach utilizes an
autoregressive Transformer to perform sequential modeling of multiple frames,
which reasons the relations between multiple frames and the corresponding
cameras to predict the next frame. To facilitate learning and ensure
consistency among generated frames, we introduce a locality constraint based on
the input cameras to guide self-attention among a large number of patches
across space and time. Our method outperforms state-of-the-art view synthesis
approaches by a large margin, especially when synthesizing long-term future in
indoor 3D scenes. Project page at https://xrenaa.github.io/look-outside-room/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos. (arXiv:2203.09463v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09463">
<div class="article-summary-box-inner">
<span><p>Current benchmarks for facial expression recognition (FER) mainly focus on
static images, while there are limited datasets for FER in videos. It is still
ambiguous to evaluate whether performances of existing methods remain
satisfactory in real-world application-oriented scenes. For example, the
"Happy" expression with high intensity in Talk-Show is more discriminating than
the same expression with low intensity in Official-Event. To fill this gap, we
build a large-scale multi-scene dataset, coined as FERV39k. We analyze the
important ingredients of constructing such a novel dataset in three aspects:
(1) multi-scene hierarchy and expression class, (2) generation of candidate
video clips, (3) trusted manual labelling process. Based on these guidelines,
we select 4 scenarios subdivided into 22 scenes, annotate 86k samples
automatically obtained from 4k videos based on the well-designed workflow, and
finally build 38,935 video clips labeled with 7 classic expressions. Experiment
benchmarks on four kinds of baseline frameworks were also provided and further
analysis on their performance across different scenes and some challenges for
future research were given. Besides, we systematically investigate key
components of DFER by ablation studies. The baseline framework and our project
are available on url.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Normalized Density Map (SNDM) for Counting Microbiological Objects. (arXiv:2203.09474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09474">
<div class="article-summary-box-inner">
<span><p>The statistical properties of the density map (DM) approach to counting
microbiological objects on images are studied in detail. The DM is given by
U$^2$-Net. Two statistical methods for deep neural networks are utilized: the
bootstrap and the Monte Carlo (MC) dropout. The detailed analysis of the
uncertainties for the DM predictions leads to a deeper understanding of the DM
model's deficiencies. Based on our investigation, we propose a
self-normalization module in the network. The improved network model, called
Self-Normalized Density Map (SNDM), can correct its output density map by
itself to accurately predict the total number of objects in the image. The SNDM
architecture outperforms the original model. Moreover, both statistical
frameworks -- bootstrap and MC dropout -- have consistent statistical results
for SNDM, which were not observed in the original model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data. (arXiv:2203.09475v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09475">
<div class="article-summary-box-inner">
<span><p>Vision-based segmentation of the robotic tool during robot-assisted surgery
enables downstream applications, such as augmented reality feedback, while
allowing for inaccuracies in robot kinematics. With the introduction of deep
learning, many methods were presented to solve instrument segmentation directly
and solely from images. While these approaches made remarkable progress on
benchmark datasets, fundamental challenges pertaining to their robustness
remain. We present CaRTS, a causality-driven robot tool segmentation algorithm,
that is designed based on a complementary causal model of the robot tool
segmentation task. Rather than directly inferring segmentation masks from
observed images, CaRTS iteratively aligns tool models with image observations
by updating the initially incorrect robot kinematic parameters through forward
kinematics and differentiable rendering to optimize image feature similarity
end-to-end. We benchmark CaRTS with competing techniques on both synthetic as
well as real data from the dVRK, generated in precisely controlled scenarios to
allow for counterfactual synthesis. On training-domain test data, CaRTS
achieves a Dice score of 93.4 that is preserved well (Dice score of 91.8) when
tested on counterfactual altered test data, exhibiting low brightness, smoke,
blood, and altered background patterns. This compares favorably to Dice scores
of 95.0 and 62.8, respectively, of a purely image-based method trained and
tested on the same data. Future work will involve accelerating CaRTS to achieve
video framerate and estimating the impact occlusion has in practice. Despite
these limitations, our results are promising: In addition to achieving high
segmentation accuracy, CaRTS provides estimates of the true robot kinematics,
which may benefit applications such as force estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision Algorithm for Predicting the Welding Efficiency of Friction Stir Welded Copper Joints from its Microstructures. (arXiv:2203.09479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09479">
<div class="article-summary-box-inner">
<span><p>Friction Stir Welding is a robust joining process, and numerous AI-based
algorithms are being developed in this field to enhance mechanical and
microstructure properties. Convolutional Neural Networks (CNNs) are Artificial
Neural Networks that use image data as input. Identical to Artificial Neural
Networks, they are composed of weights that are determined throughout learning,
neurons (activated functions), and a goal (loss function). CNN is utilized in a
variety of applications, including image recognition, semantic segmentation,
image recognition, and localization. Utilizing training on 3000 microstructure
pictures and new tests on 300 microstructure photographs, the current work
investigates the predictions of Friction Stir Welded joint effectiveness using
microstructure images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09481">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models are a promising new class of
generative models that are competitive with GANs on perceptual metrics. In this
paper, we explore their potential for sequentially generating video. Inspired
by recent advances in neural video compression, we use denoising diffusion
models to stochastically generate a residual to a deterministic next-frame
prediction. We compare this approach to two sequential VAE and two GAN
baselines on four datasets, where we test the generated frames for perceptual
quality and forecasting accuracy against ground truth frames. We find
significant improvements in terms of perceptual quality on all data and
improvements in terms of frame forecasting for complex high-resolution videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transframer: Arbitrary Frame Prediction with Generative Models. (arXiv:2203.09494v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09494">
<div class="article-summary-box-inner">
<span><p>We present a general-purpose framework for image modelling and vision tasks
based on probabilistic frame prediction. Our approach unifies a broad range of
tasks, from image segmentation, to novel view synthesis and video
interpolation. We pair this framework with an architecture we term Transframer,
which uses U-Net and Transformer components to condition on annotated context
frames, and outputs sequences of sparse, compressed image features. Transframer
is the state-of-the-art on a variety of video generation benchmarks, is
competitive with the strongest models on few-shot view synthesis, and can
generate coherent 30 second videos from a single image without any explicit
geometric information. A single generalist Transframer simultaneously produces
promising results on 8 tasks, including semantic segmentation, image
classification and optical flow prediction with no task-specific architectural
components, demonstrating that multi-task computer vision can be tackled using
probabilistic image models. Our approach can in principle be applied to a wide
range of applications that require learning the conditional structure of
annotated image-formatted data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizing Global Explanations of Point Cloud DNNs. (arXiv:2203.09505v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09505">
<div class="article-summary-box-inner">
<span><p>In the field of autonomous driving and robotics, point clouds are showing
their excellent real-time performance as raw data from most of the mainstream
3D sensors. Therefore, point cloud neural networks have become a popular
research direction in recent years. So far, however, there has been little
discussion about the explainability of deep neural networks for point clouds.
In this paper, we propose a point cloud-applicable explainability approach
based on a local surrogate model-based method to show which components
contribute to the classification. Moreover, we propose quantitative fidelity
validations for generated explanations that enhance the persuasive power of
explainability and compare the plausibility of different existing point
cloud-applicable explainability methods. Our new explainability approach
provides a fairly accurate, more semantically coherent and widely applicable
explanation for point cloud classification tasks. Our code is available at
https://github.com/Explain3D/LIME-3D
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Data-Efficient Detection Transformers. (arXiv:2203.09507v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09507">
<div class="article-summary-box-inner">
<span><p>Detection Transformers have achieved competitive performance on the
sample-rich COCO dataset. However, we show most of them suffer from significant
performance drops on small-size datasets, like Cityscapes. In other words, the
detection transformers are generally data-hungry. To tackle this problem, we
empirically analyze the factors that affect data efficiency, through a
step-by-step transition from a data-efficient RCNN variant to the
representative DETR. The empirical results suggest that sparse feature sampling
from local image areas holds the key. Based on this observation, we alleviate
the data-hungry issue of existing detection transformers by simply alternating
how key and value sequences are constructed in the cross-attention layer, with
minimum modifications to the original models. Besides, we introduce a simple
yet effective label augmentation method to provide richer supervision and
improve data efficiency. Experiments show that our method can be readily
applied to different detection transformers and improve their performance on
both small-size and sample-rich datasets. Code will be made publicly available
at \url{https://github.com/encounter1997/DE-DETRs}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DetMatch: Two Teachers are Better Than One for Joint 2D and 3D Semi-Supervised Object Detection. (arXiv:2203.09510v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09510">
<div class="article-summary-box-inner">
<span><p>While numerous 3D detection works leverage the complementary relationship
between RGB images and point clouds, developments in the broader framework of
semi-supervised object recognition remain uninfluenced by multi-modal fusion.
Current methods develop independent pipelines for 2D and 3D semi-supervised
learning despite the availability of paired image and point cloud frames.
Observing that the distinct characteristics of each sensor cause them to be
biased towards detecting different objects, we propose DetMatch, a flexible
framework for joint semi-supervised learning on 2D and 3D modalities. By
identifying objects detected in both sensors, our pipeline generates a cleaner,
more robust set of pseudo-labels that both demonstrates stronger performance
and stymies single-modality error propagation. Further, we leverage the richer
semantics of RGB images to rectify incorrect 3D class predictions and improve
localization of 3D boxes. Evaluating on the challenging KITTI and Waymo
datasets, we improve upon strong semi-supervised learning methods and observe
higher quality pseudo-labels. Code will be released at
https://github.com/Divadi/DetMatch
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Multi-Domain Long-Tailed Recognition, Generalization and Beyond. (arXiv:2203.09513v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09513">
<div class="article-summary-box-inner">
<span><p>Real-world data often exhibit imbalanced label distributions. Existing
studies on data imbalance focus on single-domain settings, i.e., samples are
from the same data distribution. However, natural data can originate from
distinct domains, where a minority class in one domain could have abundant
instances from other domains. We formalize the task of Multi-Domain Long-Tailed
Recognition (MDLT), which learns from multi-domain imbalanced data, addresses
label imbalance, domain shift, and divergent label distributions across
domains, and generalizes to all domain-class pairs. We first develop the
domain-class transferability graph, and show that such transferability governs
the success of learning in MDLT. We then propose BoDA, a theoretically grounded
learning strategy that tracks the upper bound of transferability statistics,
and ensures balanced alignment and calibration across imbalanced domain-class
distributions. We curate five MDLT benchmarks based on widely-used multi-domain
datasets, and compare BoDA to twenty algorithms that span different learning
strategies. Extensive and rigorous experiments verify the superior performance
of BoDA. Further, as a byproduct, BoDA establishes new state-of-the-art on
Domain Generalization benchmarks, improving generalization to unseen domains.
Code and data are available at
https://github.com/YyzHarry/multi-domain-imbalance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation. (arXiv:2203.09516v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09516">
<div class="article-summary-box-inner">
<span><p>Powerful priors allow us to perform inference with insufficient information.
In this paper, we propose an autoregressive prior for 3D shapes to solve
multimodal 3D tasks such as shape completion, reconstruction, and generation.
We model the distribution over 3D shapes as a non-sequential autoregressive
distribution over a discretized, low-dimensional, symbolic grid-like latent
representation of 3D shapes. This enables us to represent distributions over 3D
shapes conditioned on information from an arbitrary set of spatially anchored
query locations and thus perform shape completion in such arbitrary settings
(e.g., generating a complete chair given only a view of the back leg). We also
show that the learned autoregressive prior can be leveraged for conditional
tasks such as single-view reconstruction and language-based generation. This is
achieved by learning task-specific naive conditionals which can be approximated
by light-weight models trained on minimal paired data. We validate the
effectiveness of the proposed method using both quantitative and qualitative
evaluation and show that the proposed method outperforms the specialized
state-of-the-art methods trained for individual tasks. The project page with
code and video visualizations can be found at
https://yccyenchicheng.github.io/AutoSDF/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TensoRF: Tensorial Radiance Fields. (arXiv:2203.09517v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09517">
<div class="article-summary-box-inner">
<span><p>We present TensoRF, a novel approach to model and reconstruct radiance
fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a
scene as a 4D tensor, which represents a 3D voxel grid with per-voxel
multi-channel features. Our central idea is to factorize the 4D scene tensor
into multiple compact low-rank tensor components. We demonstrate that applying
traditional CP decomposition -- that factorizes tensors into rank-one
components with compact vectors -- in our framework leads to improvements over
vanilla NeRF. To further boost performance, we introduce a novel vector-matrix
(VM) decomposition that relaxes the low-rank constraints for two modes of a
tensor and factorizes tensors into compact vector and matrix factors. Beyond
superior rendering quality, our models with CP and VM decompositions lead to a
significantly lower memory footprint in comparison to previous and concurrent
works that directly optimize per-voxel features. Experimentally, we demonstrate
that TensoRF with CP decomposition achieves fast reconstruction (&lt;30 min) with
better rendering quality and even a smaller model size (&lt;4 MB) compared to
NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality
and outperforms previous state-of-the-art methods, while reducing the
reconstruction time (&lt;10 min) and retaining a compact model size (&lt;75 MB).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth Descent Synchronization in $\mathrm{SO}(D)$. (arXiv:2002.05299v3 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.05299">
<div class="article-summary-box-inner">
<span><p>We give robust recovery results for synchronization on the rotation group,
$\mathrm{SO}(D)$. In particular, we consider an adversarial corruption setting,
where a limited percentage of the observations are arbitrarily corrupted. We
give a novel algorithm that exploits Tukey depth in the tangent space, which
exactly recovers the underlying rotations up to an outlier percentage of
$1/(D(D-1)+2)$. This corresponds to an outlier fraction of $1/4$ for
$\mathrm{SO}(2)$ and $1/8$ for $\mathrm{SO}(3)$. In the case of $D=2$, we
demonstrate that a variant of this algorithm converges linearly to the ground
truth rotations. We finish by discussing this result in relation to a simpler
nonconvex energy minimization framework based on least absolute deviations,
which exhibits spurious fixed points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intracranial Hemorrhage Detection Using Neural Network Based Methods With Federated Learning. (arXiv:2005.08644v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.08644">
<div class="article-summary-box-inner">
<span><p>Intracranial hemorrhage, bleeding that occurs inside the cranium, is a
serious health problem requiring rapid and often intensive medical treatment.
Such a condition is traditionally diagnosed by highly-trained specialists
analyzing computed tomography (CT) scan of the patient and identifying the
location and type of hemorrhage if one exists. We propose a neural network
approach to find and classify the condition based upon the CT scan. The model
architecture implements a time distributed convolutional network. We observed
accuracy above 92% from such an architecture, provided enough data. We propose
further extensions to our approach involving the deployment of federated
learning. This would be helpful in pooling learned parameters without violating
the inherent privacy of the data involved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers. (arXiv:2012.05858v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05858">
<div class="article-summary-box-inner">
<span><p>Light-based adversarial attacks use spatial augmented reality (SAR)
techniques to fool image classifiers by altering the physical light condition
with a controllable light source, e.g., a projector. Compared with physical
attacks that place hand-crafted adversarial objects, projector-based ones
obviate modifying the physical entities, and can be performed transiently and
dynamically by altering the projection pattern. However, subtle light
perturbations are insufficient to fool image classifiers, due to the complex
environment and project-and-capture process. Thus, existing approaches focus on
projecting clearly perceptible adversarial patterns, while the more interesting
yet challenging goal, stealthy projector-based attack, remains open. In this
paper, for the first time, we formulate this problem as an end-to-end
differentiable process and propose a Stealthy Projector-based Adversarial
Attack (SPAA) solution. In SPAA, we approximate the real Project-and-Capture
process using a deep neural network named PCNet, then we include PCNet in the
optimization of projector-based attacks such that the generated adversarial
projection is physically plausible. Finally, to generate both robust and
stealthy adversarial projections, we propose an algorithm that uses minimum
perturbation and adversarial confidence thresholds to alternate between the
adversarial loss and stealthiness loss optimization. Our experimental
evaluations show that SPAA clearly outperforms other methods by achieving
higher attack success rates and meanwhile being stealthier, for both targeted
and untargeted attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. (arXiv:2103.08573v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08573">
<div class="article-summary-box-inner">
<span><p>The use of local detectors and descriptors in typical computer vision
pipelines work well until variations in viewpoint and appearance change become
extreme. Past research in this area has typically focused on one of two
approaches to this challenge: the use of projections into spaces more suitable
for feature matching under extreme viewpoint changes, and attempting to learn
features that are inherently more robust to viewpoint change. In this paper, we
present a novel framework that combines learning of invariant descriptors
through data augmentation and orthographic viewpoint projection. We propose
rotation-robust local descriptors, learnt through training data augmentation
based on rotation homographies, and a correspondence ensemble technique that
combines vanilla feature correspondences with those obtained through
rotation-robust features. Using a range of benchmark datasets as well as
contributing a new bespoke dataset for this research domain, we evaluate the
effectiveness of the proposed approach on key tasks including pose estimation
and visual place recognition. Our system outperforms a range of baseline and
state-of-the-art techniques, including enabling higher levels of place
recognition precision across opposing place viewpoints and achieves
practically-useful performance levels even under extreme viewpoint changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Loss Weight Adjustment in Object Detection. (arXiv:2103.09488v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09488">
<div class="article-summary-box-inner">
<span><p>Object detection is a typical multi-task learning application, which
optimizes classification and regression simultaneously. However, classification
loss always dominates the multi-task loss in anchor-based methods, hampering
the consistent and balanced optimization of the tasks. In this paper, we find
that shifting the bounding boxes can change the division of positive and
negative samples in classification, meaning classification depends on
regression. Moreover, we summarize three important conclusions about
fine-tuning loss weights, considering different datasets, optimizers and
regression loss functions. Based on the above conclusions, we propose Adaptive
Loss Weight Adjustment(ALWA) to solve the imbalance in optimizing anchor-based
methods according to statistical characteristics of losses. By incorporating
ALWA into previous state-of-the-art detectors, we achieve a significant
performance gain on PASCAL VOC and MS COCO, even with L1, SmoothL1 and CIoU
loss. The code is available at https://github.com/ywx-hub/ALWA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation. (arXiv:2103.09716v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09716">
<div class="article-summary-box-inner">
<span><p>Identifying the status of individual network units is critical for
understanding the mechanism of convolutional neural networks (CNNs). However,
it is still challenging to reliably give a general indication of unit status,
especially for units in different network models. To this end, we propose a
novel method for quantitatively clarifying the status of single unit in CNN
using algebraic topological tools. Unit status is indicated via the calculation
of a defined topological-based entropy, called feature entropy, which measures
the degree of chaos of the global spatial pattern hidden in the unit for a
category. In this way, feature entropy could provide an accurate indication of
status for units in different networks with diverse situations like
weight-rescaling operation. Further, we show that feature entropy decreases as
the layer goes deeper and shares almost simultaneous trend with loss during
training. We show that by investigating the feature entropy of units on only
training data, it could give discrimination between networks with different
generalization ability from the view of the effectiveness of feature
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13389">
<div class="article-summary-box-inner">
<span><p>Given a large dataset for training, GANs can achieve remarkable performance
for the image synthesis task. However, training GANs in extremely low data
regimes remains a challenge, as overfitting often occurs, leading to
memorization or training divergence. In this work, we introduce SIV-GAN, an
unconditional generative model that can generate new scene compositions from a
single training image or a single video clip. We propose a two-branch
discriminator architecture, with content and layout branches designed to judge
internal content and scene layout realism separately from each other. This
discriminator design enables synthesis of visually plausible, novel
compositions of a scene, with varying content and layout, while preserving the
context of the original sample. Compared to previous single-image GANs, our
model generates more diverse, higher quality images, while not being restricted
to a single image setting. We show that SIV-GAN successfully deals with a new
challenging task of learning from a single video, for which prior GAN models
fail to achieve synthesis of both high quality and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Anomaly Detection via Multi-task Self-Supervision. (arXiv:2104.09993v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09993">
<div class="article-summary-box-inner">
<span><p>Detecting anomalies using deep learning has become a major challenge over the
last years, and is becoming increasingly promising in several fields. The
introduction of self-supervised learning has greatly helped many methods
including anomaly detection where simple geometric transformation recognition
tasks are used. However these methods do not perform well on fine-grained
problems since they lack finer features. By combining in a multi-task framework
high-scale shape features oriented task with low-scale fine features oriented
task, our method greatly improves fine-grained anomaly detection. It
outperforms state-of-the-art with up to 31% relative error reduction measured
with AUROC on various anomaly detection problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLSD: The Global Large-Scale Ship Database and Baseline Evaluations. (arXiv:2106.02773v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02773">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a challenging global large-scale ship database
(called GLSD), designed specifically for ship detection tasks. The designed
GLSD database includes a total of 212,357 annotated instances from 152,576
images. Based on the collected images, we propose 13 ship categories that
widely exist in international routes. These categories include Sailing boat,
Fishing boat, Passenger ship, Warship, General cargo ship, Container ship, Bulk
cargo carrier, Barge, Ore carrier, Speed boat, Canoe, Oil carrier, and Tug. The
motivations of developing GLSD include the following: 1) providing a refine and
extensive ship detection database that benefits the object detection community,
2) establishing a database with exhaustive labels (bounding boxes and ship
class categories) in a uniform classification scheme, and 3) providing a
large-scale ship database with geographic information (covering more than 3000
ports and 33 routes) that benefits multi-modal analysis. In addition, we
discuss the evaluation protocols corresponding to image characteristics in GLSD
and analyze the performance of selected state-of-the-art object detection
algorithms on GSLD, aiming to establish baselines for future studies. More
information regarding the designed GLSD can be found at
https://github.com/jiaming-wang/GLSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category Contrast for Unsupervised Domain Adaptation in Visual Tasks. (arXiv:2106.02885v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02885">
<div class="article-summary-box-inner">
<span><p>Instance contrast for unsupervised representation learning has achieved great
success in recent years. In this work, we explore the idea of instance
contrastive learning in unsupervised domain adaptation (UDA) and propose a
novel Category Contrast technique (CaCo) that introduces semantic priors on top
of instance discrimination for visual UDA tasks. By considering instance
contrastive learning as a dictionary look-up operation, we construct a
semantics-aware dictionary with samples from both source and target domains
where each target sample is assigned a (pseudo) category label based on the
category priors of source samples. This allows category contrastive learning
(between target queries and the category-level dictionary) for
category-discriminative yet domain-invariant feature representations: samples
of the same category (from either source or target domain) are pulled closer
while those of different categories are pushed apart simultaneously. Extensive
UDA experiments in multiple visual tasks (e.g., segmentation, classification
and detection) show that CaCo achieves superior performance as compared with
state-of-the-art methods. The experiments also demonstrate that CaCo is
complementary to existing UDA methods and generalizable to other learning
setups such as unsupervised model adaptation, open-/partial-set adaptation etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelet-Packets for Deepfake Image Analysis and Detection. (arXiv:2106.09369v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09369">
<div class="article-summary-box-inner">
<span><p>As neural networks become able to generate realistic artificial images, they
have the potential to improve movies, music, video games and make the internet
an even more creative and inspiring place. Yet, the latest technology
potentially enables new digital ways to lie. In response, the need for a
diverse and reliable method toolbox arises to identify artificial images and
other content. Previous work primarily relies on pixel-space CNNs or the
Fourier transform. To the best of our knowledge, synthesized fake image
analysis and detection methods based on a multi-scale wavelet representation,
localized in both space and frequency, have been absent thus far. The wavelet
transform conserves spatial information to a degree, which allows us to present
a new analysis. Comparing the wavelet coefficients of real and fake images
allows interpretation. Significant differences are identified. Additionally,
this paper proposes to learn a model for the detection of synthetic images
based on the wavelet-packet representation of natural and GAN-generated images.
Our lightweight forensic classifiers exhibit competitive or improved
performance at comparatively small network sizes, as we demonstrate on the
FFHQ, CelebA and LSUN source identification problems. Furthermore, we study the
binary FaceForensics++ fake-detection problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReGO: Reference-Guided Outpainting for Scenery Image. (arXiv:2106.10601v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10601">
<div class="article-summary-box-inner">
<span><p>We aim to tackle the challenging yet practical scenery image outpainting task
in this work. Recently, generative adversarial learning has significantly
advanced the image outpainting by producing semantic consistent content for the
given image. However, the existing methods always suffer from the blurry
texture and the artifacts of the generative part, making the overall
outpainting results lack authenticity. To overcome the weakness, this work
investigates a principle way to synthesize texture-rich results by borrowing
pixels from its neighbors (i.e., reference images), named
\textbf{Re}ference-\textbf{G}uided \textbf{O}utpainting (ReGO). Particularly,
the ReGO designs an Adaptive Content Selection (ACS) module to transfer the
pixel of reference images for texture compensating of the target one. To
prevent the style of the generated part from being affected by the reference
images, a style ranking loss is further proposed to augment the ReGO to
synthesize style-consistent results. Extensive experiments on two popular
benchmarks, NS6K \cite{yangzx} and NS8K \cite{wang}, well demonstrate the
effectiveness of our ReGO. Our code will be made public available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag of Instances Aggregation Boosts Self-supervised Distillation. (arXiv:2107.01691v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01691">
<div class="article-summary-box-inner">
<span><p>Recent advances in self-supervised learning have experienced remarkable
progress, especially for contrastive learning based methods, which regard each
image as well as its augmentations as an individual class and try to
distinguish them from all other images. However, due to the large quantity of
exemplars, this kind of pretext task intrinsically suffers from slow
convergence and is hard for optimization. This is especially true for
small-scale models, in which we find the performance drops dramatically
comparing with its supervised counterpart. In this paper, we propose a simple
but effective distillation strategy for unsupervised learning. The highlight is
that the relationship among similar samples counts and can be seamlessly
transferred to the student to boost the performance. Our method, termed as
BINGO, which is short for Bag of InstaNces aGgregatiOn, targets at transferring
the relationship learned by the teacher to the student. Here bag of instances
indicates a set of similar samples constructed by the teacher and are grouped
within a bag, and the goal of distillation is to aggregate compact
representations over the student with respect to instances in a bag. Notably,
BINGO achieves new state-of-the-art performance on small-scale models, i.e.,
65.5% and 68.9% top-1 accuracies with linear evaluation on ImageNet, using
ResNet-18 and ResNet-34 as the backbones respectively, surpassing baselines
(52.5% and 57.4% top-1 accuracies) by a significant margin. The code is
available at https://github.com/haohang96/bingo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration. (arXiv:2107.05446v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05446">
<div class="article-summary-box-inner">
<span><p>Source-free domain adaptation (SFDA) aims to adapt a model trained on
labelled data in a source domain to unlabelled data in a target domain without
access to the source-domain data during adaptation. Existing methods for SFDA
leverage entropy-minimization techniques which: (i) apply only to
classification; (ii) destroy model calibration; and (iii) rely on the source
model achieving a good level of feature-space class-separation in the target
domain. We address these issues for a particularly pervasive type of domain
shift called measurement shift which can be resolved by restoring the source
features rather than extracting new ones. In particular, we propose Feature
Restoration (FR) wherein we: (i) store a lightweight and flexible approximation
of the feature distribution under the source data; and (ii) adapt the
feature-extractor such that the approximate feature distribution under the
target data realigns with that saved on the source. We additionally propose a
bottom-up training scheme which boosts performance, which we call Bottom-Up
Feature Restoration (BUFR). On real and synthetic data, we demonstrate that
BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and
data efficiency, while being less reliant on the performance of the source
model in the target domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08323">
<div class="article-summary-box-inner">
<span><p>Temporal action proposal generation is an essential and challenging task that
aims at localizing temporal intervals containing human actions in untrimmed
videos. Most of existing approaches are unable to follow the human cognitive
process of understanding the video context due to lack of attention mechanism
to express the concept of an action or an agent who performs the action or the
interaction between the agent and the environment. Based on the action
definition that a human, known as an agent, interacts with the environment and
performs an action that affects the environment, we propose a contextual
Agent-Environment Network. Our proposed contextual AEN involves (i) agent
pathway, operating at a local level to tell about which humans/agents are
acting and (ii) environment pathway operating at a global level to tell about
how the agents interact with the environment. Comprehensive evaluations on
20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different
backbone networks, i.e C3D and SlowFast, show that our method robustly exhibits
outperformance against state-of-the-art methods regardless of the employed
backbone network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AS-MLP: An Axial Shifted MLP Architecture for Vision. (arXiv:2107.08391v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08391">
<div class="article-summary-box-inner">
<span><p>An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper.
Different from MLP-Mixer, where the global spatial feature is encoded for
information flow through matrix transposition and one token-mixing MLP, we pay
more attention to the local features interaction. By axially shifting channels
of the feature map, AS-MLP is able to obtain the information flow from
different axial directions, which captures the local dependencies. Such an
operation enables us to utilize a pure MLP architecture to achieve the same
local receptive field as CNN-like architecture. We can also design the
receptive field size and dilation of blocks of AS-MLP, etc, in the same spirit
of convolutional neural networks. With the proposed AS-MLP architecture, our
model obtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the
ImageNet-1K dataset. Such a simple yet effective architecture outperforms all
MLP-based architectures and achieves competitive performance compared to the
transformer-based architectures (e.g., Swin Transformer) even with slightly
lower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be
applied to the downstream tasks (e.g., object detection and semantic
segmentation). The experimental results are also impressive. Our proposed
AS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the
ADE20K dataset, which is competitive compared to the transformer-based
architectures. Our AS-MLP establishes a strong baseline of MLP-based
architecture. Code is available at https://github.com/svip-lab/AS-MLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10904">
<div class="article-summary-box-inner">
<span><p>With recent progress in joint modeling of visual and textual representations,
Vision-Language Pretraining (VLP) has achieved impressive performance on many
multimodal downstream tasks. However, the requirement for expensive annotations
including clean image captions and regional labels limits the scalability of
existing approaches, and complicates the pretraining procedure with the
introduction of multiple dataset-specific objectives. In this work, we relax
these constraints and present a minimalist pretraining framework, named Simple
Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training
complexity by exploiting large-scale weak supervision, and is trained
end-to-end with a single prefix language modeling objective. Without utilizing
extra data or task-specific customization, the resulting model significantly
outperforms previous pretraining methods and achieves new state-of-the-art
results on a wide range of discriminative and generative vision-language
benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE
(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).
Furthermore, we demonstrate that SimVLM acquires strong generalization and
transfer ability, enabling zero-shot behavior including open-ended visual
question answering and cross-modality transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Fidelity GAN Inversion for Image Attribute Editing. (arXiv:2109.06590v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06590">
<div class="article-summary-box-inner">
<span><p>We present a novel high-fidelity generative adversarial network (GAN)
inversion framework that enables attribute editing with image-specific details
well-preserved (e.g., background, appearance, and illumination). We first
analyze the challenges of high-fidelity GAN inversion from the perspective of
lossy data compression. With a low bit-rate latent code, previous works have
difficulties in preserving high-fidelity details in reconstructed and edited
images. Increasing the size of a latent code can improve the accuracy of GAN
inversion but at the cost of inferior editability. To improve image fidelity
without compromising editability, we propose a distortion consultation approach
that employs a distortion map as a reference for high-fidelity reconstruction.
In the distortion consultation inversion (DCI), the distortion map is first
projected to a high-rate latent map, which then complements the basic low-rate
latent code with more details via consultation fusion. To achieve high-fidelity
editing, we propose an adaptive distortion alignment (ADA) module with a
self-supervised training scheme, which bridges the gap between the edited and
inversion images. Extensive experiments in the face and car domains show a
clear improvement in both inversion and editing quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAda! Temporally-Adaptive Convolutions for Video Understanding. (arXiv:2110.06178v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06178">
<div class="article-summary-box-inner">
<span><p>Spatial convolutions are widely used in numerous deep video models. It
fundamentally assumes spatio-temporal invariance, i.e., using shared weights
for every location in different frames. This work presents Temporally-Adaptive
Convolutions (TAdaConv) for video understanding, which shows that adaptive
weight calibration along the temporal dimension is an efficient way to
facilitate modelling complex temporal dynamics in videos. Specifically,
TAdaConv empowers the spatial convolutions with temporal modelling abilities by
calibrating the convolution weights for each frame according to its local and
global temporal context. Compared to previous temporal modelling operations,
TAdaConv is more efficient as it operates over the convolution kernels instead
of the features, whose dimension is an order of magnitude smaller than the
spatial resolutions. Further, the kernel calibration brings an increased model
capacity. We construct TAda2D and TAdaConvNeXt networks by replacing the 2D
convolutions in ResNet and ConvNeXt with TAdaConv, which leads to at least on
par or better performance compared to state-of-the-art approaches on multiple
video action recognition and localization benchmarks. We also demonstrate that
as a readily plug-in operation with negligible computation overhead, TAdaConv
can effectively improve many existing video models with a convincing margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes. (arXiv:2110.08059v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08059">
<div class="article-summary-box-inner">
<span><p>When designing Convolutional Neural Networks (CNNs), one must select the
size\break of the convolutional kernels before training. Recent works show CNNs
benefit from different kernel sizes at different layers, but exploring all
possible combinations is unfeasible in practice. A more efficient approach is
to learn the kernel size during training. However, existing works that learn
the kernel size have a limited bandwidth. These approaches scale kernels by
dilation, and thus the detail they can describe is limited. In this work, we
propose FlexConv, a novel convolutional operation with which high bandwidth
convolutional kernels of learnable kernel size can be learned at a fixed
parameter cost. FlexNets model long-term dependencies without the use of
pooling, achieve state-of-the-art performance on several sequential datasets,
outperform recent works with learned kernel sizes, and are competitive with
much deeper ResNets on image benchmark datasets. Additionally, FlexNets can be
deployed at higher resolutions than those seen during training. To avoid
aliasing, we propose a novel kernel parameterization with which the frequency
of the kernels can be analytically controlled. Our novel kernel
parameterization shows higher descriptive power and faster convergence speed
than existing parameterizations. This leads to important improvements in
classification accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08486">
<div class="article-summary-box-inner">
<span><p>The ability to sequence unordered events is an essential skill to comprehend
and reason about real world task procedures, which often requires thorough
understanding of temporal common sense and multimodal information, as these
procedures are often communicated through a combination of texts and images.
Such capability is essential for applications such as sequential task planning
and multi-source instruction summarization. While humans are capable of
reasoning about and sequencing unordered multimodal procedural instructions,
whether current machine learning models have such essential capability is still
an open question. In this work, we benchmark models' capability of reasoning
over and sequencing unordered multimodal instructions by curating datasets from
popular online instructional manuals and collecting comprehensive human
annotations. We find models not only perform significantly worse than humans
but also seem incapable of efficiently utilizing the multimodal information. To
improve machines' performance on multimodal event sequencing, we propose
sequentiality-aware pretraining techniques that exploit the sequential
alignment properties of both texts and images, resulting in &gt; 5% significant
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporally stable video segmentation without video annotations. (arXiv:2110.08893v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08893">
<div class="article-summary-box-inner">
<span><p>Temporally consistent dense video annotations are scarce and hard to collect.
In contrast, image segmentation datasets (and pre-trained models) are
ubiquitous, and easier to label for any novel task. In this paper, we introduce
a method to adapt still image segmentation models to video in an unsupervised
manner, by using an optical flow-based consistency measure. To ensure that the
inferred segmented videos appear more stable in practice, we verify that the
consistency measure is well correlated with human judgement via a user study.
Training a new multi-input multi-output decoder using this measure as a loss,
together with a technique for refining current image segmentation datasets and
a temporal weighted-guided filter, we observe stability improvements in the
generated segmented videos with minimal loss of accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConAM: Confidence Attention Module for Convolutional Neural Networks. (arXiv:2110.14369v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14369">
<div class="article-summary-box-inner">
<span><p>The so-called "attention" is an efficient mechanism to improve the
performance of convolutional neural networks. It uses contextual information to
recalibrate the input to strengthen the propagation of informative features.
However, the majority of the attention mechanisms only consider either local or
global contextual information, which is singular to extract features. Moreover,
many existing mechanisms directly use the contextual information to recalibrate
the input, which unilaterally enhances the propagation of the informative
features, but does not suppress the useless ones. This paper proposes a new
attention mechanism module based on the correlation between local and global
contextual information and we name this correlation as confidence. The novel
attention mechanism extracts the local and global contextual information
simultaneously, and calculates the confidence between them, then uses this
confidence to recalibrate the input pixels. The extraction of local and global
contextual information increases the diversity of features. The recalibration
with confidence suppresses useless information while enhancing the informative
one with fewer parameters. We use CIFAR-10 and CIFAR-100 in our experiments and
explore the performance of our method's components by sufficient ablation
studies. Finally, we compare our method with a various state-of-the-art
convolutional neural networks and the results show that our method completely
surpasses these models. We implement ConAM with the Python library, Pytorch,
and the code and models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer for Classification of Breast Ultrasound Images. (arXiv:2110.14731v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14731">
<div class="article-summary-box-inner">
<span><p>Medical ultrasound (US) imaging has become a prominent modality for breast
cancer imaging due to its ease-of-use, low-cost and safety. In the past decade,
convolutional neural networks (CNNs) have emerged as the method of choice in
vision applications and have shown excellent potential in automatic
classification of US images. Despite their success, their restricted local
receptive field limits their ability to learn global context information.
Recently, Vision Transformer (ViT) designs that are based on self-attention
between image patches have shown great potential to be an alternative to CNNs.
In this study, for the first time, we utilize ViT to classify breast US images
using different augmentation strategies. The results are provided as
classification accuracy and Area Under the Curve (AUC) metrics, and the
performance is compared with the state-of-the-art CNNs. The results indicate
that the ViT models have comparable efficiency with or even better than the
CNNs in classification of US breast images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDGNet: Class Distribution Guided Network for Human Parsing. (arXiv:2111.14173v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14173">
<div class="article-summary-box-inner">
<span><p>The objective of human parsing is to partition a human in an image into
constituent parts. This task involves labeling each pixel of the human image
according to the classes. Since the human body comprises hierarchically
structured parts, each body part of an image can have its sole position
distribution characteristic. Probably, a human head is less likely to be under
the feet, and arms are more likely to be near the torso. Inspired by this
observation, we make instance class distributions by accumulating the original
human parsing label in the horizontal and vertical directions, which can be
utilized as supervision signals. Using these horizontal and vertical class
distribution labels, the network is guided to exploit the intrinsic position
distribution of each class. We combine two guided features to form a spatial
guidance map, which is then superimposed onto the baseline network by
multiplication and concatenation to distinguish the human parts precisely. We
conducted extensive experiments to demonstrate the effectiveness and
superiority of our method on three well-known benchmarks: LIP, ATR, and CIHP
databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleMesh: Style Transfer for Indoor 3D Scene Reconstructions. (arXiv:2112.01530v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01530">
<div class="article-summary-box-inner">
<span><p>We apply style transfer on mesh reconstructions of indoor scenes. This
enables VR applications like experiencing 3D environments painted in the style
of a favorite artist. Style transfer typically operates on 2D images, making
stylization of a mesh challenging. When optimized over a variety of poses,
stylization patterns become stretched out and inconsistent in size. On the
other hand, model-based 3D style transfer methods exist that allow stylization
from a sparse set of images, but they require a network at inference time. To
this end, we optimize an explicit texture for the reconstructed mesh of a scene
and stylize it jointly from all available input images. Our depth- and
angle-aware optimization leverages surface normal and depth data of the
underlying mesh to create a uniform and consistent stylization for the whole
scene. Our experiments show that our method creates sharp and detailed results
for the complete scene without view-dependent artifacts. Through extensive
ablation studies, we show that the proposed 3D awareness enables style transfer
to be applied to the 3D domain of a mesh. Our method can be used to render a
stylized mesh in real-time with traditional rendering pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceFormer: Speech-Driven 3D Facial Animation with Transformers. (arXiv:2112.05329v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05329">
<div class="article-summary-box-inner">
<span><p>Speech-driven 3D facial animation is challenging due to the complex geometry
of human faces and the limited availability of 3D audio-visual data. Prior
works typically focus on learning phoneme-level features of short audio windows
with limited context, occasionally resulting in inaccurate lip movements. To
tackle this limitation, we propose a Transformer-based autoregressive model,
FaceFormer, which encodes the long-term audio context and autoregressively
predicts a sequence of animated 3D face meshes. To cope with the data scarcity
issue, we integrate the self-supervised pre-trained speech representations.
Also, we devise two biased attention mechanisms well suited to this specific
task, including the biased cross-modal multi-head (MH) attention and the biased
causal MH self-attention with a periodic positional encoding strategy. The
former effectively aligns the audio-motion modalities, whereas the latter
offers abilities to generalize to longer audio sequences. Extensive experiments
and a perceptual user study show that our approach outperforms the existing
state-of-the-arts. The code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09081">
<div class="article-summary-box-inner">
<span><p>We present a visual localization system that learns to estimate camera poses
in the real world with the help of synthetic data. Despite significant progress
in recent years, most learning-based approaches to visual localization target
at a single domain and require a dense database of geo-tagged images to
function well. To mitigate the data scarcity issue and improve the scalability
of the neural localization models, we introduce TOPO-DataGen, a versatile
synthetic data generation tool that traverses smoothly between the real and
virtual world, hinged on the geographic camera viewpoint. New large-scale
sim-to-real benchmark datasets are proposed to showcase and evaluate the
utility of the said synthetic data. Our experiments reveal that synthetic data
generically enhances the neural network performance on real data. Furthermore,
we introduce CrossLoc, a cross-modal visual representation learning approach to
pose estimation that makes full use of the scene coordinate ground truth via
self-supervision. Without any extra data, CrossLoc significantly outperforms
the state-of-the-art methods and achieves substantially higher real-data sample
efficiency. Our code and datasets are all available at
https://github.com/TOPO-EPFL/CrossLoc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Visual Tracking with Exemplar Transformers. (arXiv:2112.09686v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09686">
<div class="article-summary-box-inner">
<span><p>The design of more complex and powerful neural network models has
significantly advanced the state-of-the-art in visual object tracking. These
advances can be attributed to deeper networks, or to the introduction of new
building blocks, such as transformers. However, in the pursuit of increased
tracking performance, efficient tracking architectures have received
surprisingly little attention. In this paper, we introduce the Exemplar
Transformer, an efficient transformer for real-time visual object tracking.
E.T.Track, our visual tracker that incorporates Exemplar Transformer layers,
runs at 47 fps on a CPU. This is up to 8 times faster than other
transformer-based models, making it the only real-time transformer-based
tracker. When compared to lightweight trackers that can operate in real-time on
standard CPUs, E.T.Track consistently outperforms all other methods on the
LaSOT, OTB-100, NFS, TrackingNet and VOT-ST2020 datasets. The code will soon be
released on https://github.com/visionml/pytracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization. (arXiv:2112.11177v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11177">
<div class="article-summary-box-inner">
<span><p>For medical image segmentation, imagine if a model was only trained using MR
images in source domain, how about its performance to directly segment CT
images in target domain? This setting, namely generalizable cross-modality
segmentation, owning its clinical potential, is much more challenging than
other related settings, e.g., domain adaptation. To achieve this goal, we in
this paper propose a novel dual-normalization model by leveraging the augmented
source-similar and source-dissimilar images during our generalizable
segmentation. To be specific, given a single source domain, aiming to simulate
the possible appearance change in unseen target domains, we first utilize a
nonlinear transformation to augment source-similar and source-dissimilar
images. Then, to sufficiently exploit these two types of augmentations, our
proposed dual-normalization based model employs a shared backbone yet
independent batch normalization layer for separate normalization. Afterward, we
put forward a style-based selection scheme to automatically choose the
appropriate path in the test stage. Extensive experiments on three publicly
available datasets, i.e., BraTS, Cross-Modality Cardiac, and Abdominal
Multi-Organ datasets, have demonstrated that our method outperforms other
state-of-the-art domain generalization methods. Code is available at
https://github.com/zzzqzhou/Dual-Normalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FourierMask: Instance Segmentation using Fourier Mapping in Implicit Neural Networks. (arXiv:2112.12535v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12535">
<div class="article-summary-box-inner">
<span><p>We present FourierMask, which employs Fourier series combined with implicit
neural representations to generate instance segmentation masks. We apply a
Fourier mapping (FM) to the coordinate locations and utilize the mapped
features as inputs to an implicit representation (coordinate-based multi-layer
perceptron (MLP)). FourierMask learns to predict the coefficients of the FM for
a particular instance, and therefore adapts the FM to a specific object. This
allows FourierMask to be generalized to predict instance segmentation masks
from natural images. Since implicit functions are continuous in the domain of
input coordinates, we illustrate that by sub-sampling the input pixel
coordinates, we can generate higher resolution masks during inference.
Furthermore, we train a renderer MLP (FourierRend) on the uncertain predictions
of FourierMask and illustrate that it significantly improves the quality of the
masks. FourierMask shows competitive results on the MS COCO dataset compared to
the baseline Mask R-CNN at the same output resolution and surpasses it on
higher resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeMask: Semantically Masked Transformers for Semantic Segmentation. (arXiv:2112.12782v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12782">
<div class="article-summary-box-inner">
<span><p>Finetuning a pretrained backbone in the encoder part of an image transformer
network has been the traditional approach for the semantic segmentation task.
However, such an approach leaves out the semantic context that an image
provides during the encoding stage. This paper argues that incorporating
semantic information of the image into pretrained hierarchical
transformer-based backbones while finetuning improves the performance
considerably. To achieve this, we propose SeMask, a simple and effective
framework that incorporates semantic information into the encoder with the help
of a semantic attention operation. In addition, we use a lightweight semantic
decoder during training to provide supervision to the intermediate semantic
prior maps at every stage. Our experiments demonstrate that incorporating
semantic priors enhances the performance of the established hierarchical
encoders with a slight increase in the number of FLOPs. We provide empirical
proof by integrating SeMask into Swin Transformer and Mix Transformer backbones
as our encoder paired with different decoders. Our framework achieves a new
state-of-the-art of 58.22% mIoU on the ADE20K dataset and improvements of over
3% in the mIoU metric on the Cityscapes dataset. The code and checkpoints are
publicly available at
https://github.com/Picsart-AI-Research/SeMask-Segmentation .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04584">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of lung lesions associated with COVID-19 in CT images
requires large amount of annotated volumes. Annotations mandate expert
knowledge and are time-intensive to obtain through fully manual segmentation
methods. Additionally, lung lesions have large inter-patient variations, with
some pathologies having similar visual appearance as healthy lung tissues. This
poses a challenge when applying existing semi-automatic interactive
segmentation techniques for data labelling. To address these challenges, we
propose an efficient convolutional neural networks (CNNs) that can be learned
online while the annotator provides scribble-based interaction. To accelerate
learning from only the samples labelled through user-interactions, a
patch-based approach is used for training the network. Moreover, we use
weighted cross-entropy loss to address the class imbalance that may result from
user-interactions. During online inference, the learned network is applied to
the whole input volume using a fully convolutional approach. We compare our
proposed method with state-of-the-art using synthetic scribbles and show that
it outperforms existing methods on the task of annotating lung lesions
associated with COVID-19, achieving 16% higher Dice score while reducing
execution time by 3$\times$ and requiring 9000 lesser scribbles-based labelled
voxels. Due to the online learning aspect, our approach adapts quickly to user
input, resulting in high quality segmentation labels. Source code for ECONet is
available at: https://github.com/masadcv/ECONet-MONAILabel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Video-text Retrieval with Multiple Choice Questions. (arXiv:2201.04850v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04850">
<div class="article-summary-box-inner">
<span><p>Pre-training a model to learn transferable video-text representation for
retrieval has attracted a lot of attention in recent years. Previous dominant
works mainly adopt two separate encoders for efficient retrieval, but ignore
local associations between videos and texts. Another line of research uses a
joint encoder to interact video with texts, but results in low efficiency since
each text-video pair needs to be fed into the model. In this work, we enable
fine-grained video-text interactions while maintaining high efficiency for
retrieval via a novel pretext task, dubbed as Multiple Choice Questions (MCQ),
where a parametric module BridgeFormer is trained to answer the "questions"
constructed by the text features via resorting to the video features.
Specifically, we exploit the rich semantics of text (i.e., nouns and verbs) to
build questions, with which the video encoder can be trained to capture more
regional content and temporal dynamics. In the form of questions and answers,
the semantic associations between local video-text features can be properly
established. BridgeFormer is able to be removed for downstream retrieval,
rendering an efficient and flexible model with only two encoders. Our method
outperforms state-of-the-art methods on the popular text-to-video retrieval
task in five datasets with different experimental setups (i.e., zero-shot and
fine-tune), including HowTo100M (one million videos). We further conduct
zero-shot action recognition, which can be cast as video-to-text retrieval, and
our approach also significantly surpasses its counterparts. As an additional
benefit, our method achieves competitive results with much shorter pre-training
videos on single-modality downstream tasks, e.g., action recognition with
linear evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Pseudo Label Quality for Semi-Supervised Domain-Generalized Medical Image Segmentation. (arXiv:2201.08657v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08657">
<div class="article-summary-box-inner">
<span><p>Generalizing the medical image segmentation algorithms to unseen domains is
an important research topic for computer-aided diagnosis and surgery. Most
existing methods require a fully labeled dataset in each source domain.
Although some researchers developed a semi-supervised domain generalized
method, it still requires the domain labels. This paper presents a novel
confidence-aware cross pseudo supervision algorithm for semi-supervised domain
generalized medical image segmentation. The main goal is to enhance the pseudo
label quality for unlabeled images from unknown distributions. To achieve it,
we perform the Fourier transformation to learn low-level statistic information
across domains and augment the images to incorporate cross-domain information.
With these augmentations as perturbations, we feed the input to a
confidence-aware cross pseudo supervision network to measure the variance of
pseudo labels and regularize the network to learn with more confident pseudo
labels. Our method sets new records on public datasets, i.e., M&amp;Ms and SCGM.
Notably, without using domain labels, our method surpasses the prior art that
even uses domain labels by 11.67% on Dice on M&amp;Ms dataset with 2% labeled data.
Code is available at https://github.com/XMed-Lab/EPL_SemiDG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds. (arXiv:2201.12769v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12769">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of 3D point cloud is an essential task for autonomous
driving environment perception. The pipeline of most pointwise point cloud
semantic segmentation methods includes points sampling, neighbor searching,
feature aggregation, and classification. Neighbor searching method like
K-nearest neighbors algorithm, KNN, has been widely applied. However, the
complexity of KNN is always a bottleneck of efficiency. In this paper, we
propose an end-to-end neural architecture, Multiple View Pointwise Net,
MVP-Net, to efficiently and directly infer large-scale outdoor point cloud
without KNN or any complex pre/postprocessing. Instead, assumption-based space
filling curves and multi-rotation of point cloud methods are introduced to
point feature aggregation and receptive field expanding. Numerical experiments
show that the proposed MVP-Net is 11 times faster than the most efficient
pointwise semantic segmentation method RandLA-Net and achieves the same
accuracy on the large-scale benchmark SemanticKITTI dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation. (arXiv:2202.07191v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07191">
<div class="article-summary-box-inner">
<span><p>With rising male infertility, sperm head morphology classification becomes
critical for accurate and timely clinical diagnosis. Recent deep learning (DL)
morphology analysis methods achieve promising benchmark results, but leave
performance and robustness on the table by relying on limited and possibly
noisy class labels. To address this, we introduce a new DL training framework
that leverages anatomical and image priors from human sperm microscopy crops to
extract useful features without additional labeling cost. Our core idea is to
distill sperm head information with reliably-generated pseudo-masks and
unsupervised spatial prediction tasks. The predicted foreground masks from this
distillation step are then leveraged to regularize and reduce image and label
noise in the tuning stage. We evaluate our new approach on two public sperm
datasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy
and 96.5% HuSHeM accuracy).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDAM: Heuristic Difference Attention Module for Convolutional Neural Networks. (arXiv:2202.09556v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09556">
<div class="article-summary-box-inner">
<span><p>The attention mechanism is one of the most important priori knowledge to
enhance convolutional neural networks. Most attention mechanisms are bound to
the convolutional layer and use local or global contextual information to
recalibrate the input. This is a popular attention strategy design method.
Global contextual information helps the network to consider the overall
distribution, while local contextual information is more general. The
contextual information makes the network pay attention to the mean or maximum
value of a particular receptive field. Different from the most attention
mechanism, this article proposes a novel attention mechanism with the heuristic
difference attention module, HDAM. HDAM's input recalibration is based on the
difference between the local and global contextual information instead of the
mean and maximum values. At the same time, to make different layers have a more
suitable local receptive field size and increase the exibility of the local
receptive field design, we use genetic algorithm to heuristically produce local
receptive fields. First, HDAM extracts the mean value of the global and local
receptive fields as the corresponding contextual information. Then the
difference between the global and local contextual information is calculated.
Finally HDAM uses this difference to recalibrate the input. In addition, we use
the heuristic ability of genetic algorithm to search for the local receptive
field size of each layer. Our experiments on CIFAR-10 and CIFAR-100 show that
HDAM can use fewer parameters than other attention mechanisms to achieve higher
accuracy. We implement HDAM with the Python library, Pytorch, and the code and
models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computing Multiple Image Reconstructions with a Single Hypernetwork. (arXiv:2202.11009v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11009">
<div class="article-summary-box-inner">
<span><p>Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Adaptive SCEne Tracing. (arXiv:2202.13664v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13664">
<div class="article-summary-box-inner">
<span><p>Neural rendering with implicit neural networks has recently emerged as an
attractive proposition for scene reconstruction, achieving excellent quality
albeit at high computational cost. While the most recent generation of such
methods has made progress on the rendering (inference) times, very little
progress has been made on improving the reconstruction (training) times. In
this work, we present Neural Adaptive Scene Tracing (NAScenT), the first neural
rendering method based on directly training a hybrid explicit-implicit neural
representation. NAScenT uses a hierarchical octree representation with one
neural network per leaf node and combines this representation with a two-stage
sampling process that concentrates ray samples where they matter most near
object surfaces. As a result, NAScenT is capable of reconstructing challenging
scenes including both large, sparsely populated volumes like UAV captured
outdoor environments, as well as small scenes with high geometric complexity.
NAScenT outperforms existing neural rendering approaches in terms of both
quality and training time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Local Feature Learning for 3D Point Cloud Processing using Unary-Pairwise Attention. (arXiv:2203.00172v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00172">
<div class="article-summary-box-inner">
<span><p>We present a simple but effective attention named the unary-pairwise
attention (UPA) for modeling the relationship between 3D point clouds. Our idea
is motivated by the analysis that the standard self-attention (SA) that
operates globally tends to produce almost the same attention maps for different
query positions, revealing difficulties for learning query-independent and
query-dependent information jointly. Therefore, we reformulate the SA and
propose query-independent (Unary) and query-dependent (Pairwise) components to
facilitate the learning of both terms. In contrast to the SA, the UPA ensures
query dependence via operating locally. Extensive experiments show that the UPA
outperforms the SA consistently on various point cloud understanding tasks
including shape classification, part segmentation, and scene segmentation.
Moreover, simply equipping the popular PointNet++ method with the UPA even
outperforms or is on par with the state-of-the-art attention-based approaches.
In addition, the UPA systematically boosts the performance of both standard and
modern networks when it is integrated into them as a compositional module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement. (arXiv:2203.03622v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03622">
<div class="article-summary-box-inner">
<span><p>A stroke occurs when an artery in the brain ruptures and bleeds or when the
blood supply to the brain is cut off. Blood and oxygen cannot reach the brain's
tissues due to the rupture or obstruction resulting in tissue death. The Middle
cerebral artery (MCA) is the largest cerebral artery and the most commonly
damaged vessel in stroke. The quick onset of a focused neurological deficit
caused by interruption of blood flow in the territory supplied by the MCA is
known as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is
used to estimate the extent of early ischemic changes in patients with MCA
stroke. This study proposes a deep learning-based method to score the CT scan
for ASPECTS. Our work has three highlights. First, we propose a novel method
for medical image segmentation for stroke detection. Second, we show the
effectiveness of AI solution for fully-automated ASPECT scoring with reduced
diagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a
dice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72
for the infarcts segmentation. Lastly, we show that our model's performance is
inline with inter-reader variability between radiologists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN. (arXiv:2203.04036v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04036">
<div class="article-summary-box-inner">
<span><p>One-shot talking face generation aims at synthesizing a high-quality talking
face video from an arbitrary portrait image, driven by a video or an audio
segment. One challenging quality factor is the resolution of the output video:
higher resolution conveys more details. In this work, we investigate the latent
feature space of a pre-trained StyleGAN and discover some excellent spatial
transformation properties. Upon the observation, we explore the possibility of
using a pre-trained StyleGAN to break through the resolution limit of training
datasets. We propose a novel unified framework based on a pre-trained StyleGAN
that enables a set of powerful functionalities, i.e., high-resolution video
generation, disentangled control by driving video or audio, and flexible face
editing. Our framework elevates the resolution of the synthesized talking face
to 1024*1024 for the first time, even though the training dataset has a lower
resolution. We design a video-based motion generation module and an audio-based
one, which can be plugged into the framework either individually or jointly to
drive the video generation. The predicted motion is used to transform the
latent features of StyleGAN for visual animation. To compensate for the
transformation distortion, we propose a calibration network as well as a domain
loss to refine the features. Moreover, our framework allows two types of facial
editing, i.e., global editing via GAN inversion and intuitive editing based on
3D morphable models. Comprehensive experiments show superior video quality,
flexible controllability, and editability over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P2M: A Processing-in-Pixel-in-Memory Paradigm for Resource-Constrained TinyML Applications. (arXiv:2203.04737v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04737">
<div class="article-summary-box-inner">
<span><p>The demand to process vast amounts of data generated from state-of-the-art
high resolution cameras has motivated novel energy-efficient on-device AI
solutions. Visual data in such cameras are usually captured in the form of
analog voltages by a sensor pixel array, and then converted to the digital
domain for subsequent AI processing using analog-to-digital converters (ADC).
Recent research has tried to take advantage of massively parallel low-power
analog/digital computing in the form of near- and in-sensor processing, in
which the AI computation is performed partly in the periphery of the pixel
array and partly in a separate on-board CPU/accelerator. Unfortunately,
high-resolution input images still need to be streamed between the camera and
the AI processing unit, frame by frame, causing energy, bandwidth, and security
bottlenecks. To mitigate this problem, we propose a novel
Processing-in-Pixel-in-memory (P2M) paradigm, that customizes the pixel array
by adding support for analog multi-channel, multi-bit convolution, batch
normalization, and ReLU (Rectified Linear Units). Our solution includes a
holistic algorithm-circuit co-design approach and the resulting P2M paradigm
can be used as a drop-in replacement for embedding memory-intensive first few
layers of convolutional neural network (CNN) models within
foundry-manufacturable CMOS image sensor platforms. Our experimental results
indicate that P2M reduces data transfer bandwidth from sensors and analog to
digital conversions by ~21x, and the energy-delay product (EDP) incurred in
processing a MobileNetV2 model on a TinyML use case for visual wake words
dataset (VWW) by up to ~11x compared to standard near-processing or in-sensor
implementations, without any significant drop in test accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Transformer for Hyperspectral Image Classification. (arXiv:2203.04771v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04771">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images (HSI) not only have a broad macroscopic field of view
but also contain rich spectral information, and the types of surface objects
can be identified through spectral information, which is one of the main
applications in hyperspectral image related research.In recent years, more and
more deep learning methods have been proposed, among which convolutional neural
networks (CNN) are the most influential. However, CNN-based methods are
difficult to capture long-range dependencies, and also require a large amount
of labeled data for model training.Besides, most of the self-supervised
training methods in the field of HSI classification are based on the
reconstruction of input samples, and it is difficult to achieve effective use
of unlabeled samples. To address the shortcomings of CNN networks, we propose a
noval multi-scale convolutional embedding module for HSI to realize effective
extraction of spatial-spectral information, which can be better combined with
Transformer network.In order to make more efficient use of unlabeled data, we
propose a new self-supervised pretask. Similar to Mask autoencoder, but our
pre-training method only masks the corresponding token of the central pixel in
the encoder, and inputs the remaining token into the decoder to reconstruct the
spectral information of the central pixel.Such a pretask can better model the
relationship between the central feature and the domain feature, and obtain
more stable training results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalisation for Object Detection. (arXiv:2203.05294v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05294">
<div class="article-summary-box-inner">
<span><p>Domain generalisation aims to promote the learning of domain-invariant
features while suppressing domain specific features, so that a model can
generalise well on previously unseen target domains. This paper studies domain
generalisation in the object detection setting. We propose new terms for
handling both the bounding box detector and domain belonging, and incorporate
them with consistency regularisation. This allows us to learn a domain agnostic
feature representation for object detection, applicable to the problem of
domain generalisation. The proposed approach is evaluated using four standard
object detection datasets with available domain metadata, namely GWHD,
Cityscapes, BDD100K, Sim10K and exhibits consistently superior generalisation
performance over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Bi-directional Skip Connections in Encoder-Decoder Architectures and Beyond. (arXiv:2203.05709v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05709">
<div class="article-summary-box-inner">
<span><p>U-Net, as an encoder-decoder architecture with forward skip connections, has
achieved promising results in various medical image analysis tasks. Many recent
approaches have also extended U-Net with more complex building blocks, which
typically increase the number of network parameters considerably. Such
complexity makes the inference stage highly inefficient for clinical
applications. Towards an effective yet economic segmentation network design, in
this work, we propose backward skip connections that bring decoded features
back to the encoder. Our design can be jointly adopted with forward skip
connections in any encoder-decoder architecture forming a recurrence structure
without introducing extra parameters. With the backward skip connections, we
propose a U-Net based network family, namely Bi-directional O-shape networks,
which set new benchmarks on multiple public medical imaging segmentation
datasets. On the other hand, with the most plain architecture (BiO-Net),
network computations inevitably increase along with the pre-set recurrence
time. We have thus studied the deficiency bottleneck of such recurrent design
and propose a novel two-phase Neural Architecture Search (NAS) algorithm,
namely BiX-NAS, to search for the best multi-scale bi-directional skip
connections. The ineffective skip connections are then discarded to reduce
computational costs and speed up network inference. The finally searched
BiX-Net yields the least network complexity and outperforms other
state-of-the-art counterparts by large margins. We evaluate our methods on both
2D and 3D segmentation tasks in a total of six datasets. Extensive ablation
studies have also been conducted to provide a comprehensive analysis for our
proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Protocol Matters: Towards Accurate Scene Text Recognition via Training Protocol Searching. (arXiv:2203.06696v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06696">
<div class="article-summary-box-inner">
<span><p>The development of scene text recognition (STR) in the era of deep learning
has been mainly focused on novel architectures of STR models. However, training
protocol (i.e., settings of the hyper-parameters involved in the training of
STR models), which plays an equally important role in successfully training a
good STR model, is under-explored for scene text recognition. In this work, we
attempt to improve the accuracy of existing STR models by searching for optimal
training protocol. Specifically, we develop a training protocol search
algorithm, based on a newly designed search space and an efficient search
algorithm using evolutionary optimization and proxy tasks. Experimental results
show that our searched training protocol can improve the recognition accuracy
of mainstream STR models by 2.7%~3.9%. In particular, with the searched
training protocol, TRBA-Net achieves 2.1% higher accuracy than the
state-of-the-art STR model (i.e., EFIFSTR), while the inference speed is 2.3x
and 3.7x faster on CPU and GPU respectively. Extensive experiments are
conducted to demonstrate the effectiveness of the proposed method and the
generalization ability of the training protocol found by our search method.
Code is available at https://github.com/VDIGPKU/STR_TPSearch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06717">
<div class="article-summary-box-inner">
<span><p>We revisit large kernel design in modern convolutional neural networks
(CNNs). Inspired by recent advances of vision transformers (ViTs), in this
paper, we demonstrate that using a few large convolutional kernels instead of a
stack of small kernels could be a more powerful paradigm. We suggested five
guidelines, e.g., applying re-parameterized large depth-wise convolutions, to
design efficient high-performance large-kernel CNNs. Following the guidelines,
we propose RepLKNet, a pure CNN architecture whose kernel size is as large as
31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the
performance gap between CNNs and ViTs, e.g., achieving comparable or superior
results than Swin Transformer on ImageNet and a few typical downstream tasks,
with lower latency. RepLKNet also shows nice scalability to big data and large
models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K,
which is very competitive among the state-of-the-arts with similar model sizes.
Our study further reveals that, in contrast to small-kernel CNNs, large-kernel
CNNs have much larger effective receptive fields, and higher shape bias rather
than texture bias. Code &amp; models at
https://github.com/megvii-research/RepLKNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimMatch: Semi-supervised Learning with Similarity Matching. (arXiv:2203.06915v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06915">
<div class="article-summary-box-inner">
<span><p>Learning with few labeled data has been a longstanding problem in the
computer vision and machine learning research community. In this paper, we
introduced a new semi-supervised learning framework, SimMatch, which
simultaneously considers semantic similarity and instance similarity. In
SimMatch, the consistency regularization will be applied on both semantic-level
and instance-level. The different augmented views of the same instance are
encouraged to have the same class prediction and similar similarity
relationship respected to other instances. Next, we instantiated a labeled
memory buffer to fully leverage the ground truth labels on instance-level and
bridge the gaps between the semantic and instance similarities. Finally, we
proposed the \textit{unfolding} and \textit{aggregation} operation which allows
these two similarities be isomorphically transformed with each other. In this
way, the semantic and instance pseudo-labels can be mutually propagated to
generate more high-quality and reliable matching targets. Extensive
experimental results demonstrate that SimMatch improves the performance of
semi-supervised learning tasks across different benchmark datasets and
different settings. Notably, with 400 epochs of training, SimMatch achieves
67.2\%, and 74.4\% Top-1 Accuracy with 1\% and 10\% labeled examples on
ImageNet, which significantly outperforms the baseline methods and is better
than previous semi-supervised learning frameworks. Code and pre-trained models
are available at https://github.com/KyleZheng1997/simmatch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting associations and meanings of objects depicted in artworks through bi-modal deep networks. (arXiv:2203.07026v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07026">
<div class="article-summary-box-inner">
<span><p>We present a novel bi-modal system based on deep networks to address the
problem of learning associations and simple meanings of objects depicted in
"authored" images, such as fine art paintings and drawings. Our overall system
processes both the images and associated texts in order to learn associations
between images of individual objects, their identities and the abstract
meanings they signify. Unlike past deep nets that describe depicted objects and
infer predicates, our system identifies meaning-bearing objects ("signifiers")
and their associations ("signifieds") as well as basic overall meanings for
target artworks. Our system had precision of 48% and recall of 78% with an F1
metric of 0.6 on a curated set of Dutch vanitas paintings, a genre celebrated
for its concentration on conveying a meaning of great import at the time of
their execution. We developed and tested our system on fine art paintings but
our general methods can be applied to other authored images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Non-Rigid 3D Registration. (arXiv:2203.07858v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07858">
<div class="article-summary-box-inner">
<span><p>Non-rigid registration computes an alignment between a source surface with a
target surface in a non-rigid manner. In the past decade, with the advances in
3D sensing technologies that can measure time-varying surfaces, non-rigid
registration has been applied for the acquisition of deformable shapes and has
a wide range of applications. This survey presents a comprehensive review of
non-rigid registration methods for 3D shapes, focusing on techniques related to
dynamic shape acquisition and reconstruction. In particular, we review
different approaches for representing the deformation field, and the methods
for computing the desired deformation. Both optimization-based and
learning-based methods are covered. We also review benchmarks and datasets for
evaluating non-rigid registration methods, and discuss potential future
research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting. (arXiv:2203.07918v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07918">
<div class="article-summary-box-inner">
<span><p>While 6D object pose estimation has recently made a huge leap forward, most
methods can still only handle a single or a handful of different objects, which
limits their applications. To circumvent this problem, category-level object
pose estimation has recently been revamped, which aims at predicting the 6D
pose as well as the 3D metric size for previously unseen instances from a given
set of object classes. This is, however, a much more challenging task due to
severe intra-class shape variations. To address this issue, we propose
GPV-Pose, a novel framework for robust category-level pose estimation,
harnessing geometric insights to enhance the learning of category-level
pose-sensitive features. First, we introduce a decoupled confidence-driven
rotation representation, which allows geometry-aware recovery of the associated
rotation matrix. Second, we propose a novel geometry-guided point-wise voting
paradigm for robust retrieval of the 3D object bounding box. Finally,
leveraging these different output streams, we can enforce several geometric
consistency terms, further increasing performance, especially for non-symmetric
categories. GPV-Pose produces superior results to state-of-the-art competitors
on common public benchmarks, whilst almost achieving real-time inference speed
at 20 FPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intrinsic Neural Fields: Learning Functions on Manifolds. (arXiv:2203.07967v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07967">
<div class="article-summary-box-inner">
<span><p>Neural fields have gained significant attention in the computer vision
community due to their excellent performance in novel view synthesis, geometry
reconstruction, and generative modeling. Some of their advantages are a sound
theoretic foundation and an easy implementation in current deep learning
frameworks. While neural fields have been applied to signals on manifolds,
e.g., for texture reconstruction, their representation has been limited to
extrinsically embedding the shape into Euclidean space. The extrinsic embedding
ignores known intrinsic manifold properties and is inflexible wrt. transfer of
the learned function. To overcome these limitations, this work introduces
intrinsic neural fields, a novel and versatile representation for neural fields
on manifolds. Intrinsic neural fields combine the advantages of neural fields
with the spectral properties of the Laplace-Beltrami operator. We show
theoretically that intrinsic neural fields inherit many desirable properties of
the extrinsic neural field framework but exhibit additional intrinsic
qualities, like isometry invariance. In experiments, we show intrinsic neural
fields can reconstruct high-fidelity textures from images with state-of-the-art
quality and are robust to the discretization of the underlying manifold. We
demonstrate the versatility of intrinsic neural fields by tackling various
applications: texture transfer between deformed shapes &amp; different shapes,
texture reconstruction from real-world images with view dependence, and
discretization-agnostic learning on meshes and point clouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motif Mining: Finding and Summarizing Remixed Image Content. (arXiv:2203.08327v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08327">
<div class="article-summary-box-inner">
<span><p>On the internet, images are no longer static; they have become dynamic
content. Thanks to the availability of smartphones with cameras and easy-to-use
editing software, images can be remixed (i.e., redacted, edited, and recombined
with other content) on-the-fly and with a world-wide audience that can repeat
the process. From digital art to memes, the evolution of images through time is
now an important topic of study for digital humanists, social scientists, and
media forensics specialists. However, because typical data sets in computer
vision are composed of static content, the development of automated algorithms
to analyze remixed content has been limited. In this paper, we introduce the
idea of Motif Mining - the process of finding and summarizing remixed image
content in large collections of unlabeled and unsorted data. In this paper,
this idea is formalized and a reference implementation is introduced.
Experiments are conducted on three meme-style data sets, including a newly
collected set associated with the information war in the Russo-Ukrainian
conflict. The proposed motif mining approach is able to identify related
remixed content that, when compared to similar approaches, more closely aligns
with the preferences and expectations of human observers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Efficient 3D Learner via Knowledge Transferred from 2D Model. (arXiv:2203.08479v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08479">
<div class="article-summary-box-inner">
<span><p>Collecting and labeling the registered 3D point cloud is costly. As a result,
3D resources for training are typically limited in quantity compared to the 2D
images counterpart. In this work, we deal with the data scarcity challenge of
3D tasks by transferring knowledge from strong 2D models via RGB-D images.
Specifically, we utilize a strong and well-trained semantic segmentation model
for 2D images to augment RGB-D images with pseudo-label. The augmented dataset
can then be used to pre-train 3D models. Finally, by simply fine-tuning on a
few labeled 3D instances, our method already outperforms existing
state-of-the-art that is tailored for 3D label efficiency. We also show that
the results of mean-teacher and entropy minimization can be improved by our
pre-training, suggesting that the transferred knowledge is helpful in
semi-supervised setting. We verify the effectiveness of our approach on two
popular 3D models and three different tasks. On ScanNet official evaluation, we
establish new state-of-the-art semantic segmentation results on the
data-efficient track.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complexity Reduction of Learned In-Loop Filtering in Video Coding. (arXiv:2203.08650v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08650">
<div class="article-summary-box-inner">
<span><p>In video coding, in-loop filters are applied on reconstructed video frames to
enhance their perceptual quality, before storing the frames for output.
Conventional in-loop filters are obtained by hand-crafted methods. Recently,
learned filters based on convolutional neural networks that utilize attention
mechanisms have been shown to improve upon traditional techniques. However,
these solutions are typically significantly more computationally expensive,
limiting their potential for practical applications. The proposed method uses a
novel combination of sparsity and structured pruning for complexity reduction
of learned in-loop filters. This is done through a three-step training process
of magnitude-guidedweight pruning, insignificant neuron identification and
removal, and fine-tuning. Through initial tests we find that network parameters
can be significantly reduced with a minimal impact on network performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Flow: Cross-layer Graph Flow Distillation for Dual-Efficient Medical Image Segmentation. (arXiv:2203.08667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08667">
<div class="article-summary-box-inner">
<span><p>With the development of deep convolutional neural networks, medical image
segmentation has achieved a series of breakthroughs in recent years. However,
the higher-performance convolutional neural networks always mean numerous
parameters and expensive computation costs, which will hinder the applications
in clinical scenarios. Meanwhile, the scarceness of large-scale annotated
medical image datasets further impedes the application of high-performance
networks. To tackle these problems, we propose Graph Flow, a novel
comprehensive knowledge distillation method, to exploit the cross-layer graph
flow knowledge for both network-efficient and annotation-efficient medical
image segmentation. Specifically, our Graph Flow Distillation constructs a
variation graph which is employed to measure the flow of channel-wise salience
features between different layers. Next, the knowledge included in the
variation graph is transferred from a well-trained cumbersome teacher network
to a non-trained compact student network. In addition, an unsupervised
Paraphraser Module is designed to refine the knowledge of the teacher network,
which is also beneficial for the stabilization of training procedure.
Furthermore, we build a unified distillation framework by integrating the
adversarial distillation and the vanilla logits distillation, which can further
promote the final performance respectively. As a result, extensive experiments
conducted on Gastric Cancer Segmentation Dataset and Synapse Multi-organ
Segmentation Dataset demonstrate the prominent ability of our method which
achieves state-of-the-art performance on these different-modality and
multi-category medical image datasets. Moreover, we demonstrate the
effectiveness of our Graph Flow through a new semi-supervised paradigm for
dual-efficient medical image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Exploration for Neural Global Illumination of Variable Scenes. (arXiv:2203.08272v1 [cs.GR] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08272">
<div class="article-summary-box-inner">
<span><p>Neural rendering algorithms introduce a fundamentally new approach for
photorealistic rendering, typically by learning a neural representation of
illumination on large numbers of ground truth images. When training for a given
variable scene, i.e., changing objects, materials, lights and viewpoint, the
space D of possible training data instances quickly becomes unmanageable as the
dimensions of variable parameters increase. We introduce a novel Active
Exploration method using Markov Chain Monte Carlo, which explores D, generating
samples (i.e., ground truth renderings) that best help training and interleaves
training and on-the-fly sample data generation. We introduce a self-tuning
sample reuse strategy to minimize the expensive step of rendering training
samples. We apply our approach on a neural generator that learns to render
novel scene instances given an explicit parameterization of the scene
configuration. Our results show that Active Exploration trains our network much
more efficiently than uniformly sampling, and together with our resolution
enhancement approach, achieves better quality than uniform sampling at
convergence. Our method allows interactive rendering of hard light transport
paths (e.g., complex caustics) -- that require very high samples counts to be
captured -- and provides dynamic scene navigation and manipulation, after
training for 5-18 hours depending on required quality and variations.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-20 23:07:26.634876154 UTC">2022-03-20 23:07:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>