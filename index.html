<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-10T01:30:00Z">03-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media. (arXiv:2203.04356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04356">
<div class="article-summary-box-inner">
<span><p>Social media is a breeding ground for threat narratives and related
conspiracy theories. In these, an outside group threatens the integrity of an
inside group, leading to the emergence of sharply defined group identities:
Insiders -- agents with whom the authors identify and Outsiders -- agents who
threaten the insiders. Inferring the members of these groups constitutes a
challenging new NLP task: (i) Information is distributed over many
poorly-constructed posts; (ii) Threats and threat agents are highly contextual,
with the same post potentially having multiple agents assigned to membership in
either group; (iii) An agent's identity is often implicit and transitive; and
(iv) Phrases used to imply Outsider status often do not follow common negative
sentiment patterns. To address these challenges, we define a novel
Insider-Outsider classification task. Because we are not aware of any
appropriate existing datasets or attendant models, we introduce a labeled
dataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages
pretrained language modeling to classify Insiders and Outsiders. NP2IO is shown
to be robust, generalizing to noun phrases not seen during training, and
exceeding the performance of non-trivial baseline models by $20\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's AI Match: A Two-Step Approach for Schema Matching Using Embeddings. (arXiv:2203.04366v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04366">
<div class="article-summary-box-inner">
<span><p>Since data is often stored in different sources, it needs to be integrated to
gather a global view that is required in order to create value and derive
knowledge from it. A critical step in data integration is schema matching which
aims to find semantic correspondences between elements of two schemata. In
order to reduce the manual effort involved in schema matching, many solutions
for the automatic determination of schema correspondences have already been
developed.
</p>
<p>In this paper, we propose a novel end-to-end approach for schema matching
based on neural embeddings. The main idea is to use a two-step approach
consisting of a table matching step followed by an attribute matching step. In
both steps we use embeddings on different levels either representing the whole
table or single attributes. Our results show that our approach is able to
determine correspondences in a robust and reliable way and compared to
traditional schema matching approaches can find non-trivial correspondences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iSEA: An Interactive Pipeline for Semantic Error Analysis of NLP Models. (arXiv:2203.04408v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04408">
<div class="article-summary-box-inner">
<span><p>Error analysis in NLP models is essential to successful model development and
deployment. One common approach for diagnosing errors is to identify
subpopulations in the dataset where the model produces the most errors.
However, existing approaches typically define subpopulations based on
pre-defined features, which requires users to form hypotheses of errors in
advance. To complement these approaches, we propose iSEA, an Interactive
Pipeline for Semantic Error Analysis in NLP Models, which automatically
discovers semantically-grounded subpopulations with high error rates in the
context of a human-in-the-loop interactive system. iSEA enables model
developers to learn more about their model errors through discovered
subpopulations, validate the sources of errors through interactive analysis on
the discovered subpopulations, and test hypotheses about model errors by
defining custom subpopulations. The tool supports semantic descriptions of
error-prone subpopulations at the token and concept level, as well as
pre-defined higher-level features. Through use cases and expert interviews, we
demonstrate how iSEA can assist error understanding and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating the Uncertainty in Emotion Class Labels with Utterance-Specific Dirichlet Priors. (arXiv:2203.04443v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04443">
<div class="article-summary-box-inner">
<span><p>Emotion recognition is a key attribute for artificial intelligence systems
that need to naturally interact with humans. However, the task definition is
still an open problem due to inherent ambiguity of emotions. In this paper, a
novel Bayesian training loss based on per-utterance Dirichlet prior
distributions is proposed for verbal emotion recognition, which models the
uncertainty in one-hot labels created when human annotators assign the same
utterance to different emotion classes. An additional metric is used to
evaluate the performance by detecting test utterances with high labelling
uncertainty. This removes a major limitation that emotion classification
systems only consider utterances with majority labels.Furthermore, a
frequentist approach is studied to leverage the continuous-valued "soft" labels
obtained by averaging the one-hot labels. We propose a two-branch model
structure for emotion classification on a per-utterance basis. Experiments with
the widely used IEMOCAP dataset demonstrate that the two-branch structure
achieves state-of-the-art classification results with all common IEMOCAP test
setups. Based on this, uncertainty estimation experiments were performed. The
best performance in terms of the area under the precision-recall curve when
detecting utterances with high uncertainty was achieved by interpolating the
Bayesian training loss with the Kullback-Leibler divergence training loss for
the soft labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Networks and Unsupervised Ranking of Sentences. (arXiv:2203.04459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04459">
<div class="article-summary-box-inner">
<span><p>We construct a contextual network to represent a document with syntactic and
semantic relations between word-sentence pairs, based on which we devise an
unsupervised algorithm called CNATAR (Contextual Network And Text Analysis
Rank) to score sentences, and rank them through a bi-objective 0-1 knapsack
maximization problem over topic analysis and sentence scores. We show that
CNATAR outperforms the combined ranking of the three human judges provided on
the SummBank dataset under both ROUGE and BLEU metrics, which in term
significantly outperforms each individual judge's ranking. Moreover, CNATAR
produces so far the highest ROUGE scores over DUC-02, and outperforms previous
supervised algorithms on the CNN/DailyMail and NYT datasets. We also compare
the performance of CNATAR and the latest supervised neural-network
summarization models and compute oracle results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Evaluation of Answer-Agnostic Paragraph-level Multi-Question Generation. (arXiv:2203.04464v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04464">
<div class="article-summary-box-inner">
<span><p>We study the task of predicting a set of salient questions from a given
paragraph without any prior knowledge of the precise answer. We make two main
contributions. First, we propose a new method to evaluate a set of predicted
questions against the set of references by using the Hungarian algorithm to
assign predicted questions to references before scoring the assigned pairs. We
show that our proposed evaluation strategy has better theoretical and practical
properties compared to prior methods because it can properly account for the
coverage of references. Second, we compare different strategies to utilize a
pre-trained seq2seq model to generate and select a set of questions related to
a given paragraph. The code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boilerplate Detection via Semantic Classification of TextBlocks. (arXiv:2203.04467v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04467">
<div class="article-summary-box-inner">
<span><p>We present a hierarchical neural network model called SemText to detect HTML
boilerplate based on a novel semantic representation of HTML tags, class names,
and text blocks. We train SemText on three published datasets of news webpages
and fine-tune it using a small number of development data in CleanEval and
GoogleTrends-2017. We show that SemText achieves the state-of-the-art accuracy
on these datasets. We then demonstrate the robustness of SemText by showing
that it also detects boilerplate effectively on out-of-domain community-based
question-answer webpages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Onception: Active Learning with Expert Advice for Real World Machine Translation. (arXiv:2203.04507v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04507">
<div class="article-summary-box-inner">
<span><p>Active learning can play an important role in low-resource settings (i.e.,
where annotated data is scarce), by selecting which instances may be more
worthy to annotate. Most active learning approaches for Machine Translation
assume the existence of a pool of sentences in a source language, and rely on
human annotators to provide translations or post-edits, which can still be
costly. In this paper, we assume a real world human-in-the-loop scenario in
which: (i) the source sentences may not be readily available, but instead
arrive in a stream; (ii) the automatic translations receive feedback in the
form of a rating, instead of a correct/edited translation, since the
human-in-the-loop might be a user looking for a translation, but not be able to
provide one. To tackle the challenge of deciding whether each incoming pair
source-translations is worthy to query for human feedback, we resort to a
number of stream-based active learning query strategies. Moreover, since we not
know in advance which query strategy will be the most adequate for a certain
language pair and set of Machine Translation models, we propose to dynamically
combine multiple strategies using prediction with expert advice. Our
experiments show that using active learning allows to converge to the best
Machine Translation systems with fewer human interactions. Furthermore,
combining multiple strategies using prediction with expert advice often
outperforms several individual active learning strategies with even fewer
interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04592">
<div class="article-summary-box-inner">
<span><p>Benchmarks are crucial to measuring and steering progress in artificial
intelligence (AI). However, recent studies raised concerns over the state of AI
benchmarking, reporting issues such as benchmark overfitting, benchmark
saturation and increasing centralization of benchmark dataset creation. To
facilitate monitoring of the health of the AI benchmarking ecosystem, we
introduce methodologies for creating condensed maps of the global dynamics of
benchmark creation and saturation. We curated data for 1688 benchmarks covering
the entire domains of computer vision and natural language processing, and show
that a large fraction of benchmarks quickly trended towards near-saturation,
that many benchmarks fail to find widespread utilization, and that benchmark
performance gains for different AI tasks were prone to unforeseen bursts. We
conclude that future work should focus on large-scale community collaboration
and on mapping benchmark performance gains to real-world utility and impact of
AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of Deep Transformers for Patronizing and Condescending Language Detection. (arXiv:2203.04616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04616">
<div class="article-summary-box-inner">
<span><p>Patronizing and condescending language (PCL) has a large harmful impact and
is difficult to detect, both for human judges and existing NLP systems. At
SemEval-2022 Task 4, we propose a novel Transformer-based model and its
ensembles to accurately understand such language context for PCL detection. To
facilitate comprehension of the subtle and subjective nature of PCL, two
fine-tuning strategies are applied to capture discriminative features from
diverse linguistic behaviour and categorical distribution. The system achieves
remarkable results on the official ranking, namely 1st in Subtask 1 and 5th in
Subtask 2. Extensive experiments on the task demonstrate the effectiveness of
our system and its strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEBP -- Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents. (arXiv:2203.04637v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04637">
<div class="article-summary-box-inner">
<span><p>People always desire an embodied agent that can perform a task by
understanding language instruction. Moreover, they also want to monitor and
expect agents to understand commands the way they expected. But, how to build
such an embodied agent is still unclear. Recently, people can explore this
problem with the Vision-and-Language Interaction benchmark ALFRED, which
requires an agent to perform complicated daily household tasks following
natural language instructions in unseen scenes. In this paper, we propose LEBP
-- Language Expectation and Binding Policy Module to tackle the ALFRED. The
LEBP contains a two-stream process: 1) It first conducts a language expectation
module to generate an expectation describing how to perform tasks by
understanding the language instruction. The expectation consists of a sequence
of sub-steps for the task (e.g., Pick an apple). The expectation allows people
to access and check the understanding results of instructions before the agent
takes actual actions, in case the task might go wrong. 2) Then, it uses the
binding policy module to bind sub-steps in expectation to actual actions to
specific scenarios. Actual actions include navigation and object manipulation.
Experimental results suggest our approach achieves comparable performance to
currently published SOTA methods and can avoid large decay from seen scenarios
to unseen scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory Efficient Continual Learning for Neural Text Classification. (arXiv:2203.04640v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04640">
<div class="article-summary-box-inner">
<span><p>Learning text classifiers based on pre-trained language models has become the
standard practice in natural language processing applications. Unfortunately,
training large neural language models, such as transformers, from scratch is
very costly and requires a vast amount of training data, which might not be
available in the application domain of interest. Moreover, in many real-world
scenarios, classes are uncovered as more data is seen, calling for
class-incremental modelling approaches. In this work we devise a method to
perform text classification using pre-trained models on a sequence of
classification tasks provided in sequence. We formalize the problem as a
continual learning problem where the algorithm learns new tasks without
performance degradation on the previous ones and without re-training the model
from scratch. We empirically demonstrate that our method requires significantly
less model parameters compared to other state of the art methods and that it is
significantly faster at inference time. The tight control on the number of
model parameters, and so the memory, is not only improving efficiency. It is
making possible the usage of the algorithm in real-world applications where
deploying a solution with a constantly increasing memory consumption is just
unrealistic. While our method suffers little forgetting, it retains a
predictive performance on-par with state of the art but less memory efficient
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slangvolution: A Causal Analysis of Semantic Change and Frequency Dynamics in Slang. (arXiv:2203.04651v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04651">
<div class="article-summary-box-inner">
<span><p>Languages are continuously undergoing changes, and the mechanisms that
underlie these changes are still a matter of debate. In this work, we approach
language evolution through the lens of causality in order to model not only how
various distributional factors associate with language change, but how they
causally affect it. In particular, we study slang, which is an informal
language that is typically restricted to a specific group or social setting. We
analyze the semantic change and frequency shift of slang words and compare them
to those of standard, nonslang words. With causal discovery and causal
inference techniques, we measure the effect that word type (slang/nonslang) has
on both semantic change and frequency shift, as well as its relationship to
frequency, polysemy and part of speech. Our analysis provides some new insights
in the study of semantic change, e.g., we show that slang words undergo less
semantic change but tend to have larger frequency shifts over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASET: Ad-hoc Structured Exploration of Text Collections [Extended Abstract]. (arXiv:2203.04663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04663">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new system called ASET that allows users to
perform structured explorations of text collections in an ad-hoc manner. The
main idea of ASET is to use a new two-phase approach that first extracts a
superset of information nuggets from the texts using existing extractors such
as named entity recognizers and then matches the extractions to a structured
table definition as requested by the user based on embeddings. In our
evaluation, we show that ASET is thus able to extract structured data from
real-world text collections in high quality without the need to design
extraction pipelines upfront.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Named Entity Recognition as Latent Lexicalized Constituency Parsing. (arXiv:2203.04665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04665">
<div class="article-summary-box-inner">
<span><p>Nested named entity recognition (NER) has been receiving increasing
attention. Recently, (Fu et al, 2021) adapt a span-based constituency parser to
tackle nested NER. They treat nested entities as partially-observed
constituency trees and propose the masked inside algorithm for partial
marginalization. However, their method cannot leverage entity heads, which have
been shown useful in entity mention detection and entity typing. In this work,
we resort to more expressive structures, lexicalized constituency trees in
which constituents are annotated by headwords, to model nested entities. We
leverage the Eisner-Satta algorithm to perform partial marginalization and
inference efficiently. In addition, we propose to use (1) a two-stage strategy
(2) a head regularization loss and (3) a head-aware labeling loss in order to
enhance the performance. We make a thorough ablation study to investigate the
functionality of each component. Experimentally, our method achieves the
state-of-the-art performance on ACE2004, ACE2005 and NNE, and competitive
performance on GENIA, and meanwhile has a fast inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Diversity: Visible to Humans, Exploitable by Machines. (arXiv:2203.04723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04723">
<div class="article-summary-box-inner">
<span><p>The Universal Knowledge Core (UKC) is a large multilingual lexical database
with a focus on language diversity and covering over a thousand languages. The
aim of the database, as well as its tools and data catalogue, is to make the
somewhat abstract notion of diversity visually understandable for humans and
formally exploitable by machines. The UKC website lets users explore millions
of individual words and their meanings, but also phenomena of cross-lingual
convergence and divergence, such as shared interlingual meanings, lexicon
similarities, cognate clusters, or lexical gaps. The UKC LiveLanguage
Catalogue, in turn, provides access to the underlying lexical data in a
computer-processable form, ready to be reused in cross-lingual applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretrained Domain-Specific Language Model for General Information Retrieval Tasks in the AEC Domain. (arXiv:2203.04729v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04729">
<div class="article-summary-box-inner">
<span><p>As an essential task for the architecture, engineering, and construction
(AEC) industry, information retrieval (IR) from unstructured textual data based
on natural language processing (NLP) is gaining increasing attention. Although
various deep learning (DL) models for IR tasks have been investigated in the
AEC domain, it is still unclear how domain corpora and domain-specific
pretrained DL models can improve performance in various IR tasks. To this end,
this work systematically explores the impacts of domain corpora and various
transfer learning techniques on the performance of DL models for IR tasks and
proposes a pretrained domain-specific language model for the AEC domain. First,
both in-domain and close-domain corpora are developed. Then, two types of
pretrained models, including traditional wording embedding models and
BERT-based models, are pretrained based on various domain corpora and transfer
learning strategies. Finally, several widely used DL models for IR tasks are
further trained and tested based on various configurations and pretrained
models. The result shows that domain corpora have opposite effects on
traditional word embedding models for text classification and named entity
recognition tasks but can further improve the performance of BERT-based models
in all tasks. Meanwhile, BERT-based models dramatically outperform traditional
methods in all IR tasks, with maximum improvements of 5.4% and 10.1% in the F1
score, respectively. This research contributes to the body of knowledge in two
ways: 1) demonstrating the advantages of domain corpora and pretrained DL
models and 2) opening the first domain-specific dataset and pretrained language
model for the AEC domain, to the best of our knowledge. Thus, this work sheds
light on the adoption and application of pretrained models in the AEC domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhance Topics Analysis based on Keywords Properties. (arXiv:2203.04786v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04786">
<div class="article-summary-box-inner">
<span><p>Topic Modelling is one of the most prevalent text analysis technique used to
explore and retrieve collection of documents. The evaluation of the topic model
algorithms is still a very challenging tasks due to the absence of
gold-standard list of topics to compare against for every corpus. In this work,
we present a specificity score based on keywords properties that is able to
select the most informative topics. This approach helps the user to focus on
the most informative topics. In the experiments, we show that we are able to
compress the state-of-the-art topic modelling results of different factors with
an information loss that is much lower than the solution based on the recent
coherence score presented in literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-Shot Learning from a Demonstration with Hierarchical Latent Language. (arXiv:2203.04806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04806">
<div class="article-summary-box-inner">
<span><p>Humans have the capability, aided by the expressive compositionality of their
language, to learn quickly by demonstration. They are able to describe unseen
task-performing procedures and generalize their execution to other contexts. In
this work, we introduce DescribeWorld, an environment designed to test this
sort of generalization skill in grounded agents, where tasks are linguistically
and procedurally composed of elementary concepts. The agent observes a single
task demonstration in a Minecraft-like grid world, and is then asked to carry
out the same task in a new map. To enable such a level of generalization, we
propose a neural agent infused with hierarchical latent language--both at the
level of task inference and subtask planning. Our agent first generates a
textual description of the demonstrated unseen task, then leverages this
description to replicate it. Through multiple evaluation scenarios and a suite
of generalization tests, we find that agents that perform text-based inference
are better equipped for the challenge under a random split of tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Sub-structured Knowledge Distillation. (arXiv:2203.04825v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04825">
<div class="article-summary-box-inner">
<span><p>Structured prediction models aim at solving a type of problem where the
output is a complex structure, rather than a single variable. Performing
knowledge distillation for such models is not trivial due to their
exponentially large output space. In this work, we propose an approach that is
much simpler in its formulation and far more efficient for training than
existing approaches. Specifically, we transfer the knowledge from a teacher
model to its student model by locally matching their predictions on all
sub-structures, instead of the whole output space. In this manner, we avoid
adopting some time-consuming techniques like dynamic programming (DP) for
decoding output structures, which permits parallel computation and makes the
training process even faster in practice. Besides, it encourages the student
model to better mimic the internal behavior of the teacher model. Experiments
on two structured prediction tasks demonstrate that our approach outperforms
previous methods and halves the time cost for one training epoch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Language Identification for Celtic Texts. (arXiv:2203.04831v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04831">
<div class="article-summary-box-inner">
<span><p>Language identification is an important Natural Language Processing task. It
has been thoroughly researched in the literature. However, some issues are
still open. This work addresses the identification of the related low-resource
languages on the example of the Celtic language family.
</p>
<p>This work's main goals were: (1) to collect the dataset of three Celtic
languages; (2) to prepare a method to identify the languages from the Celtic
family, i.e. to train a successful classification model; (3) to evaluate the
influence of different feature extraction methods, and explore the
applicability of the unsupervised models as a feature extraction technique; (4)
to experiment with the unsupervised feature extraction on a reduced annotated
set.
</p>
<p>We collected a new dataset including Irish, Scottish, Welsh and English
records. We tested supervised models such as SVM and neural networks with
traditional statistical features alongside the output of clustering,
autoencoder, and topic modelling methods. The analysis showed that the
unsupervised features could serve as a valuable extension to the n-gram feature
vectors. It led to an improvement in performance for more entangled classes.
The best model achieved a 98\% F1 score and 97\% MCC. The dense neural network
consistently outperformed the SVM model.
</p>
<p>The low-resource languages are also challenging due to the scarcity of
available annotated training data. This work evaluated the performance of the
classifiers using the unsupervised feature extraction on the reduced labelled
dataset to handle this issue. The results uncovered that the unsupervised
feature vectors are more robust to the labelled set reduction. Therefore, they
proved to help achieve comparable classification performance with much less
labelled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">'Beach' to 'Bitch': Inadvertent Unsafe Transcription of Kids' Content on YouTube. (arXiv:2203.04837v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04837">
<div class="article-summary-box-inner">
<span><p>Over the last few years, YouTube Kids has emerged as one of the highly
competitive alternatives to television for children's entertainment.
Consequently, YouTube Kids' content should receive an additional level of
scrutiny to ensure children's safety. While research on detecting offensive or
inappropriate content for kids is gaining momentum, little or no current work
exists that investigates to what extent AI applications can (accidentally)
introduce content that is inappropriate for kids.
</p>
<p>In this paper, we present a novel (and troubling) finding that well-known
automatic speech recognition (ASR) systems may produce text content highly
inappropriate for kids while transcribing YouTube Kids' videos. We dub this
phenomenon as \emph{inappropriate content hallucination}. Our analyses suggest
that such hallucinations are far from occasional, and the ASR systems often
produce them with high confidence. We release a first-of-its-kind data set of
audios for which the existing state-of-the-art ASR systems hallucinate
inappropriate content for kids. In addition, we demonstrate that some of these
errors can be fixed using language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference. (arXiv:2203.04857v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04857">
<div class="article-summary-box-inner">
<span><p>We introduce a neuro-symbolic natural logic framework based on reinforcement
learning with introspective revision. The model samples and rewards specific
reasoning paths through policy gradient, in which the introspective revision
algorithm modifies intermediate symbolic reasoning steps to discover
reward-earning operations as well as leverages external knowledge to alleviate
spurious reasoning and training inefficiency. The framework is supported by
properly designed local relation models to avoid input entangling, which helps
ensure the interpretability of the proof paths. The proposed model has built-in
interpretability and shows superior capability in monotonicity inference,
systematic generalization, and interpretability, compared to previous models on
the existing datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PET: A new Dataset for Process Extraction from Natural Language Text. (arXiv:2203.04860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04860">
<div class="article-summary-box-inner">
<span><p>Although there is a long tradition of work in NLP on extracting entities and
relations from text, to date there exists little work on the acquisition of
business processes from unstructured data such as textual corpora of process
descriptions. With this work we aim at filling this gap and establishing the
first steps towards bridging data-driven information extraction methodologies
from Natural Language Processing and the model-based formalization that is
aimed from Business Process Management. For this, we develop the first corpus
of business process descriptions annotated with activities, gateways, actors
and flow information. We present our new resource, including a detailed
overview of the annotation schema and guidelines, as well as a variety of
baselines to benchmark the difficulty and challenges of business process
extraction from text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Alignment of Distributional Word Embeddings. (arXiv:2203.04863v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04863">
<div class="article-summary-box-inner">
<span><p>Cross-domain alignment play a key roles in tasks ranging from machine
translation to transfer learning. Recently, purely unsupervised methods
operating on monolingual embeddings have successfully been used to infer a
bilingual lexicon without relying on supervision. However, current state-of-the
art methods only focus on point vectors although distributional embeddings have
proven to embed richer semantic information when representing words. In this
paper, we propose stochastic optimization approach for aligning probabilistic
embeddings. Finally, we evaluate our method on the problem of unsupervised word
translation, by aligning word embeddings trained on monolingual data. We show
that the proposed approach achieves good performance on the bilingual lexicon
induction task across several language pairs and performs better than the
point-vector based approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04904">
<div class="article-summary-box-inner">
<span><p>Despite achieving state-of-the-art zero-shot performance, existing
vision-language models, e.g., CLIP, still fall short of domain-specific
classification tasks, e.g., Fungi Classification. In the context of few-shot
transfer learning, traditional fine-tuning fails to prevent highly expressive
model from exploiting spurious correlations in the training data. On the other
hand, although model-agnostic meta-learning (MAML) presents as a natural
alternative for transfer learning, the expensive computation due to implicit
second-order optimization limits its use in large-scale models and datasets. In
this work we aim to further improve the generalization of existing
vision-language models on unseen tasks via a simple yet efficient fine-tuning
strategy based on uniform task sampling. We term our method as Model-Agnostic
Multitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level
optimization and uses only first-order gradients, which makes it easily
scalable and computationally efficient. Due to the uniform task sampling
procedure, MAMF consistently outperforms the classical fine-tuning method for
few-shot transfer learning on five benchmark datasets. Empirically, we further
discover that the effectiveness of first-order MAML is highly dependent on the
zero-shot performance of the pretrained model, and our simple algorithm can
outperform first-order MAML on more challenging datasets with low zero-shot
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose Guided Multi-person Image Generation From Text. (arXiv:2203.04907v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04907">
<div class="article-summary-box-inner">
<span><p>Transformers have recently been shown to generate high quality images from
texts. However, existing methods struggle to create high fidelity full-body
images, especially multiple people. A person's pose has a high degree of
freedom that is difficult to describe using words only; this creates errors in
the generated image, such as incorrect body proportions and pose. We propose a
pose-guided text-to-image model, using pose as an additional input constraint.
Using the proposed Keypoint Pose Encoding (KPE) to encode human pose into low
dimensional representation, our model can generate novel multi-person images
accurately representing the pose and text descriptions provided, with minimal
errors. We demonstrate that KPE is invariant to changes in the target image
domain and image resolution; we show results on the Deepfashion dataset and
create a new multi-person Deepfashion dataset to demonstrate the
multi-capabilities of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DUAL: Textless Spoken Question Answering with Speech Discrete Unit Adaptive Learning. (arXiv:2203.04911v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04911">
<div class="article-summary-box-inner">
<span><p>Spoken Question Answering (SQA) has gained research attention and made
remarkable progress in recent years. However, existing SQA methods rely on
Automatic Speech Recognition (ASR) transcripts, which are time and
cost-prohibitive to collect. This work proposes an ASR transcript-free SQA
framework named Discrete Unit Adaptive Learning (DUAL), which leverages
unlabeled data for pre-training and is fine-tuned by the SQA downstream task.
DAUL can directly predict the time interval of the spoken answer from the
spoken document. We also release a new SQA benchmark corpus Natural
Multi-speaker Spoken Question Answering (NMSQA) for testing SQA in realistic
scenarios. The experimental results show that DUAL performs competitively with
the cascade approach (ASR + text QA), and DUAL is robust to real-world speech.
We will open-source our code and model to inspire more SQA innovations from the
community
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05377">
<div class="article-summary-box-inner">
<span><p>In this demonstration, we present an efficient BERT-based multi-task (MT)
framework that is particularly suitable for iterative and incremental
development of the tasks. The proposed framework is based on the idea of
partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the
other layers frozen. For each task, we train independently a single-task (ST)
model using partial fine-tuning. Then we compress the task-specific layers in
each ST model using knowledge distillation. Those compressed ST models are
finally merged into one MT model so that the frozen layers of the former are
shared across the tasks. We exemplify our approach on eight GLUE tasks,
demonstrating that it is able to achieve both strong performance and
efficiency. We have implemented our method in the utterance understanding
system of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate
that our model reduces the overall serving cost by 86%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Headed-Span-Based Projective Dependency Parsing. (arXiv:2108.04750v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04750">
<div class="article-summary-box-inner">
<span><p>We propose a new method for projective dependency parsing based on headed
spans. In a projective dependency tree, the largest subtree rooted at each word
covers a contiguous sequence (i.e., a span) in the surface order. We call such
a span marked by a root word \textit{headed span}.
</p>
<p>A projective dependency tree can be represented as a collection of headed
spans. We decompose the score of a dependency tree into the scores of the
headed spans and design a novel $O(n^3)$ dynamic programming algorithm to
enable global training and exact inference. Our model achieves state-of-the-art
or competitive results on PTB, CTB, and UD. Our code is publicly available at
\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining (second-order) graph-based and headed-span-based projective dependency parsing. (arXiv:2108.05838v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05838">
<div class="article-summary-box-inner">
<span><p>Graph-based methods, which decompose the score of a dependency tree into
scores of dependency arcs, are popular in dependency parsing for decades.
Recently, \citet{Yang2022Span} propose a headed-span-based method that
decomposes the score of a dependency tree into scores of headed spans. They
show improvement over first-order graph-based methods. However, their method
does not score dependency arcs at all, and dependency arcs are implicitly
induced by their cubic-time algorithm, which is possibly sub-optimal since
modeling dependency arcs is intuitively useful. In this work, we aim to combine
graph-based and headed-span-based methods, incorporating both arc scores and
headed span scores into our model. First, we show a direct way to combine with
$O(n^4)$ parsing complexity. To decrease complexity, inspired by the classical
head-splitting trick, we show two $O(n^3)$ dynamic programming algorithms to
combine first- and second-order graph-based and headed-span-based methods. Our
experiments on PTB, CTB, and UD show that combining first-order graph-based and
headed-span-based methods is effective. We also confirm the effectiveness of
second-order graph-based parsing in the deep learning age, however, we observe
marginal or no improvement when combining second-order graph-based and
headed-span-based methods. Our code is publicly available at
\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks. (arXiv:2110.05419v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05419">
<div class="article-summary-box-inner">
<span><p>Constituency parsing and nested named entity recognition (NER) are similar
tasks since they both aim to predict a collection of nested and non-crossing
spans. In this work, we cast nested NER to constituency parsing and propose a
novel pointing mechanism for bottom-up parsing to tackle both tasks. The key
idea is based on the observation that if we traverse a constituency tree in
post-order, i.e., visiting a parent after its children, then two consecutively
visited spans would share a boundary. Our model tracks the shared boundaries
and predicts the next boundary at each step by leveraging a pointer network. As
a result, it needs only linear steps to parse and thus is efficient. It also
maintains a parsing configuration for structural consistency, i.e., always
outputting valid trees. Experimentally, our model achieves the state-of-the-art
performance on PTB among all BERT-based models (96.01 F1 score) and competitive
performance on CTB7 in constituency parsing; and it also achieves strong
performance on three benchmark datasets of nested NER: ACE2004, ACE2005, and
GENIA. Our code is publicly available at
\url{https://github.com/sustcsonglin/pointer-net-for-nested}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition. (arXiv:2110.07480v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07480">
<div class="article-summary-box-inner">
<span><p>Nested entities are observed in many domains due to their compositionality,
which cannot be easily recognized by the widely-used sequence labeling
framework. A natural solution is to treat the task as a span classification
problem. To learn better span representation and increase classification
performance, it is crucial to effectively integrate heterogeneous factors
including inside tokens, boundaries, labels, and related spans which could be
contributing to nested entities recognition. To fuse these heterogeneous
factors, we propose a novel triaffine mechanism including triaffine attention
and scoring. Triaffine attention uses boundaries and labels as queries and uses
inside tokens and related spans as keys and values for span representations.
Triaffine scoring interacts with boundaries and span representations for
classification. Experiments show that our proposed method outperforms previous
span-based methods, achieves the state-of-the-art $F_1$ scores on nested NER
datasets GENIA and KBP2017, and shows comparable results on ACE2004 and
ACE2005.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prix-LM: Pretraining for Multilingual Knowledge Base Construction. (arXiv:2110.08443v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08443">
<div class="article-summary-box-inner">
<span><p>Knowledge bases (KBs) contain plenty of structured world and commonsense
knowledge. As such, they often complement distributional text-based information
and facilitate various downstream tasks. Since their manual construction is
resource- and time-intensive, recent efforts have tried leveraging large
pretrained language models (PLMs) to generate additional monolingual knowledge
facts for KBs. However, such methods have not been attempted for building and
enriching multilingual KBs. Besides wider application, such multilingual KBs
can provide richer combined knowledge than monolingual (e.g., English) KBs.
Knowledge expressed in different languages may be complementary and unequally
distributed: this implies that the knowledge available in high-resource
languages can be transferred to low-resource ones. To achieve this, it is
crucial to represent multilingual knowledge in a shared/unified space. To this
end, we propose a unified representation model, Prix-LM, for multilingual KB
construction and completion. We leverage two types of knowledge, monolingual
triples and cross-lingual links, extracted from existing multilingual KBs, and
tune a multilingual language encoder XLM-R via a causal language modeling
objective. Prix-LM integrates useful multilingual and KB-based factual
knowledge into a single model. Experiments on standard entity-related tasks,
such as link prediction in multiple languages, cross-lingual entity linking and
bilingual lexicon induction, demonstrate its effectiveness, with gains reported
over strong task-specialised baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trajectory Prediction with Linguistic Representations. (arXiv:2110.09741v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09741">
<div class="article-summary-box-inner">
<span><p>Language allows humans to build mental models that interpret what is
happening around them resulting in more accurate long-term predictions. We
present a novel trajectory prediction model that uses linguistic intermediate
representations to forecast trajectories, and is trained using trajectory
samples with partially-annotated captions. The model learns the meaning of each
of the words without direct per-word supervision. At inference time, it
generates a linguistic description of trajectories which captures maneuvers and
interactions over an extended time interval. This generated description is used
to refine predictions of the trajectories of multiple agents. We train and
validate our model on the Argoverse dataset, and demonstrate improved accuracy
results in trajectory prediction. In addition, our model is more interpretable:
it presents part of its reasoning in plain language as captions, which can aid
model development and can aid in building confidence in the model before
deploying it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel knowledge graph development for industry design: A case study on indirect coal liquefaction process. (arXiv:2111.13854v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13854">
<div class="article-summary-box-inner">
<span><p>Hazard and operability analysis (HAZOP) is a remarkable representative in
industrial safety engineering. However, a great storehouse of industrial safety
knowledge (ISK) in HAZOP reports has not been thoroughly exploited. In order to
reuse and unlock the value of ISK and optimize HAZOP, we have developed a novel
knowledge graph for industrial safety (ISKG) with HAZOP as the carrier through
bridging data science and engineering design. Specifically, firstly,
considering that the knowledge contained in HAZOP reports of different
processes in industry is not the same, we creatively develope a general ISK
standardization framework, it provides a practical scheme for integrating HAZOP
reports from various processes and uniformly representing the ISK with diverse
expressions. Secondly, we conceive a novel and reliable information extraction
model based on deep learning combined with data science, it can effectively
mine ISK from HAZOP reports, which alleviates the obstacle of ISK extraction
caused by the particularity of HAZOP text. Finally, we build ISK triples and
store them in the Neo4j graph database. We take indirect coal liquefaction
process as a case study to develop ISKG, and its oriented applications can
optimize HAZOP and mine the potential of ISK, which is of great significance to
improve the security of the system and enhance prevention awareness for people.
ISKG containing the ISK standardization framework and the information
extraction model sets an example of the interaction between data science and
engineering design, which can enlighten other researchers and extend the
perspectives of industrial safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Editing for Counterfactual Stories. (arXiv:2112.05417v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05417">
<div class="article-summary-box-inner">
<span><p>Creating what-if stories requires reasoning about prior statements and
possible outcomes of the changed conditions. One can easily generate coherent
endings under new conditions, but it would be challenging for current systems
to do it with minimal changes to the original story. Therefore, one major
challenge is the trade-off between generating a logical story and rewriting
with minimal-edits. In this paper, we propose EDUCAT, an editing-based
unsupervised approach for counterfactual story rewriting. EDUCAT includes a
target position detection strategy based on estimating causal effects of the
what-if conditions, which keeps the causal invariant parts of the story. EDUCAT
then generates the stories under fluency, coherence and minimal-edits
constraints. We also propose a new metric to alleviate the shortcomings of
current automatic metrics and better evaluate the trade-off. We evaluate EDUCAT
on a public counterfactual story rewriting benchmark. Experiments show that
EDUCAT achieves the best trade-off over unsupervised SOTA methods according to
both automatic and human evaluation. The resources of EDUCAT are available at:
https://github.com/jiangjiechen/EDUCAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19. (arXiv:2201.07423v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07423">
<div class="article-summary-box-inner">
<span><p>Loneliness has been associated with negative outcomes for physical and mental
health. Understanding how people express and cope with various forms of
loneliness is critical for early screening and targeted interventions to reduce
loneliness, particularly among vulnerable groups such as young adults. To
examine how different forms of loneliness and coping strategies manifest in
loneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained
Loneliness) by using Reddit posts in two young adult-focused forums and two
loneliness related forums consisting of a diverse age group. We provide
annotations by trained human annotators for binary and fine-grained loneliness
classifications of the posts. Trained on FIG-Loneliness, two BERT-based models
were used to understand loneliness forms and authors' coping strategies in
these forums. Our binary loneliness classification achieved an accuracy above
97%, and fine-grained loneliness category classification reached an average
accuracy of 77% across all labeled categories. With FIG-Loneliness and model
predictions, we found that loneliness expressions in the young adult related
forums are distinct from other forums. Those in young adult-focused forums are
more likely to express concerns pertaining to peer relationship, and are
potentially more sensitive to geographical isolation impacted by the COVID-19
pandemic lockdown. Also, we show that different forms of loneliness have
differential use in coping strategies. For example, those who experienced
transient and social loneliness had stronger desires to reach out and connect
with others, whereas individuals who experienced physical and romantic
loneliness were more likely to seek advice from others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01993">
<div class="article-summary-box-inner">
<span><p>Visual question answering is the task of answering questions about images. We
introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually
grounds answers to visual questions asked by people with visual impairments. We
analyze our dataset and compare it with five VQA-Grounding datasets to
demonstrate what makes it similar and different. We then evaluate the SOTA VQA
and VQA-Grounding models and demonstrate that current SOTA algorithms often
fail to identify the correct visual evidence where the answer is located. These
models regularly struggle when the visual evidence occupies a small fraction of
the image, for images that are higher quality, as well as for visual questions
that require skills in text recognition. The dataset, evaluation server, and
leaderboard all can be found at the following link:
https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification. (arXiv:2203.02225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02225">
<div class="article-summary-box-inner">
<span><p>Generating new events given context with correlated ones plays a crucial role
in many event-centric reasoning tasks. Existing works either limit their scope
to specific scenarios or overlook event-level correlations. In this paper, we
propose to pre-train a general Correlation-aware context-to-Event Transformer
(ClarET) for event-centric reasoning. To achieve this, we propose three novel
event-centric objectives, i.e., whole event recovering, contrastive
event-correlation encoding and prompt-based event locating, which highlight
event-level correlations with effective training. The proposed ClarET is
applicable to a wide range of event-centric reasoning scenarios, considering
its versatility of (i) event-correlation types (e.g., causal, temporal,
contrast), (ii) application formulations (i.e., generation and classification),
and (iii) reasoning types (e.g., abductive, counterfactual and ending
reasoning). Empirical fine-tuning results, as well as zero- and few-shot
learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4
reasoning types with diverse event correlations), verify its effectiveness and
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClueGraphSum: Let Key Clues Guide the Cross-Lingual Abstractive Summarization. (arXiv:2203.02797v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02797">
<div class="article-summary-box-inner">
<span><p>Cross-Lingual Summarization (CLS) is the task to generate a summary in one
language for an article in a different language. Previous studies on CLS mainly
take pipeline methods or train the end-to-end model using the translated
parallel data. However, the quality of generated cross-lingual summaries needs
more further efforts to improve, and the model performance has never been
evaluated on the hand-written CLS dataset. Therefore, we first propose a
clue-guided cross-lingual abstractive summarization method to improve the
quality of cross-lingual summaries, and then construct a novel hand-written CLS
dataset for evaluation. Specifically, we extract keywords, named entities, etc.
of the input article as key clues for summarization and then design a
clue-guided algorithm to transform an article into a graph with less noisy
sentences. One Graph encoder is built to learn sentence semantics and article
structures and one Clue encoder is built to encode and translate key clues,
ensuring the information of important parts are reserved in the generated
summary. These two encoders are connected by one decoder to directly learn
cross-lingual semantics. Experimental results show that our method has stronger
robustness for longer inputs and substantially improves the performance over
the strong baseline, achieving an improvement of 8.55 ROUGE-1
(English-to-Chinese summarization) and 2.13 MoverScore (Chinese-to-English
summarization) scores over the existing SOTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ILDAE: Instance-Level Difficulty Analysis of Evaluation Data. (arXiv:2203.03073v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03073">
<div class="article-summary-box-inner">
<span><p>Knowledge of questions' difficulty level helps a teacher in several ways,
such as estimating students' potential quickly by asking carefully selected
questions and improving quality of examination by modifying trivial and hard
questions. Can we extract such benefits of instance difficulty in NLP? To this
end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE)
in a large-scale setup of 23 datasets and demonstrate its five novel
applications: 1) conducting efficient-yet-accurate evaluations with fewer
instances saving computational cost and time, 2) improving quality of existing
evaluation datasets by repairing erroneous and trivial instances, 3) selecting
the best model based on application requirements, 4) analyzing dataset
characteristics for guiding future data creation, 5) estimating Out-of-Domain
performance reliably. Comprehensive experiments for these applications result
in several interesting findings, such as evaluation using just 5% instances
(selected via ILDAE) achieves as high as 0.93 Kendall correlation with
evaluation using complete dataset and computing weighted accuracy using
difficulty scores leads to 5.2% higher correlation with Out-of-Domain
performance. We release the difficulty scores and hope our analyses and
findings will bring more attention to this important yet understudied field of
leveraging instance difficulty in evaluations.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Aligner Network. (arXiv:2203.04290v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04290">
<div class="article-summary-box-inner">
<span><p>Image registration is important for medical imaging, the estimation of the
spatial transformation between different images. Many previous studies have
used learning-based methods for coarse-to-fine registration to efficiently
perform 3D image registration. The coarse-to-fine approach, however, is limited
when dealing with the different motions of nearby objects. Here we propose a
novel Motion-Aware (MA) structure that captures the different motions in a
region. The MA structure incorporates a novel Residual Aligner (RA) module
which predicts the multi-head displacement field used to disentangle the
different motions of multiple neighbouring objects. Compared with other deep
learning methods, the network based on the MA structure and RA module achieve
one of the most accurate unsupervised inter-subject registration on the 9
organs of assorted sizes in abdominal CT scans, with the highest-ranked
registration of the veins (Dice Similarity Coefficient / Average surface
distance: 62\%/4.9mm for the vena cava and 34\%/7.9mm for the portal and
splenic vein), with a half-sized structure and more efficient computation.
Applied to the segmentation of lungs in chest CT scans, the new network
achieves results which were indistinguishable from the best-ranked networks
(94\%/3.0mm). Additionally, the theorem on predicted motion pattern and the
design of MA structure are validated by further analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Few Examples: A Summary of Approaches to Few-Shot Learning. (arXiv:2203.04291v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04291">
<div class="article-summary-box-inner">
<span><p>Few-Shot Learning refers to the problem of learning the underlying pattern in
the data just from a few training samples. Requiring a large number of data
samples, many deep learning solutions suffer from data hunger and extensively
high computation time and resources. Furthermore, data is often not available
due to not only the nature of the problem or privacy concerns but also the cost
of data preparation. Data collection, preprocessing, and labeling are strenuous
human tasks. Therefore, few-shot learning that could drastically reduce the
turnaround time of building machine learning applications emerges as a low-cost
solution. This survey paper comprises a representative list of recently
proposed few-shot learning algorithms. Given the learning dynamics and
characteristics, the approaches to few-shot learning problems are discussed in
the perspectives of meta-learning, transfer learning, and hybrid approaches
(i.e., different variations of the few-shot learning problem).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards performant and reliable undersampled MR reconstruction via diffusion model sampling. (arXiv:2203.04292v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04292">
<div class="article-summary-box-inner">
<span><p>Magnetic Resonance (MR) image reconstruction from under-sampled acquisition
promises faster scanning time. To this end, current State-of-The-Art (SoTA)
approaches leverage deep neural networks and supervised training to learn a
recovery model. While these approaches achieve impressive performances, the
learned model can be fragile on unseen degradation, e.g. when given a different
acceleration factor. These methods are also generally deterministic and provide
a single solution to an ill-posed problem; as such, it can be difficult for
practitioners to understand the reliability of the reconstruction. We introduce
DiffuseRecon, a novel diffusion model-based MR reconstruction method.
DiffuseRecon guides the generation process based on the observed signals and a
pre-trained diffusion model, and does not require additional training on
specific acceleration factors. DiffuseRecon is stochastic in nature and
generates results from a distribution of fully-sampled MR images; as such, it
allows us to explicitly visualize different potential reconstruction solutions.
Lastly, DiffuseRecon proposes an accelerated, coarse-to-fine Monte-Carlo
sampling scheme to approximate the most likely reconstruction candidate. The
proposed DiffuseRecon achieves SoTA performances reconstructing from raw
acquisition signals in fastMRI and SKM-TEA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NaviAirway: a bronchiole-sensitive deep learning-based airway segmentation pipeline for planning of navigation bronchoscopy. (arXiv:2203.04294v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04294">
<div class="article-summary-box-inner">
<span><p>Navigation bronchoscopy is a minimally invasive procedure in which doctors
pass a bronchoscope into a subject's airways to sample the target pulmonary
lesion. A three-dimensional (3D) airway roadmap reconstructed from Computer
Tomography (CT) scans is a prerequisite for this procedure, especially when the
target is distally located. Therefore, an accurate and efficient airway
segmentation algorithm is essential to reduce bronchoscopists' burden of
pre-procedural airway identification as well as patients' discomfort during the
prolonged procedure. However, airway segmentation remains a challenging task
because of the intrinsic complex tree-like structure, imbalanced sizes of
airway branches, potential domain shifts of CT scans, and few available labeled
images. To address these problems, we present a deep learning-based pipeline,
denoted as NaviAirway, which finds finer bronchioles through four major novel
components - feature extractor modules in model architecture design, a
bronchiole-sensitive loss function, a human-vision-inspired iterative training
strategy, and a semi-supervised learning framework to utilize unlabeled CT
images. Experimental results showed that NaviAirway outperformed existing
methods, particularly in identification of higher generation bronchioles and
robustness to new CT scans. On average, NaviAirway takes five minutes to
segment the CT scans of one patient on a GPU-embedded computer. Moreover, we
propose two new metrics to complement conventional ones for a more
comprehensive and fairer evaluation of deep learning-based airway segmentation
approaches. The code is publicly available on
https://github.com/AntonotnaWang/NaviAirway.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region Specific Optimization (RSO)-based Deep Interactive Registration. (arXiv:2203.04295v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04295">
<div class="article-summary-box-inner">
<span><p>Medical image registration is a fundamental and vital task which will affect
the efficacy of many downstream clinical tasks. Deep learning (DL)-based
deformable image registration (DIR) methods have been investigated, showing
state-of-the-art performance. A test time optimization (TTO) technique was
proposed to further improve the DL models' performance. Despite the substantial
accuracy improvement with this TTO technique, there still remained some regions
that exhibited large registration errors even after many TTO iterations. To
mitigate this challenge, we firstly identified the reason why the TTO technique
was slow, or even failed, to improve those regions' registration results. We
then proposed a two-levels TTO technique, i.e., image-specific optimization
(ISO) and region-specific optimization (RSO), where the region can be
interactively indicated by the clinician during the registration result
reviewing process. For both efficiency and accuracy, we further envisioned a
three-step DL-based image registration workflow. Experimental results showed
that our proposed method outperformed the conventional method qualitatively and
quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-free Domain Adaptation for Multi-site and Lifespan Brain Skull Stripping. (arXiv:2203.04299v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04299">
<div class="article-summary-box-inner">
<span><p>Skull stripping is a crucial prerequisite step in the analysis of brain
magnetic resonance (MR) images. Although many excellent works or tools have
been proposed, they suffer from low generalization capability. For instance,
the model trained on a dataset with specific imaging parameters (source domain)
cannot be well applied to other datasets with different imaging parameters
(target domain). Especially, for the lifespan datasets, the model trained on an
adult dataset is not applicable to an infant dataset due to the large domain
difference. To address this issue, numerous domain adaptation (DA) methods have
been proposed to align the extracted features between the source and target
domains, requiring concurrent access to the input images of both domains.
Unfortunately, it is problematic to share the images due to privacy. In this
paper, we design a source-free domain adaptation framework (SDAF) for
multi-site and lifespan skull stripping that can accomplish domain adaptation
without access to source domain images. Our method only needs to share the
source labels as shape dictionaries and the weights trained on the source data,
without disclosing private information from source domain subjects. To deal
with the domain shift between multi-site lifespan datasets, we take advantage
of the brain shape prior which is invariant to imaging parameters and ages.
Experiments demonstrate that our framework can significantly outperform the
state-of-the-art methods on multi-site lifespan datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UENAS: A Unified Evolution-based NAS Framework. (arXiv:2203.04300v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04300">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) has gained significant attention for
automatic network design in recent years. Previous NAS methods suffer from
limited search spaces, which may lead to sub-optimal results. In this paper, we
propose UENAS, an evolution-based NAS framework with a broader search space
that supports optimizing network architectures, pruning strategies, and
hyperparameters simultaneously. To alleviate the huge search cost caused by the
expanded search space, three strategies are adopted: First, an adaptive pruning
strategy that iteratively trims the average model size in the population
without compromising performance. Second, child networks share weights of
overlapping layers with pre-trained parent networks to reduce the training
epochs. Third, an online predictor scores the joint representations of
architecture, pruning strategy, and hyperparameters to filter out inferior
combos. By the proposed three strategies, the search efficiency is
significantly improved and more well-performed compact networks with tailored
hyper-parameters are derived. In experiments, UENAS achieves error rates of
2.81% on CIFAR-10, 20.24% on CIFAR-100, and 33% on Tiny-ImageNet, which shows
the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Live Laparoscopic Video Retrieval with Compressed Uncertainty. (arXiv:2203.04301v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04301">
<div class="article-summary-box-inner">
<span><p>Searching through large volumes of medical data to retrieve relevant
information is a challenging yet crucial task for clinical care. However the
primitive and most common approach to retrieval, involving text in the form of
keywords, is severely limited when dealing with complex media formats.
Content-based retrieval offers a way to overcome this limitation, by using rich
media as the query itself. Surgical video-to-video retrieval in particular is a
new and largely unexplored research problem with high clinical value,
especially in the real-time case: using real-time video hashing, search can be
achieved directly inside of the operating room. Indeed, the process of hashing
converts large data entries into compact binary arrays or hashes, enabling
large-scale search operations at a very fast rate. However, due to fluctuations
over the course of a video, not all bits in a given hash are equally reliable.
In this work, we propose a method capable of mitigating this uncertainty while
maintaining a light computational footprint. We present superior retrieval
results (3-4 % top 10 mean average precision) on a multi-task evaluation
protocol for surgery, using cholecystectomy phases, bypass phases, and coming
from an entirely new dataset introduced here, critical events across six
different surgery types. Success on this multi-task benchmark shows the
generalizability of our approach for surgical video retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperPoint features in endoscopy. (arXiv:2203.04302v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04302">
<div class="article-summary-box-inner">
<span><p>There is often a significant gap between research results and applicability
in routine medical practice. This work studies the performance of well-known
local features on a medical dataset captured during routine colonoscopy
procedures. Local feature extraction and matching is a key step for many
computer vision applications, specially regarding 3D modelling. In the medical
domain, handcrafted local features such as SIFT, with public pipelines such as
COLMAP, are still a predominant tool for this kind of tasks. We explore the
potential of the well known self-supervised approach SuperPoint, present an
adapted variation for the endoscopic domain and propose a challenging
evaluation framework. SuperPoint based models achieve significantly higher
matching quality than commonly used local features in this domain. Our adapted
model avoids features within specularity regions, a frequent and problematic
artifact in endoscopic images, with consequent benefits for matching and
reconstruction results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Dual-Output Diffusion Models. (arXiv:2203.04304v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04304">
<div class="article-summary-box-inner">
<span><p>Iterative denoising-based generation, also known as denoising diffusion
models, has recently been shown to be comparable in quality to other classes of
generative models, and even surpass them. Including, in particular, Generative
Adversarial Networks, which are currently the state of the art in many
sub-tasks of image generation. However, a major drawback of this method is that
it requires hundreds of iterations to produce a competitive result. Recent
works have proposed solutions that allow for faster generation with fewer
iterations, but the image quality gradually deteriorates with increasingly
fewer iterations being applied during generation. In this paper, we reveal some
of the causes that affect the generation quality of diffusion models,
especially when sampling with few iterations, and come up with a simple, yet
effective, solution to mitigate them. We consider two opposite equations for
the iterative denoising, the first predicts the applied noise, and the second
predicts the image directly. Our solution takes the two options and learns to
dynamically alternate between them through the denoising process. Our proposed
solution is general and can be applied to any existing diffusion model. As we
show, when applied to various SOTA architectures, our solution immediately
improves their generation quality, with negligible added complexity and
parameters. We experiment on multiple datasets and configurations and run an
extensive ablation study to support these findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Models for Medical Anomaly Detection. (arXiv:2203.04306v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04306">
<div class="article-summary-box-inner">
<span><p>In medical applications, weakly supervised anomaly detection methods are of
great interest, as only image-level annotations are required for training.
Current anomaly detection methods mainly rely on generative adversarial
networks or autoencoder models. Those models are often complicated to train or
have difficulties to preserve fine details in the image. We present a novel
weakly supervised anomaly detection method based on denoising diffusion
implicit models. We combine the deterministic iterative noising and denoising
scheme with classifier guidance for image-to-image translation between diseased
and healthy subjects. Our method generates very detailed anomaly maps without
the need for a complex training procedure. We evaluate our method on the
BRATS2020 dataset for brain tumor detection and the CheXpert dataset for
detecting pleural effusions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breast cancer detection using artificial intelligence techniques: A systematic literature review. (arXiv:2203.04308v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04308">
<div class="article-summary-box-inner">
<span><p>Cancer is one of the most dangerous diseases to humans, and yet no permanent
cure has been developed for it. Breast cancer is one of the most common cancer
types. According to the National Breast Cancer foundation, in 2020 alone, more
than 276,000 new cases of invasive breast cancer and more than 48,000
non-invasive cases were diagnosed in the US. To put these figures in
perspective, 64% of these cases are diagnosed early in the disease's cycle,
giving patients a 99% chance of survival. Artificial intelligence and machine
learning have been used effectively in detection and treatment of several
dangerous diseases, helping in early diagnosis and treatment, and thus
increasing the patient's chance of survival. Deep learning has been designed to
analyze the most important features affecting detection and treatment of
serious diseases. For example, breast cancer can be detected using genes or
histopathological imaging. Analysis at the genetic level is very expensive, so
histopathological imaging is the most common approach used to detect breast
cancer. In this research work, we systematically reviewed previous work done on
detection and treatment of breast cancer using genetic sequencing or
histopathological imaging with the help of deep learning and machine learning.
We also provide recommendations to researchers who will work in this field
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Scale Adaptive Network for Single Image Denoising. (arXiv:2203.04313v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04313">
<div class="article-summary-box-inner">
<span><p>Multi-scale architectures have shown effectiveness in a variety of tasks
including single image denoising, thanks to appealing cross-scale
complementarity. However, existing methods treat different scale features
equally without considering their scale-specific characteristics, i.e., the
within-scale characteristics are ignored. In this paper, we reveal this missing
piece for multi-scale architecture design and accordingly propose a novel
Multi-Scale Adaptive Network (MSANet) for single image denoising. To be
specific, MSANet simultaneously embraces the within-scale characteristics and
the cross-scale complementarity thanks to three novel neural blocks, i.e.,
adaptive feature block (AFeB), adaptive multi-scale block (AMB), and adaptive
fusion block (AFuB). In brief, AFeB is designed to adaptively select details
and filter noises, which is highly expected for fine-grained features. AMB
could enlarge the receptive field and aggregate the multi-scale information,
which is designed to satisfy the demands of both fine- and coarse-grained
features. AFuB devotes to adaptively sampling and transferring the features
from one scale to another scale, which is used to fuse the features with
varying characteristics from coarse to fine. Extensive experiments on both
three real and six synthetic noisy image datasets show the superiority of
MSANet compared with 12 methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PyNET-QxQ: A Distilled PyNET for QxQ Bayer Pattern Demosaicing in CMOS Image Sensor. (arXiv:2203.04314v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04314">
<div class="article-summary-box-inner">
<span><p>The deep learning-based ISP models for mobile cameras produce high-quality
images comparable to the professional DSLR camera. However, many of them are
computationally expensive, which may not be appropriate for mobile
environments. Also, the recent mobile cameras adopt non-Bayer CFAs (e.g., Quad
Bayer, Nona Bayer, and QxQ Bayer) to improve image quality; however, most deep
learning-based ISP models mainly focus on standard Bayer CFA. In this work, we
propose PyNET-QxQ based on PyNET, a light-weighted ISP explicitly designed for
the QxQ CFA pattern. The number of parameters of PyNET-QxQ is less than 2.5% of
PyNET. We also introduce a novel knowledge distillation technique, progressive
distillation, to train the compressed network effectively. Finally, experiments
with QxQ images (obtained by an actual QxQ camera sensor, under development)
demonstrate the outstanding performance of PyNET-QxQ despite significant
parameter reductions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent. (arXiv:2203.04317v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04317">
<div class="article-summary-box-inner">
<span><p>Image registration is the process of bringing different images into a common
coordinate system - a technique widely used in various applications of computer
vision, such as remote sensing, image retrieval, and most commonly in medical
imaging. Deep Learning based techniques have been applied successfully to
tackle various complex medical image processing problems, including medical
image registration. Over the years, several image registration techniques have
been proposed using deep learning. Deformable image registration techniques
such as Voxelmorph have been successful in capturing finer changes and
providing smoother deformations. However, Voxelmorph, as well as ICNet and
FIRE, do not explicitly encode global dependencies (i.e. the overall anatomical
view of the supplied image) and therefore can not track large deformations. In
order to tackle the aforementioned problems, this paper extends the Voxelmorph
approach in three different ways. To improve the performance in case of small
as well as large deformations, supervision of the model at different
resolutions have been integrated using a multi-scale UNet. To support the
network to learn and encode the minute structural co-relations of the given
image-pairs, a self-constructing graph network (SCGNet) has been used as the
latent of the multi-scale UNet - which can improve the learning process of the
model and help the model to generalise better. And finally, to make the
deformations inverse-consistent, cycle consistency loss has been employed. On
the task of registration of brain MRIs, the proposed method achieved
significant improvements over ANTs and VoxelMorph, obtaining a Dice score of
0.8013$\pm$0.0243 for intramodal and 0.6211$\pm$0.0309 for intermodal, while
VoxelMorph achieved 0.7747$\pm$0.0260 and 0.6071$\pm$0.0510, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unrolled Primal-Dual Networks for Lensless Cameras. (arXiv:2203.04353v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04353">
<div class="article-summary-box-inner">
<span><p>Conventional image reconstruction models for lensless cameras often assume
that each measurement results from convolving a given scene with a single
experimentally measured point-spread function. These image reconstruction
models fall short in simulating lensless cameras truthfully as these models are
not sophisticated enough to account for optical aberrations or scenes with
depth variations. Our work shows that learning a supervised primal-dual
reconstruction method results in image quality matching state of the art in the
literature without demanding a large network capacity. This improvement stems
from our primary finding that embedding learnable forward and adjoint models in
a learned primal-dual optimization framework can even improve the quality of
reconstructed images (+5dB PSNR) compared to works that do not correct for the
model error. In addition, we built a proof-of-concept lensless camera prototype
that uses a pseudo-random phase mask to demonstrate our point. Finally, we
share the extensive evaluation of our learned model based on an open dataset
and a dataset from our proof-of-concept lensless camera prototype.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches. (arXiv:2203.04412v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04412">
<div class="article-summary-box-inner">
<span><p>Adversarial patches are optimized contiguous pixel blocks in an input image
that cause a machine-learning model to misclassify it. However, their
optimization is computationally demanding, and requires careful hyperparameter
tuning, potentially leading to suboptimal robustness evaluations. To overcome
these issues, we propose ImageNet-Patch, a dataset to benchmark
machine-learning models against adversarial patches. It consists of a set of
patches, optimized to generalize across different models, and readily
applicable to ImageNet data after preprocessing them with affine
transformations. This process enables an approximate yet faster robustness
evaluation, leveraging the transferability of adversarial perturbations. We
showcase the usefulness of this dataset by testing the effectiveness of the
computed patches against 127 models. We conclude by discussing how our dataset
could be used as a benchmark for robustness, and how our methodology can be
generalized to other domains. We open source our dataset and evaluation code at
https://github.com/pralab/ImageNet-Patch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Flag Median and FlagIRLS. (arXiv:2203.04437v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04437">
<div class="article-summary-box-inner">
<span><p>Finding prototypes (e.g., mean and median) for a dataset is central to a
number of common machine learning algorithms. Subspaces have been shown to
provide useful, robust representations for datasets of images, videos and more.
Since subspaces correspond to points on a Grassmann manifold, one is led to
consider the idea of a subspace prototype for a Grassmann-valued dataset. While
a number of different subspace prototypes have been described, the calculation
of some of these prototypes has proven to be computationally expensive while
other prototypes are affected by outliers and produce highly imperfect
clustering on noisy data. This work proposes a new subspace prototype, the flag
median, and introduces the FlagIRLS algorithm for its calculation. We provide
evidence that the flag median is robust to outliers and can be used effectively
in algorithms like Linde-Buzo-Grey (LBG) to produce improved clusterings on
Grassmannians. Numerical experiments include a synthetic dataset, the MNIST
handwritten digits dataset, the Mind's Eye video dataset and the UCF YouTube
action dataset. The flag median is compared the other leading algorithms for
computing prototypes on the Grassmannian, namely, the $\ell_2$-median and to
the flag mean. We find that using FlagIRLS to compute the flag median converges
in $4$ iterations on a synthetic dataset. We also see that Grassmannian LBG
with a codebook size of $20$ and using the flag median produces at least a
$10\%$ improvement in cluster purity over Grassmannian LBG using the flag mean
or $\ell_2$-median on the Mind's Eye dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pointillism: Accurate 3D bounding box estimation with multi-radars. (arXiv:2203.04440v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04440">
<div class="article-summary-box-inner">
<span><p>Autonomous perception requires high-quality environment sensing in the form
of 3D bounding boxes of dynamic objects. The primary sensors used in automotive
systems are light-based cameras and LiDARs. However, they are known to fail in
adverse weather conditions. Radars can potentially solve this problem as they
are barely affected by adverse weather conditions. However, specular
reflections of wireless signals cause poor performance of radar point clouds.
We introduce Pointillism, a system that combines data from multiple spatially
separated radars with an optimal separation to mitigate these problems. We
introduce a novel concept of Cross Potential Point Clouds, which uses the
spatial diversity induced by multiple radars and solves the problem of noise
and sparsity in radar point clouds. Furthermore, we present the design of
RP-net, a novel deep learning architecture, designed explicitly for radar's
sparse data distribution, to enable accurate 3D bounding box estimation. The
spatial techniques designed and proposed in this paper are fundamental to
radars point cloud distribution and would benefit other radar sensing
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervision, Remote Sensing and Abstraction: Representation Learning Across 3 Million Locations. (arXiv:2203.04445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04445">
<div class="article-summary-box-inner">
<span><p>Self-supervision based deep learning classification approaches have received
considerable attention in academic literature. However, the performance of such
methods on remote sensing imagery domains remains under-explored. In this work,
we explore contrastive representation learning methods on the task of
imagery-based city classification, an important problem in urban computing. We
use satellite and map imagery across 2 domains, 3 million locations and more
than 1500 cities. We show that self-supervised methods can build a
generalizable representation from as few as 200 cities, with representations
achieving over 95\% accuracy in unseen cities with minimal additional training.
We also find that the performance discrepancy of such methods, when compared to
supervised methods, induced by the domain discrepancy between natural imagery
and abstract imagery is significant for remote sensing imagery. We compare all
analysis against existing supervised models from academic literature and
open-source our models for broader usage and further criticism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tune your Place Recognition: Self-Supervised Domain Calibration via Robust SLAM. (arXiv:2203.04446v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04446">
<div class="article-summary-box-inner">
<span><p>Visual place recognition techniques based on deep learning, which have
imposed themselves as the state-of-the-art in recent years, do not always
generalize well to environments that are visually different from the training
set. Thus, to achieve top performance, it is sometimes necessary to fine-tune
the networks to the target environment. To this end, we propose a completely
self-supervised domain calibration procedure based on robust pose graph
estimation from Simultaneous Localization and Mapping (SLAM) as the supervision
signal without requiring GPS or manual labeling. We first show that the
training samples produced by our technique are sufficient to train a visual
place recognition system from a pre-trained classification model. Then, we show
that our approach can improve the performance of a state-of-the-art technique
on a target environment dissimilar from the training set. We believe that this
approach will help practitioners to deploy more robust place recognition
solutions in real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIDER: Exploiting Hyperspherical Embeddings for Out-of-Distribution Detection. (arXiv:2203.04450v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04450">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection is a critical task for reliable machine
learning. Recent advances in representation learning give rise to developments
in distance-based OOD detection, where testing samples are detected as OOD if
they are relatively far away from the centroids or prototypes of
in-distribution (ID) classes. However, prior methods directly take
off-the-shelf loss functions that suffice for classifying ID samples, but are
not optimally designed for OOD detection. In this paper, we propose CIDER, a
simple and effective representation learning framework by exploiting
hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses
to promote strong ID-OOD separability: (1) a dispersion loss that promotes
large angular distances among different class prototypes, and (2) a compactness
loss that encourages samples to be close to their class prototypes. We show
that CIDER is effective under various settings and establishes state-of-the-art
performance. On a hard OOD detection task CIFAR-100 vs. CIFAR-10, our method
substantially improves the AUROC by 14.20% compared to the embeddings learned
by the cross-entropy loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Rotation Representation With an Efficiently Computable Bingham Loss Function and Its Application to Pose Estimation. (arXiv:2203.04456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04456">
<div class="article-summary-box-inner">
<span><p>In recent years, a deep learning framework has been widely used for object
pose estimation. While quaternion is a common choice for rotation
representation of 6D pose, it cannot represent an uncertainty of the
observation. In order to handle the uncertainty, Bingham distribution is one
promising solution because this has suitable features, such as a smooth
representation over SO(3), in addition to the ambiguity representation.
However, it requires the complex computation of the normalizing constants. This
is the bottleneck of loss computation in training neural networks based on
Bingham representation. As such, we propose a fast-computable and
easy-to-implement loss function for Bingham distribution. We also show not only
to examine the parametrization of Bingham distribution but also an application
based on our loss function.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Mosquito Habitat Detection Using Satellite Imagery and Convolutional Neural Networks for Disease Risk Mapping. (arXiv:2203.04463v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04463">
<div class="article-summary-box-inner">
<span><p>Mosquitoes are known vectors for disease transmission that cause over one
million deaths globally each year. The majority of natural mosquito habitats
are areas containing standing water that are challenging to detect using
conventional ground-based technology on a macro scale. Contemporary approaches,
such as drones, UAVs, and other aerial imaging technology are costly when
implemented and are only most accurate on a finer spatial scale whereas the
proposed convolutional neural network(CNN) approach can be applied for disease
risk mapping and further guide preventative efforts on a more global scale. By
assessing the performance of autonomous mosquito habitat detection technology,
the transmission of mosquito-borne diseases can be prevented in a
cost-effective manner. This approach aims to identify the spatiotemporal
distribution of mosquito habitats in extensive areas that are difficult to
survey using ground-based technology by employing computer vision on satellite
imagery for proof of concept. The research presents an evaluation and the
results of 3 different CNN models to determine their accuracy of predicting
large-scale mosquito habitats. For this approach, a dataset was constructed
containing a variety of geographical features. Larger land cover variables such
as ponds/lakes, inlets, and rivers were utilized to classify mosquito habitats
while minute sites were omitted for higher accuracy on a larger scale. Using
the dataset, multiple CNN networks were trained and evaluated for accuracy of
habitat prediction. Utilizing a CNN-based approach on readily available
satellite imagery is cost-effective and scalable, unlike most aerial imaging
technology. Testing revealed that YOLOv4 obtained greater accuracy in mosquito
habitat detection for identifying large-scale mosquito habitats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks. (arXiv:2203.04466v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04466">
<div class="article-summary-box-inner">
<span><p>Neural networks tend to achieve better accuracy with training if they are
larger -- even if the resulting models are overparameterized. Nevertheless,
carefully removing such excess parameters before, during, or after training may
also produce models with similar or even improved accuracy. In many cases, that
can be curiously achieved by heuristics as simple as removing a percentage of
the weights with the smallest absolute value -- even though magnitude is not a
perfect proxy for weight relevance. With the premise that obtaining
significantly better performance from pruning depends on accounting for the
combined effect of removing multiple weights, we revisit one of the classic
approaches for impact-based pruning: the Optimal Brain Surgeon~(OBS). We
propose a tractable heuristic for solving the combinatorial extension of OBS,
in which we select weights for simultaneous removal, as well as a systematic
update of the remaining weights. Our selection method outperforms other methods
under high sparsity, and the weight update is advantageous even when combined
with the other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Part-level Action Parsing via a Pose-guided Coarse-to-Fine Framework. (arXiv:2203.04476v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04476">
<div class="article-summary-box-inner">
<span><p>Action recognition from videos, i.e., classifying a video into one of the
pre-defined action types, has been a popular topic in the communities of
artificial intelligence, multimedia, and signal processing. However, existing
methods usually consider an input video as a whole and learn models, e.g.,
Convolutional Neural Networks (CNNs), with coarse video-level class labels.
These methods can only output an action class for the video, but cannot provide
fine-grained and explainable cues to answer why the video shows a specific
action. Therefore, researchers start to focus on a new task, Part-level Action
Parsing (PAP), which aims to not only predict the video-level action but also
recognize the frame-level fine-grained actions or interactions of body parts
for each person in the video. To this end, we propose a coarse-to-fine
framework for this challenging task. In particular, our framework first
predicts the video-level class of the input video, then localizes the body
parts and predicts the part-level action. Moreover, to balance the accuracy and
computation in part-level action parsing, we propose to recognize the
part-level actions by segment-level features. Furthermore, to overcome the
ambiguity of body parts, we propose a pose-guided positional embedding method
to accurately localize body parts. Through comprehensive experiments on a
large-scale dataset, i.e., Kinetics-TPS, our framework achieves
state-of-the-art performance and outperforms existing methods over a 31.10% ROC
score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3SD: Self-Supervised Saliency Detection With No Labels. (arXiv:2203.04478v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04478">
<div class="article-summary-box-inner">
<span><p>We present a conceptually simple self-supervised method for saliency
detection. Our method generates and uses pseudo-ground truth labels for
training. The generated pseudo-GT labels don't require any kind of human
annotations (e.g., pixel-wise labels or weak labels like scribbles). Recent
works show that features extracted from classification tasks provide important
saliency cues like structure and semantic information of salient objects in the
image. Our method, called 3SD, exploits this idea by adding a branch for a
self-supervised classification task in parallel with salient object detection,
to obtain class activation maps (CAM maps). These CAM maps along with the edges
of the input image are used to generate the pseudo-GT saliency maps to train
our 3SD network. Specifically, we propose a contrastive learning-based training
on multiple image patches for the classification task. We show the multi-patch
classification with contrastive loss improves the quality of the CAM maps
compared to naive classification on the entire image. Experiments on six
benchmark datasets demonstrate that without any labels, our 3SD method
outperforms all existing weakly supervised and unsupervised methods, and its
performance is on par with the fully-supervised methods. Code is available at
:https://github.com/rajeevyasarla/3SD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Steganography based on Style Transfer. (arXiv:2203.04500v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04500">
<div class="article-summary-box-inner">
<span><p>Image steganography is the art and science of using images as cover for
covert communications. With the development of neural networks, traditional
image steganography is more likely to be detected by deep learning-based
steganalysis. To improve upon this, we propose image steganography network
based on style transfer, and the embedding of secret messages can be disguised
as image stylization. We embed secret information while transforming the
content image style. In latent space, the secret information is integrated into
the latent representation of the cover image to generate the stego images,
which are indistinguishable from normal stylized images. It is an end-to-end
unsupervised model without pre-training. Extensive experiments on the benchmark
dataset demonstrate the reliability, quality and security of stego images
generated by our steganographic network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Update Compression for Deep Neural Networks on the Edge. (arXiv:2203.04516v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04516">
<div class="article-summary-box-inner">
<span><p>An increasing number of artificial intelligence (AI) applications involve the
execution of deep neural networks (DNNs) on edge devices. Many practical
reasons motivate the need to update the DNN model on the edge device
post-deployment, such as refining the model, concept drift, or outright change
in the learning task. In this paper, we consider the scenario where retraining
can be done on the server side based on a copy of the DNN model, with only the
necessary data transmitted to the edge to update the deployed model. However,
due to bandwidth constraints, we want to minimise the transmission required to
achieve the update. We develop a simple approach based on matrix factorisation
to compress the model update -- this differs from compressing the model itself.
The key idea is to preserve existing knowledge in the current model and
optimise only small additional parameters for the update which can be used to
reconstitute the model on the edge. We compared our method to similar
techniques used in federated learning; our method usually requires less than
half of the update size of existing methods to achieve the same accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Road Segmentation via Uncertainty-aware Symmetric Network. (arXiv:2203.04537v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04537">
<div class="article-summary-box-inner">
<span><p>The high performance of RGB-D based road segmentation methods contrasts with
their rare application in commercial autonomous driving, which is owing to two
reasons: 1) the prior methods cannot achieve high inference speed and high
accuracy in both ways; 2) the different properties of RGB and depth data are
not well-exploited, limiting the reliability of predicted road. In this paper,
based on the evidence theory, an uncertainty-aware symmetric network (USNet) is
proposed to achieve a trade-off between speed and accuracy by fully fusing RGB
and depth data. Firstly, cross-modal feature fusion operations, which are
indispensable in the prior RGB-D based methods, are abandoned. We instead
separately adopt two light-weight subnetworks to learn road representations
from RGB and depth inputs. The light-weight structure guarantees the real-time
inference of our method. Moreover, a multiscale evidence collection (MEC)
module is designed to collect evidence in multiple scales for each modality,
which provides sufficient evidence for pixel class determination. Finally, in
uncertainty-aware fusion (UAF) module, the uncertainty of each modality is
perceived to guide the fusion of the two subnetworks. Experimental results
demonstrate that our method achieves a state-of-the-art accuracy with real-time
inference speed of 43+ FPS. The source code is available at
https://github.com/morancyc/USNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Depth Distribution Alignment with Low Computation. (arXiv:2203.04538v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04538">
<div class="article-summary-box-inner">
<span><p>The performance of monocular depth estimation generally depends on the amount
of parameters and computational cost. It leads to a large accuracy contrast
between light-weight networks and heavy-weight networks, which limits their
application in the real world. In this paper, we model the majority of accuracy
contrast between them as the difference of depth distribution, which we call
"Distribution drift". To this end, a distribution alignment network (DANet) is
proposed. We firstly design a pyramid scene transformer (PST) module to capture
inter-region interaction in multiple scales. By perceiving the difference of
depth features between every two regions, DANet tends to predict a reasonable
scene structure, which fits the shape of distribution to ground truth. Then, we
propose a local-global optimization (LGO) scheme to realize the supervision of
global range of scene depth. Thanks to the alignment of depth distribution
shape and scene depth range, DANet sharply alleviates the distribution drift,
and achieves a comparable performance with prior heavy-weight methods, but uses
only 1% floating-point operations per second (FLOPs) of them. The experiments
on two datasets, namely the widely used NYUDv2 dataset and the more challenging
iBims-1 dataset, demonstrate the effectiveness of our method. The source code
is available at https://github.com/YiLiM1/DANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChiTransformer:Towards Reliable Stereo from Cues. (arXiv:2203.04554v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04554">
<div class="article-summary-box-inner">
<span><p>Current stereo matching techniques are challenged by restricted searching
space, occluded regions, and sheer size. While single image depth estimation is
spared from these challenges and can achieve satisfactory results with the
extracted monocular cues, the lack of stereoscopic relationship renders the
monocular prediction less reliable on its own, especially in highly dynamic or
cluttered environments. To address these issues in both scenarios, we present
an optic-chiasm-inspired self-supervised binocular depth estimation method,
wherein vision transformer (ViT) with a gated positional cross-attention (GPCA)
layer is designed to enable feature-sensitive pattern retrieval between views
while retaining the extensive context information aggregated through
self-attentions. Monocular cues from a single view are thereafter conditionally
rectified by a blending layer with the retrieved pattern pairs. This crossover
design is biologically analogous to the optic-chasma structure in human visual
system and hence the name, ChiTransformer. Our experiments show that this
architecture yields substantial improvements over state-of-the-art
self-supervised stereo approaches by 11%, and can be used on both rectilinear
and non-rectilinear (e.g., fisheye) images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Temporal Consistency for Source-Free Video Domain Adaptation. (arXiv:2203.04559v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04559">
<div class="article-summary-box-inner">
<span><p>Video-based Unsupervised Domain Adaptation (VUDA) methods improve the
robustness of video models, enabling them to be applied to action recognition
tasks across different environments. However, these methods require constant
access to source data during the adaptation process. Yet in many real-world
applications, subjects and scenes in the source video domain should be
irrelevant to those in the target video domain. With the increasing emphasis on
data privacy, such methods that require source data access would raise serious
privacy issues. Therefore, to cope with such concern, a more practical domain
adaptation scenario is formulated as the Source-Free Video-based Domain
Adaptation (SFVDA). Though there are a few methods for Source-Free Domain
Adaptation (SFDA) on image data, these methods yield degenerating performance
in SFVDA due to the multi-modality nature of videos, with the existence of
additional temporal features. In this paper, we propose a novel Attentive
Temporal Consistent Network (ATCoN) to address SFVDA by learning temporal
consistency, guaranteed by two novel consistency objectives, namely feature
consistency and source prediction consistency, performed across local temporal
features. ATCoN further constructs effective overall temporal features by
attending to local temporal features based on prediction confidence. Empirical
results demonstrate the state-of-the-art performance of ATCoN across various
cross-domain action recognition benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Self-Semi-Supervised Learning for Few Labeled Samples Fast Training. (arXiv:2203.04560v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04560">
<div class="article-summary-box-inner">
<span><p>Faster training and fewer annotations are two key issues for applying deep
models to various practical domains. Now, semi-supervised learning has achieved
great success in training with few annotations. However, low-quality labeled
samples produced by random sampling make it difficult to continue to reduce the
number of annotations. In this paper we propose an active self-semi-supervised
training framework that bootstraps semi-supervised models with good prior
pseudo-labels, where the priors are obtained by label propagation over
self-supervised features. Because the accuracy of the prior is not only
affected by the quality of features, but also by the selection of the labeled
samples. We develop active learning and label propagation strategies to obtain
better prior pseudo-labels. Consequently, our framework can greatly improve the
performance of models with few annotations and greatly reduce the training
time. Experiments on three semi-supervised learning benchmarks demonstrate
effectiveness. Our method achieves similar accuracy to standard semi-supervised
approaches in about 1/3 of the training time, and even outperform them when
fewer annotations are available (84.10\% in CIFAR-10 with 10 labels).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLNav: Learning to Safely Navigate on Martian Terrains. (arXiv:2203.04563v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04563">
<div class="article-summary-box-inner">
<span><p>We present MLNav, a learning-enhanced path planning framework for
safety-critical and resource-limited systems operating in complex environments,
such as rovers navigating on Mars. MLNav makes judicious use of machine
learning to enhance the efficiency of path planning while fully respecting
safety constraints. In particular, the dominant computational cost in such
safety-critical settings is running a model-based safety checker on the
proposed paths. Our learned search heuristic can simultaneously predict the
feasibility for all path options in a single run, and the model-based safety
checker is only invoked on the top-scoring paths. We validate in high-fidelity
simulations using both real Martian terrain data collected by the Perseverance
rover, as well as a suite of challenging synthetic terrains. Our experiments
show that: (i) compared to the baseline ENav path planner on board the
Perserverance rover, MLNav can provide a significant improvement in multiple
key metrics, such as a 10x reduction in collision checks when navigating real
Martian terrains, despite being trained with synthetic terrains; and (ii) MLNav
can successfully navigate highly challenging terrains where the baseline ENav
fails to find a feasible path before timing out.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-Aware Face Swapping. (arXiv:2203.04564v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04564">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to
achieve identity-consistent harmonious high-resolution face generation in a
local-global manner: \textbf{1)} Local Facial Region-Aware (FRA) branch
augments local identity-relevant features by introducing the Transformer to
effectively model misaligned cross-scale semantic interaction. \textbf{2)}
Global Source Feature-Adaptive (SFA) branch further complements global
identity-relevant cues for generating identity-consistent swapped faces.
Besides, we propose a \textit{Face Mask Predictor} (FMP) module incorporated
with StyleGAN2 to predict identity-relevant soft facial masks in an
unsupervised manner that is more practical for generating harmonious
high-resolution faces. Abundant experiments qualitatively and quantitatively
demonstrate the superiority of our method for generating more
identity-consistent high-resolution swapped faces over SOTA methods, \eg,
obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\uparrow$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All You Need is LUV: Unsupervised Collection of Labeled Images using Invisible UV Fluorescent Indicators. (arXiv:2203.04566v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04566">
<div class="article-summary-box-inner">
<span><p>Large-scale semantic image annotation is a significant challenge for
learning-based perception systems in robotics. Current approaches often rely on
human labelers, which can be expensive, or simulation data, which can visually
or physically differ from real data. This paper proposes Labels from
UltraViolet (LUV), a novel framework that enables rapid, labeled data
collection in real manipulation environments without human labeling. LUV uses
transparent, ultraviolet-fluorescent paint with programmable ultraviolet LEDs
to collect paired images of a scene in standard lighting and UV lighting to
autonomously extract segmentation masks and keypoints via color segmentation.
We apply LUV to a suite of diverse robot perception tasks to evaluate its
labeling quality, flexibility, and data collection rate. Results suggest that
LUV is 180-2500 times faster than a human labeler across the tasks. We show
that LUV provides labels consistent with human annotations on unpainted test
images. The networks trained on these labels are used to smooth and fold
crumpled towels with 83% success rate and achieve 1.7mm position error with
respect to human labels on a surgical needle pose estimation task. The low cost
of LUV makes it ideal as a lightweight replacement for human labeling systems,
with the one-time setup costs at $300 equivalent to the cost of collecting
around 200 semantic segmentation labels on Amazon Mechanical Turk. Code,
datasets, visualizations, and supplementary material can be found at
https://sites.google.com/berkeley.edu/luv
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04568">
<div class="article-summary-box-inner">
<span><p>The success of Transformer in computer vision has attracted increasing
attention in the medical imaging community. Especially for medical image
segmentation, many excellent hybrid architectures based on convolutional neural
networks (CNNs) and Transformer have been presented and achieve impressive
performance. However, most of these methods, which embed modular Transformer
into CNNs, struggle to reach their full potential. In this paper, we propose a
novel hybrid architecture for medical image segmentation called PHTrans, which
parallelly hybridizes Transformer and CNN in main building blocks to produce
hierarchical representations from global and local features and adaptively
aggregate them, aiming to fully exploit their strengths to obtain better
segmentation performance. Specifically, PHTrans follows the U-shaped
encoder-decoder design and introduces the parallel hybird module in deep
stages, where convolution blocks and the modified 3D Swin Transformer learn
local features and global dependencies separately, then a sequence-to-volume
operation unifies the dimensions of the outputs to achieve feature aggregation.
Extensive experimental results on both Multi-Atlas Labeling Beyond the Cranial
Vault and Automated Cardiac Diagnosis Challeng datasets corroborate its
effectiveness, consistently outperforming state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction. (arXiv:2203.04570v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04570">
<div class="article-summary-box-inner">
<span><p>Vision transformer (ViT) has achieved competitive accuracy on a variety of
computer vision applications, but its computational cost impedes the deployment
on resource-limited mobile devices.
</p>
<p>We explore the sparsity in ViT and observe that informative patches and heads
are sufficient for accurate image recognition.
</p>
<p>In this paper, we propose a cascade pruning framework named CP-ViT by
predicting sparsity in ViT models progressively and dynamically to reduce
computational redundancy while minimizing the accuracy loss. Specifically, we
define the cumulative score to reserve the informative patches and heads across
the ViT model for better accuracy. We also propose the dynamic pruning ratio
adjustment technique based on layer-aware attention range. CP-ViT has great
general applicability for practical deployment, which can be applied to a wide
range of ViT models and can achieve superior accuracy with or without
fine-tuning.
</p>
<p>Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various
pre-trained models have demonstrated the effectiveness and efficiency of
CP-ViT. By progressively pruning 50\% patches, our CP-ViT method reduces over
40\% FLOPs while maintaining accuracy loss within 1\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neuro-vector-symbolic Architecture for Solving Raven's Progressive Matrices. (arXiv:2203.04571v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04571">
<div class="article-summary-box-inner">
<span><p>Neither deep neural networks nor symbolic AI alone have approached the kind
of intelligence expressed in humans. This is mainly because neural networks are
not able to decompose distinct objects from their joint representation (the
so-called binding problem), while symbolic AI suffers from exhaustive rule
searches, among other problems. These two problems are still pronounced in
neuro-symbolic AI which aims to combine the best of the two paradigms. Here, we
show that the two problems can be addressed with our proposed
neuro-vector-symbolic architecture (NVSA) by exploiting its powerful operators
on fixed-width holographic vectorized representations that serve as a common
language between neural networks and symbolic logical reasoning. The efficacy
of NVSA is demonstrated by solving the Raven's progressive matrices. NVSA
achieves a new record of 97.7% average accuracy in RAVEN, and 98.8% in I-RAVEN
datasets, with two orders of magnitude faster execution than the symbolic
logical reasoning on CPUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Brain Tumor Segmentation via Missing Modality Synthesis and Modality-level Attention Fusion. (arXiv:2203.04586v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04586">
<div class="article-summary-box-inner">
<span><p>Multi-modal magnetic resonance (MR) imaging provides great potential for
diagnosing and analyzing brain gliomas. In clinical scenarios, common MR
sequences such as T1, T2 and FLAIR can be obtained simultaneously in a single
scanning process. However, acquiring contrast enhanced modalities such as T1ce
requires additional time, cost, and injection of contrast agent. As such, it is
clinically meaningful to develop a method to synthesize unavailable modalities
which can also be used as additional inputs to downstream tasks (e.g., brain
tumor segmentation) for performance enhancing. In this work, we propose an
end-to-end framework named Modality-Level Attention Fusion Network (MAF-Net),
wherein we innovatively conduct patchwise contrastive learning for extracting
multi-modal latent features and dynamically assigning attention weights to fuse
different modalities. Through extensive experiments on BraTS2020, our proposed
MAF-Net is found to yield superior T1ce synthesis performance (SSIM of 0.8879
and PSNR of 22.78) and accurate brain tumor segmentation (mean Dice scores of
67.9%, 41.8% and 88.0% on segmenting the tumor core, enhancing tumor and whole
tumor).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04592">
<div class="article-summary-box-inner">
<span><p>Benchmarks are crucial to measuring and steering progress in artificial
intelligence (AI). However, recent studies raised concerns over the state of AI
benchmarking, reporting issues such as benchmark overfitting, benchmark
saturation and increasing centralization of benchmark dataset creation. To
facilitate monitoring of the health of the AI benchmarking ecosystem, we
introduce methodologies for creating condensed maps of the global dynamics of
benchmark creation and saturation. We curated data for 1688 benchmarks covering
the entire domains of computer vision and natural language processing, and show
that a large fraction of benchmarks quickly trended towards near-saturation,
that many benchmarks fail to find widespread utilization, and that benchmark
performance gains for different AI tasks were prone to unforeseen bursts. We
conclude that future work should focus on large-scale community collaboration
and on mapping benchmark performance gains to real-world utility and impact of
AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization using Pretrained Models without Fine-tuning. (arXiv:2203.04600v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04600">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pretrained models is a common practice in domain generalization
(DG) tasks. However, fine-tuning is usually computationally expensive due to
the ever-growing size of pretrained models. More importantly, it may cause
over-fitting on source domain and compromise their generalization ability as
shown in recent works. Generally, pretrained models possess some level of
generalization ability and can achieve decent performance regarding specific
domains and samples. However, the generalization performance of pretrained
models could vary significantly over different test domains even samples, which
raises challenges for us to best leverage pretrained models in DG tasks. In
this paper, we propose a novel domain generalization paradigm to better
leverage various pretrained models, named specialized ensemble learning for
domain generalization (SEDGE). It first trains a linear label space adapter
upon fixed pretrained models, which transforms the outputs of the pretrained
model to the label space of the target domain. Then, an ensemble network aware
of model specialty is proposed to dynamically dispatch proper pretrained models
to predict each test sample. Experimental studies on several benchmarks show
that SEDGE achieves significant performance improvements comparing to strong
baselines including state-of-the-art method in DG tasks and reduces the
trainable parameters by ~99% and the training time by ~99.5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-effective multiple instance learning on weakly stem cell colony segmentation. (arXiv:2203.04606v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04606">
<div class="article-summary-box-inner">
<span><p>The detection of induced pluripotent stem cell (iPSC) colonies often needs
the precise extraction of the colony features. However, existing computerized
systems relied on segmentation of contours by preprocessing for classifying the
colony conditions were task-extensive. To maximize the efficiency in
categorizing colony conditions, we propose a multiple instance learning (MIL)
in weakly supervised settings. It is designed in a single model to produce weak
segmentation and classification of colonies without using finely labeled
samples. As a single model, we employ a U-net-like convolution neural network
(CNN) to train on binary image-level labels for MIL colonies classification.
Furthermore, to specify the object of interest we used a simple post-processing
method. The proposed approach is compared over conventional methods using
five-fold cross-validation and receiver operating characteristic (ROC) curve.
The maximum accuracy of the MIL-net is 95%, which is 15 % higher than the
conventional methods. Furthermore, the ability to interpret the location of the
iPSC colonies based on the image level label without using a pixel-wise ground
truth image is more appealing and cost-effective in colony condition
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical No-box Adversarial Attacks with Training-free Hybrid Image Transformation. (arXiv:2203.04607v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04607">
<div class="article-summary-box-inner">
<span><p>In recent years, the adversarial vulnerability of deep neural networks (DNNs)
has raised increasing attention. Among all the threat models, no-box attacks
are the most practical but extremely challenging since they neither rely on any
knowledge of the target model or similar substitute model, nor access the
dataset for training a new substitute model. Although a recent method has
attempted such an attack in a loose sense, its performance is not good enough
and computational overhead of training is expensive. In this paper, we move a
step forward and show the existence of a \textbf{training-free} adversarial
perturbation under the no-box threat model, which can be successfully used to
attack different DNNs in real-time. Motivated by our observation that
high-frequency component (HFC) domains in low-level features and plays a
crucial role in classification, we attack an image mainly by manipulating its
frequency components. Specifically, the perturbation is manipulated by
suppression of the original HFC and adding of noisy HFC. We empirically and
experimentally analyze the requirements of effective noisy HFC and show that it
should be regionally homogeneous, repeating and dense. Extensive experiments on
the ImageNet dataset demonstrate the effectiveness of our proposed no-box
method. It attacks ten well-known models with a success rate of
\textbf{98.13\%} on average, which outperforms state-of-the-art no-box attacks
by \textbf{29.39\%}. Furthermore, our method is even competitive to mainstream
transfer-based black-box attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-Based Visual Camera Pose Estimation From Ellipsoidal Model and 3D-Aware Ellipse Prediction. (arXiv:2203.04613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04613">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a method for initial camera pose estimation from
just a single image which is robust to viewing conditions and does not require
a detailed model of the scene. This method meets the growing need of easy
deployment of robotics or augmented reality applications in any environments,
especially those for which no accurate 3D model nor huge amount of ground truth
data are available. It exploits the ability of deep learning techniques to
reliably detect objects regardless of viewing conditions. Previous works have
also shown that abstracting the geometry of a scene of objects by an ellipsoid
cloud allows to compute the camera pose accurately enough for various
application needs. Though promising, these approaches use the ellipses fitted
to the detection bounding boxes as an approximation of the imaged objects. In
this paper, we go one step further and propose a learning-based method which
detects improved elliptic approximations of objects which are coherent with the
3D ellipsoids in terms of perspective projection. Experiments prove that the
accuracy of the computed pose significantly increases thanks to our method.
This is achieved with very little effort in terms of training data acquisition
- a few hundred calibrated images of which only three need manual object
annotation. Code and models are released at
https://gitlab.inria.fr/tangram/3d-aware-ellipses-for-visual-localization
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification. (arXiv:2203.04614v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04614">
<div class="article-summary-box-inner">
<span><p>A large-scale labeled dataset is a key factor for the success of supervised
deep learning in computer vision. However, a limited number of annotated data
is very common, especially in ophthalmic image analysis, since manual
annotation is time-consuming and labor-intensive. Self-supervised learning
(SSL) methods bring huge opportunities for better utilizing unlabeled data, as
they do not need massive annotations. With an attempt to use as many as
possible unlabeled ophthalmic images, it is necessary to break the dimension
barrier, simultaneously making use of both 2D and 3D images. In this paper, we
propose a universal self-supervised Transformer framework, named Uni4Eye, to
discover the inherent image property and capture domain-specific feature
embedding in ophthalmic images. Uni4Eye can serve as a global feature
extractor, which builds its basis on a Masked Image Modeling task with a Vision
Transformer (ViT) architecture. We employ a Unified Patch Embedding module to
replace the origin patch embedding module in ViT for jointly processing both 2D
and 3D input images. Besides, we design a dual-branch multitask decoder module
to simultaneously perform two reconstruction tasks on the input image and its
gradient map, delivering discriminative representations for better convergence.
We evaluate the performance of our pre-trained Uni4Eye encoder by fine-tuning
it on six downstream ophthalmic image classification tasks. The superiority of
Uni4Eye is successfully established through comparisons to other
state-of-the-art SSL pre-training methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation and Generation of Physical Adversarial Patch. (arXiv:2203.04623v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04623">
<div class="article-summary-box-inner">
<span><p>Recent studies have revealed the vulnerability of face recognition models
against physical adversarial patches, which raises security concerns about the
deployed face recognition systems. However, it is still challenging to ensure
the reproducibility for most attack algorithms under complex physical
conditions, which leads to the lack of a systematic evaluation of the existing
methods. It is therefore imperative to develop a framework that can enable a
comprehensive evaluation of the vulnerability of face recognition in the
physical world. To this end, we propose to simulate the complex transformations
of faces in the physical world via 3D-face modeling, which serves as a digital
counterpart of physical faces. The generic framework allows us to control
different face variations and physical conditions to conduct reproducible
evaluations comprehensively. With this digital simulator, we further propose a
Face3DAdv method considering the 3D face transformations and realistic physical
variations. Extensive experiments validate that Face3DAdv can significantly
improve the effectiveness of diverse physically realizable adversarial patches
in both simulated and physical environments, against various white-box and
black-box face recognition models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Dense Face Alignment with Fused Features by Aggregating CNNs and GCNs. (arXiv:2203.04643v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04643">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel multi-level aggregation network to regress
the coordinates of the vertices of a 3D face from a single 2D image in an
end-to-end manner. This is achieved by seamlessly combining standard
convolutional neural networks (CNNs) with Graph Convolution Networks (GCNs). By
iteratively and hierarchically fusing the features across different layers and
stages of the CNNs and GCNs, our approach can provide a dense face alignment
and 3D face reconstruction simultaneously for the benefit of direct feature
learning of 3D face mesh. Experiments on several challenging datasets
demonstrate that our method outperforms state-of-the-art approaches on both 2D
and 3D face alignment tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Normal and Visibility Estimation of Human Face from a Single Image. (arXiv:2203.04647v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04647">
<div class="article-summary-box-inner">
<span><p>Recent work on the intrinsic image of humans starts to consider the
visibility of incident illumination and encodes the light transfer function by
spherical harmonics. In this paper, we show that such a light transfer function
can be further decomposed into visibility and cosine terms related to surface
normal. Such decomposition allows us to recover the surface normal in addition
to visibility. We propose a deep learning-based approach with a reconstruction
loss for training on real-world images. Results show that compared with
previous works, the reconstruction of human face from our method better reveals
the surface normal and shading details especially around regions where
visibility effect is strong.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ray Tracing-Guided Design of Plenoptic Cameras. (arXiv:2203.04660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04660">
<div class="article-summary-box-inner">
<span><p>The design of a plenoptic camera requires the combination of two dissimilar
optical systems, namely a main lens and an array of microlenses. And while the
construction process of a conventional camera is mainly concerned with focusing
the image onto a single plane, in the case of plenoptic cameras there can be
additional requirements such as a predefined depth of field or a desired range
of disparities in neighboring microlens images. Due to this complexity, the
manual creation of multiple plenoptic camera setups is often a time-consuming
task. In this work we assume a simulation framework as well as the main lens
data given and present a method to calculate the remaining aperture, sensor and
microlens array parameters under different sets of constraints. Our ray
tracing-based approach is shown to result in models outperforming their
pendants generated with the commonly used paraxial approximations in terms of
image quality, while still meeting the desired constraints. Both the
implementation and evaluation setup including 30 plenoptic camera designs are
made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creating Realistic Ground Truth Data for the Evaluation of Calibration Methods for Plenoptic and Conventional Cameras. (arXiv:2203.04661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04661">
<div class="article-summary-box-inner">
<span><p>Camera calibration methods usually consist of capturing images of known
calibration patterns and using the detected correspondences to optimize the
parameters of the assumed camera model. A meaningful evaluation of these
methods relies on the availability of realistic synthetic data. In previous
works concerned with conventional cameras the synthetic data was mainly created
by rendering perfect images with a pinhole camera and subsequently adding
distortions and aberrations to the renderings and correspondences according to
the assumed camera model. This method can bias the evaluation since not every
camera perfectly complies with an assumed model. Furthermore, in the field of
plenoptic camera calibration there is no synthetic ground truth data available
at all. We address these problems by proposing a method based on backward ray
tracing to create realistic ground truth data that can be used for an unbiased
evaluation of calibration methods for both types of cameras.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulation of Plenoptic Cameras. (arXiv:2203.04662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04662">
<div class="article-summary-box-inner">
<span><p>Plenoptic cameras enable the capturing of spatial as well as angular color
information which can be used for various applications among which are image
refocusing and depth calculations. However, these cameras are expensive and
research in this area currently lacks data for ground truth comparisons. In
this work we describe a flexible, easy-to-use Blender model for the different
plenoptic camera types which is on the one hand able to provide the ground
truth data for research and on the other hand allows an inexpensive assessment
of the cameras usefulness for the desired applications. Furthermore we show
that the rendering results exhibit the same image degradation effects as real
cameras and make our simulation publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inadequately Pre-trained Models are Better Feature Extractors. (arXiv:2203.04668v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04668">
<div class="article-summary-box-inner">
<span><p>Pre-training has been a popular learning paradigm in deep learning era,
especially in annotation-insufficient scenario. Better ImageNet pre-trained
models have been demonstrated, from the perspective of architecture, by
previous research to have better transferability to downstream tasks. However,
in this paper, we found that during the same pre-training process, models at
middle epochs, which is inadequately pre-trained, can outperform fully trained
models when used as feature extractors (FE), while the fine-tuning (FT)
performance still grows with the source performance. This reveals that there is
not a solid positive correlation between top-1 accuracy on ImageNet and the
transferring result on target data. Based on the contradictory phenomenon
between FE and FT that better feature extractor fails to be fine-tuned better
accordingly, we conduct comprehensive analyses on features before softmax layer
to provide insightful explanations. Our discoveries suggest that, during
pre-training, models tend to first learn spectral components corresponding to
large singular values and the residual components contribute more when
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-Aware Flow Generation for Human Body Reshaping. (arXiv:2203.04670v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04670">
<div class="article-summary-box-inner">
<span><p>Body reshaping is an important procedure in portrait photo retouching. Due to
the complicated structure and multifarious appearance of human bodies, existing
methods either fall back on the 3D domain via body morphable model or resort to
keypoint-based image deformation, leading to inefficiency and unsatisfied
visual quality. In this paper, we address these limitations by formulating an
end-to-end flow generation architecture under the guidance of body structural
priors, including skeletons and Part Affinity Fields, and achieve
unprecedentedly controllable performance under arbitrary poses and garments. A
compositional attention mechanism is introduced for capturing both visual
perceptual correlations and structural associations of the human body to
reinforce the manipulation consistency among related parts. For a comprehensive
evaluation, we construct the first large-scale body reshaping dataset, namely
BR-5K, which contains 5,000 portrait photos as well as professionally retouched
targets. Extensive experiments demonstrate that our approach significantly
outperforms existing state-of-the-art methods in terms of visual performance,
controllability, and efficiency. The dataset is available at our website:
https://github.com/JianqiangRen/BodyReshaping5K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Align-Deform-Subtract: An Interventional Framework for Explaining Object Differences. (arXiv:2203.04694v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04694">
<div class="article-summary-box-inner">
<span><p>Given two object images, how can we explain their differences in terms of the
underlying object properties? To address this question, we propose
Align-Deform-Subtract (ADS) -- an interventional framework for explaining
object differences. By leveraging semantic alignments in image-space as
counterfactual interventions on the underlying object properties, ADS
iteratively quantifies and removes differences in object properties. The result
is a set of "disentangled" error measures which explain object differences in
terms of their underlying properties. Experiments on real and synthetic data
illustrate the efficacy of the framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexIT: Towards Flexible Semantic Image Translation. (arXiv:2203.04705v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04705">
<div class="article-summary-box-inner">
<span><p>Deep generative models, like GANs, have considerably improved the state of
the art in image synthesis, and are able to generate near photo-realistic
images in structured domains such as human faces. Based on this success, recent
work on image editing proceeds by projecting images to the GAN latent space and
manipulating the latent vector. However, these approaches are limited in that
only images from a narrow domain can be transformed, and with only a limited
number of editing operations. We propose FlexIT, a novel method which can take
any input image and a user-defined text instruction for editing. Our method
achieves flexible and natural editing, pushing the limits of semantic image
translation. First, FlexIT combines the input image and text into a single
target point in the CLIP multimodal embedding space. Via the latent space of an
auto-encoder, we iteratively transform the input image toward the target point,
ensuring coherence and quality with a variety of novel regularization terms. We
propose an evaluation protocol for semantic image translation, and thoroughly
evaluate our method on ImageNet. Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Transformer Framework for Group-based Segmentation: Co-Segmentation, Co-Saliency Detection and Video Salient Object Detection. (arXiv:2203.04708v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04708">
<div class="article-summary-box-inner">
<span><p>Humans tend to mine objects by learning from a group of images or several
frames of video since we live in a dynamic world. In the computer vision area,
many researches focus on co-segmentation (CoS), co-saliency detection (CoSD)
and video salient object detection (VSOD) to discover the co-occurrent objects.
However, previous approaches design different networks on these similar tasks
separately, and they are difficult to apply to each other, which lowers the
upper bound of the transferability of deep learning frameworks. Besides, they
fail to take full advantage of the cues among inter- and intra-feature within a
group of images. In this paper, we introduce a unified framework to tackle
these issues, term as UFO (Unified Framework for Co-Object Segmentation).
Specifically, we first introduce a transformer block, which views the image
feature as a patch token and then captures their long-range dependencies
through the self-attention mechanism. This can help the network to excavate the
patch structured similarities among the relevant objects. Furthermore, we
propose an intra-MLP learning module to produce self-mask to enhance the
network to avoid partial activation. Extensive experiments on four CoS
benchmarks (PASCAL, iCoseg, Internet and MSRC), three CoSD benchmarks
(Cosal2015, CoSOD3k, and CocA) and four VSOD benchmarks (DAVIS16, FBMS, ViSal
and SegV2) show that our method outperforms other state-of-the-arts on three
different tasks in both accuracy and speed by using the same network
architecture , which can reach 140 FPS in real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Black-box Skeleton-based Human Activity Classifiers. (arXiv:2203.04713v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04713">
<div class="article-summary-box-inner">
<span><p>Deep learning has been regarded as the `go to' solution for many tasks today,
but its intrinsic vulnerability to malicious attacks has become a major
concern. The vulnerability is affected by a variety of factors including
models, tasks, data, and attackers. Consequently, methods such as Adversarial
Training and Randomized Smoothing have been proposed to tackle the problem in a
wide range of applications. In this paper, we investigate skeleton-based Human
Activity Recognition, which is an important type of time-series data but
under-explored in defense against attacks. Our method is featured by (1) a new
Bayesian Energy-based formulation of robust discriminative classifiers, (2) a
new parameterization of the adversarial sample manifold of actions, and (3) a
new post-train Bayesian treatment on both the adversarial samples and the
classifier. We name our framework Bayesian Energy-based Adversarial Training or
BEAT. BEAT is straightforward but elegant, which turns vulnerable black-box
classifiers into robust ones without sacrificing accuracy. It demonstrates
surprising and universal effectiveness across a wide range of action
classifiers and datasets, under various attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting conversion of mild cognitive impairment to Alzheimer's disease. (arXiv:2203.04725v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04725">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) is the most common age-related dementia. Mild
cognitive impairment (MCI) is the early stage of cognitive decline before AD.
It is crucial to predict the MCI-to-AD conversion for precise management, which
remains challenging due to the diversity of patients. Previous evidence shows
that the brain network generated from diffusion MRI promises to classify
dementia using deep learning. However, the limited availability of diffusion
MRI challenges the model training. In this study, we develop a self-supervised
contrastive learning approach to generate structural brain networks from
routine anatomical MRI under the guidance of diffusion MRI. The generated brain
networks are applied to train a learning framework for predicting the MCI-to-AD
conversion. Instead of directly modelling the AD brain networks, we train a
graph encoder and a variational autoencoder to model the healthy ageing
trajectories from brain networks of healthy controls. To predict the MCI-to-AD
conversion, we further design a recurrent neural networks based approach to
model the longitudinal deviation of patients' brain networks from the healthy
ageing trajectory. Numerical results show that the proposed methods outperform
the benchmarks in the prediction task. We also visualize the model
interpretation to explain the prediction and identify abnormal changes of white
matter tracts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P2M: A Processing-in-Pixel-in-Memory Paradigm for Resource-Constrained TinyML Applications. (arXiv:2203.04737v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04737">
<div class="article-summary-box-inner">
<span><p>The demand to process vast amounts of data generated from state-of-the-art
high resolution cameras has motivated novel energy-efficient on-device AI
solutions. Visual data in such cameras are usually captured in the form of
analog voltages by a sensor pixel array, and then converted to the digital
domain for subsequent AI processing using analog-to-digital converters (ADC).
Recent research has tried to take advantage of massively parallel low-power
analog/digital computing in the form of near- and in-sensor processing, in
which the AI computation is performed partly in the periphery of the pixel
array and partly in a separate on-board CPU/accelerator. Unfortunately,
high-resolution input images still need to be streamed between the camera and
the AI processing unit, frame by frame, causing energy, bandwidth, and security
bottlenecks. To mitigate this problem, we propose a novel
Processing-in-Pixel-in-memory (P2M) paradigm, that customizes the pixel array
by adding support for analog multi-channel, multi-bit convolution and ReLU
(Rectified Linear Units). Our solution includes a holistic algorithm-circuit
co-design approach and the resulting P2M paradigm can be used as a drop-in
replacement for embedding memory-intensive first few layers of convolutional
neural network (CNN) models within foundry-manufacturable CMOS image sensor
platforms. Our experimental results indicate that P2M reduces data transfer
bandwidth from sensors and analog to digital conversions by ~21x, and the
energy-delay product (EDP) incurred in processing a MobileNetV2 model on a
TinyML use case for visual wake words dataset (VWW) by up to ~11x compared to
standard near-processing or in-sensor implementations, without any significant
drop in test accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences. (arXiv:2203.04738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04738">
<div class="article-summary-box-inner">
<span><p>Parallelizing Gated Recurrent Unit (GRU) networks is a challenging task, as
the training procedure of GRU is inherently sequential. Prior efforts to
parallelize GRU have largely focused on conventional parallelization strategies
such as data-parallel and model-parallel training algorithms. However, when the
given sequences are very long, existing approaches are still inevitably
performance limited in terms of training time. In this paper, we present a
novel parallel training scheme (called parallel-in-time) for GRU based on a
multigrid reduction in time (MGRIT) solver. MGRIT partitions a sequence into
multiple shorter sub-sequences and trains the sub-sequences on different
processors in parallel. The key to achieving speedup is a hierarchical
correction of the hidden state to accelerate end-to-end communication in both
the forward and backward propagation phases of gradient descent. Experimental
results on the HMDB51 dataset, where each video is an image sequence,
demonstrate that the new parallel training scheme achieves up to 6.5$\times$
speedup over a serial approach. As efficiency of our new parallelization
strategy is associated with the sequence length, our parallel GRU algorithm
achieves significant performance improvement as the sequence length increases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters. (arXiv:2203.04746v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04746">
<div class="article-summary-box-inner">
<span><p>This work presents SkinningNet, an end-to-end Two-Stream Graph Neural Network
architecture that computes skinning weights from an input mesh and its
associated skeleton, without making any assumptions on shape class and
structure of the provided mesh. Whereas previous methods pre-compute
handcrafted features that relate the mesh and the skeleton or assume a fixed
topology of the skeleton, the proposed method extracts this information in an
end-to-end learnable fashion by jointly learning the best relationship between
mesh vertices and skeleton joints. The proposed method exploits the benefits of
the novel Multi-Aggregator Graph Convolution that combines the results of
different aggregators during the summarizing step of the Message-Passing
scheme, helping the operation to generalize for unseen topologies. Experimental
results demonstrate the effectiveness of the contributions of our novel
architecture, with SkinningNet outperforming current state-of-the-art
alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Human Gaze For Surgical Activity Recognition. (arXiv:2203.04752v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04752">
<div class="article-summary-box-inner">
<span><p>Automatically recognizing surgical activities plays an important role in
providing feedback to surgeons, and is a fundamental step towards
computer-aided surgical systems. Human gaze and visual saliency carry important
information about visual attention, and can be used in computer vision systems.
Although state-of-the-art surgical activity recognition models learn spatial
temporal features, none of these models make use of human gaze and visual
saliency. In this study, we propose to use human gaze with a spatial temporal
attention mechanism for activity recognition in surgical videos. Our model
consists of an I3D-based architecture, learns spatio-temporal features using 3D
convolutions, as well as learning an attention map using human gaze. We
evaluated our model on the Suturing task of JIGSAWS which is a publicly
available surgical video understanding dataset. Our evaluations on a subset of
random video segments in this task suggest that our approach achieves promising
results with an accuracy of 86.2%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Image Classificationtion. (arXiv:2203.04771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04771">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images (HSI) not only have a broad macroscopic field of view
but also contain rich spectral information, and the types of surface objects
can be identified through spectral information, which is one of the main
applications in hyperspectral image related research.In recent years, more and
more deep learning methods have been proposed, among which convolutional neural
networks (CNN) are the most influential. However, CNN-based methods are
difficult to capture long-range dependencies, and also require a large amount
of labeled data for model training.Besides, most of the self-supervised
training methods in the field of HSI classification are based on the
reconstruction of input samples, and it is difficult to achieve effective use
of unlabeled samples. To address the shortcomings of CNN networks, we propose a
noval multi-scale convolutional embedding module for HSI to realize effective
extraction of spatial-spectral information, which can be better combined with
Transformer network.In order to make more efficient use of unlabeled data, we
propose a new self-supervised pretask. Similar to Mask autoencoder, but our
pre-training method only masks the corresponding token of the central pixel in
the encoder, and inputs the remaining token into the decoder to reconstruct the
spectral information of the central pixel.Such a pretask can better model the
relationship between the central feature and the domain feature, and obtain
more stable training results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How many Observations are Enough? Knowledge Distillation for Trajectory Forecasting. (arXiv:2203.04781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04781">
<div class="article-summary-box-inner">
<span><p>Accurate prediction of future human positions is an essential task for modern
video-surveillance systems. Current state-of-the-art models usually rely on a
"history" of past tracked locations (e.g., 3 to 5 seconds) to predict a
plausible sequence of future locations (e.g., up to the next 5 seconds). We
feel that this common schema neglects critical traits of realistic
applications: as the collection of input trajectories involves machine
perception (i.e., detection and tracking), incorrect detection and
fragmentation errors may accumulate in crowded scenes, leading to tracking
drifts. On this account, the model would be fed with corrupted and noisy input
data, thus fatally affecting its prediction performance.
</p>
<p>In this regard, we focus on delivering accurate predictions when only few
input observations are used, thus potentially lowering the risks associated
with automatic perception. To this end, we conceive a novel distillation
strategy that allows a knowledge transfer from a teacher network to a student
one, the latter fed with fewer observations (just two ones). We show that a
properly defined teacher supervision allows a student network to perform
comparably to state-of-the-art approaches that demand more observations.
Besides, extensive experiments on common trajectory forecasting datasets
highlight that our student network better generalizes to unseen scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of YOLO Models with Sliced Inference for Small Object Detection. (arXiv:2203.04799v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04799">
<div class="article-summary-box-inner">
<span><p>Small object detection has major applications in the fields of UAVs,
surveillance, farming and many others. In this work we investigate the
performance of state of the art Yolo based object detection models for the task
of small object detection as they are one of the most popular and easy to use
object detection models. We evaluated YOLOv5 and YOLOX models in this study. We
also investigate the effects of slicing aided inference and fine-tuning the
model for slicing aided inference. We used the VisDrone2019Det dataset for
training and evaluating our models. This dataset is challenging in the sense
that most objects are relatively small compared to the image sizes. This work
aims to benchmark the YOLOv5 and YOLOX models for small object detection. We
have seen that sliced inference increases the AP50 score in all experiments,
this effect was greater for the YOLOv5 models compared to the YOLOX models. The
effects of sliced fine-tuning and sliced inference combined produced
substantial improvement for all models. The highest AP50 score was achieved by
the YOLOv5- Large model on the VisDrone2019Det test-dev subset with the score
being 48.8.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRF-Pose: A First-Reconstruct-Then-Regress Approach for Weakly-supervised 6D Object Pose Estimation. (arXiv:2203.04802v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04802">
<div class="article-summary-box-inner">
<span><p>Pose estimation of 3D objects in monocular images is a fundamental and
long-standing problem in computer vision. Existing deep learning approaches for
6D pose estimation typically rely on the assumption of availability of 3D
object models and 6D pose annotations. However, precise annotation of 6D poses
in real data is intricate, time-consuming and not scalable, while synthetic
data scales well but lacks realism. To avoid these problems, we present a
weakly-supervised reconstruction-based pipeline, named NeRF-Pose, which needs
only 2D object segmentation and known relative camera poses during training.
Following the first-reconstruct-then-regress idea, we first reconstruct the
objects from multiple views in the form of an implicit neural representation.
Then, we train a pose regression network to predict pixel-wise 2D-3D
correspondences between images and the reconstructed model. At inference, the
approach only needs a single image as input. A NeRF-enabled PnP+RANSAC
algorithm is used to estimate stable and accurate pose from the predicted
correspondences. Experiments on LineMod and LineMod-Occlusion show that the
proposed method has state-of-the-art accuracy in comparison to the best 6D pose
estimation methods in spite of being trained only with weak labels. Besides, we
extend the Homebrewed DB dataset with more real training images to support the
weakly supervised task and achieve compelling results on this dataset. The
extended dataset and code will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A high-precision self-supervised monocular visual odometry in foggy weather based on robust cycled generative adversarial networks and multi-task learning aided depth estimation. (arXiv:2203.04812v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04812">
<div class="article-summary-box-inner">
<span><p>This paper proposes a high-precision self-supervised monocular VO, which is
specifically designed for navigation in foggy weather. A cycled generative
adversarial network is designed to obtain high-quality self-supervised loss via
forcing the forward and backward half-cycle to output consistent estimation.
Moreover, gradient-based loss and perceptual loss are introduced to eliminate
the interference of complex photometric change on self-supervised loss in foggy
weather. To solve the ill-posed problem of depth estimation, a self-supervised
multi-task learning aided depth estimation module is designed based on the
strong correlation between the depth estimation and transmission map
calculation of hazy images in foggy weather. The experimental results on the
synthetic foggy KITTI dataset show that the proposed self-supervised monocular
VO performs better in depth and pose estimation than other state-of-the-art
monocular VO in the literature, indicating the designed method is more suitable
for foggy weather.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-DIAE: Degradation Invariant Autoencoders for Text Recognition and Document Enhancement. (arXiv:2203.04814v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04814">
<div class="article-summary-box-inner">
<span><p>In this work, we propose Text-Degradation Invariant Auto Encoder (Text-DIAE)
aimed to solve two tasks, text recognition (handwritten or scene-text) and
document image enhancement. We define three pretext tasks as learning
objectives to be optimized during pre-training without the usage of labelled
data. Each of the pre-text objectives is specifically tailored for the final
downstream tasks. We conduct several ablation experiments that show the
importance of each degradation for a specific domain. Exhaustive
experimentation shows that our method does not have limitations of previous
state-of-the-art based on contrastive losses while at the same time requiring
essentially fewer data samples to converge. Finally, we demonstrate that our
method surpasses the state-of-the-art significantly in existing supervised and
self-supervised settings in handwritten and scene text recognition and document
image enhancement. Our code and trained models will be made publicly available
at~\url{ <a href="http://Upon_Acceptance">this http URL</a>}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A high-precision underwater object detection based on joint self-supervised deblurring and improved spatial transformer network. (arXiv:2203.04822v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04822">
<div class="article-summary-box-inner">
<span><p>Deep learning-based underwater object detection (UOD) remains a major
challenge due to the degraded visibility and difficulty to obtain sufficient
underwater object images captured from various perspectives for training. To
address these issues, this paper presents a high-precision UOD based on joint
self-supervised deblurring and improved spatial transformer network. A
self-supervised deblurring subnetwork is introduced into the designed
multi-task learning aided object detection architecture to force the shared
feature extraction module to output clean features for detection subnetwork.
Aiming at alleviating the limitation of insufficient photos from different
perspectives, an improved spatial transformer network is designed based on
perspective transformation, adaptively enriching image features within the
network. The experimental results show that the proposed UOD approach achieved
47.9 mAP in URPC2017 and 70.3 mAP in URPC2018, outperforming many
state-of-the-art UOD methods and indicating the designed method is more
suitable for UOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers. (arXiv:2203.04838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04838">
<div class="article-summary-box-inner">
<span><p>The performance of semantic segmentation of RGB images can be advanced by
exploiting informative features from supplementary modalities. In this work, we
propose CMX, a vision-transformer-based cross-modal fusion framework for RGB-X
semantic segmentation. To generalize to different sensing modalities
encompassing various uncertainties, we consider that comprehensive cross-modal
interactions should be provided. CMX is built with two streams to extract
features from RGB images and the complementary modality (X-modality). In each
feature extraction stage, we design a Cross-Modal Feature Rectification Module
(CM-FRM) to calibrate the feature of the current modality by combining the
feature from the other modality, in spatial- and channel-wise dimensions. With
rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix them
for the final semantic prediction. FFM is constructed with a cross-attention
mechanism, which enables exchange of long-range contexts, enhancing both
modalities' features at a global level. Extensive experiments show that CMX
generalizes to diverse multi-modal combinations, achieving state-of-the-art
performances on four RGB-Depth benchmarks, as well as RGB-Thermal and
RGB-Polarization datasets. Besides, to investigate the generalizability to
dense-sparse data fusion, we establish a RGB-Event semantic segmentation
benchmark based on the EventScape dataset, on which CMX sets the new
state-of-the-art. Code is available at
https://github.com/huaaaliu/RGBX_Semantic_Segmentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction. (arXiv:2203.04845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04845">
<div class="article-summary-box-inner">
<span><p>Many algorithms have been developed to solve the inverse problem of coded
aperture snapshot spectral imaging (CASSI), i.e., recovering the 3D
hyperspectral images (HSIs) from a 2D compressive measurement. In recent years,
learning-based methods have demonstrated promising performance and dominated
the mainstream research direction. However, existing CNN-based methods show
limitations in capturing long-range dependencies and non-local self-similarity.
Previous Transformer-based methods densely sample tokens, some of which are
uninformative, and calculate the multi-head self-attention (MSA) between some
tokens that are unrelated in content. This does not fit the spatially sparse
nature of HSI signals and limits the model scalability. In this paper, we
propose a novel Transformer-based method, coarse-to-fine sparse Transformer
(CST), firstly embedding HSI sparsity into deep learning for HSI
reconstruction. In particular, CST uses our proposed spectra-aware screening
mechanism (SASM) for coarse patch selecting. Then the selected patches are fed
into our customized spectra-aggregation hashing multi-head self-attention
(SAH-MSA) for fine pixel clustering and self-similarity capturing.
Comprehensive experiments show that our CST significantly outperforms
state-of-the-art methods while requiring cheaper computational costs. The code
and models will be made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CEU-Net: Ensemble Semantic Segmentation of Hyperspectral Images Using Clustering. (arXiv:2203.04873v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04873">
<div class="article-summary-box-inner">
<span><p>Most semantic segmentation approaches of Hyperspectral images (HSIs) use and
require preprocessing steps in the form of patching to accurately classify
diversified land cover in remotely sensed images. These approaches use patching
to incorporate the rich neighborhood information in images and exploit the
simplicity and segmentability of the most common HSI datasets. In contrast,
most landmasses in the world consist of overlapping and diffused classes,
making neighborhood information weaker than what is seen in common HSI
datasets. To combat this issue and generalize the segmentation models to more
complex and diverse HSI datasets, in this work, we propose our novel flagship
model: Clustering Ensemble U-Net (CEU-Net). CEU-Net uses the ensemble method to
combine spectral information extracted from convolutional neural network (CNN)
training on a cluster of landscape pixels. Our CEU-Net model outperforms
existing state-of-the-art HSI semantic segmentation methods and gets
competitive performance with and without patching when compared to baseline
models. We highlight CEU-Net's high performance across Botswana, KSC, and
Salinas datasets compared to HybridSN and AeroRIT methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VGQ-CNN: Moving Beyond Fixed Cameras and Top-Grasps for Grasp Quality Prediction. (arXiv:2203.04874v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04874">
<div class="article-summary-box-inner">
<span><p>We present the Versatile Grasp Quality Convolutional Neural Network
(VGQ-CNN), a grasp quality prediction network for 6-DOF grasps. VGQ-CNN can be
used when evaluating grasps for objects seen from a wide range of camera poses
or mobile robots without the need to retrain the network. By defining the grasp
orientation explicitly as an input to the network, VGQ-CNN can evaluate 6-DOF
grasp poses, moving beyond the 4-DOF grasps used in most image-based grasp
evaluation methods like GQ-CNN. We train VGQ-CNN on our new Versatile Grasp
dataset (VG-dset), containing 6-DOF grasps observed from a wide range of camera
poses. VGQ-CNN achieves a balanced accuracy of 82.1% on our test-split while
generalising to a variety of camera poses. Meanwhile, it achieves competitive
performance for overhead cameras and top-grasps with a balanced accuracy of
74.2% compared to GQ-CNN's 76.6%. We also propose a modified network
architecture, FAST-VGQ-CNN, that speeds up inference using a shared encoder
architecture and can make 128 grasp quality predictions in 12ms on a CPU. Code
and data are available at https://figshare.com/s/b12b37b14b747b10524e.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Image Representation Learning with Federated Sampled Softmax. (arXiv:2203.04888v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04888">
<div class="article-summary-box-inner">
<span><p>Learning image representations on decentralized data can bring many benefits
in cases where data cannot be aggregated across data silos. Softmax cross
entropy loss is highly effective and commonly used for learning image
representations. Using a large number of classes has proven to be particularly
beneficial for the descriptive power of such representations in centralized
learning. However, doing so on decentralized data with Federated Learning is
not straightforward as the demand on FL clients' computation and communication
increases proportionally to the number of classes. In this work we introduce
federated sampled softmax (FedSS), a resource-efficient approach for learning
image representation with Federated Learning. Specifically, the FL clients
sample a set of classes and optimize only the corresponding model parameters
with respect to a sampled softmax objective that approximates the global full
softmax objective. We examine the loss formulation and empirically show that
our method significantly reduces the number of parameters transferred to and
optimized by the client devices, while performing on par with the standard full
softmax method. This work creates a possibility for efficiently learning image
representations on decentralized data with a large number of classes under the
federated setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-light Image and Video Enhancement via Selective Manipulation of Chromaticity. (arXiv:2203.04889v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04889">
<div class="article-summary-box-inner">
<span><p>Image acquisition in low-light conditions suffers from poor quality and
significant degradation in visual aesthetics. This affects the visual
perception of the acquired image and the performance of various computer vision
and image processing algorithms applied after acquisition. Especially for
videos, the additional temporal domain makes it more challenging, wherein we
need to preserve quality in a temporally coherent manner. We present a simple
yet effective approach for low-light image and video enhancement. To this end,
we introduce "Adaptive Chromaticity", which refers to an adaptive computation
of image chromaticity. The above adaptivity allows us to avoid the costly step
of low-light image decomposition into illumination and reflectance, employed by
many existing techniques. All stages in our method consist of only point-based
operations and high-pass or low-pass filtering, thereby ensuring that the
amount of temporal incoherence is negligible when applied on a per-frame basis
for videos. Our results on standard lowlight image datasets show the efficacy
of our algorithm and its qualitative and quantitative superiority over several
state-of-the-art techniques. For videos captured in the wild, we perform a user
study to demonstrate the preference for our method in comparison to
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction. (arXiv:2203.04895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04895">
<div class="article-summary-box-inner">
<span><p>Benefiting from color independence, illumination invariance and location
discrimination attributed by the depth map, it can provide important
supplemental information for extracting salient objects in complex
environments. However, high-quality depth sensors are expensive and can not be
widely applied. While general depth sensors produce the noisy and sparse depth
information, which brings the depth-based networks with irreversible
interference. In this paper, we propose a novel multi-task and multi-modal
filtered transformer (MMFT) network for RGB-D salient object detection (SOD).
Specifically, we unify three complementary tasks: depth estimation, salient
object detection and contour estimation. The multi-task mechanism promotes the
model to learn the task-aware features from the auxiliary tasks. In this way,
the depth information can be completed and purified. Moreover, we introduce a
multi-modal filtered transformer (MFT) module, which equips with three
modality-specific filters to generate the transformer-enhanced feature for each
modality. The proposed model works in a depth-free style during the testing
phase. Experiments show that it not only significantly surpasses the
depth-based RGB-D SOD methods on multiple datasets, but also precisely predicts
a high-quality depth map and salient contour at the same time. And, the
resulted depth map can help existing RGB-D SOD methods obtain significant
performance gain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04904">
<div class="article-summary-box-inner">
<span><p>Despite achieving state-of-the-art zero-shot performance, existing
vision-language models, e.g., CLIP, still fall short of domain-specific
classification tasks, e.g., Fungi Classification. In the context of few-shot
transfer learning, traditional fine-tuning fails to prevent highly expressive
model from exploiting spurious correlations in the training data. On the other
hand, although model-agnostic meta-learning (MAML) presents as a natural
alternative for transfer learning, the expensive computation due to implicit
second-order optimization limits its use in large-scale models and datasets. In
this work we aim to further improve the generalization of existing
vision-language models on unseen tasks via a simple yet efficient fine-tuning
strategy based on uniform task sampling. We term our method as Model-Agnostic
Multitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level
optimization and uses only first-order gradients, which makes it easily
scalable and computationally efficient. Due to the uniform task sampling
procedure, MAMF consistently outperforms the classical fine-tuning method for
few-shot transfer learning on five benchmark datasets. Empirically, we further
discover that the effectiveness of first-order MAML is highly dependent on the
zero-shot performance of the pretrained model, and our simple algorithm can
outperform first-order MAML on more challenging datasets with low zero-shot
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Matters For Meta-Learning Vision Regression Tasks?. (arXiv:2203.04905v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04905">
<div class="article-summary-box-inner">
<span><p>Meta-learning is widely used in few-shot classification and function
regression due to its ability to quickly adapt to unseen tasks. However, it has
not yet been well explored on regression tasks with high dimensional inputs
such as images. This paper makes two main contributions that help understand
this barely explored area. \emph{First}, we design two new types of
cross-category level vision regression tasks, namely object discovery and pose
estimation of unprecedented complexity in the meta-learning domain for computer
vision. To this end, we (i) exhaustively evaluate common meta-learning
techniques on these tasks, and (ii) quantitatively analyze the effect of
various deep learning techniques commonly used in recent meta-learning
algorithms in order to strengthen the generalization capability: data
augmentation, domain randomization, task augmentation and meta-regularization.
Finally, we (iii) provide some insights and practical recommendations for
training meta-learning algorithms on vision regression tasks. \emph{Second}, we
propose the addition of functional contrastive learning (FCL) over the task
representations in Conditional Neural Processes (CNPs) and train in an
end-to-end fashion. The experimental results show that the results of prior
work are misleading as a consequence of a poor choice of the loss function as
well as too small meta-training sets. Specifically, we find that CNPs
outperform MAML on most tasks without fine-tuning. Furthermore, we observe that
naive task augmentation without a tailored design results in underfitting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose Guided Multi-person Image Generation From Text. (arXiv:2203.04907v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04907">
<div class="article-summary-box-inner">
<span><p>Transformers have recently been shown to generate high quality images from
texts. However, existing methods struggle to create high fidelity full-body
images, especially multiple people. A person's pose has a high degree of
freedom that is difficult to describe using words only; this creates errors in
the generated image, such as incorrect body proportions and pose. We propose a
pose-guided text-to-image model, using pose as an additional input constraint.
Using the proposed Keypoint Pose Encoding (KPE) to encode human pose into low
dimensional representation, our model can generate novel multi-person images
accurately representing the pose and text descriptions provided, with minimal
errors. We demonstrate that KPE is invariant to changes in the target image
domain and image resolution; we show results on the Deepfashion dataset and
create a new multi-person Deepfashion dataset to demonstrate the
multi-capabilities of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking data-driven point spread function modeling with a differentiable optical model. (arXiv:2203.04908v1 [astro-ph.IM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04908">
<div class="article-summary-box-inner">
<span><p>In astronomy, upcoming space telescopes with wide-field optical instruments
have a spatially varying point spread function (PSF). Certain scientific goals
require a high-fidelity estimation of the PSF at target positions where no
direct measurement of the PSF is provided. Even though observations of the PSF
are available at some positions of the field of view (FOV), they are
undersampled, noisy, and integrated in wavelength in the instrument's passband.
PSF modeling requires building a model from these observations that can infer a
super-resolved PSF at any wavelength and any position in the FOV. Current
data-driven PSF models can tackle spatial variations and super-resolution, but
are not capable of capturing chromatic variations. Our model, coined WaveDiff,
proposes a paradigm shift in the data-driven modeling of the point spread
function field of telescopes. By adding a differentiable optical forward model
into the modeling framework, we change the data-driven modeling space from the
pixels to the wavefront. The proposed model relies on efficient automatic
differentiation technology as well as modern stochastic first-order
optimization techniques recently developed by the thriving machine-learning
community. Our framework paves the way to building powerful models that are
physically motivated and do not require special calibration data. This paper
demonstrates the WaveDiff model on a simplified setting of a space telescope.
The proposed framework represents a performance breakthrough with respect to
existing data-driven approaches. The pixel reconstruction errors decrease
6-fold at observation resolution and 44-fold for a 3x super-resolution. The
ellipticity errors are reduced by a factor of at least 20 and the size error by
a factor of more than 250. By only using noisy broad-band in-focus
observations, we successfully capture the PSF chromatic variations due to
diffraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers. (arXiv:2203.04913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04913">
<div class="article-summary-box-inner">
<span><p>Algorithmic fairness is frequently motivated in terms of a trade-off in which
overall performance is decreased so as to improve performance on disadvantaged
groups where the algorithm would otherwise be less accurate. Contrary to this,
we find that applying existing fairness approaches to computer vision improve
fairness by degrading the performance of classifiers across all groups (with
increased degradation on the best performing groups).
</p>
<p>Extending the bias-variance decomposition for classification to fairness, we
theoretically explain why the majority of fairness classifiers designed for low
capacity models should not be used in settings involving high-capacity models,
a scenario common to computer vision. We corroborate this analysis with
extensive experimental support that shows that many of the fairness heuristics
used in computer vision also degrade performance on the most disadvantaged
groups. Building on these insights, we propose an adaptive augmentation
strategy that, uniquely, of all methods tested, improves performance for the
disadvantaged groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Triangular Character Animation Sampling with Motion, Emotion, and Relation. (arXiv:2203.04930v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04930">
<div class="article-summary-box-inner">
<span><p>Dramatic progress has been made in animating individual characters. However,
we still lack automatic control over activities between characters, especially
those involving interactions. In this paper, we present a novel energy-based
framework to sample and synthesize animations by associating the characters'
body motions, facial expressions, and social relations. We propose a
Spatial-Temporal And-Or graph (ST-AOG), a stochastic grammar model, to encode
the contextual relationship between motion, emotion, and relation, forming a
triangle in a conditional random field. We train our model from a labeled
dataset of two-character interactions. Experiments demonstrate that our method
can recognize the social relation between two characters and sample new scenes
of vivid motion and emotion using Markov Chain Monte Carlo (MCMC) given the
social relation. Thus, our method can provide animators with an automatic way
to generate 3D character animations, help synthesize interactions between
Non-Player Characters (NPCs), and enhance machine emotion intelligence (EQ) in
virtual reality (VR).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the surprising tradeoff between ImageNet accuracy and perceptual similarity. (arXiv:2203.04946v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04946">
<div class="article-summary-box-inner">
<span><p>Perceptual distances between images, as measured in the space of pre-trained
deep features, have outperformed prior low-level, pixel-based metrics on
assessing image similarity. While the capabilities of older and less accurate
models such as AlexNet and VGG to capture perceptual similarity are well known,
modern and more accurate models are less studied. First, we observe a
surprising inverse correlation between ImageNet accuracy and Perceptual Scores
of modern networks such as ResNets, EfficientNets, and Vision Transformers:
that is better classifiers achieve worse Perceptual Scores. Then, we perform a
large-scale study and examine the ImageNet accuracy/Perceptual Score
relationship on varying the depth, width, number of training steps, weight
decay, label smoothing, and dropout. Higher accuracy improves Perceptual Score
up to a certain point, but we uncover a Pareto frontier between accuracies and
Perceptual Score in the mid-to-high accuracy regime. We explore this
relationship further using distortion invariance, spatial frequency
sensitivity, and alternative perceptual functions. Interestingly we discover
shallow ResNets, trained for less than 5 epochs only on ImageNet, whose
emergent Perceptual Score matches the prior best networks trained directly on
supervised human perceptual judgements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Deep Networks: Lagrangian Optimization via Log-Barrier Extensions. (arXiv:1904.04205v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.04205">
<div class="article-summary-box-inner">
<span><p>This study investigates imposing hard inequality constraints on the outputs
of convolutional neural networks (CNN) during training. Several recent works
showed that the theoretical and practical advantages of Lagrangian optimization
over simple penalties do not materialize in practice when dealing with modern
CNNs involving millions of parameters. Therefore, constrained CNNs are
typically handled with penalties. We propose *log-barrier extensions*, which
approximate Lagrangian optimization of constrained-CNN problems with a sequence
of unconstrained losses. Unlike standard interior-point and log-barrier
methods, our formulation does not need an initial feasible solution. The
proposed extension yields an upper bound on the duality gap -- generalizing the
result of standard log-barriers -- and yielding sub-optimality certificates for
feasible solutions. While sub-optimality is not guaranteed for non-convex
problems, this result shows that log-barrier extensions are a principled way to
approximate Lagrangian optimization for constrained CNNs via implicit dual
variables. We report weakly supervised image segmentation experiments, with
various constraints, showing that our formulation outperforms substantially the
existing constrained-CNN methods, in terms of accuracy, constraint satisfaction
and training stability, more so when dealing with a large number of
constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U$^2$-Net: Going Deeper with Nested U-Structure for Salient Object Detection. (arXiv:2005.09007v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.09007">
<div class="article-summary-box-inner">
<span><p>In this paper, we design a simple yet powerful deep network architecture,
U$^2$-Net, for salient object detection (SOD). The architecture of our
U$^2$-Net is a two-level nested U-structure. The design has the following
advantages: (1) it is able to capture more contextual information from
different scales thanks to the mixture of receptive fields of different sizes
in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the
whole architecture without significantly increasing the computational cost
because of the pooling operations used in these RSU blocks. This architecture
enables us to train a deep network from scratch without using backbones from
image classification tasks. We instantiate two models of the proposed
architecture, U$^2$-Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and
U$^2$-Net$^{\dagger}$ (4.7 MB, 40 FPS), to facilitate the usage in different
environments. Both models achieve competitive performance on six SOD datasets.
The code is available: https://github.com/NathanUA/U-2-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UKPGAN: A General Self-Supervised Keypoint Detector. (arXiv:2011.11974v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11974">
<div class="article-summary-box-inner">
<span><p>Keypoint detection is an essential component for the object registration and
alignment. In this work, we reckon keypoint detection as information
compression, and force the model to distill out irrelevant points of an object.
Based on this, we propose UKPGAN, a general self-supervised 3D keypoint
detector where keypoints are detected so that they could reconstruct the
original object shape. Two modules: GAN-based keypoint sparsity control and
salient information distillation modules are proposed to locate those important
keypoints. Extensive experiments show that our keypoints align well with human
annotated keypoint labels, and can be applied to SMPL human bodies under
various non-rigid deformations. Furthermore, our keypoint detector trained on
clean object collections generalizes well to real-world scenarios, thus further
improves geometric registration when combined with off-the-shelf point
descriptors. Repeatability experiments show that our model is stable under both
rigid and non-rigid transformations, with local reference frame estimation. Our
code is available on https://github.com/qq456cvb/UKPGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes. (arXiv:2011.12001v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12001">
<div class="article-summary-box-inner">
<span><p>3D object detection has attracted much attention thanks to the advances in
sensors and deep learning methods for point clouds. Current state-of-the-art
methods like VoteNet regress direct offset towards object centers and box
orientations with an additional Multi-Layer-Perceptron network. Both their
offset and orientation predictions are not accurate due to the fundamental
difficulty in rotation classification. In the work, we disentangle the direct
offset into Local Canonical Coordinates (LCC), box scales and box orientations.
Only LCC and box scales are regressed, while box orientations are generated by
a canonical voting scheme. Finally, an LCC-aware back-projection checking
algorithm iteratively cuts out bounding boxes from the generated vote maps,
with the elimination of false positives. Our model achieves state-of-the-art
performance on three standard real-world benchmarks: ScanNet, SceneNN and SUN
RGB-D. Our code is available on https://github.com/qq456cvb/CanonicalVoting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection. (arXiv:2103.00128v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00128">
<div class="article-summary-box-inner">
<span><p>With ever-increasing dataset sizes, subset selection techniques are becoming
increasingly important for a plethora of tasks. It is often necessary to guide
the subset selection to achieve certain desiderata, which includes focusing or
targeting certain data points, while avoiding others. Examples of such problems
include: i)targeted learning, where the goal is to find subsets with rare
classes or rare attributes on which the model is underperforming, and ii)guided
summarization, where data (e.g., image collection, text, document or video) is
summarized for quicker human consumption with specific additional user intent.
Motivated by such applications, we present PRISM, a rich class of PaRameterIzed
Submodular information Measures. Through novel functions and their
parameterizations, PRISM offers a variety of modeling capabilities that enable
a trade-off between desired qualities of a subset like diversity or
representation and similarity/dissimilarity with a set of data points. We
demonstrate how PRISM can be applied to the two real-world problems mentioned
above, which require guided subset selection. In doing so, we show that PRISM
interestingly generalizes some past work, therein reinforcing its broad
utility. Through extensive experiments on diverse datasets, we demonstrate the
superiority of PRISM over the state-of-the-art in targeted learning and in
guided image-collection summarization
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOTR: End-to-End Multiple-Object Tracking with Transformer. (arXiv:2105.03247v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03247">
<div class="article-summary-box-inner">
<span><p>Temporal modeling of objects is a key challenge in multiple-object tracking
(MOT). Existing methods track by associating detections through motion-based
and appearance-based similarity heuristics. The post-processing nature of
association prevents end-to-end exploitation of temporal variations in video
sequence. In this paper, we propose MOTR, which extends DETR and introduces
``track query'' to model the tracked instances in the entire video. Track query
is transferred and updated frame-by-frame to perform iterative prediction over
time. We propose tracklet-aware label assignment to train track queries and
newborn object queries. We further propose temporal aggregation network and
collective average loss to enhance temporal relation modeling. Experimental
results on DanceTrack show that MOTR significantly outperforms state-of-the-art
method, ByteTrack by 6.5% on HOTA metric. On MOT17, MOTR outperforms our
concurrent works, TrackFormer and TransTrack, on association performance. MOTR
can serve as a stronger baseline for future research on temporal modeling and
Transformer-based trackers.Code is available at
https://github.com/megvii-model/MOTR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation. (arXiv:2105.04447v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04447">
<div class="article-summary-box-inner">
<span><p>We propose a novel scene flow estimation approach to capture and infer 3D
motions from point clouds. Estimating 3D motions for point clouds is
challenging, since a point cloud is unordered and its density is significantly
non-uniform. Such unstructured data poses difficulties in matching
corresponding points between point clouds, leading to inaccurate flow
estimation. We propose a novel architecture named Sparse
Convolution-Transformer Network (SCTN) that equips the sparse convolution with
the transformer. Specifically, by leveraging the sparse convolution, SCTN
transfers irregular point cloud into locally consistent flow features for
estimating continuous and consistent motions within an object/local object
part. We further propose to explicitly learn point relations using a point
transformer module, different from exiting methods. We show that the learned
relation-based contextual information is rich and helpful for matching
corresponding points, benefiting scene flow estimation. In addition, a novel
loss function is proposed to adaptively encourage flow consistency according to
feature similarity. Extensive experiments demonstrate that our proposed
approach achieves a new state of the art in scene flow estimation. Our approach
achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene
Flow respectively, which significantly outperforms previous methods by large
margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUPR-GAN: SUrgical PRediction GAN for Event Anticipation in Laparoscopic and Robotic Surgery. (arXiv:2105.04642v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04642">
<div class="article-summary-box-inner">
<span><p>Comprehension of surgical workflow is the foundation upon which artificial
intelligence (AI) and machine learning (ML) holds the potential to assist
intraoperative decision-making and risk mitigation. In this work, we move
beyond mere identification of past surgical phases, into the prediction of
future surgical steps and specification of the transitions between them. We use
a novel Generative Adversarial Network (GAN) formulation to sample future
surgical phases trajectories conditioned on past video frames from laparoscopic
cholecystectomy (LC) videos and compare it to state-of-the-art approaches for
surgical video analysis and alternative prediction methods. We demonstrate the
GAN formulation's effectiveness through inferring and predicting the progress
of LC videos. We quantify the horizon-accuracy trade-off and explored average
performance, as well as the performance on the more challenging, and clinically
relevant transitions between phases. Furthermore, we conduct a survey, asking
16 surgeons of different specialties and educational levels to qualitatively
evaluate predicted surgery phases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Map for Active Semantic Goal Navigation. (arXiv:2106.15648v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15648">
<div class="article-summary-box-inner">
<span><p>We consider the problem of object goal navigation in unseen environments.
Solving this problem requires learning of contextual semantic priors, a
challenging endeavour given the spatial and semantic variability of indoor
environments. Current methods learn to implicitly encode these priors through
goal-oriented navigation policy functions operating on spatial representations
that are limited to the agent's observable areas. In this work, we propose a
novel framework that actively learns to generate semantic maps outside the
field of view of the agent and leverages the uncertainty over the semantic
classes in the unobserved areas to decide on long term goals. We demonstrate
that through this spatial prediction strategy, we are able to learn semantic
priors in scenes that can be leveraged in unknown environments. Additionally,
we show how different objectives can be defined by balancing exploration with
exploitation during searching for semantic targets. Our method is validated in
the visually realistic environments of the Matterport3D dataset and show
improved results on object goal navigation over competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening. (arXiv:2108.03815v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03815">
<div class="article-summary-box-inner">
<span><p>Anomaly detection plays a pivotal role in numerous real-world scenarios, such
as industrial automation and manufacturing intelligence. Recently, variational
inference-based anomaly analysis has attracted researchers' and developers'
attention. It aims to model the defect-free distribution so that anomalies can
be classified as out-of-distribution samples. Nevertheless, there are two
disturbing factors that need us to prioritize: (i) the simplistic prior latent
distribution inducing limited expressive capability; (ii) the strong
probability distance notion results in collapsed features. In this paper, we
propose a novel Patch-wise Wasserstein AutoEncoder (P-WAE) architecture to
alleviate those challenges. In particular, a patch-wise variational inference
model coupled with solving the jigsaw puzzle is designed, which is a simple yet
effective way to increase the expressiveness of the latent manifold. This makes
using the model on high-dimensional practical data possible. In addition, we
leverage a weaker measure, sliced-Wasserstein distance, to achieve the
equilibrium between the reconstruction fidelity and generalized
representations. Comprehensive experiments, conducted on the MVTec AD dataset,
demonstrate the superior performance of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FEDI: Few-shot learning based on Earth Mover's Distance algorithm combined with deep residual network to identify diabetic retinopathy. (arXiv:2108.09711v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09711">
<div class="article-summary-box-inner">
<span><p>Diabetic retinopathy(DR) is the main cause of blindness in diabetic patients.
However, DR can easily delay the occurrence of blindness through the diagnosis
of the fundus. In view of the reality, it is difficult to collect a large
amount of diabetic retina data in clinical practice. This paper proposes a
few-shot learning model of a deep residual network based on Earth Mover's
Distance algorithm to assist in diagnosing DR. We build training and validation
classification tasks for few-shot learning based on 39 categories of 1000
sample data, train deep residual networks, and obtain experience maximization
pre-training models. Based on the weights of the pre-trained model, the Earth
Mover's Distance algorithm calculates the distance between the images, obtains
the similarity between the images, and changes the model's parameters to
improve the accuracy of the training model. Finally, the experimental
construction of the small sample classification task of the test set to
optimize the model further, and finally, an accuracy of 93.5667% on the
3way10shot task of the diabetic retina test set. For the experimental code and
results, please refer to:
https://github.com/panliangrui/few-shot-learning-funds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSNet: A Dual-Stream Framework for Weakly-Supervised Gigapixel Pathology Image Analysis. (arXiv:2109.05788v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05788">
<div class="article-summary-box-inner">
<span><p>We present a novel weakly-supervised framework for classifying whole slide
images (WSIs). WSIs, due to their gigapixel resolution, are commonly processed
by patch-wise classification with patch-level labels. However, patch-level
labels require precise annotations, which is expensive and usually unavailable
on clinical data. With image-level labels only, patch-wise classification would
be sub-optimal due to inconsistency between the patch appearance and
image-level label. To address this issue, we posit that WSI analysis can be
effectively conducted by integrating information at both high magnification
(local) and low magnification (regional) levels. We auto-encode the visual
signals in each patch into a latent embedding vector representing local
information, and down-sample the raw WSI to hardware-acceptable thumbnails
representing regional information. The WSI label is then predicted with a
Dual-Stream Network (DSNet), which takes the transformed local patch embeddings
and multi-scale thumbnail images as inputs and can be trained by the
image-level label only. Experiments conducted on two large-scale public
datasets demonstrate that our method outperforms all recent state-of-the-art
weakly-supervised WSI classification methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Foreground Object Structure Transfer for Unsupervised Domain Adaptation. (arXiv:2109.06543v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06543">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation aims to train a classification model from the
labeled source domain for the unlabeled target domain. Since the data
distributions of the two domains are different, the model often performs poorly
on the target domain. Existing methods align the feature distributions of the
source and target domains and learn domain-invariant features to improve the
performance of the model. However, the features are usually aligned as a whole,
and the domain adaptation task fails to serve the classification, which will
ignore the class information and lead to misalignment.In this paper, we
investigate those features that should be used for domain alignment, introduce
prior knowledge to extract foreground features to guide the domain adaptation
task for classification tasks, and perform alignment in the local structure of
objects. We propose a method called Foreground Object Structure Transfer(FOST).
The key to FOST is the new clustering based condition, which combines the
relative position relationship of foreground objects. Based on this conditions,
FOST makes the data distribution of the same class more compact in geometry. In
practice, since the label of the target domain is not available, we use the
clustering information of the source domain to assign pseudo labels to the
target domain samples, and then according to the source domain data prior
knowledge guides those positive features to maximum the inter-class distance
between different classes and mimimum the intra-class distance. Extensive
experimental results on various benchmarks ($i.e.$ ImageCLEF-DA, Office-31,
Office-Home, Visda-2017) under different domain adaptation settings prove that
our FOST compares favorably against the existing state-of-the-art domain
adaptation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A review of deep learning methods for MRI reconstruction. (arXiv:2109.08618v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08618">
<div class="article-summary-box-inner">
<span><p>Following the success of deep learning in a wide range of applications,
neural network-based machine-learning techniques have received significant
interest for accelerating magnetic resonance imaging (MRI) acquisition and
reconstruction strategies. A number of ideas inspired by deep learning
techniques for computer vision and image processing have been successfully
applied to nonlinear image reconstruction in the spirit of compressed sensing
for accelerated MRI. Given the rapidly growing nature of the field, it is
imperative to consolidate and summarize the large number of deep learning
methods that have been reported in the literature, to obtain a better
understanding of the field in general. This article provides an overview of the
recent developments in neural-network based approaches that have been proposed
specifically for improving parallel imaging. A general background and
introduction to parallel MRI is also given from a classical view of k-space
based reconstruction methods. Image domain based techniques that introduce
improved regularizers are covered along with k-space based methods which focus
on better interpolation strategies using neural networks. While the field is
rapidly evolving with plenty of papers published each year, in this review, we
attempt to cover broad categories of methods that have shown good performance
on publicly available data sets. Limitations and open problems are also
discussed and recent efforts for producing open data sets and benchmarks for
the community are examined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirLoop: Lifelong Loop Closure Detection. (arXiv:2109.08975v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08975">
<div class="article-summary-box-inner">
<span><p>Loop closure detection is an important building block that ensures the
accuracy and robustness of simultaneous localization and mapping (SLAM)
systems. Due to their generalization ability, CNN-based approaches have
received increasing attention. Although they normally benefit from training on
datasets that are diverse and reflective of the environments, new environments
often emerge after the model is deployed. It is therefore desirable to
incorporate the data newly collected during operation for incremental learning.
Nevertheless, simply finetuning the model on new data is infeasible since it
may cause the model's performance on previously learned data to degrade over
time, which is also known as the problem of catastrophic forgetting. In this
paper, we present AirLoop, a method that leverages techniques from lifelong
learning to minimize forgetting when training loop closure detection models
incrementally. We experimentally demonstrate the effectiveness of AirLoop on
TartanAir, Nordland, and RobotCar datasets. To the best of our knowledge,
AirLoop is one of the first works to achieve lifelong learning of deep loop
closure detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirDOS: Dynamic SLAM benefits from Articulated Objects. (arXiv:2109.09903v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09903">
<div class="article-summary-box-inner">
<span><p>Dynamic Object-aware SLAM (DOS) exploits object-level information to enable
robust motion estimation in dynamic environments. Existing methods mainly focus
on identifying and excluding dynamic objects from the optimization. In this
paper, we show that feature-based visual SLAM systems can also benefit from the
presence of dynamic articulated objects by taking advantage of two
observations: (1) The 3D structure of each rigid part of articulated object
remains consistent over time; (2) The points on the same rigid part follow the
same motion. In particular, we present AirDOS, a dynamic object-aware system
that introduces rigidity and motion constraints to model articulated objects.
By jointly optimizing the camera pose, object motion, and the object 3D
structure, we can rectify the camera pose estimation, preventing tracking loss,
and generate 4D spatio-temporal maps for both dynamic objects and static
scenes. Experiments show that our algorithm improves the robustness of visual
SLAM algorithms in challenging crowded urban environments. To the best of our
knowledge, AirDOS is the first dynamic object-aware SLAM system demonstrating
that camera pose estimation can be improved by incorporating dynamic
articulated objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focal and Global Knowledge Distillation for Detectors. (arXiv:2111.11837v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11837">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation has been applied to image classification successfully.
However, object detection is much more sophisticated and most knowledge
distillation methods have failed on it. In this paper, we point out that in
object detection, the features of the teacher and student vary greatly in
different areas, especially in the foreground and background. If we distill
them equally, the uneven differences between feature maps will negatively
affect the distillation. Thus, we propose Focal and Global Distillation (FGD).
Focal distillation separates the foreground and background, forcing the student
to focus on the teacher's critical pixels and channels. Global distillation
rebuilds the relation between different pixels and transfers it from teachers
to students, compensating for missing global information in focal distillation.
As our method only needs to calculate the loss on the feature map, FGD can be
applied to various detectors. We experiment on various detectors with different
backbones and the results show that the student detector achieves excellent mAP
improvement. For example, ResNet-50 based RetinaNet, Faster RCNN, RepPoints and
Mask RCNN with our distillation method achieve 40.7%, 42.0%, 42.0% and 42.1%
mAP on COCO2017, which are 3.3, 3.6, 3.4 and 2.9 higher than the baseline,
respectively. Our codes are available at https://github.com/yzd-v/FGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstruction Student with Attention for Student-Teacher Pyramid Matching. (arXiv:2111.15376v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15376">
<div class="article-summary-box-inner">
<span><p>Anomaly detection and localization are important problems in computer vision.
Recently, Convolutional Neural Network (CNN) has been used for visual
inspection. In particular, the scarcity of anomalous samples increases the
difficulty of this task, and unsupervised leaning based methods are attracting
attention. We focus on Student-Teacher Feature Pyramid Matching (STPM) which
can be trained from only normal images with small number of epochs. Here we
proposed a powerful method which compensates for the shortcomings of STPM.
Proposed method consists of two students and two teachers that a pair of
student-teacher network is the same as STPM. The other student-teacher network
has a role to reconstruct the features of normal products. By reconstructing
the features of normal products from an abnormal image, it is possible to
detect abnormalities with higher accuracy by taking the difference between
them. The new student-teacher network uses attention modules and different
teacher network from the original STPM. Attention mechanism acts to
successfully reconstruct the normal regions in an input image. Different
teacher network prevents looking at the same regions as the original STPM. Six
anomaly maps obtained from the two student-teacher networks are used to
calculate the final anomaly map. Student-teacher network for reconstructing
features improved AUC scores for pixel level and image level in comparison with
the original STPM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Theoretic Representation Distillation. (arXiv:2112.00459v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00459">
<div class="article-summary-box-inner">
<span><p>Despite the empirical success of knowledge distillation, current
state-of-the-art methods are computationally expensive to train, which makes
them difficult to adopt in practice. To address this problem, we introduce two
distinct complementary losses inspired by a cheap entropy-like estimator. These
losses aim to maximise the correlation and mutual information between the
student and teacher representations. Our method incurs significantly less
training overheads than other approaches and achieves competitive performance
to state-of-the-art on the knowledge distillation and cross-model transfer
tasks. We further demonstrate the effectiveness of our method on a binary
distillation task, whereby it leads to a new state-of-the-art for binary
quantisation and approaches the performance of a full precision model. The
code, evaluation protocols, and trained models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Domain Adaptation: An Energy-Based Approach. (arXiv:2112.01406v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01406">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation has recently emerged as an effective paradigm
for generalizing deep neural networks to new target domains. However, there is
still enormous potential to be tapped to reach the fully supervised
performance. In this paper, we present a novel active learning strategy to
assist knowledge transfer in the target domain, dubbed active domain
adaptation. We start from an observation that energy-based models exhibit
\textit{free energy biases} when training (source) and test (target) data come
from different distributions. Inspired by this inherent mechanism, we
empirically reveal that a simple yet efficient energy-based sampling strategy
sheds light on selecting the most valuable target samples than existing
approaches requiring particular architectures or computation of the distances.
Our algorithm, Energy-based Active Domain Adaptation (EADA), queries groups of
target data that incorporate both domain characteristic and instance
uncertainty into every selection round. Meanwhile, by aligning the free energy
of target data compact around the source domain via a regularization term,
domain gap can be implicitly diminished. Through extensive experiments, we show
that EADA surpasses state-of-the-art methods on well-known challenging
benchmarks with substantial improvements, making it a useful option in the open
world. Code is available at https://github.com/BIT-DA/EADA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generate Point Clouds with Multiscale Details from Graph-Represented Structures. (arXiv:2112.06433v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06433">
<div class="article-summary-box-inner">
<span><p>As details are missing in most representations of structures, the lack of
controllability to details is one of the major weaknesses in structure-based
controllable point cloud generation. It is observable that definitions of
details and structures are subjective. Details can be treated as structures on
small scales. To represent structures in different scales at the same time, we
present a graph-based representation of structures called the Multiscale
Structure Graph(MSG). Given structures in multiple scales, similar patterns of
local structures can be found at different scales, positions, and angles. The
knowledge learned from a local structure pattern shall be transferred to other
similar patterns. An encoding and generation mechanism, namely the Multiscale
Structure-based Point Cloud Generator(MSPCG) is proposed, which can
simultaneously learn point cloud generation from local patterns with
miscellaneous spatial properties. By editing the MSG, the proposed method
supports multiscale editions on point clouds. By generating point clouds from
local structures and learning simultaneously in multiple scales, our MSPCG has
better generalization ability and scalability. Trained on the ShapeNet, our
MSPCG is able to generate point clouds for unseen categories and generate
indoor scenes from a given structure. The experimental results show that our
method significantly outperforms baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions. (arXiv:2112.08088v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08088">
<div class="article-summary-box-inner">
<span><p>Though deep learning-based object detection methods have achieved promising
results on the conventional datasets, it is still challenging to locate objects
from the low-quality images captured in adverse weather conditions. The
existing methods either have difficulties in balancing the tasks of image
enhancement and object detection, or often ignore the latent information
beneficial for detection. To alleviate this problem, we propose a novel
Image-Adaptive YOLO (IA-YOLO) framework, where each image can be adaptively
enhanced for better detection performance. Specifically, a differentiable image
processing (DIP) module is presented to take into account the adverse weather
conditions for YOLO detector, whose parameters are predicted by a small
convolutional neural net-work (CNN-PP). We learn CNN-PP and YOLOv3 jointly in
an end-to-end fashion, which ensures that CNN-PP can learn an appropriate DIP
to enhance the image for detection in a weakly supervised manner. Our proposed
IA-YOLO approach can adaptively process images in both normal and adverse
weather conditions. The experimental results are very encouraging,
demonstrating the effectiveness of our proposed IA-YOLO method in both foggy
and low-light scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Learning of Multi-category 3D Pose and Shape Estimation. (arXiv:2112.10196v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10196">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the representation of the shape and pose of objects
using their keypoints. Therefore, we propose an end-to-end method that
simultaneously detects 2D keypoints from an image and lifts them to 3D. The
proposed method learns both 2D detection and 3D lifting only from 2D keypoints
annotations. In addition to being end-to-end from images to 3D keypoints, our
method also handles objects from multiple categories using a single neural
network. We use a Transformer-based architecture to detect the keypoints, as
well as to summarize the visual context of the image. This visual context
information is then used while lifting the keypoints to 3D, to allow
context-based reasoning for better performance. Our method can handle
occlusions as well as a wide variety of object classes. Our experiments on
three benchmarks demonstrate that our method performs better than the
state-of-the-art. Our source code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Fine-grained Class Clustering via Generative Adversarial Networks. (arXiv:2112.14971v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14971">
<div class="article-summary-box-inner">
<span><p>Unsupervised fine-grained class clustering is a practical yet challenging
task due to the difficulty of feature representations learning of subtle object
details. We introduce C3-GAN, a method that leverages the categorical inference
power of InfoGAN with contrastive learning. We aim to learn feature
representations that encourage a dataset to form distinct cluster boundaries in
the embedding space, while also maximizing the mutual information between the
latent code and its image observation. Our approach is to train a
discriminator, which is also used for inferring clusters, to optimize the
contrastive loss, where image-latent pairs that maximize the mutual information
are considered as positive pairs and the rest as negative pairs. Specifically,
we map the input of a generator, which was sampled from the categorical
distribution, to the embedding space of the discriminator and let them act as a
cluster centroid. In this way, C3-GAN succeeded in learning a
clustering-friendly embedding space where each cluster is distinctively
separable. Experimental results show that C3-GAN achieved the state-of-the-art
clustering performance on four fine-grained image datasets, while also
alleviating the mode collapse phenomenon. Code is available at
https://github.com/naver-ai/c3-gan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseTact: Optical Tactile Sensor for Dense Shape Reconstruction. (arXiv:2201.01367v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01367">
<div class="article-summary-box-inner">
<span><p>Increasing the performance of tactile sensing in robots enables versatile,
in-hand manipulation. Vision-based tactile sensors have been widely used as
rich tactile feedback has been shown to be correlated with increased
performance in manipulation tasks. Existing tactile sensor solutions with high
resolution have limitations that include low accuracy, expensive components, or
lack of scalability. In this paper, an inexpensive, scalable, and compact
tactile sensor with high-resolution surface deformation modeling for surface
reconstruction of the 3D sensor surface is proposed. By measuring the image
from the fisheye camera, it is shown that the sensor can successfully estimate
the surface deformation in real-time (1.8ms) by using deep convolutional neural
networks. This sensor in its design and sensing abilities represents a
significant step toward better object in-hand localization, classification, and
surface estimation all enabled by high-resolution shape reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Compressive Imaging Reconstruction Using Convolution and Contextual Transformer. (arXiv:2201.05768v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05768">
<div class="article-summary-box-inner">
<span><p>Spectral compressive imaging (SCI) is able to encode the high-dimensional
hyperspectral image to a 2D measurement, and then uses algorithms to
reconstruct the spatio-spectral data-cube. At present, the main bottleneck of
SCI is the reconstruction algorithm, and the state-of-the-art (SOTA)
reconstruction methods generally face the problem of long reconstruction time
and/or poor detail recovery. In this paper, we propose a novel hybrid network
module, namely CCoT (Convolution and Contextual Transformer) block, which can
acquire the inductive bias ability of convolution and the powerful modeling
ability of transformer simultaneously,and is conducive to improving the quality
of reconstruction to restore fine details. We integrate the proposed CCoT block
into deep unfolding framework based on the generalized alternating projection
algorithm, and further propose the GAP-CCoT network. Finally, we apply the
GAP-CCoT algorithm to SCI reconstruction. Through the experiments of extensive
synthetic and real data, our proposed model achieves higher reconstruction
quality ($&gt;$2dB in PSNR on simulated benchmark datasets) and shorter running
time than existing SOTA algorithms by a large margin. The code and models will
be released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-arbitrary Invertible Image Downscaling. (arXiv:2201.12576v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12576">
<div class="article-summary-box-inner">
<span><p>Conventional social media platforms usually downscale the HR images to
restrict their resolution to a specific size for saving transmission/storage
cost, which leads to the super-resolution (SR) being highly ill-posed. Recent
invertible image downscaling methods jointly model the downscaling/upscaling
problems and achieve significant improvements. However, they only consider
fixed integer scale factors that cannot downscale HR images with various
resolutions to meet the resolution restriction of social media platforms. In
this paper, we propose a scale-Arbitrary Invertible image Downscaling Network
(AIDN), to natively downscale HR images with arbitrary scale factors.
Meanwhile, the HR information is embedded in the downscaled low-resolution (LR)
counterparts in a nearly imperceptible form such that our AIDN can also restore
the original HR images solely from the LR images. The key to supporting
arbitrary scale factors is our proposed Conditional Resampling Module (CRM)
that conditions the downscaling/upscaling kernels and sampling locations on
both scale factors and image content. Extensive experimental results
demonstrate that our AIDN achieves top performance for invertible downscaling
with both arbitrary integer and non-integer scale factors. Code will be
released upon publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13392">
<div class="article-summary-box-inner">
<span><p>The mortality of lung cancer has ranked high among cancers for many years.
Early detection of lung cancer is critical for disease prevention, cure, and
mortality rate reduction. However, existing detection methods on pulmonary
nodules introduce an excessive number of false positive proposals in order to
achieve high sensitivity, which is not practical in clinical situations. In
this paper, we propose the multi-head detection and spatial
squeeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to
aid doctors in the early diagnosis of lung cancers. Specifically, we first
introduce multi-head detectors and skip connections to customize for the
variety of nodules in sizes, shapes and types and capture multi-scale features.
Then, we implement a spatial attention module to enable the network to focus on
different regions differently inspired by how experienced clinicians screen CT
images, which results in fewer false positive proposals. Lastly, we present a
lightweight but effective false positive reduction module with the Linear
Regression model to cut down the number of false positive proposals, without
any constraints on the front network. Extensive experimental results compared
with the state-of-the-art models have shown the superiority of the MHSnet in
terms of the average FROC, sensitivity and especially false discovery rate
(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%
and 28.33% decrease in terms of false discovery rate and average candidates per
scan). The false positive reduction module significantly decreases the average
number of candidates generated per scan by 68.11% and the false discovery rate
by 13.48%, which is promising to reduce distracted proposals for the downstream
tasks based on the detection results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01993">
<div class="article-summary-box-inner">
<span><p>Visual question answering is the task of answering questions about images. We
introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually
grounds answers to visual questions asked by people with visual impairments. We
analyze our dataset and compare it with five VQA-Grounding datasets to
demonstrate what makes it similar and different. We then evaluate the SOTA VQA
and VQA-Grounding models and demonstrate that current SOTA algorithms often
fail to identify the correct visual evidence where the answer is located. These
models regularly struggle when the visual evidence occupies a small fraction of
the image, for images that are higher quality, as well as for visual questions
that require skills in text recognition. The dataset, evaluation server, and
leaderboard all can be found at the following link:
https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Roto-Translation Equivariant Super-Resolution of Two-Dimensional Flows Using Convolutional Neural Networks. (arXiv:2202.11099v2 [physics.flu-dyn] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11099">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) often apparently process vectors as
quantities that have no direction such as colors in images. This study
investigates the effect of considering vectors as geometric objects in terms of
super-resolution of velocity on two-dimensional fluids. Vector is distinguished
from scalar by the transformation law associated with a change in basis, which
can be incorporated as the prior knowledge using the equivariant deep learning.
The existing CNNs are converted into equivariant ones by rendering each layer
equivariant with respect to rotation and translation. The training data in the
low- and high-resolution are generated with the downsampling or spectral
nudging. With the date inheriting the rotational symmetry, the equivariant CNNs
exhibit comparable accuracy with the non-equivariant ones. Owing to the smaller
number of parameters in the equivariant CNNs, these models are trainable with a
smaller size of the data. In this case, the transformation law of vector should
be incorporated as the prior knowledge, where vector is treated as a quantity
having direction. Two examples are presented to demonstrate the possibility of
symmetry of the data being broken. In the first case, the downsampling method
renders the correspondence between low- and high-resolution patterns dependent
on the orientation. In the second case, the input data are insufficient to
recognize the rotation of coordinates. In both cases, the accuracy of the CNNs
deteriorates if the equivariance is forcefully imposed, and the conventional
CNNs should be used, where vector is not processed as a quantity having
direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Deep Learning for Image Classification with Distribution Mismatch: A Survey. (arXiv:2203.00190v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00190">
<div class="article-summary-box-inner">
<span><p>Deep learning methodologies have been employed in several different fields,
with an outstanding success in image recognition applications, such as material
quality control, medical imaging, autonomous driving, etc. Deep learning models
rely on the abundance of labelled observations to train a prospective model.
These models are composed of millions of parameters to estimate, increasing the
need of more training observations. Frequently it is expensive to gather
labelled observations of data, making the usage of deep learning models not
ideal, as the model might over-fit data. In a semi-supervised setting,
unlabelled data is used to improve the levels of accuracy and generalization of
a model with small labelled datasets. Nevertheless, in many situations
different unlabelled data sources might be available. This raises the risk of a
significant distribution mismatch between the labelled and unlabelled datasets.
Such phenomena can cause a considerable performance hit to typical
semi-supervised deep learning frameworks, which often assume that both labelled
and unlabelled datasets are drawn from similar distributions. Therefore, in
this paper we study the latest approaches for semi-supervised deep learning for
image recognition. Emphasis is made in semi-supervised deep learning models
designed to deal with a distribution mismatch between the labelled and
unlabelled datasets. We address open challenges with the aim to encourage the
community to tackle them, and overcome the high data demand of traditional deep
learning pipelines under real-world usage settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive Image Generation using Residual Quantization. (arXiv:2203.01941v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01941">
<div class="article-summary-box-inner">
<span><p>For autoregressive (AR) modeling of high-resolution images, vector
quantization (VQ) represents an image as a sequence of discrete codes. A short
sequence length is important for an AR model to reduce its computational costs
to consider long-range interactions of codes. However, we postulate that
previous VQ cannot shorten the code sequence and generate high-fidelity images
together in terms of the rate-distortion trade-off. In this study, we propose
the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and
RQ-Transformer, to effectively generate high-resolution images. Given a fixed
codebook size, RQ-VAE can precisely approximate a feature map of an image and
represent the image as a stacked map of discrete codes. Then, RQ-Transformer
learns to predict the quantized feature vector at the next position by
predicting the next stack of codes. Thanks to the precise approximation of
RQ-VAE, we can represent a 256$\times$256 image as 8$\times$8 resolution of the
feature map, and RQ-Transformer can efficiently reduce the computational costs.
Consequently, our framework outperforms the existing AR models on various
benchmarks of unconditional and conditional image generation. Our approach also
has a significantly faster sampling speed than previous AR models to generate
high-quality images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Neural Architecture Search for Lightweight Dense Prediction Networks. (arXiv:2203.01994v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01994">
<div class="article-summary-box-inner">
<span><p>We present LDP, a lightweight dense prediction neural architecture search
(NAS) framework. Starting from a pre-defined generic backbone, LDP applies the
novel Assisted Tabu Search for efficient architecture exploration. LDP is fast
and suitable for various dense estimation problems, unlike previous NAS methods
that are either computational demanding or deployed only for a single subtask.
The performance of LPD is evaluated on monocular depth estimation, semantic
segmentation, and image super-resolution tasks on diverse datasets, including
NYU-Depth-v2, KITTI, Cityscapes, COCO-stuff, DIV2K, Set5, Set14, BSD100,
Urban100. Experiments show that the proposed framework yields consistent
improvements on all tested dense prediction tasks, while being $5\%-315\%$ more
compact in terms of the number of model parameters than prior arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformations in Learned Image Compression from a Modulation Perspective. (arXiv:2203.02158v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02158">
<div class="article-summary-box-inner">
<span><p>In this paper, a unified transformation method in learned image
compression(LIC) is proposed from the perspective of modulation. Firstly, the
quantization in LIC is considered as a generalized channel with additive
uniform noise. Moreover, the LIC is interpreted as a particular communication
system according to the consistency in structures and optimization objectives.
Thus, the technology of communication systems can be applied to guide the
design of modules in LIC. Furthermore, a unified transform method based on
signal modulation (TSM) is defined. In the view of TSM, the existing
transformation methods are mathematically reduced to a linear modulation. A
series of transformation methods, e.g. TPM and TJM, are obtained by extending
to nonlinear modulation. The experimental results on various datasets and
backbone architectures verify that the effectiveness and robustness of the
proposed method. More importantly, it further confirms the feasibility of
guiding LIC design from a communication perspective. For example, when backbone
architecture is hyperprior combining context model, our method achieves
3.52$\%$ BD-rate reduction over GDN on Kodak dataset without increasing
complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPAL: Occlusion Pattern Aware Loss for Unsupervised Light Field Disparity Estimation. (arXiv:2203.02231v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02231">
<div class="article-summary-box-inner">
<span><p>Light field disparity estimation is an essential task in computer vision with
various applications. Although supervised learning-based methods have achieved
both higher accuracy and efficiency than traditional optimization-based
methods, the dependency on ground-truth disparity for training limits the
overall generalization performance not to say for real-world scenarios where
the ground-truth disparity is hard to capture. In this paper, we argue that
unsupervised methods can achieve comparable accuracy, but, more importantly,
much higher generalization capacity and efficiency than supervised methods.
Specifically, we present the Occlusion Pattern Aware Loss, named OPAL, which
successfully extracts and encodes the general occlusion patterns inherent in
the light field for loss calculation. OPAL enables: i) accurate and robust
estimation by effectively handling occlusions without using any ground-truth
information for training and ii) much efficient performance by significantly
reducing the network parameters required for accurate inference. Besides, a
transformer-based network and a refinement module are proposed for achieving
even more accurate results. Extensive experiments demonstrate our method not
only significantly improves the accuracy compared with the SOTA unsupervised
methods, but also possesses strong generalization capacity, even for real-world
data, compared with supervised methods. Our code will be made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highly Accurate Dichotomous Image Segmentation. (arXiv:2203.03041v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03041">
<div class="article-summary-box-inner">
<span><p>We present a systematic study on a new task called dichotomous image
segmentation (DIS), which aims to segment highly accurate objects from natural
images. To this end, we collected the first large-scale dataset, called DIS5K,
which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images covering
camouflaged, salient, or meticulous objects in various backgrounds. All images
are annotated with extremely fine-grained labels. In addition, we introduce a
simple intermediate supervision baseline (IS-Net) using both feature-level and
mask-level guidance for DIS model training. Without tricks, IS-Net outperforms
various cutting-edge baselines on the proposed DIS5K, making it a general
self-learned supervision network that can help facilitate future research in
DIS. Further, we design a new metric called human correction efforts (HCE)
which approximates the number of mouse clicking operations required to correct
the false positives and false negatives. HCE is utilized to measure the gap
between models and real-world applications and thus can complement existing
metrics. Finally, we conduct the largest-scale benchmark, evaluating 16
representative segmentation models, providing a more insightful discussion
regarding object complexities, and showing several potential applications
(e.g., background removal, art design, 3D reconstruction). Hoping these efforts
can open up promising directions for both academic and industries. We will
release our DIS5K dataset, IS-Net baseline, HCE metric, and the complete
benchmark results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehensive Review of Deep Learning-Based 3D Point Cloud Completion Processing and Analysis. (arXiv:2203.03311v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03311">
<div class="article-summary-box-inner">
<span><p>Point cloud completion is a generation and estimation issue derived from the
partial point clouds, which plays a vital role in the applications in 3D
computer vision. The progress of deep learning (DL) has impressively improved
the capability and robustness of point cloud completion. However, the quality
of completed point clouds is still needed to be further enhanced to meet the
practical utilization. Therefore, this work aims to conduct a comprehensive
survey on various methods, including point-based, convolution-based,
graph-based, and generative model-based approaches, etc. And this survey
summarizes the comparisons among these methods to provoke further research
insights. Besides, this review sums up the commonly used datasets and
illustrates the applications of point cloud completion. Eventually, we also
discussed possible research trends in this promptly expanding field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Student Become Decathlon Master in Retinal Vessel Segmentation via Dual-teacher Multi-target Domain Adaptation. (arXiv:2203.03631v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03631">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation has been proposed recently to tackle the
so-called domain shift between training data and test data with different
distributions. However, most of them only focus on single-target domain
adaptation and cannot be applied to the scenario with multiple target domains.
In this paper, we propose RVms, a novel unsupervised multi-target domain
adaptation approach to segment retinal vessels (RVs) from multimodal and
multicenter retinal images. RVms mainly consists of a style augmentation and
transfer (SAT) module and a dual-teacher knowledge distillation (DTKD) module.
SAT augments and clusters images into source-similar domains and
source-dissimilar domains via B\'ezier and Fourier transformations. DTKD
utilizes the augmented and transformed data to train two teachers, one for
source-similar domains and the other for source-dissimilar domains. Afterwards,
knowledge distillation is performed to iteratively distill different domain
knowledge from teachers to a generic student. The local relative intensity
transformation is employed to characterize RVs in a domain invariant manner and
promote the generalizability of teachers and student models. Moreover, we
construct a new multimodal and multicenter vascular segmentation dataset from
existing publicly-available datasets, which can be used to benchmark various
domain adaptation and domain generalization methods. Through extensive
experiments, RVms is found to be very close to the target-trained Oracle in
terms of segmenting the RVs, largely outperforming other state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon. (arXiv:2203.03818v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03818">
<div class="article-summary-box-inner">
<span><p>Estimating the risk level of adversarial examples is essential for safely
deploying machine learning models in the real world. One popular approach for
physical-world attacks is to adopt the "sticker-pasting" strategy, which
however suffers from some limitations, including difficulties in access to the
target or printing by valid colors. A new type of non-invasive attacks emerged
recently, which attempt to cast perturbation onto the target by optics based
tools, such as laser beam and projector. However, the added optical patterns
are artificial but not natural. Thus, they are still conspicuous and
attention-grabbed, and can be easily noticed by humans. In this paper, we study
a new type of optical adversarial examples, in which the perturbations are
generated by a very common natural phenomenon, shadow, to achieve naturalistic
and stealthy physical-world adversarial attack under the black-box setting. We
extensively evaluate the effectiveness of this new attack on both simulated and
real-world environments. Experimental results on traffic sign recognition
demonstrate that our algorithm can generate adversarial examples effectively,
reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets
respectively, while continuously misleading a moving camera over 95% of the
time in real-world scenarios. We also offer discussions about the limitations
and the defense mechanism of this attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention. (arXiv:2203.03937v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03937">
<div class="article-summary-box-inner">
<span><p>Recently, Transformers have shown promising performance in various vision
tasks. To reduce the quadratic computation complexity caused by each query
attending to all keys/values, various methods have constrained the range of
attention within local regions, where each query only attends to keys/values
within a hand-crafted window. However, these hand-crafted window partition
mechanisms are data-agnostic and ignore their input content, so it is likely
that one query maybe attends to irrelevant keys/values. To address this issue,
we propose a Dynamic Group Attention (DG-Attention), which dynamically divides
all queries into multiple groups and selects the most relevant keys/values for
each group. Our DG-Attention can flexibly model more relevant dependencies
without any spatial constraint that is used in hand-crafted window based
attention. Built on the DG-Attention, we develop a general vision transformer
backbone named Dynamic Group Transformer (DGT). Extensive experiments show that
our models can outperform the state-of-the-art methods on multiple common
vision tasks, including image classification, semantic segmentation, object
detection, and instance segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding person identification via gait. (arXiv:2203.04179v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04179">
<div class="article-summary-box-inner">
<span><p>Gait recognition is the process of identifying humans from their bipedal
locomotion such as walking or running. As such gait data is privacy sensitive
information and should be anonymized. With the rise of more and higher quality
gait recording techniques, such as depth cameras or motion capture suits, an
increasing amount of high-quality gait data becomes available which requires
anonymization. As a first step towards developing anonymization techniques for
high-quality gait data, we study different aspects of movement data to quantify
their contribution to the gait recognition process. We first extract categories
of features from the literature on human gait perception and then design
computational experiments for each of the categories which we run against a
gait recognition system. Our results show that gait anonymization is a
challenging process as the data is highly redundant and interdependent.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-10 23:08:15.769062990 UTC">2022-03-10 23:08:15 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>