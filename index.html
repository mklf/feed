<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-02T01:30:00Z">03-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reducing the Need for Speech Training Data To Build Spoken Language Understanding Systems. (arXiv:2203.00006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00006">
<div class="article-summary-box-inner">
<span><p>The lack of speech data annotated with labels required for spoken language
understanding (SLU) is often a major hurdle in building end-to-end (E2E)
systems that can directly process speech inputs. In contrast, large amounts of
text data with suitable labels are usually available. In this paper, we propose
a novel text representation and training methodology that allows E2E SLU
systems to be effectively constructed using these text resources. With very
limited amounts of additional speech, we show that these models can be further
improved to perform at levels close to similar systems built on the full speech
datasets. The efficacy of our proposed approach is demonstrated on both intent
and entity tasks using three different SLU datasets. With text-only training,
the proposed system achieves up to 90% of the performance possible with full
speech training. With just an additional 10% of speech data, these models
significantly improve further to 97% of full performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LISA: Learning Interpretable Skill Abstractions from Language. (arXiv:2203.00054v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00054">
<div class="article-summary-box-inner">
<span><p>Learning policies that effectually utilize language instructions in complex,
multi-task environments is an important problem in imitation learning. While it
is possible to condition on the entire language instruction directly, such an
approach could suffer from generalization issues. To encode complex
instructions into skills that can generalize to unseen instructions, we propose
Learning Interpretable Skill Abstractions (LISA), a hierarchical imitation
learning framework that can learn diverse, interpretable skills from
language-conditioned demonstrations. LISA uses vector quantization to learn
discrete skill codes that are highly correlated with language instructions and
the behavior of the learned policy. In navigation and robotic manipulation
environments, LISA is able to outperform a strong non-hierarchical baseline in
the low data regime and compose learned skills to solve tasks containing unseen
long-range instructions. Our method demonstrates a more natural way to
condition on language in sequential decision-making problems and achieve
interpretable and controllable behavior with the learned skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on Explanations in Out-of-Domain Settings. (arXiv:2203.00056v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00056">
<div class="article-summary-box-inner">
<span><p>Recent work in Natural Language Processing has focused on developing
approaches that extract faithful explanations, either via identifying the most
important tokens in the input (i.e. post-hoc explanations) or by designing
inherently faithful models that first select the most important tokens and then
use them to predict the correct label (i.e. select-then-predict models).
Currently, these approaches are largely evaluated on in-domain settings. Yet,
little is known about how post-hoc explanations and inherently faithful models
perform in out-of-domain settings. In this paper, we conduct an extensive
empirical study that examines: (1) the out-of-domain faithfulness of post-hoc
explanations, generated by five feature attribution methods; and (2) the
out-of-domain performance of two inherently faithful models over six datasets.
Contrary to our expectations, results show that in many cases out-of-domain
post-hoc explanation faithfulness measured by sufficiency and comprehensiveness
is higher compared to in-domain. We find this misleading and suggest using a
random baseline as a yardstick for evaluating post-hoc explanation
faithfulness. Our findings also show that select-then predict models
demonstrate comparable predictive performance in out-of-domain settings to
full-text trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Choosing on Sequences. (arXiv:2203.00070v1 [econ.TH])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00070">
<div class="article-summary-box-inner">
<span><p>The standard economic model of choice assumes that a decision maker chooses
from sets of alternatives. A new branch of literature has considered the
problem of choosing from lists i.e. ordered sets. In this paper, we propose a
new framework that considers choice from infinite sequences. Our framework
provides a natural way to model decision making in settings where choice relies
on a string of recommendations. We introduce three broad classes of choice
rules in this framework. Our main result shows that bounded attention is due to
the continuity of the choice functions with respect to a natural topology. We
introduce some natural choice rules in this framework and provide their
axiomatic characterizations. Finally, we introduce the notion of computability
of a choice function using Turing machines and show that computable choice
rules can be implemented by a finite automaton.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure Extraction in Task-Oriented Dialogues with Slot Clustering. (arXiv:2203.00073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00073">
<div class="article-summary-box-inner">
<span><p>Extracting structure information from dialogue data can help us better
understand user and system behaviors. In task-oriented dialogues, dialogue
structure has often been considered as transition graphs among dialogue states.
However, annotating dialogue states manually is expensive and time-consuming.
In this paper, we propose a simple yet effective approach for structure
extraction in task-oriented dialogues. We first detect and cluster possible
slot tokens with a pre-trained model to approximate dialogue ontology for a
target domain. Then we track the status of each identified token group and
derive a state transition structure. Empirical results show that our approach
outperforms unsupervised baseline models by far in dialogue structure
extraction. In addition, we show that data augmentation based on extracted
structures enriches the surface formats of training data and can achieve a
significant performance boost in dialogue response generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing. (arXiv:2203.00130v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00130">
<div class="article-summary-box-inner">
<span><p>When seeking information not covered in patient-friendly documents, like
medical pamphlets, healthcare consumers may turn to the research literature.
Reading medical papers, however, can be a challenging experience. To improve
access to medical papers, we introduce a novel interactive interface-Paper
Plain-with four features powered by natural language processing: definitions of
unfamiliar terms, in-situ plain language section summaries, a collection of key
questions that guide readers to answering passages, and plain language
summaries of the answering passages. We evaluate Paper Plain, finding that
participants who use Paper Plain have an easier time reading and understanding
research papers without a loss in paper comprehension compared to those who use
a typical PDF reader. Altogether, the study results suggest that guiding
readers to relevant passages and providing plain language summaries, or
"gists," alongside the original paper content can make reading medical papers
easier and give readers more confidence to approach these papers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Sentence Composition Reasoning for Multi-Hop Question Answering. (arXiv:2203.00160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00160">
<div class="article-summary-box-inner">
<span><p>Due to the lack of insufficient data, existing multi-hop open domain question
answering systems require to effectively find out relevant supporting facts
according to each question. To alleviate the challenges of semantic factual
sentences retrieval and multi-hop context expansion, we present a semantic
sentence composition reasoning approach for a multi-hop question answering
task, which consists of two key modules: a multi-stage semantic matching module
(MSSM) and a factual sentence composition module (FSC). With the combination of
factual sentences and multi-stage semantic retrieval, our approach can provide
more comprehensive contextual information for model training and reasoning.
Experimental results demonstrate our model is able to incorporate existing
pre-trained language models and outperform the existing SOTA method on the QASC
task with an improvement of about 9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Transformers use variable binding?. (arXiv:2203.00162v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00162">
<div class="article-summary-box-inner">
<span><p>Increasing the explainability of deep neural networks (DNNs) requires
evaluating whether they implement symbolic computation. One central symbolic
capacity is variable binding: linking an input value to an abstract variable
held in system-internal memory. Prior work on the computational abilities of
DNNs has not resolved the question of whether their internal processes involve
variable binding. We argue that the reason for this is fundamental, inherent in
the way experiments in prior work were designed. We provide the first
systematic evaluation of the variable binding capacities of the
state-of-the-art Transformer networks BERT and RoBERTa. Our experiments are
designed such that the model must generalize a rule across disjoint subsets of
the input vocabulary, and cannot rely on associative pattern matching alone.
The results show a clear discrepancy between classification and
sequence-to-sequence tasks: BERT and RoBERTa can easily learn to copy or
reverse strings even when trained on task-specific vocabularies that are
switched in the test set; but both models completely fail to generalize across
vocabularies in similar sequence classification tasks. These findings indicate
that the effectiveness of Transformers in sequence modelling may lie in their
extensive use of the input itself as an external "memory" rather than
network-internal symbolic operations involving variable binding. Therefore, we
propose a novel direction for future work: augmenting the inputs available to
circumvent the lack of network-internal variable binding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EPPAC: Entity Pre-typing Relation Classification with Prompt AnswerCentralizing. (arXiv:2203.00193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00193">
<div class="article-summary-box-inner">
<span><p>Relation classification (RC) aims to predict the relationship between a pair
of subject and object in a given context. Recently, prompt tuning approaches
have achieved high performance in RC. However, existing prompt tuning
approaches have the following issues: (1) numerous categories decrease RC
performance; (2) manually designed prompts require intensive labor. To address
these issues, a novel paradigm, Entity Pre-typing Relation Classification with
Prompt Answer Centralizing(EPPAC) is proposed in this paper. The entity
pre-tying in EPPAC is presented to address the first issue using a double-level
framework that pre-types entities before RC and prompt answer centralizing is
proposed to address the second issue. Extensive experiments show that our
proposed EPPAC outperformed state-of-the-art approaches on TACRED and TACREV by
14.4% and 11.1%, respectively. The code is provided in the Supplementary
Materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RMBR: A Regularized Minimum Bayes Risk Reranking Framework for Machine Translation. (arXiv:2203.00201v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00201">
<div class="article-summary-box-inner">
<span><p>Beam search is the most widely used decoding method for neural machine
translation (NMT). In practice, the top-1 candidate with the highest
log-probability among the n candidates is selected as the preferred one.
However, this top-1 candidate may not be the best overall translation among the
n-best list. Recently, Minimum Bayes Risk (MBR) decoding has been proposed to
improve the quality for NMT, which seeks for a consensus translation that is
closest on average to other candidates from the n-best list. We argue that MBR
still suffers from the following problems: The utility function only considers
the lexical-level similarity between candidates; The expected utility considers
the entire n-best list which is time-consuming and inadequate candidates in the
tail list may hurt the performance; Only the relationship between candidates is
considered. To solve these issues, we design a regularized MBR reranking
framework (RMBR), which considers semantic-based similarity and computes the
expected utility for each candidate by truncating the list. We expect the
proposed framework to further consider the translation quality and model
uncertainty of each candidate. Thus the proposed quality regularizer and
uncertainty regularizer are incorporated into the framework. Extensive
experiments on multiple translation tasks demonstrate the effectiveness of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings. (arXiv:2203.00211v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00211">
<div class="article-summary-box-inner">
<span><p>In order to equip NLP systems with selective prediction capability, several
task-specific approaches have been proposed. However, which approaches work
best across tasks or even if they consistently outperform the simplest baseline
'MaxProb' remains to be explored. To this end, we systematically study
'selective prediction' in a large-scale setup of 17 datasets across several NLP
tasks. Through comprehensive experiments under in-domain (IID), out-of-domain
(OOD), and adversarial (ADV) settings, we show that despite leveraging
additional resources (held-out data/computation), none of the existing
approaches consistently and considerably outperforms MaxProb in all three
settings. Furthermore, their performance does not translate well across tasks.
For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate
Detection datasets but does not fare well on NLI datasets, especially in the
OOD setting. Thus, we recommend that future selective prediction approaches
should be evaluated across tasks and settings for reliable estimation of their
capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extended Graph Temporal Classification for Multi-Speaker End-to-End ASR. (arXiv:2203.00232v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00232">
<div class="article-summary-box-inner">
<span><p>Graph-based temporal classification (GTC), a generalized form of the
connectionist temporal classification loss, was recently proposed to improve
automatic speech recognition (ASR) systems using graph-based supervision. For
example, GTC was first used to encode an N-best list of pseudo-label sequences
into a graph for semi-supervised learning. In this paper, we propose an
extension of GTC to model the posteriors of both labels and label transitions
by a neural network, which can be applied to a wider range of tasks. As an
example application, we use the extended GTC (GTC-e) for the multi-speaker
speech recognition task. The transcriptions and speaker information of
multi-speaker speech are represented by a graph, where the speaker information
is associated with the transitions and ASR outputs with the nodes. Using GTC-e,
multi-speaker ASR modelling becomes very similar to single-speaker ASR
modeling, in that tokens by multiple speakers are recognized as a single merged
sequence in chronological order. For evaluation, we perform experiments on a
simulated multi-speaker speech dataset derived from LibriSpeech, obtaining
promising results with performance close to classical benchmarks for the task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRILLsson: Distilled Universal Paralinguistic Speech Representations. (arXiv:2203.00236v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00236">
<div class="article-summary-box-inner">
<span><p>Recent advances in self-supervision have dramatically improved the quality of
speech representations. However, deployment of state-of-the-art embedding
models on devices has been restricted due to their limited public availability
and large resource footprint. Our work addresses these issues by publicly
releasing a collection of paralinguistic speech models that are small and near
state-of-the-art performance. Our approach is based on knowledge distillation,
and our models are distilled on public data only. We explore different
architectures and thoroughly evaluate our models on the Non-Semantic Speech
(NOSS) benchmark. Our largest distilled model is less than 15% the size of the
original model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7
tasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB)
and achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the
open source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model
outperforms the open source Wav2Vec 2.0 on both emotion recognition tasks
despite being 7% the size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment. (arXiv:2203.00242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00242">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language (V+L) pre-training models have achieved tremendous
success in recent years on various multi-modal benchmarks. However, the
majority of existing models require pre-training on a large set of parallel
image-text data, which is costly to collect, compared to image-only or
text-only data. In this paper, we explore unsupervised Vision-and-Language
pre-training (UVLP) to learn the cross-modal representation from non-parallel
image and text datasets. We found two key factors that lead to good
unsupervised V+L pre-training without parallel data: (i) joint image-and-text
input (ii) overall image-text alignment (even for non-parallel data).
Accordingly, we propose a novel unsupervised V+L pre-training curriculum for
non-parallel texts and images. We first construct a weakly aligned image-text
corpus via a retrieval-based approach, then apply a set of multi-granular
alignment pre-training tasks, including region-to-tag, region-to-phrase, and
image-to-sentence alignment, to bridge the gap between the two modalities. A
comprehensive ablation study shows each granularity is helpful to learn a
stronger pre-trained model. We adapt our pre-trained model to a set of V+L
downstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. Our
model achieves the state-of-art performance in all these tasks under the
unsupervised setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring and Adapting Chinese GPT to Pinyin Input Method. (arXiv:2203.00249v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00249">
<div class="article-summary-box-inner">
<span><p>While GPT has become the de-facto method for text generation tasks, its
application to pinyin input method remains unexplored. In this work, we make
the first exploration to leverage Chinese GPT for pinyin input method. We find
that a frozen GPT achieves state-of-the-art performance on perfect pinyin.
However, the performance drops dramatically when the input includes abbreviated
pinyin. A reason is that an abbreviated pinyin can be mapped to many perfect
pinyin, which links to even larger number of Chinese characters. We mitigate
this issue with two strategies, including enriching the context with pinyin and
optimizing the training process to help distinguish homophones. To further
facilitate the evaluation of pinyin input method, we create a dataset
consisting of 270K instances from 15 domains. Results show that our approach
improves performance on abbreviated pinyin across all domains. Model analysis
demonstrates that both strategies contribute to the performance boost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs. (arXiv:2203.00255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00255">
<div class="article-summary-box-inner">
<span><p>Question answering over temporal knowledge graphs (KGs) efficiently uses
facts contained in a temporal KG, which records entity relations and when they
occur in time, to answer natural language questions (e.g., "Who was the
president of the US before Obama?"). These questions often involve three
time-related challenges that previous work fail to adequately address: 1)
questions often do not specify exact timestamps of interest (e.g., "Obama"
instead of 2000); 2) subtle lexical differences in time relations (e.g.,
"before" vs "after"); 3) off-the-shelf temporal KG embeddings that previous
work builds on ignore the temporal order of timestamps, which is crucial for
answering temporal-order related questions. In this paper, we propose a
time-sensitive question answering (TSQA) framework to tackle these problems.
TSQA features a timestamp estimation module to infer the unwritten timestamp
from the question. We also employ a time-sensitive KG encoder to inject
ordering information into the temporal KG embeddings that TSQA is based on.
With the help of techniques to reduce the search space for potential answers,
TSQA significantly outperforms the previous state of the art on a new benchmark
for question answering over temporal KGs, especially achieving a 32% (absolute)
error reduction on complex questions that require multiple steps of reasoning
over facts in the temporal KG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors. (arXiv:2203.00257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00257">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis has attracted increasing attention and lots of
models have been proposed. However, the performance of the state-of-the-art
models decreases sharply when they are deployed in the real world. We find that
the main reason is that real-world applications can only access the text
outputs by the automatic speech recognition (ASR) models, which may be with
errors because of the limitation of model capacity. Through further analysis of
the ASR outputs, we find that in some cases the sentiment words, the key
sentiment elements in the textual modality, are recognized as other words,
which makes the sentiment of the text change and hurts the performance of
multimodal sentiment models directly. To address this problem, we propose the
sentiment word aware multimodal refinement model (SWRM), which can dynamically
refine the erroneous sentiment words by leveraging multimodal sentiment clues.
Specifically, we first use the sentiment word position detection module to
obtain the most possible position of the sentiment word in the text and then
utilize the multimodal sentiment word refinement module to dynamically refine
the sentiment word embeddings. The refined embeddings are taken as the textual
inputs of the multimodal feature fusion module to predict the sentiment labels.
We conduct extensive experiments on the real-world datasets including
MOSI-Speechbrain, MOSI-IBM, and MOSI-iFlytek and the results demonstrate the
effectiveness of our model, which surpasses the current state-of-the-art models
on three datasets. Furthermore, our approach can be adapted for other
multimodal feature fusion models easily. Data and code are available at
https://github.com/albertwy/SWRM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArabGend: Gender Analysis and Inference on Arabic Twitter. (arXiv:2203.00271v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00271">
<div class="article-summary-box-inner">
<span><p>Gender analysis of Twitter can reveal important socio-cultural differences
between male and female users. There has been a significant effort to analyze
and automatically infer gender in the past for most widely spoken languages'
content, however, to our knowledge very limited work has been done for Arabic.
In this paper, we perform an extensive analysis of differences between male and
female users on the Arabic Twitter-sphere. We study differences in user
engagement, topics of interest, and the gender gap in professions. Along with
gender analysis, we also propose a method to infer gender by utilizing
usernames, profile pictures, tweets, and networks of friends. In order to do
so, we manually annotated gender and locations for ~166K Twitter accounts
associated with ~92K user location, which we plan to make publicly available at
<a href="http://anonymous.com.">this http URL</a> Our proposed gender inference method achieve an F1 score
of 82.1%, which is 47.3% higher than majority baseline. In addition, we also
developed a demo and made it publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TableFormer: Robust Transformer Modeling for Table-Text Encoding. (arXiv:2203.00274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00274">
<div class="article-summary-box-inner">
<span><p>Understanding tables is an important aspect of natural language
understanding. Existing models for table understanding require linearization of
the table structure, where row or column order is encoded as an unwanted bias.
Such spurious biases make the model vulnerable to row and column order
perturbations. Additionally, prior work has not thoroughly modeled the table
structures or table-text alignments, hindering the table-text understanding
ability. In this work, we propose a robust and structurally aware table-text
encoding architecture TableFormer, where tabular structural biases are
incorporated completely through learnable attention biases. TableFormer is (1)
strictly invariant to row and column orders, and, (2) could understand tables
better due to its tabular inductive biases. Our evaluations showed that
TableFormer outperforms strong baselines in all settings on SQA, WTQ and
TabFact table reasoning datasets, and achieves state-of-the-art performance on
SQA, especially when facing answer-invariant row and column order perturbations
(6% improvement over the best baseline), because previous SOTA models'
performance drops by 4% - 6% when facing such perturbations while TableFormer
is not affected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation. (arXiv:2203.00281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00281">
<div class="article-summary-box-inner">
<span><p>Recently CKY-based models show great potential in unsupervised grammar
induction thanks to their human-like encoding paradigm, which runs recursively
and hierarchically, but requires $O(n^3)$ time-complexity. Recursive
Transformer based on Differentiable Trees (R2D2) makes it possible to scale to
large language model pre-training even with complex tree encoder by introducing
a heuristic pruning method. However, the rule-based pruning approach suffers
from local optimum and slow inference issues. In this paper, we fix those
issues in a unified method. We propose to use a top-down parser as a
model-based pruning method, which also enables parallel encoding during
inference. Typically, our parser casts parsing as a split point scoring task,
which first scores all split points for a given sentence, and then recursively
splits a span into two by picking a split point with the highest score in the
current span. The reverse order of the splits is considered as the order of
pruning in R2D2 encoder. Beside the bi-directional language model loss, we also
optimize the parser by minimizing the KL distance between tree probabilities
from parser and R2D2. Our experiments show that our Fast-R2D2 improves
performance significantly in grammar induction and achieves competitive results
in downstream classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Is Whole Word Masking Always Better for Chinese BERT?": Probing on Chinese Grammatical Error Correction. (arXiv:2203.00286v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00286">
<div class="article-summary-box-inner">
<span><p>Whole word masking (WWM), which masks all subwords corresponding to a word at
once, makes a better English BERT model. For the Chinese language, however,
there is no subword because each token is an atomic character. The meaning of a
word in Chinese is different in that a word is a compositional unit consisting
of multiple characters. Such difference motivates us to investigate whether WWM
leads to better context understanding ability for Chinese BERT. To achieve
this, we introduce two probing tasks related to grammatical error correction
and ask pretrained models to revise or insert tokens in a masked language
modeling manner. We construct a dataset including labels for 19,075 tokens in
10,448 sentences. We train three Chinese BERT models with standard
character-level masking (CLM), WWM, and a combination of CLM and WWM,
respectively. Our major findings are as follows: First, when one character
needs to be inserted or replaced, the model trained with CLM performs the best.
Second, when more than one character needs to be handled, WWM is the key to
better performance. Finally, when being fine-tuned on sentence-level downstream
tasks, models trained with different masking strategies perform comparably.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VScript: Controllable Script Generation with Audio-Visual Presentation. (arXiv:2203.00314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00314">
<div class="article-summary-box-inner">
<span><p>Automatic script generation could save a considerable amount of resources and
offer inspiration to professional scriptwriters. We present VScript, a
controllable pipeline that generates complete scripts including dialogues and
scene descriptions, and presents visually using video retrieval and aurally
using text-to-speech for spoken dialogue. With an interactive interface, our
system allows users to select genres and input starting words that control the
theme and development of the generated script. We adopt a hierarchical
structure, which generates the plot, then the script and its audio-visual
presentation. We also introduce a novel approach to plot-guided dialogue
generation by treating it as an inverse dialogue summarization. Experiment
results show that our approach outperforms the baselines on both automatic and
human evaluations, especially in terms of genre control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-LID: Leveraging BERT to Improve Spoken Language Identification. (arXiv:2203.00328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00328">
<div class="article-summary-box-inner">
<span><p>Language identification is a task of automatically determining the identity
of a language conveyed by a spoken segment. It has a profound impact on the
multilingual interoperability of an intelligent speech system. Despite language
identification attaining high accuracy on medium or long utterances (&gt;3s), the
performance on short utterances (&lt;=1s) is still far from satisfactory. We
propose an effective BERT-based language identification system (BERT-LID) to
improve language identification performance, especially on short-duration
speech segments. To adapt BERT into the LID pipeline, we drop in a conjunction
network prior to BERT to accommodate the frame-level Phonetic
Posteriorgrams(PPG) derived from the frontend phone recognizer and then
fine-tune the conjunction network and BERT pre-trained model together. We
evaluate several variations within this piped framework, including combining
BERT with CNN, LSTM, DPCNN, and RCNN. The experimental results demonstrate that
the best-performing model is RCNN-BERT. Compared with the prior works, our
RCNN-BERT model can improve the accuracy by about 5% on long-segment
identification and 18% on short-segment identification. The outperformance of
our model, especially on the short-segment task, demonstrates the applicability
of our proposed BERT-based approach on language identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Read before Generate! Faithful Long Form Question Answering with Machine Reading. (arXiv:2203.00343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00343">
<div class="article-summary-box-inner">
<span><p>Long-form question answering (LFQA) aims to generate a paragraph-length
answer for a given question. While current work on LFQA using large pre-trained
model for generation are effective at producing fluent and somewhat relevant
content, one primary challenge lies in how to generate a faithful answer that
has less hallucinated content. We propose a new end-to-end framework that
jointly models answer generation and machine reading. The key idea is to
augment the generation model with fine-grained, answer-related salient
information which can be viewed as an emphasis on faithful facts.
State-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate
the effectiveness of our method, in comparison with strong baselines on
automatic and human evaluation metrics. A detailed analysis further proves the
competency of our methods in generating fluent, relevant, and more faithful
answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Performance of Automated Essay Scoring by using back-translation essays and adjusted scores. (arXiv:2203.00354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00354">
<div class="article-summary-box-inner">
<span><p>Automated essay scoring plays an important role in judging students' language
abilities in education. Traditional approaches use handcrafted features to
score and are time-consuming and complicated. Recently, neural network
approaches have improved performance without any feature engineering. Unlike
other natural language processing tasks, only a small number of datasets are
publicly available for automated essay scoring, and the size of the dataset is
not sufficiently large. Considering that the performance of a neural network is
closely related to the size of the dataset, the lack of data limits the
performance improvement of the automated essay scoring model. In this paper, we
proposed a method to increase the number of essay-score pairs using
back-translation and score adjustment and applied it to the Automated Student
Assessment Prize dataset for augmentation. We evaluated the effectiveness of
the augmented data using models from prior work. In addition, performance was
evaluated in a model using long short-term memory, which is widely used for
automated essay scoring. The performance of the models was improved by using
augmented data to train the models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning. (arXiv:2203.00357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00357">
<div class="article-summary-box-inner">
<span><p>Logical reasoning is of vital importance to natural language understanding.
Previous studies either employ graph-based models to incorporate prior
knowledge about logical relations, or introduce symbolic logic into neural
models through data augmentation. These methods, however, heavily depend on
annotated training data, and thus suffer from over-fitting and poor
generalization problems due to the dataset sparsity. To address these two
problems, in this paper, we propose MERIt, a MEta-path guided contrastive
learning method for logical ReasonIng of text, to perform self-supervised
pre-training on abundant unlabeled text data. Two novel strategies serve as
indispensable components of our method. In particular, a strategy based on
meta-path is devised to discover the logical structure in natural texts,
followed by a counterfactual data augmentation strategy to eliminate the
information shortcut induced by pre-training. The experimental results on two
challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate
that our method outperforms the SOTA baselines with significant improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition. (arXiv:2203.00545v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00545">
<div class="article-summary-box-inner">
<span><p>The MultiCoNER shared task aims at detecting semantically ambiguous and
complex named entities in short and low-context settings for multiple
languages. The lack of contexts makes the recognition of ambiguous named
entities challenging. To alleviate this issue, our team DAMO-NLP proposes a
knowledge-based system, where we build a multilingual knowledge base based on
Wikipedia to provide related context information to the named entity
recognition (NER) model. Given an input sentence, our system effectively
retrieves related contexts from the knowledge base. The original input
sentences are then augmented with such context information, allowing
significantly better contextualized token representations to be captured. Our
system wins 10 out of 13 tracks in the MultiCoNER shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepNet: Scaling Transformers to 1,000 Layers. (arXiv:2203.00555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00555">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a simple yet effective method to stabilize
extremely deep Transformers. Specifically, we introduce a new normalization
function (DeepNorm) to modify the residual connection in Transformer,
accompanying with theoretically derived initialization. In-depth theoretical
analysis shows that model updates can be bounded in a stable way. The proposed
method combines the best of two worlds, i.e., good performance of Post-LN and
stable training of Pre-LN, making DeepNorm a preferred alternative. We
successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and
feed-forward network sublayers) without difficulty, which is one order of
magnitude deeper than previous deep Transformers. Remarkably, on a multilingual
benchmark with 7,482 translation directions, our 200-layer model with 3.2B
parameters significantly outperforms the 48-layer state-of-the-art model with
12B parameters by 5 BLEU points, which indicates a promising scaling direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topological Data Analysis for Word Sense Disambiguation. (arXiv:2203.00565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00565">
<div class="article-summary-box-inner">
<span><p>We develop and test a novel unsupervised algorithm for word sense induction
and disambiguation which uses topological data analysis. Typical approaches to
the problem involve clustering, based on simple low level features of distance
in word embeddings. Our approach relies on advanced mathematical concepts in
the field of topology which provides a richer conceptualization of clusters for
the word sense induction tasks. We use a persistent homology barcode algorithm
on the SemCor dataset and demonstrate that our approach gives low relative
error on word sense induction. This shows the promise of topological algorithms
for natural language processing and we advocate for future work in this
promising area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural invariants and semantic fingerprints in the "ego network" of words. (arXiv:2203.00588v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00588">
<div class="article-summary-box-inner">
<span><p>Well-established cognitive models coming from anthropology have shown that,
due to the cognitive constraints that limit our "bandwidth" for social
interactions, humans organize their social relations according to a regular
structure. In this work, we postulate that similar regularities can be found in
other cognitive processes, such as those involving language production. In
order to investigate this claim, we analyse a dataset containing tweets of a
heterogeneous group of Twitter users (regular users and professional writers).
Leveraging a methodology similar to the one used to uncover the
well-established social cognitive constraints, we find regularities at both the
structural and semantic level. At the former, we find that a concentric layered
structure (which we call ego network of words, in analogy to the ego network of
social relationships) very well captures how individuals organise the words
they use. The size of the layers in this structure regularly grows
(approximately 2-3 times with respect to the previous one) when moving
outwards, and the two penultimate external layers consistently account for
approximately 60% and 30% of the used words, irrespective of the number of the
total number of layers of the user. For the semantic analysis, each ring of
each ego network is described by a semantic profile, which captures the topics
associated with the words in the ring. We find that ring #1 has a special role
in the model. It is semantically the most dissimilar and the most diverse among
the rings. We also show that the topics that are important in the innermost
ring also have the characteristic of being predominant in each of the other
rings, as well as in the entire ego network. In this respect, ring #1 can be
seen as the semantic fingerprint of the ego network of words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Common Speech Analysis Engine. (arXiv:2203.00613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00613">
<div class="article-summary-box-inner">
<span><p>Recent innovations in self-supervised representation learning have led to
remarkable advances in natural language processing. That said, in the speech
processing domain, self-supervised representation learning-based systems are
not yet considered state-of-the-art. We propose leveraging recent advances in
self-supervised-based speech processing to create a common speech analysis
engine. Such an engine should be able to handle multiple speech processing
tasks, using a single architecture, to obtain state-of-the-art accuracy. The
engine must also enable support for new tasks with small training datasets.
Beyond that, a common engine should be capable of supporting distributed
training with client in-house private data. We present the architecture for a
common speech analysis engine based on the HuBERT self-supervised speech
representation. Based on experiments, we report our results for language
identification and emotion recognition on the standard evaluations NIST-LRE 07
and IEMOCAP. Our results surpass the state-of-the-art performance reported so
far on these tasks. We also analyzed our engine on the emotion recognition task
using reduced amounts of training data and show how to achieve improved
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale. (arXiv:2203.00633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00633">
<div class="article-summary-box-inner">
<span><p>Transformer language models that are trained on vast amounts of data have
achieved remarkable success at various NLP benchmarks. Intriguingly, this
success is achieved by models that lack an explicit modeling of hierarchical
syntactic structures, which were hypothesized by decades of linguistic research
to be necessary for good generalization. This naturally leaves a question: to
what extent can we further improve the performance of Transformer language
models, through an inductive bias that encourages the model to explain the data
through the lens of recursive syntactic compositions? Although the benefits of
modeling recursive syntax have been shown at the small data and model scales,
it remains an open question whether -- and to what extent -- a similar design
principle is still beneficial in the case of powerful Transformer language
models that work well at scale. To answer these questions, we introduce
Transformer Grammars -- a novel class of Transformer language models that
combine: (i) the expressive power, scalability, and strong performance of
Transformers, and (ii) recursive syntactic compositions, which here are
implemented through a special attention mask. We find that Transformer Grammars
outperform various strong baselines on multiple syntax-sensitive language
modeling evaluation metrics, in addition to sentence-level language modeling
perplexity. Nevertheless, we find that the recursive syntactic composition
bottleneck harms perplexity on document-level modeling, providing evidence that
a different kind of memory mechanism -- that works independently of syntactic
structures -- plays an important role in the processing of long-form text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Impact of Individual Domain Factors in Self-Supervised Pre-Training. (arXiv:2203.00648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00648">
<div class="article-summary-box-inner">
<span><p>Human speech data comprises a rich set of domain factors such as accent,
syntactic and semantic variety, or acoustic environment. Previous work explores
the effect of domain mismatch in automatic speech recognition between
pre-training and fine-tuning as a whole but does not dissect the contribution
of individual factors. In this paper, we present a controlled study to better
understand the effect of such factors on the performance of pre-trained
representations. To do so, we pre-train models either on modified natural
speech or synthesized audio, with a single domain factor modified, and then
measure performance on automatic speech recognition after fine tuning. Results
show that phonetic domain factors play an important role during pre-training
while grammatical and syntactic factors are far less important. To our
knowledge, this is the first study to better understand the domain
characteristics in self-supervised pre-training for speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing an Interdisciplinary Science of Conversation: Insights from a Large Multimodal Corpus of Human Speech. (arXiv:2203.00674v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00674">
<div class="article-summary-box-inner">
<span><p>People spend a substantial portion of their lives engaged in conversation,
and yet our scientific understanding of conversation is still in its infancy.
In this report we advance an interdisciplinary science of conversation, with
findings from a large, novel, multimodal corpus of 1,656 recorded conversations
in spoken English. This 7+ million word, 850 hour corpus totals over 1TB of
audio, video, and transcripts, with moment-to-moment measures of vocal, facial,
and semantic expression, along with an extensive survey of speaker post
conversation reflections. We leverage the considerable scope of the corpus to
(1) extend key findings from the literature, such as the cooperativeness of
human turn-taking; (2) define novel algorithmic procedures for the segmentation
of speech into conversational turns; (3) apply machine learning insights across
various textual, auditory, and visual features to analyze what makes
conversations succeed or fail; and (4) explore how conversations are related to
well-being across the lifespan. We also report (5) a comprehensive mixed-method
report, based on quantitative analysis and qualitative review of each
recording, that showcases how individuals from diverse backgrounds alter their
communication patterns and find ways to connect. We conclude with a discussion
of how this large-scale public dataset may offer new directions for future
research, especially across disciplinary boundaries, as scholars from a variety
of fields appear increasingly interested in the study of conversation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04293">
<div class="article-summary-box-inner">
<span><p>Humans are able to perceive, understand and reason about causal events.
Developing models with similar physical and causal understanding capabilities
is a long-standing goal of artificial intelligence. As a step towards this
direction, we introduce CRAFT, a new video question answering dataset that
requires causal reasoning about physical forces and object interactions. It
contains 58K video and question pairs that are generated from 10K videos from
20 different virtual environments, containing various objects in motion that
interact with each other and the scene. Two question categories in CRAFT
include previously studied descriptive and counterfactual questions.
Additionally, inspired by the Force Dynamics Theory in cognitive linguistics,
we introduce a new causal question category that involves understanding the
causal interactions between objects through notions like cause, enable, and
prevent. Our results show that even though the questions in CRAFT are easy for
humans, the tested baseline models, including existing state-of-the-art
methods, do not yet deal with the challenges posed in our benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariance, encodings, and generalization: learning identity effects with neural networks. (arXiv:2101.08386v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08386">
<div class="article-summary-box-inner">
<span><p>Often in language and other areas of cognition, whether two components of an
object are identical or not determines if it is well formed. We call such
constraints identity effects. When developing a system to learn well-formedness
from examples, it is easy enough to build in an identify effect. But can
identity effects be learned from the data without explicit guidance? We provide
a framework in which we can rigorously prove that algorithms satisfying simple
criteria cannot make the correct inference. We then show that a broad class of
learning algorithms including deep feedforward neural networks trained via
gradient-based algorithms (such as stochastic gradient descent or the Adam
method) satisfy our criteria, dependent on the encoding of inputs. In some
broader circumstances we are able to provide adversarial examples that the
network necessarily classifies incorrectly. Finally, we demonstrate our theory
with computational experiments in which we explore the effect of different
input encodings on the ability of algorithms to generalize to novel inputs.
This allows us to show similar effects to those predicted by theory for more
realistic methods that violate some of the conditions of our theoretical
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotating Columns with Pre-trained Language Models. (arXiv:2104.01785v2 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01785">
<div class="article-summary-box-inner">
<span><p>Inferring meta information about tables, such as column headers or
relationships between columns, is an active research topic in data management
as we find many tables are missing some of this information. In this paper, we
study the problem of annotating table columns (i.e., predicting column types
and the relationships between columns) using only information from the table
itself. We develop a multi-task learning framework (called Doduo) based on
pre-trained language models, which takes the entire table as input and predicts
column types/relations using a single model. Experimental results show that
Doduo establishes new state-of-the-art performance on two benchmarks for the
column type prediction and column relation prediction tasks with up to 4.0% and
11.9% improvements, respectively. We report that Doduo can already outperform
the previous state-of-the-art performance with a minimal number of tokens, only
8 tokens per column. We release a toolbox
(https://github.com/megagonlabs/doduo) and confirm the effectiveness of Doduo
on a real-world data science problem through a case study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Transport-based Adaptation in Dysarthric Speech Tasks. (arXiv:2104.02535v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02535">
<div class="article-summary-box-inner">
<span><p>In many real-world applications, the mismatch between distributions of
training data (source) and test data (target) significantly degrades the
performance of machine learning algorithms. In speech data, causes of this
mismatch include different acoustic environments or speaker characteristics. In
this paper, we address this issue in the challenging context of dysarthric
speech, by multi-source domain/speaker adaptation (MSDA/MSSA). Specifically, we
propose the use of an optimal-transport based approach, called MSDA via
Weighted Joint Optimal Transport (MSDA-WDJOT). We confront the mismatch problem
in dysarthria detection for which the proposed approach outperforms both the
Baseline and the state-of-the-art MSDA models, improving the detection accuracy
of 0.9% over the best competitor method. We then employ MSDA-WJDOT for
dysarthric speaker adaptation in command speech recognition. This provides a
Command Error Rate relative reduction of 16% and 7% over the baseline and the
best competitor model, respectively. Interestingly, MSDA-WJDOT provides a
similarity score between the source and the target, i.e. between speakers in
this case. We leverage this similarity measure to define a Dysarthric and
Healthy score of the target speaker and diagnose the dysarthria with an
accuracy of 95%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Membership Inference Attacks on Knowledge Graphs. (arXiv:2104.08273v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08273">
<div class="article-summary-box-inner">
<span><p>Membership inference attacks (MIAs) infer whether a specific data record is
used for target model training. MIAs have provoked many discussions in the
information security community since they give rise to severe data privacy
issues, especially for private and sensitive datasets. Knowledge Graphs (KGs),
which describe domain-specific subjects and relationships among them, are
valuable and sensitive, such as medical KGs constructed from electronic health
records. However, the privacy threat to knowledge graphs is critical but rarely
explored. In this paper, we conduct the first empirical evaluation of privacy
threats to knowledge graphs triggered by knowledge graph embedding methods
(KGEs). We propose three types of membership inference attacks: transfer
attacks (TAs), prediction loss-based attacks (PLAs), and prediction
correctness-based attacks (PCAs), according to attack difficulty levels. In the
experiments, we conduct three inference attacks against four standard KGE
methods over three benchmark datasets. In addition, we also propose the attacks
against medical KG and financial KG. The results demonstrate that the proposed
attack methods can easily explore the privacy leakage of knowledge graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring diachronic sense change: new models and Monte Carlo methods for Bayesian inference. (arXiv:2105.00819v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00819">
<div class="article-summary-box-inner">
<span><p>In a bag-of-words model, the senses of a word with multiple meanings, e.g.
"bank" (used either in a river-bank or an institution sense), are represented
as probability distributions over context words, and sense prevalence is
represented as a probability distribution over senses. Both of these may change
with time. Modelling and measuring this kind of sense change is challenging due
to the typically high-dimensional parameter space and sparse datasets. A
recently published corpus of ancient Greek texts contains expert-annotated
sense labels for selected target words. Automatic sense-annotation for the word
"kosmos" (meaning decoration, order or world) has been used as a test case in
recent work with related generative models and Monte Carlo methods. We adapt an
existing generative sense change model to develop a simpler model for the main
effects of sense and time, and give MCMC methods for Bayesian inference on all
these models that are more efficient than existing methods. We carry out
automatic sense-annotation of snippets containing "kosmos" using our model, and
measure the time-evolution of its three senses and their prevalence. As far as
we are aware, ours is the first analysis of this data, within the class of
generative models we consider, that quantifies uncertainty and returns credible
sets for evolving sense prevalence in good agreement with those given by expert
annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weighted Training for Cross-Task Learning. (arXiv:2105.14095v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14095">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted
training algorithm for cross-task learning based on minimizing a
representation-based task distance between the source and target tasks. We show
that TAWT is easy to implement, is computationally efficient, requires little
hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees.
The effectiveness of TAWT is corroborated through extensive experiments with
BERT on four sequence tagging tasks in natural language processing (NLP),
including part-of-speech (PoS) tagging, chunking, predicate detection, and
named entity recognition (NER). As a byproduct, the proposed
representation-based task distance allows one to reason in a theoretically
principled way about several critical aspects of cross-task learning, such as
the choice of the source data and the impact of fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Temperature Matters in Abstractive Summarization Distillation. (arXiv:2106.03441v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03441">
<div class="article-summary-box-inner">
<span><p>Recent progress of abstractive text summarization largely relies on large
pre-trained sequence-to-sequence Transformer models, which are computationally
expensive. This paper aims to distill these large models into smaller ones for
faster inference and minimal performance loss. Pseudo-labeling based methods
are popular in sequence-to-sequence model distillation. In this paper, we find
simply manipulating attention temperatures in Transformers can make pseudo
labels easier to learn for student models. Our experiments on three
summarization datasets show our proposed method consistently improves over
vanilla pseudo-labeling based methods. We also find that both the pseudo labels
and summaries produced by our students are shorter and more abstractive. Our
code is available at \url{https://github.com/Shengqiang-Zhang/plate}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network. (arXiv:2108.02352v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02352">
<div class="article-summary-box-inner">
<span><p>Aspect-level sentiment classification (ASC) aims to predict the fine-grained
sentiment polarity towards a given aspect mentioned in a review. Despite recent
advances in ASC, enabling machines to preciously infer aspect sentiments is
still challenging. This paper tackles two challenges in ASC: (1) due to lack of
aspect knowledge, aspect representation derived in prior works is inadequate to
represent aspect's exact meaning and property information; (2) prior works only
capture either local syntactic information or global relational information,
thus missing either one of them leads to insufficient syntactic information. To
tackle these challenges, we propose a novel ASC model which not only end-to-end
embeds and leverages aspect knowledge but also marries the two kinds of
syntactic information and lets them compensate for each other. Our model
includes three key components: (1) a knowledge-aware gated recurrent memory
network recurrently integrates dynamically summarized aspect knowledge; (2) a
dual syntax graph network combines both kinds of syntactic information to
comprehensively capture sufficient syntactic information; (3) a knowledge
integrating gate re-enhances the final representation with further needed
aspect knowledge; (4) an aspect-to-context attention mechanism aggregates the
aspect-related semantics from all hidden states into the final representation.
Experimental results on several benchmark datasets demonstrate the
effectiveness of our model, which overpass previous state-of-the-art models by
large margins in terms of both Accuracy and Macro-F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues. (arXiv:2109.00430v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00430">
<div class="article-summary-box-inner">
<span><p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a
range of professional medical services, i.e., diagnosis, treatment and
consultation. The development of MDSs is hindered because of a lack of
resources. In particular. (1) there is no dataset with large-scale medical
dialogues that covers multiple medical services and contains fine-grained
medical labels (i.e., intents, actions, slots, values), and (2) there is no set
of established benchmarks for MDSs for multi-domain, multi-service medical
dialogues. In this paper, we present ReMeDi, a set of resource for medical
dialogues. ReMeDi consists of two parts, the ReMeDi dataset and the ReMeDi
benchmarks. The ReMeDi dataset contains 96,965 conversations between doctors
and patients, including 1,557 conversations with fine-gained labels. It covers
843 types of diseases, 5,228 medical entities, and 3 specialties of medical
services across 40 domains. To the best of our knowledge, the ReMeDi dataset is
the only medical dialogue dataset that covers multiple domains and services,
and has fine-grained medical labels. The second part of the ReMeDi resources
consists of a set of state-of-the-art models for (medical) dialogue generation.
The ReMeDi benchmark has the following methods: (1) pretrained models (i.e.,
BERT-WWM, BERT-MED, GPT2, and MT5) trained, validated, and tested on the ReMeDi
dataset, and (2) a self-supervised contrastive learning(SCL) method to expand
the ReMeDi dataset and enhance the training of the state-of-the-art pretrained
models. We describe the creation of the ReMeDi dataset, the ReMeDi benchmarking
methods, and establish experimental results using the ReMeDi benchmarking
methods on the ReMeDi dataset for future research to compare against. With this
paper, we share the dataset, implementations of the benchmarks, and evaluation
scripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System. (arXiv:2109.14739v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14739">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been recently shown to benefit task-oriented
dialogue (TOD) systems. Despite their success, existing methods often formulate
this task as a cascaded generation problem which can lead to error accumulation
across different sub-tasks and greater data annotation overhead. In this study,
we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In
addition, we introduce a new dialogue multi-task pre-training strategy that
allows the model to learn the primary TOD task completion skills from
heterogeneous dialog corpora. We extensively test our model on three benchmark
TOD tasks, including end-to-end dialogue modelling, dialogue state tracking,
and intent classification. Experimental results show that PPTOD achieves new
state of the art on all evaluated tasks in both high-resource and low-resource
scenarios. Furthermore, comparisons against previous SOTA methods show that the
responses generated by PPTOD are more factually correct and semantically
coherent as judged by human annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval. (arXiv:2110.07524v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07524">
<div class="article-summary-box-inner">
<span><p>Training dense passage representations via contrastive learning has been
shown effective for Open-Domain Passage Retrieval (ODPR). Existing studies
focus on further optimizing by improving negative sampling strategy or extra
pretraining. However, these studies keep unknown in capturing passage with
internal representation conflicts from improper modeling granularity. This work
thus presents a refined model on the basis of a smaller granularity, contextual
sentences, to alleviate the concerned conflicts. In detail, we introduce an
in-passage negative sampling strategy to encourage a diverse generation of
sentence representations within the same passage. Experiments on three
benchmark datasets verify the efficacy of our method, especially on datasets
where conflicts are severe. Extensive experiments further present good
transferability of our method across datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answering Open-Domain Multi-Answer Questions via a Recall-then-Verify Framework. (arXiv:2110.08544v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08544">
<div class="article-summary-box-inner">
<span><p>Open-domain questions are likely to be open-ended and ambiguous, leading to
multiple valid answers. Existing approaches typically adopt the
rerank-then-read framework, where a reader reads top-ranking evidence to
predict answers. According to our empirical analysis, this framework faces
three problems: first, to leverage a large reader under a memory constraint,
the reranker should select only a few relevant passages to cover diverse
answers, while balancing relevance and diversity is non-trivial; second, the
small reading budget prevents the reader from accessing valuable retrieved
evidence filtered out by the reranker; third, when using a generative reader to
predict answers all at once based on all selected evidence, whether a valid
answer will be predicted also pathologically depends on the evidence of some
other valid answer(s). To address these issues, we propose to answer
open-domain multi-answer questions with a recall-then-verify framework, which
separates the reasoning process of each answer so that we can make better use
of retrieved evidence while also leveraging large models under the same memory
constraint. Our framework achieves state-of-the-art results on two multi-answer
datasets, and predicts significantly more gold answers than a rerank-then-read
system that uses an oracle reranker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAIR: Data Augmented Invariant Regularization. (arXiv:2110.11205v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11205">
<div class="article-summary-box-inner">
<span><p>While deep learning through empirical risk minimization (ERM) has succeeded
at achieving human-level performance at a variety of complex tasks, ERM
generalizes poorly to distribution shift. This is partly explained by
overfitting to spurious features such as background in images or named entities
in natural language. Synthetic data augmentation followed by empirical risk
minimization (DA-ERM) is a simple and widely used solution to remedy this
problem. In addition, consistency regularization could be applied to further
promote model performance to be consistent on the augmented sample and the
original one. In this paper, we propose data augmented invariant regularization
(DAIR), a simple form of consistency regularization that is applied directly on
the loss function rather than intermediate features, making it widely
applicable regardless of network architecture or problem setup. We apply DAIR
to multiple real-world learning problems, namely robust regression, visual
question answering, robust deep neural network training, and neural
task-oriented dialog modeling. Our experiments show that DAIR consistently
outperforms ERM and DA-ERM with little marginal cost and sets new
state-of-the-art results in several benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientWord-Net: An Open Source Hotword Detection Engine based on One-shot Learning. (arXiv:2111.00379v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00379">
<div class="article-summary-box-inner">
<span><p>Voice assistants like Siri, Google Assistant, Alexa etc. are used widely
across the globe for home automation, these require the use of special phrases
also known as hotwords to wake it up and perform an action like "Hey Alexa!",
"Ok Google!" and "Hey Siri!" etc. These hotwords are detected with lightweight
real-time engines whose purpose is to detect the hotwords uttered by the user.
This paper presents the design and implementation of a hotword detection engine
based on one-shot learning which detects the hotword uttered by the user in
real-time with just one or few training samples of the hotword. This approach
is efficient when compared to existing implementations because the process of
adding a new hotword in the existing systems requires enormous amounts of
positive and negative training samples and the model needs to retrain for every
hotword. This makes the existing implementations inefficient in terms of
computation and cost. The architecture proposed in this paper has achieved an
accuracy of 94.51%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis. (arXiv:2111.03133v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03133">
<div class="article-summary-box-inner">
<span><p>Generating images that fit a given text description using machine learning
has improved greatly with the release of technologies such as the CLIP
image-text encoder model; however, current methods lack artistic control of the
style of image to be generated. We introduce StyleCLIPDraw which adds a style
loss to the CLIPDraw text-to-drawing synthesis model to allow artistic control
of the synthesized drawings in addition to control of the content via text.
Whereas performing decoupled style transfer on a generated image only affects
the texture, our proposed coupled approach is able to capture a style in both
texture and shape, suggesting that the style of the drawing is coupled with the
drawing process itself. More results and our code are available at
https://github.com/pschaldenbrand/StyleCLIPDraw
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning for Monolingual End-to-End Automatic Speech Recognition. (arXiv:2112.09427v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09427">
<div class="article-summary-box-inner">
<span><p>Adapting Automatic Speech Recognition (ASR) models to new domains leads to a
deterioration of performance on the original domain(s), a phenomenon called
Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to
new accents, dialects, topics, etc. without suffering from CF, making them
unable to be continually enhanced without storing all past data. Fortunately,
Continual Learning (CL) methods, which aim to enable continual adaptation while
overcoming CF, can be used. In this paper, we implement an extensive number of
CL methods for End-to-End ASR and test and compare their ability to extend a
monolingual Hybrid CTC-Transformer model across four new tasks. We find that
the best performing CL method closes the gap between the fine-tuned model
(lower bound) and the model trained jointly on all tasks (upper bound) by more
than 40%, while requiring access to only 0.6% of the original data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval. (arXiv:2201.05409v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05409">
<div class="article-summary-box-inner">
<span><p>Ad-hoc search calls for the selection of appropriate answers from a
massive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a
promising solution, where deep learning based document representation and ANN
search techniques are allied to handle this task. However, a major challenge is
that the ANN index can be too large to fit into memory, given the considerable
size of answer corpus. In this work, we tackle this problem with Bi-Granular
Document Representation, where the lightweight sparse embeddings are indexed
and standby in memory for coarse-grained candidate search, and the heavyweight
dense embeddings are hosted in disk for fine-grained post verification. For the
best of retrieval accuracy, a Progressive Optimization framework is designed.
The sparse embeddings are learned ahead for high-quality search of candidates.
Conditioned on the candidate distribution induced by the sparse embeddings, the
dense embeddings are continuously learned to optimize the discrimination of
ground-truth from the shortlisted candidates. Besides, two techniques: the
contrastive quantization and the locality-centric sampling are introduced for
the learning of sparse and dense embeddings, which substantially contribute to
their performances. Thanks to the above features, our method effectively
handles massive-scale EBR with strong advantages in accuracy: with up to +4.3%
recall gain on million-scale corpus, and up to +17.5% recall gain on
billion-scale corpus. Besides, Our method is applied to a major sponsored
search platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and
CTR (+0.49%). Our code is available at https://github.com/microsoft/BiDR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Typical Decoding for Natural Language Generation. (arXiv:2202.00666v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00666">
<div class="article-summary-box-inner">
<span><p>Despite achieving incredibly low perplexities on myriad natural language
corpora, today's language models still often underperform when used to generate
text. This dichotomy has puzzled the language generation community for the last
few years. In this work, we posit that the abstraction of natural language as a
communication channel (\`a la Shannon, 1948) can provide new insights into the
behaviors of probabilistic language generators, e.g., why high-probability
texts can be dull or repetitive. Humans use language as a means of
communicating information, and do so in a simultaneously efficient and
error-minimizing manner; they choose each word in a string with this (perhaps
subconscious) goal in mind. We propose that generation from probabilistic
models should mimic this behavior. Rather than always choosing words from the
high-probability region of the distribution--which have a low Shannon
information content--we sample from the set of words with information content
close to the conditional entropy of our model, i.e., close to the expected
information content. This decision criterion can be realized through a simple
and efficient implementation, which we call typical sampling. Automatic and
human evaluations show that, in comparison to nucleus and top-k sampling,
typical sampling offers competitive performance in terms of quality while
consistently reducing the number of degenerate repetitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. (arXiv:2202.01279v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01279">
<div class="article-summary-box-inner">
<span><p>PromptSource is a system for creating, sharing, and using natural language
prompts. Prompts are functions that map an example from a dataset to a natural
language input and target output. Using prompts to train and query language
models is an emerging area in NLP that requires new tools that let users
develop and refine these prompts collaboratively. PromptSource addresses the
emergent challenges in this new setting with (1) a templating language for
defining data-linked prompts, (2) an interface that lets users quickly iterate
on prompt development by observing outputs of their prompts on many examples,
and (3) a community-driven set of guidelines for contributing new prompts to a
common pool. Over 2,000 prompts for roughly 170 datasets are already available
in PromptSource. PromptSource is available at
https://github.com/bigscience-workshop/promptsource.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding. (arXiv:2202.13093v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13093">
<div class="article-summary-box-inner">
<span><p>Contrastive learning is emerging as a powerful technique for extracting
knowledge from unlabeled data. This technique requires a balanced mixture of
two ingredients: positive (similar) and negative (dissimilar) samples. This is
typically achieved by maintaining a queue of negative samples during training.
Prior works in the area typically uses a fixed-length negative sample queue,
but how the negative sample size affects the model performance remains unclear.
The opaque impact of the number of negative samples on performance when
employing contrastive learning aroused our in-depth exploration. This paper
presents a momentum contrastive learning model with negative sample queue for
sentence embedding, namely MoCoSE. We add the prediction layer to the online
branch to make the model asymmetric and together with EMA update mechanism of
the target branch to prevent model from collapsing. We define a maximum
traceable distance metric, through which we learn to what extent the text
contrastive learning benefits from the historical information of negative
samples. Our experiments find that the best results are obtained when the
maximum traceable distance is at a certain range, demonstrating that there is
an optimal range of historical information for a negative sample queue. We
evaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS)
task and obtain an average Spearman's correlation of $77.27\%$. Source code is
available at https://github.com/xbdxwyh/mocose
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Modular Skills in Multitask Learning. (arXiv:2202.13914v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13914">
<div class="article-summary-box-inner">
<span><p>A modular design encourages neural models to disentangle and recombine
different facets of knowledge to generalise more systematically to new tasks.
In this work, we assume that each task is associated with a subset of latent
discrete skills from a (potentially small) inventory. In turn, skills
correspond to parameter-efficient (sparse / low-rank) model parameterisations.
By jointly learning these and a task-skill allocation matrix, the network for
each task is instantiated as the average of the parameters of active skills. To
favour non-trivial soft partitions of skills across tasks, we experiment with a
series of inductive biases, such as an Indian Buffet Process prior and a
two-speed learning rate. We evaluate our latent-skill model on two main
settings: 1) multitask reinforcement learning for grounded instruction
following on 8 levels of the BabyAI platform; and 2) few-shot adaptation of
pre-trained text-to-text generative models on CrossFit, a benchmark comprising
160 NLP tasks. We find that the modular design of a network significantly
increases sample efficiency in reinforcement learning and few-shot
generalisation in supervised learning, compared to baselines with fully shared,
task-specific, or conditionally generated parameters where knowledge is
entangled across tasks. In addition, we show how discrete skills help
interpretability, as they yield an explicit hierarchy of tasks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-temporal Vision Transformer for Super-resolution Microscopy. (arXiv:2203.00030v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00030">
<div class="article-summary-box-inner">
<span><p>Structured illumination microscopy (SIM) is an optical super-resolution
technique that enables live-cell imaging beyond the diffraction limit.
Reconstruction of SIM data is prone to artefacts, which becomes problematic
when imaging highly dynamic samples because previous methods rely on the
assumption that samples are static. We propose a new transformer-based
reconstruction method, VSR-SIM, that uses shifted 3-dimensional window
multi-head attention in addition to channel attention mechanism to tackle the
problem of video super-resolution (VSR) in SIM. The attention mechanisms are
found to capture motion in sequences without the need for common motion
estimation techniques such as optical flow. We take an approach to training the
network that relies solely on simulated data using videos of natural scenery
with a model for SIM image formation. We demonstrate a use case enabled by
VSR-SIM referred to as rolling SIM imaging, which increases temporal resolution
in SIM by a factor of 9. Our method can be applied to any SIM setup enabling
precise recordings of dynamic processes in biomedical research with high
temporal resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voxelmorph++ Going beyond the cranial vault with keypoint supervision and multi-channel instance optimisation. (arXiv:2203.00046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00046">
<div class="article-summary-box-inner">
<span><p>The majority of current research in deep learning based image registration
addresses inter-patient brain registration with moderate deformation
magnitudes. The recent Learn2Reg medical registration benchmark has
demonstrated that single-scale U-Net architectures, such as VoxelMorph that
directly employ a spatial transformer loss, often do not generalise well beyond
the cranial vault and fall short of state-of-the-art performance for abdominal
or intra-patient lung registration. Here, we propose two straightforward steps
that greatly reduce this gap in accuracy. First, we employ keypoint
self-supervision with a novel network head that predicts a discretised heatmap
and robustly reduces large deformations for better robustness. Second, we
replace multiple learned fine-tuning steps by a single instance optimisation
with hand-crafted features and the Adam optimiser. Different to other related
work, including FlowNet or PDD-Net, our approach does not require a fully
discretised architecture with correlation layer. Our ablation study
demonstrates the importance of keypoints in both self-supervised and
unsupervised (using only a MIND metric) settings. On a multi-centric
inspiration-exhale lung CT dataset, including very challenging COPD scans, our
method outperforms VoxelMorph by improving nonlinear alignment by 77% compared
to 19% - reaching target registration errors of 2 mm that outperform all but
one learning methods published to date. Extending the method to semantic
features sets new stat-of-the-art performance on inter-subject abdominal CT
registration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local and Global GANs with Semantic-Aware Upsampling for Image Generation. (arXiv:2203.00047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00047">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the task of semantic-guided image generation. One
challenge common to most existing image-level generation methods is the
difficulty in generating small objects and detailed local textures. To address
this, in this work we consider generating images using local context. As such,
we design a local class-specific generative network using semantic maps as
guidance, which separately constructs and learns subgenerators for different
classes, enabling it to capture finer details. To learn more discriminative
class-specific feature representations for the local generation, we also
propose a novel classification module. To combine the advantages of both global
image-level and local class-specific generation, a joint generation network is
designed with an attention fusion module and a dual-discriminator structure
embedded. Lastly, we propose a novel semantic-aware upsampling method, which
has a larger receptive field and can take far-away pixels that are semantically
related for feature upsampling, enabling it to better preserve semantic
consistency for instances with the same semantic labels. Extensive experiments
on two image generation tasks show the superior performance of the proposed
method. State-of-the-art results are established by large margins on both tasks
and on nine challenging public benchmarks. The source code and trained models
are available at https://github.com/Ha0Tang/LGGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Alignment using Representation Codebook. (arXiv:2203.00048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00048">
<div class="article-summary-box-inner">
<span><p>Aligning signals from different modalities is an important step in
vision-language representation learning as it affects the performance of later
stages such as cross-modality fusion. Since image and text typically reside in
different regions of the feature space, directly aligning them at instance
level is challenging especially when features are still evolving during
training. In this paper, we propose to align at a higher and more stable level
using cluster representation. Specifically, we treat image and text as two
"views" of the same entity, and encode them into a joint vision-language coding
space spanned by a dictionary of cluster centers (codebook). We contrast
positive and negative samples via their cluster assignments while
simultaneously optimizing the cluster centers. To further smooth out the
learning process, we adopt a teacher-student distillation paradigm, where the
momentum teacher of one view guides the student learning of the other. We
evaluated our approach on common vision language benchmarks and obtain new SoTA
on zero-shot cross modality retrieval while being competitive on various other
transfer tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Targeted Change Detection with Heterogeneous Remote Sensing Images for Forest Mortality Mapping. (arXiv:2203.00049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00049">
<div class="article-summary-box-inner">
<span><p>In this paper we develop a method for mapping forest mortality in the
forest-tundra ecotone using satellite data from heterogeneous sensors. We use
medium resolution imagery in order to provide the complex pattern of forest
mortality in this sparsely forested area, which has been induced by an outbreak
of geometrid moths. Specifically, Landsat-5 Thematic Mapper images from before
the event are used, with RADARSAT-2 providing the post-event images. We obtain
the difference images for both multispectral optical and synthetic aperture
radar (SAR) by using a recently developed deep learning method for translating
between the two domains. These differences are stacked with the original pre-
and post-event images in order to let our algorithm also learn how the areas
appear before and after the change event. By doing this, and focusing on
learning only the changes of interest with one-class classification (OCC), we
obtain good results with very little training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERF: Explicit Radiance Field Reconstruction From Scratch. (arXiv:2203.00051v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00051">
<div class="article-summary-box-inner">
<span><p>We propose a novel explicit dense 3D reconstruction approach that processes a
set of images of a scene with sensor poses and calibrations and estimates a
photo-real digital model. One of the key innovations is that the underlying
volumetric representation is completely explicit in contrast to neural
network-based (implicit) alternatives. We encode scenes explicitly using clear
and understandable mappings of optimization variables to scene geometry and
their outgoing surface radiance. We represent them using hierarchical
volumetric fields stored in a sparse voxel octree. Robustly reconstructing such
a volumetric scene model with millions of unknown variables from registered
scene images only is a highly non-convex and complex optimization problem. To
this end, we employ stochastic gradient descent (Adam) which is steered by an
inverse differentiable renderer.
</p>
<p>We demonstrate that our method can reconstruct models of high quality that
are comparable to state-of-the-art implicit methods. Importantly, we do not use
a sequential reconstruction pipeline where individual steps suffer from
incomplete or unreliable information from previous stages, but start our
optimizations from uniformed initial solutions with scene geometry and radiance
that is far off from the ground truth. We show that our method is general and
practical. It does not require a highly controlled lab setup for capturing, but
allows for reconstructing scenes with a vast variety of objects, including
challenging ones, such as outdoor plants or furry toys. Finally, our
reconstructed scene models are versatile thanks to their explicit design. They
can be edited interactively which is computationally too costly for implicit
alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Transport-based Graph Matching for 3D retinal OCT image registration. (arXiv:2203.00069v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00069">
<div class="article-summary-box-inner">
<span><p>Registration of longitudinal optical coherence tomography (OCT) images
assists disease monitoring and is essential in image fusion applications. Mouse
retinal OCT images are often collected for longitudinal study of eye disease
models such as uveitis, but their quality is often poor compared with human
imaging. This paper presents a novel but efficient framework involving an
optimal transport based graph matching (OT-GM) method for 3D mouse OCT image
registration. We first perform registration of fundus-like images obtained by
projecting all b-scans of a volume on a plane orthogonal to them, hereafter
referred to as the x-y plane. We introduce Adaptive Weighted Vessel Graph
Descriptors (AWVGD) and 3D Cube Descriptors (CD) to identify the correspondence
between nodes of graphs extracted from segmented vessels within the OCT
projection images. The AWVGD comprises scaling, translation and rotation, which
are computationally efficient, whereas CD exploits 3D spatial and frequency
domain information. The OT-GM method subsequently performs the correct
alignment in the x-y plane. Finally, registration along the direction
orthogonal to the x-y plane (the z-direction) is guided by the segmentation of
two important anatomical features peculiar to mouse b-scans, the Internal
Limiting Membrane (ILM) and the hyaloid remnant (HR). Both subjective and
objective evaluation results demonstrate that our framework outperforms other
well-established methods on mouse OCT images within a reasonable execution
time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Model is All You Need: Multi-Task Learning Enables Simultaneous Histology Image Segmentation and Classification. (arXiv:2203.00077v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00077">
<div class="article-summary-box-inner">
<span><p>The recent surge in performance for image analysis of digitised pathology
slides can largely be attributed to the advance of deep learning. Deep models
can be used to initially localise various structures in the tissue and hence
facilitate the extraction of interpretable features for biomarker discovery.
However, these models are typically trained for a single task and therefore
scale poorly as we wish to adapt the model for an increasing number of
different tasks. Also, supervised deep learning models are very data hungry and
therefore rely on large amounts of training data to perform well. In this paper
we present a multi-task learning approach for segmentation and classification
of nuclei, glands, lumen and different tissue regions that leverages data from
multiple independent data sources. While ensuring that our tasks are aligned by
the same tissue type and resolution, we enable simultaneous prediction with a
single network. As a result of feature sharing, we also show that the learned
representation can be used to improve downstream tasks, including nuclear
classification and signet ring cell detection. As part of this work, we use a
large dataset consisting of over 600K objects for segmentation and 440K patches
for classification and make the data publicly available. We use our approach to
process the colorectal subset of TCGA, consisting of 599 whole-slide images, to
localise 377 million, 900K and 2.1 million nuclei, glands and lumen
respectively. We make this resource available to remove a major barrier in the
development of explainable models for computational pathology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Camera Pose Regression Using Pseudo-LiDAR. (arXiv:2203.00080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00080">
<div class="article-summary-box-inner">
<span><p>An accurate and robust large-scale localization system is an integral
component for active areas of research such as autonomous vehicles and
augmented reality. To this end, many learning algorithms have been proposed
that predict 6DOF camera pose from RGB or RGB-D images. However, previous
methods that incorporate depth typically treat the data the same way as RGB
images, often adding depth maps as additional channels to RGB images and
passing them through convolutional neural networks (CNNs). In this paper, we
show that converting depth maps into pseudo-LiDAR signals, previously shown to
be useful for 3D object detection, is a better representation for camera
localization tasks by projecting point clouds that can accurately determine
6DOF camera pose. This is demonstrated by first comparing localization
accuracies of a network operating exclusively on pseudo-LiDAR representations,
with networks operating exclusively on depth maps. We then propose FusionLoc, a
novel architecture that uses pseudo-LiDAR to regress a 6DOF camera pose.
FusionLoc is a dual stream neural network, which aims to remedy common issues
with typical 2D CNNs operating on RGB-D images. The results from this
architecture are compared against various other state-of-the-art deep pose
regression implementations using the 7 Scenes dataset. The findings are that
FusionLoc performs better than a number of other camera localization methods,
with a notable improvement being, on average, 0.33m and 4.35{\deg} more
accurate than RGB-D PoseNet. By proving the validity of using pseudo-LiDAR
signals over depth maps for localization, there are new considerations when
implementing large-scale localization systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual Image Assessment. (arXiv:2203.00108v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00108">
<div class="article-summary-box-inner">
<span><p>DeepFakes are synthetic videos generated by swapping a face of an original
image with the face of somebody else. In this paper, we describe our work to
develop general, deep learning-based models to classify DeepFake content. We
propose a novel framework for using Generative Adversarial Network (GAN)-based
models, we call MRI-GAN, that utilizes perceptual differences in images to
detect synthesized videos. We test our MRI-GAN approach and a
plain-frames-based model using the DeepFake Detection Challenge Dataset. Our
plain frames-based-model achieves 91% test accuracy and a model which uses our
MRI-GAN framework with Structural Similarity Index Measurement (SSIM) for the
perceptual differences achieves 74% test accuracy. The results of MRI-GAN are
preliminary and may be improved further by modifying the choice of loss
function, tuning hyper-parameters, or by using a more advanced perceptual
similarity metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Right Spin: Learning Object Motion from Rotation-Compensated Flow Fields. (arXiv:2203.00115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00115">
<div class="article-summary-box-inner">
<span><p>Both a good understanding of geometrical concepts and a broad familiarity
with objects lead to our excellent perception of moving objects. The human
ability to detect and segment moving objects works in the presence of multiple
objects, complex background geometry, motion of the observer and even
camouflage. How humans perceive moving objects so reliably is a longstanding
research question in computer vision and borrows findings from related areas
such as psychology, cognitive science and physics. One approach to the problem
is to teach a deep network to model all of these effects. This contrasts with
the strategy used by human vision, where cognitive processes and body design
are tightly coupled and each is responsible for certain aspects of correctly
identifying moving objects. Similarly from the computer vision perspective,
there is evidence that classical, geometry-based techniques are better suited
to the "motion-based" parts of the problem, while deep networks are more
suitable for modeling appearance. In this work, we argue that the coupling of
camera rotation and camera translation can create complex motion fields that
are difficult for a deep network to untangle directly. We present a novel
probabilistic model to estimate the camera's rotation given the motion field.
We then rectify the flow field to obtain a rotation-compensated motion field
for subsequent segmentation. This strategy of first estimating camera motion,
and then allowing a network to learn the remaining parts of the problem, yields
improved results on the widely used DAVIS benchmark as well as the recently
published motion segmentation data set MoCA (Moving Camouflaged Animals).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectiveness of Delivered Information Trade Study. (arXiv:2203.00116v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00116">
<div class="article-summary-box-inner">
<span><p>The sensor to shooter timeline is affected by two main variables: satellite
positioning and asset positioning. Speeding up satellite positioning by adding
more sensors or by decreasing processing time is important only if there is a
prepared shooter, otherwise the main source of time is getting the shooter into
position. However, the intelligence community should work towards the
exploitation of sensors to the highest speed and effectiveness possible.
Achieving a high effectiveness while keeping speed high is a tradeoff that must
be considered in the sensor to shooter timeline. In this paper we investigate
two main ideas, increasing the effectiveness of satellite imagery through image
manipulation and how on-board image manipulation would affect the sensor to
shooter timeline. We cover these ideas in four scenarios: Discrete Event
Simulation of onboard processing versus ground station processing, quality of
information with cloud cover removal, information improvement with super
resolution, and data reduction with image to caption. This paper will show how
image manipulation techniques such as Super Resolution, Cloud Removal, and
Image to Caption will improve the quality of delivered information in addition
to showing how those processes effect the sensor to shooter timeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rectifying homographies for stereo vision: analytical solution for minimal distortion. (arXiv:2203.00123v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00123">
<div class="article-summary-box-inner">
<span><p>Stereo rectification is the determination of two image transformations (or
homographies) that map corresponding points on the two images, projections of
the same point in the 3D space, onto the same horizontal line in the
transformed images. Rectification is used to simplify the subsequent stereo
correspondence problem and speeding up the matching process. Rectifying
transformations, in general, introduce perspective distortion on the obtained
images, which shall be minimised to improve the accuracy of the following
algorithm dealing with the stereo correspondence problem. The search for the
optimal transformations is usually carried out relying on numerical
optimisation. This work proposes a closed-form solution for the rectifying
homographies that minimise perspective distortion. The experimental comparison
confirms its capability to solve the convergence issues of the previous
formulation. Its Python implementation is provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BlazeNeo: Blazing fast polyp segmentation and neoplasm detection. (arXiv:2203.00129v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00129">
<div class="article-summary-box-inner">
<span><p>In recent years, computer-aided automatic polyp segmentation and neoplasm
detection have been an emerging topic in medical image analysis, providing
valuable support to colonoscopy procedures. Attentions have been paid to
improving the accuracy of polyp detection and segmentation. However, not much
focus has been given to latency and throughput for performing these tasks on
dedicated devices, which can be crucial for practical applications. This paper
introduces a novel deep neural network architecture called BlazeNeo, for the
task of polyp segmentation and neoplasm detection with an emphasis on
compactness and speed while maintaining high accuracy. The model leverages the
highly efficient HarDNet backbone alongside lightweight Receptive Field Blocks
for computational efficiency, and an auxiliary training mechanism to take full
advantage of the training data for the segmentation quality. Our experiments on
a challenging dataset show that BlazeNeo achieves improvements in latency and
model size while maintaining comparable accuracy against state-of-the-art
methods. When deploying on the Jetson AGX Xavier edge device in INT8 precision,
our BlazeNeo achieves over 155 fps while yielding the best accuracy among all
compared methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-scale Transformer for Medical Image Segmentation: Architectures, Model Efficiency, and Benchmarks. (arXiv:2203.00131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00131">
<div class="article-summary-box-inner">
<span><p>Transformers have emerged to be successful in a number of natural language
processing and vision tasks, but their potential applications to medical
imaging remain largely unexplored due to the unique difficulties of this field.
In this study, we present UTNetV2, a simple yet powerful backbone model that
combines the strengths of the convolutional neural network and Transformer for
enhancing performance and efficiency in medical image segmentation. The
critical design of UTNetV2 includes three innovations: (1) We used a hybrid
hierarchical architecture by introducing depthwise separable convolution to
projection and feed-forward network in the Transformer block, which brings
local relationship modeling and desirable properties of CNNs (translation
invariance) to Transformer, thus eliminate the requirement of large-scale
pre-training. (2) We proposed efficient bidirectional attention (B-MHA) that
reduces the quadratic computation complexity of self-attention to linear by
introducing an adaptively updated semantic map. The efficient attention makes
it possible to capture long-range relationship and correct the fine-grained
errors in high-resolution token maps. (3) The semantic maps in the B-MHA allow
us to perform semantically and spatially global multi-scale feature fusion
without introducing much computational overhead. Furthermore, we provide a fair
comparison codebase of CNN-based and Transformer-based on various medical image
segmentation tasks to evaluate the merits and defects of both architectures.
UTNetV2 demonstrated state-of-the-art performance across various settings,
including large-scale datasets, small-scale datasets, 2D and 3D settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Cross-Video Neural Representations for High-Quality Frame Interpolation. (arXiv:2203.00137v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00137">
<div class="article-summary-box-inner">
<span><p>This paper considers the problem of temporal video interpolation, where the
goal is to synthesize a new video frame given its two neighbors. We propose
Cross-Video Neural Representation (CURE) as the first video interpolation
method based on neural fields (NF). NF refers to the recent class of methods
for the neural representation of complex 3D scenes that has seen widespread
success and application across computer vision. CURE represents the video as a
continuous function parameterized by a coordinate-based neural network, whose
inputs are the spatiotemporal coordinates and outputs are the corresponding RGB
values. CURE introduces a new architecture that conditions the neural network
on the input frames for imposing space-time consistency in the synthesized
video. This not only improves the final interpolation quality, but also enables
CURE to learn a prior across multiple videos. Experimental evaluations show
that CURE achieves the state-of-the-art performance on video interpolation on
several benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatiotemporal Transformer Attention Network for 3D Voxel Level Joint Segmentation and Motion Prediction in Point Cloud. (arXiv:2203.00138v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00138">
<div class="article-summary-box-inner">
<span><p>Environment perception including detection, classification, tracking, and
motion prediction are key enablers for automated driving systems and
intelligent transportation applications. Fueled by the advances in sensing
technologies and machine learning techniques, LiDAR-based sensing systems have
become a promising solution. The current challenges of this solution are how to
effectively combine different perception tasks into a single backbone and how
to efficiently learn the spatiotemporal features directly from point cloud
sequences. In this research, we propose a novel spatiotemporal attention
network based on a transformer self-attention mechanism for joint semantic
segmentation and motion prediction within a point cloud at the voxel level. The
network is trained to simultaneously outputs the voxel level class and
predicted motion by learning directly from a sequence of point cloud datasets.
The proposed backbone includes both a temporal attention module (TAM) and a
spatial attention module (SAM) to learn and extract the complex spatiotemporal
features. This approach has been evaluated with the nuScenes dataset, and
promising performance has been achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preemptive Motion Planning for Human-to-Robot Indirect Placement Handovers. (arXiv:2203.00156v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00156">
<div class="article-summary-box-inner">
<span><p>As technology advances, the need for safe, efficient, and collaborative
human-robot-teams has become increasingly important. One of the most
fundamental collaborative tasks in any setting is the object handover.
Human-to-robot handovers can take either of two approaches: (1) direct
hand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach
ensures minimal contact between the human and robot but can also result in
increased idle time due to having to wait for the object to first be placed
down on a surface. To minimize such idle time, the robot must preemptively
predict the human intent of where the object will be placed. Furthermore, for
the robot to preemptively act in any sort of productive manner, predictions and
motion planning must occur in real-time. We introduce a novel
prediction-planning pipeline that allows the robot to preemptively move towards
the human agent's intended placement location using gaze and gestures as model
inputs. In this paper, we investigate the performance and drawbacks of our
early intent predictor-planner as well as the practical benefits of using such
a pipeline through a human-robot case study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Semantic and Instance Segmentation for Colon Nuclei Identification and Counting. (arXiv:2203.00157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00157">
<div class="article-summary-box-inner">
<span><p>We address the problem of automated nuclear segmentation, classification, and
quantification from Haematoxylin and Eosin stained histology images, which is
of great relevance for several downstream computational pathology applications.
In this work, we present a solution framed as a simultaneous semantic and
instance segmentation framework. Our solution is part of the Colon Nuclei
Identification and Counting (CoNIC) Challenge. We first train a semantic and
instance segmentation model separately. Our framework uses as backbone HoverNet
and Cascade Mask-RCNN models. We then ensemble the results with a custom
Non-Maximum Suppression embedding (NMS). In our framework, the semantic model
computes a class prediction for the cells whilst the instance model provides a
refined segmentation. We demonstrate, through our experimental results, that
our model outperforms the provided baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nuclear Segmentation and Classification Model with Imbalanced Classes for CoNiC Challenge. (arXiv:2203.00171v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00171">
<div class="article-summary-box-inner">
<span><p>Nuclear segmentation and classification is an essential step for
computational pathology. TIA lab from Warwick University organized a nuclear
segmentation and classification challenge (CoNiC) for H&amp;E stained
histopathology images in colorectal cancer based on the Lizard dataset. In this
challenge, computer algorithms should be able to segment and recognize six
types of nuclei, including Epithelial, Lymphocyte, Plasma, Eosinophil,
Neutrophil, Connective tissue. This challenge introduces two highly correlated
tasks, nuclei segmentation and classification task and prediction of cellular
composition task. There are a few obstacles we have to address in this
challenge, 1) imbalanced annotations with few training samples on minority
classes, 2) color variation of the images from multiple centers or scanners, 3)
limited training samples, 4) similar morphological appearance among classes. To
deal with these challenges, we proposed a systematic pipeline for nuclear
segmentation and classification. First, we built a GAN-based model to
automatically generate pseudo images for data augmentation. Then we trained a
self-supervised stain normalization model to solve the color variation problem.
Next we constructed a baseline model HoVer-Net with cost-sensitive loss to
encourage the model pay more attention on the minority classes. According to
the results of the leaderboard, our proposed pipeline achieves 0.40665 mPQ+
(Rank 33rd) and 0.62199 r2 (Rank 4th) in the preliminary test phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Local Feature Learning for 3D Point Cloud Processing using Unary-Pairwise Attention. (arXiv:2203.00172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00172">
<div class="article-summary-box-inner">
<span><p>We present a simple but effective attention named the unary-pairwise
attention (UPA) for modeling the relationship between 3D point clouds. Our idea
is motivated by the analysis that the standard self-attention (SA) that
operates globally tends to produce almost the same attention maps for different
query positions, revealing difficulties for learning query-independent and
query-dependent information jointly. Therefore, we reformulate the SA and
propose query-independent (Unary) and query-dependent (Pairwise) components to
facilitate the learning of both terms. In contrast to the SA, the UPA ensures
query dependence via operating locally. Extensive experiments show that the UPA
outperforms the SA consistently on various point cloud understanding tasks
including shape classification, part segmentation, and scene segmentation.
Moreover, simply equipping the popular PointNet++ method with the UPA even
outperforms or is on par with the state-of-the-art attention-based approaches.
In addition, the UPA systematically boosts the performance of both standard and
modern networks when it is integrated into them as a compositional module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACTIVE:Augmentation-Free Graph Contrastive Learning for Partial Multi-View Clustering. (arXiv:2203.00186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00186">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an augmentation-free graph contrastive learning
framework, namely ACTIVE, to solve the problem of partial multi-view
clustering. Notably, we suppose that the representations of similar samples
(i.e., belonging to the same cluster) and their multiply views features should
be similar. This is distinct from the general unsupervised contrastive learning
that assumes an image and its augmentations share a similar representation.
Specifically, relation graphs are constructed using the nearest neighbours to
identify existing similar samples, then the constructed inter-instance relation
graphs are transferred to the missing views to build graphs on the
corresponding missing data. Subsequently, two main components, within-view
graph contrastive learning (WGC) and cross-view graph consistency learning
(CGC), are devised to maximize the mutual information of different views within
a cluster. The proposed approach elevates instance-level contrastive learning
and missing data inference to the cluster-level, effectively mitigating the
impact of individual missing data on clustering. Experiments on several
challenging datasets demonstrate the superiority of our proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robots Autonomously Detecting People: A Multimodal Deep Contrastive Learning Method Robust to Intraclass Variations. (arXiv:2203.00187v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00187">
<div class="article-summary-box-inner">
<span><p>Robotic detection of people in crowded and/or cluttered human-centered
environments including hospitals, long-term care, stores and airports is
challenging as people can become occluded by other people or objects, and
deform due to variations in clothing or pose. There can also be loss of
discriminative visual features due to poor lighting. In this paper, we present
a novel multimodal person detection architecture to address the mobile robot
problem of person detection under intraclass variations. We present a two-stage
training approach using 1) a unique pretraining method we define as Temporal
Invariant Multimodal Contrastive Learning (TimCLR), and 2) a Multimodal Faster
R-CNN (MFRCNN) detector. TimCLR learns person representations that are
invariant under intraclass variations through unsupervised learning. Our
approach is unique in that it generates image pairs from natural variations
within multimodal image sequences, in addition to synthetic data augmentation,
and contrasts crossmodal features to transfer invariances between different
modalities. These pretrained features are used by the MFRCNN detector for
finetuning and person detection from RGB-D images. Extensive experiments
validate the performance of our DL architecture in both human-centered crowded
and cluttered environments. Results show that our method outperforms existing
unimodal and multimodal person detection approaches in terms of detection
accuracy in detecting people with body occlusions and pose deformations in
different lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Deep Learning for Image Classification with Distribution Mismatch: A Survey. (arXiv:2203.00190v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00190">
<div class="article-summary-box-inner">
<span><p>Deep learning methodologies have been employed in several different fields,
with an outstanding success in image recognition applications, such as material
quality control, medical imaging, autonomous driving, etc. Deep learning models
rely on the abundance of labelled observations to train a prospective model.
These models are composed of millions of parameters to estimate, increasing the
need of more training observations. Frequently it is expensive to gather
labelled observations of data, making the usage of deep learning models not
ideal, as the model might over-fit data. In a semi-supervised setting,
unlabelled data is used to improve the levels of accuracy and generalization of
a model with small labelled datasets. Nevertheless, in many situations
different unlabelled data sources might be available. This raises the risk of a
significant distribution mismatch between the labelled and unlabelled datasets.
Such phenomena can cause a considerable performance hit to typical
semi-supervised deep learning frameworks, which often assume that both labelled
and unlabelled datasets are drawn from similar distributions. Therefore, in
this paper we study the latest approaches for semi-supervised deep learning for
image recognition. Emphasis is made in semi-supervised deep learning models
designed to deal with a distribution mismatch between the labelled and
unlabelled datasets. We address open challenges with the aim to encourage the
community to tackle them, and overcome the high data demand of traditional deep
learning pipelines under real-world usage settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Challenges When 3D Semantic Segmentation Faces Class Imbalanced and OOD Data. (arXiv:2203.00214v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00214">
<div class="article-summary-box-inner">
<span><p>3D semantic segmentation (3DSS) is an essential process in the creation of a
safe autonomous driving system. However, deep learning models for 3D semantic
segmentation often suffer from the class imbalance problem and
out-of-distribution (OOD) data. In this study, we explore how the class
imbalance problem affects 3DSS performance and whether the model can detect the
category prediction correctness, or whether data is ID (in-distribution) or
OOD. For these purposes, we conduct two experiments using three representative
3DSS models and five trust scoring methods, and conduct both a confusion and
feature analysis of each class. Furthermore, a data augmentation method for the
3D LiDAR dataset is proposed to create a new dataset based on SemanticKITTI and
SemanticPOSS, called AugKITTI. We propose the wPre metric and TSD for a more
in-depth analysis of the results, and follow are proposals with an insightful
discussion. Based on the experimental results, we find that: (1) the classes
are not only imbalanced in their data size but also in the basic properties of
each semantic category. (2) The intraclass diversity and interclass ambiguity
make class learning difficult and greatly limit the models' performance,
creating the challenges of semantic and data gaps. (3) The trust scores are
unreliable for classes whose features are confused with other classes. For 3DSS
models, those misclassified ID classes and OODs may also be given high trust
scores, making the 3DSS predictions unreliable, and leading to the challenges
in judging 3DSS result trustworthiness. All of these outcomes point to several
research directions for improving the performance and reliability of the 3DSS
models used for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How certain are your uncertainties?. (arXiv:2203.00238v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00238">
<div class="article-summary-box-inner">
<span><p>Having a measure of uncertainty in the output of a deep learning method is
useful in several ways, such as in assisting with interpretation of the
outputs, helping build confidence with end users, and for improving the
training and performance of the networks. Therefore, several different methods
have been proposed to capture various types of uncertainty, including epistemic
(relating to the model used) and aleatoric (relating to the data) sources, with
the most commonly used methods for estimating these being test-time dropout for
epistemic uncertainty and test-time augmentation for aleatoric uncertainty.
However, these methods are parameterised (e.g. amount of dropout or type and
level of augmentation) and so there is a whole range of possible uncertainties
that could be calculated, even with a fixed network and dataset. This work
investigates the stability of these uncertainty measurements, in terms of both
magnitude and spatial pattern. In experiments using the well characterised
BraTS challenge, we demonstrate substantial variability in the magnitude and
spatial pattern of these uncertainties, and discuss the implications for
interpretability, repeatability and confidence in results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment. (arXiv:2203.00242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00242">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language (V+L) pre-training models have achieved tremendous
success in recent years on various multi-modal benchmarks. However, the
majority of existing models require pre-training on a large set of parallel
image-text data, which is costly to collect, compared to image-only or
text-only data. In this paper, we explore unsupervised Vision-and-Language
pre-training (UVLP) to learn the cross-modal representation from non-parallel
image and text datasets. We found two key factors that lead to good
unsupervised V+L pre-training without parallel data: (i) joint image-and-text
input (ii) overall image-text alignment (even for non-parallel data).
Accordingly, we propose a novel unsupervised V+L pre-training curriculum for
non-parallel texts and images. We first construct a weakly aligned image-text
corpus via a retrieval-based approach, then apply a set of multi-granular
alignment pre-training tasks, including region-to-tag, region-to-phrase, and
image-to-sentence alignment, to bridge the gap between the two modalities. A
comprehensive ablation study shows each granularity is helpful to learn a
stronger pre-trained model. We adapt our pre-trained model to a set of V+L
downstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. Our
model achieves the state-of-art performance in all these tasks under the
unsupervised setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When A Conventional Filter Meets Deep Learning: Basis Composition Learning on Image Filters. (arXiv:2203.00258v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00258">
<div class="article-summary-box-inner">
<span><p>Image filters are fast, lightweight and effective, which make these
conventional wisdoms preferable as basic tools in vision tasks. In practical
scenarios, users have to tweak parameters multiple times to obtain satisfied
results. This inconvenience heavily discounts the efficiency and user
experience. We propose basis composition learning on single image filters to
automatically determine their optimal formulas. The feasibility is based on a
two-step strategy: first, we build a set of filtered basis (FB) consisting of
approximations under selected parameter configurations; second, a dual-branch
composition module is proposed to learn how the candidates in FB are combined
to better approximate the target image. Our method is simple yet effective in
practice; it renders filters to be user-friendly and benefits fundamental
low-level vision problems including denoising, deraining and texture removal.
Extensive experiments demonstrate that our method achieves an appropriate
balance among the performance, time complexity and memory efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection. (arXiv:2203.00259v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00259">
<div class="article-summary-box-inner">
<span><p>Density-based and classification-based methods have ruled unsupervised
anomaly detection in recent years, while reconstruction-based methods are
rarely mentioned for the poor reconstruction ability and low performance.
However, the latter requires no costly extra training samples for the
unsupervised training that is more practical, so this paper focuses on
improving this kind of method and proposes a novel Omni-frequency
Channel-selection Reconstruction (OCR-GAN) network to handle anomaly detection
task in a perspective of frequency. Concretely, we propose a Frequency
Decoupling (FD) module to decouple the input image into different frequency
components and model the reconstruction process as a combination of parallel
omni-frequency image restorations, as we observe a significant difference in
the frequency distribution of normal and abnormal images. Given the correlation
among multiple frequencies, we further propose a Channel Selection (CS) module
that performs frequency interaction among different encoders by adaptively
selecting different channels. Abundant experiments demonstrate the
effectiveness and superiority of our approach over different kinds of methods,
e.g., achieving a new state-of-the-art 98.3 detection AUC on the MVTec AD
dataset without extra training data that markedly surpasses the
reconstruction-based baseline by +38.1 and the current SOTA method by +0.3.
Source code will be available at https://github.com/zhangzjn/OCR-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separable-HoverNet and Instance-YOLO for Colon Nuclei Identification and Counting. (arXiv:2203.00262v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00262">
<div class="article-summary-box-inner">
<span><p>Nuclear segmentation, classification and quantification within Haematoxylin &amp;
Eosin stained histology images enables the extraction of interpretable
cell-based features that can be used in downstream explainable models in
computational pathology (CPath). However, automatic recognition of different
nuclei is faced with a major challenge in that there are several different
types of nuclei, some of them exhibiting large intraclass variability. In this
work, we propose an approach that combine Separable-HoverNet and
Instance-YOLOv5 to indentify colon nuclei small and unbalanced. Our approach
can achieve mPQ+ 0.389 on the Segmentation and Classification-Preliminary Test
Dataset and r2 0.599 on the Cellular Composition-Preliminary Test Dataset on
ISBI 2022 CoNIC Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProgressLabeller: Visual Data Stream Annotation for Training Object-Centric 3D Perception. (arXiv:2203.00283v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00283">
<div class="article-summary-box-inner">
<span><p>Visual perception tasks often require vast amounts of labelled data,
including 3D poses and image space segmentation masks. The process of creating
such training data sets can prove difficult or time-intensive to scale up to
efficacy for general use. Consider the task of pose estimation for rigid
objects. Deep neural network based approaches have shown good performance when
trained on large, public datasets. However, adapting these networks for other
novel objects, or fine-tuning existing models for different environments,
requires significant time investment to generate newly labelled instances.
Towards this end, we propose ProgressLabeller as a method for more efficiently
generating large amounts of 6D pose training data from color images sequences
for custom scenes in a scalable manner. ProgressLabeller is intended to also
support transparent or translucent objects, for which the previous methods
based on depth dense reconstruction will fail. We demonstrate the effectiveness
of ProgressLabeller by rapidly create a dataset of over 1M samples with which
we fine-tune a state-of-the-art pose estimation network in order to markedly
improve the downstream robotic grasp success rates. ProgressLabeller will be
made publicly available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Globally-Optimal Correspondence-Less Visual Odometry for Planar Ground Vehicles. (arXiv:2203.00291v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00291">
<div class="article-summary-box-inner">
<span><p>The motion of planar ground vehicles is often non-holonomic, and as a result
may be modelled by the 2 DoF Ackermann steering model. We analyse the
feasibility of estimating such motion with a downward facing camera that exerts
fronto-parallel motion with respect to the ground plane. This turns the motion
estimation into a simple image registration problem in which we only have to
identify a 2-parameter planar homography. However, one difficulty that arises
from this setup is that ground-plane features are indistinctive and thus hard
to match between successive views. We encountered this difficulty by
introducing the first globally-optimal, correspondence-less solution to
plane-based Ackermann motion estimation. The solution relies on the
branch-and-bound optimisation technique. Through the low-dimensional
parametrisation, a derivation of tight bounds, and an efficient implementation,
we demonstrate how this technique is eventually amenable to accurate real-time
motion estimation. We prove its property of global optimality and analyse the
impact of assuming a locally constant centre of rotation. Our results on real
data finally demonstrate a significant advantage over the more traditional,
correspondence-based hypothesise-and-test schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FP-Loc: Lightweight and Drift-free Floor Plan-assisted LiDAR Localization. (arXiv:2203.00292v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00292">
<div class="article-summary-box-inner">
<span><p>We present a novel framework for floor plan-based, full six degree-of-freedom
LiDAR localization. Our approach relies on robust ceiling and ground plane
detection, which solves part of the pose and supports the segmentation of
vertical structure elements such as walls and pillars. Our core contribution is
a novel nearest neighbour data structure for an efficient look-up of nearest
vertical structure elements from the floor plan. The registration is realized
as a pair-wise regularized windowed pose graph optimization. Highly efficient,
accurate and drift-free long-term localization is demonstrated on multiple
scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial samples for deep monocular 6D object pose estimation. (arXiv:2203.00302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00302">
<div class="article-summary-box-inner">
<span><p>Estimating object 6D pose from an RGB image is important for many real-world
applications such as autonomous driving and robotic grasping, where robustness
of the estimation is crucial. In this work, for the first time, we study
adversarial samples that can fool state-of-the-art (SOTA) deep learning based
6D pose estimation models. In particular, we propose a Unified 6D pose
estimation Attack, namely U6DA, which can successfully attack all the three
main categories of models for 6D pose estimation. The key idea of our U6DA is
to fool the models to predict wrong results for object shapes that are
essential for correct 6D pose estimation. Specifically, we explore a
transfer-based black-box attack to 6D pose estimation. By shifting the
segmentation attention map away from its original position, adversarial samples
are crafted. We show that such adversarial samples are not only effective for
the direct 6D pose estimation models, but also able to attack the two-stage
based models regardless of their robust RANSAC modules. Extensive experiments
were conducted to demonstrate the effectiveness of our U6DA with large-scale
public benchmarks. We also introduce a new U6DA-Linemod dataset for robustness
study of the 6D pose estimation task. Our codes and dataset will be available
at \url{https://github.com/cuge1995/U6DA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehensive Analysis of the Object Detection Pipeline on UAVs. (arXiv:2203.00306v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00306">
<div class="article-summary-box-inner">
<span><p>An object detection pipeline comprises a camera that captures the scene and
an object detector that processes these images. The quality of the images
directly affects the performance of the object detector. Many works nowadays
focus either on improving the image quality or improving the object detection
models independently, but neglect the importance of joint optimization of the
two subsystems. In this paper, we first empirically analyze the influence of
seven parameters (quantization, compression, resolution, color model, image
distortion, gamma correction, additional channels) in remote sensing
applications. For our experiments, we utilize three UAV data sets from
different domains and a mixture of large and small state-of-the-art object
detector models to provide an extensive evaluation of the influence of the
pipeline parameters. Additionally, we realize an object detection pipeline
prototype on an embedded platform for an UAV and give a best practice
recommendation for building object detection pipelines based on our findings.
We show that not all parameters have an equal impact on detection accuracy and
data throughput, and that by using a suitable compromise between parameters we
are able to improve detection accuracy for lightweight object detection models,
while keeping the same data throughput.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Perceiver: A General Architecture for Arbitrary Boundary Detection. (arXiv:2203.00307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00307">
<div class="article-summary-box-inner">
<span><p>Generic Boundary Detection (GBD) aims at locating general boundaries that
divide videos into semantically coherent and taxonomy-free units, and could
server as an important pre-processing step for long-form video understanding.
Previous research separately handle these different-level generic boundaries
with specific designs of complicated deep networks from simple CNN to LSTM.
Instead, in this paper, our objective is to develop a general yet simple
architecture for arbitrary boundary detection in videos. To this end, we
present Temporal Perceiver, a general architecture with Transformers, offering
a unified solution to the detection of arbitrary generic boundaries. The core
design is to introduce a small set of latent feature queries as anchors to
compress the redundant input into fixed dimension via cross-attention blocks.
Thanks to this fixed number of latent units, it reduces the quadratic
complexity of attention operation to a linear form of input frames.
Specifically, to leverage the coherence structure of videos, we construct two
types of latent feature queries: boundary queries and context queries, which
handle the semantic incoherence and coherence regions accordingly. Moreover, to
guide the learning of latent feature queries, we propose an alignment loss on
cross-attention to explicitly encourage the boundary queries to attend on the
top possible boundaries. Finally, we present a sparse detection head on the
compressed representations and directly output the final boundary detection
results without any post-processing module. We test our Temporal Perceiver on a
variety of detection benchmarks, ranging from shot-level, event-level, to
scene-level GBD. Our method surpasses the previous state-of-the-art methods on
all benchmarks, demonstrating the generalization ability of our temporal
perceiver.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards IID representation learning and its application on biomedical data. (arXiv:2203.00332v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00332">
<div class="article-summary-box-inner">
<span><p>Due to the heterogeneity of real-world data, the widely accepted independent
and identically distributed (IID) assumption has been criticized in recent
studies on causality. In this paper, we argue that instead of being a
questionable assumption, IID is a fundamental task-relevant property that needs
to be learned. Consider $k$ independent random vectors $\mathsf{X}^{i = 1,
\ldots, k}$, we elaborate on how a variety of different causal questions can be
reformulated to learning a task-relevant function $\phi$ that induces IID among
$\mathsf{Z}^i := \phi \circ \mathsf{X}^i$, which we term IID representation
learning.
</p>
<p>For proof of concept, we examine the IID representation learning on
Out-of-Distribution (OOD) generalization tasks. Concretely, by utilizing the
representation obtained via the learned function that induces IID, we conduct
prediction of molecular characteristics (molecular prediction) on two
biomedical datasets with real-world distribution shifts introduced by a)
preanalytical variation and b) sampling protocol. To enable reproducibility and
for comparison to the state-of-the-art (SOTA) methods, this is done by
following the OOD benchmarking guidelines recommended from WILDS. Compared to
the SOTA baselines supported in WILDS, the results confirm the superior
performance of IID representation learning on OOD tasks. The code is publicly
accessible via https://github.com/CTPLab/IID_representation_learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affordance Learning from Play for Sample-Efficient Policy Learning. (arXiv:2203.00352v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00352">
<div class="article-summary-box-inner">
<span><p>Robots operating in human-centered environments should have the ability to
understand how objects function: what can be done with each object, where this
interaction may occur, and how the object is used to achieve a goal. To this
end, we propose a novel approach that extracts a self-supervised visual
affordance model from human teleoperated play data and leverages it to enable
efficient policy learning and motion planning. We combine model-based planning
with model-free deep reinforcement learning (RL) to learn policies that favor
the same object regions favored by people, while requiring minimal robot
interactions with the environment. We evaluate our algorithm, Visual
Affordance-guided Policy Optimization (VAPO), with both diverse simulation
manipulation tasks and real world robot tidy-up experiments to demonstrate the
effectiveness of our affordance-guided policies. We find that our policies
train 4x faster than the baselines and generalize better to novel objects
because our visual affordance model can anticipate their affordance regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tempera: Spatial Transformer Feature Pyramid Network for Cardiac MRI Segmentation. (arXiv:2203.00355v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00355">
<div class="article-summary-box-inner">
<span><p>Assessing the structure and function of the right ventricle (RV) is important
in the diagnosis of several cardiac pathologies. However, it remains more
challenging to segment the RV than the left ventricle (LV). In this paper, we
focus on segmenting the RV in both short (SA) and long-axis (LA) cardiac MR
images simultaneously. For this task, we propose a new multi-input/output
architecture, hybrid 2D/3D geometric spatial TransformEr Multi-Pass fEature
pyRAmid (Tempera). Our feature pyramid extends current designs by allowing not
only a multi-scale feature output but multi-scale SA and LA input images as
well. Tempera transfers learned features between SA and LA images via layer
weight sharing and incorporates a geometric target transformer to map the
predicted SA segmentation to LA space. Our model achieves an average Dice score
of 0.836 and 0.798 for the SA and LA, respectively, and 26.31 mm and 31.19 mm
Hausdorff distances. This opens up the potential for the incorporation of RV
segmentation models into clinical workflows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Wilderness Using Explainable Machine Learning in Satellite Imagery. (arXiv:2203.00379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00379">
<div class="article-summary-box-inner">
<span><p>Wilderness areas offer important ecological and social benefits, and
therefore warrant monitoring and preservation. Yet, what makes a place "wild"
is vaguely defined, making the detection and monitoring of wilderness areas via
remote sensing techniques a challenging task. In this article, we explore the
characteristics and appearance of the vague concept of wilderness areas via
multispectral satellite imagery. For this, we apply a novel explainable machine
learning technique on a curated dataset, which is sophisticated for the task to
investigate wild and anthropogenic areas in Fennoscandia. The dataset contains
Sentinel-2 images of areas representing 1) protected areas with the aim of
preserving and retaining the natural character and 2) anthropogenic areas
consisting of artificial and agricultural landscapes. With our technique, we
predict continuous, detailed and high-resolution sensitivity maps of unseen
remote sensing data in regards to wild and anthropogenic characteristics. Our
neural network provides an interpretable activation space in which regions are
semantically arranged in regards to wild and anthropogenic characteristics and
certain land cover classes. This increases confidence in the method and allows
for new explanations in regards to the investigated concept. Our model advances
explainable machine learning for remote sensing, offers opportunities for
comprehensive analyses of existing wilderness, and practical relevance for
conservation efforts. Code and data are available at
<a href="http://rs.ipb.uni-bonn.de/data">this http URL</a> and
https://gitlab.jsc.fz-juelich.de/kiste/wilderness, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP. (arXiv:2203.00386v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00386">
<div class="article-summary-box-inner">
<span><p>Training a text-to-image generator in the general domain (e.g., Dall.e,
CogView) requires huge amounts of paired text-image data, which is too
expensive to collect. In this paper, we propose a self-supervised scheme named
as CLIP-GEN for general text-to-image generation with the language-image priors
extracted with a pre-trained CLIP model. In our approach, we only require a set
of unlabeled images in the general domain to train a text-to-image generator.
Specifically, given an image without text labels, we first extract the
embedding of the image in the united language-vision embedding space with the
image encoder of CLIP. Next, we convert the image into a sequence of discrete
tokens in the VQGAN codebook space (the VQGAN model can be trained with the
unlabeled image dataset in hand). Finally, we train an autoregressive
transformer that maps the image tokens from its unified language-vision
representation. Once trained, the transformer can generate coherent image
tokens based on the text embedding extracted from the text encoder of CLIP upon
an input text. Such a strategy enables us to train a strong and general
text-to-image generator with large text-free image dataset such as ImageNet.
Qualitative and quantitative evaluations verify that our method significantly
outperforms optimization-based text-to-image methods in terms of image quality
while not compromising the text-image matching. Our method can even achieve
comparable performance as flagship supervised models like CogView.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-aware Dynamic Graph Neural Network for Video Compressive Sensing. (arXiv:2203.00387v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00387">
<div class="article-summary-box-inner">
<span><p>Video snapshot compressive imaging (SCI) utilizes a 2D detector to capture
sequential video frames and compresses them into a single measurement. Various
reconstruction methods have been developed to recover the high-speed video
frames from the snapshot measurement. However, most existing reconstruction
methods are incapable of capturing long-range spatial and temporal
dependencies, which are critical for video processing. In this paper, we
propose a flexible and robust approach based on graph neural network (GNN) to
efficiently model non-local interactions between pixels in space as well as
time regardless of the distance. Specifically, we develop a motion-aware
dynamic GNN for better video representation, i.e., represent each pixel as the
aggregation of relative nodes under the guidance of frame-by-frame motions,
which consists of motion-aware dynamic sampling, cross-scale node sampling and
graph aggregation. Extensive results on both simulation and real data
demonstrate both the effectiveness and efficiency of the proposed approach, and
the visualization clearly illustrates the intrinsic dynamic sampling operations
of our proposed model for boosting the video SCI reconstruction results. The
code and models will be released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beam-Shape Effects and Noise Removal from THz Time-Domain Images in Reflection Geometry in the 0.25-6 THz Range. (arXiv:2203.00417v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00417">
<div class="article-summary-box-inner">
<span><p>The increasing need of restoring high-resolution Hyper-Spectral (HS) images
is determining a growing reliance on Computer Vision-based processing to
enhance the clarity of the image content. HS images can, in fact, suffer from
degradation effects or artefacts caused by instrument limitations. This paper
focuses on a procedure aimed at reducing the degradation effects,
frequency-dependent blur and noise, in Terahertz Time-Domain Spectroscopy
(THz-TDS) images in reflection geometry. It describes the application of a
joint deblurring and denoising approach that had been previously proved to be
effective for the restoration of THz-TDS images in transmission geometry, but
that had never been tested in reflection modality. This mode is often the only
one that can be effectively used in most cases, for example when analyzing
objects that are either opaque in the THz range, or that cannot be displaced
from their location (e.g., museums), such as those of cultural interest.
Compared to transmission mode, reflection geometry introduces, however, further
distortion to THz data, neglected in existing literature. In this work, we
successfully implement image deblurring and denoising of both uniform-shape
samples (a contemporary 1 Euro cent coin and an inlaid pendant) and samples
with the uneven reliefs and corrosion products on the surface which make the
analysis of the object particularly complex (an ancient Roman silver coin). The
study demonstrates the ability of image processing to restore data in the 0.25
- 6 THz range, spanning over more than four octaves, and providing the
foundation for future analytical approaches of cultural heritage using the
far-infrared spectrum still not sufficiently investigated in literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">se-Shweshwe Inspired Fashion Generation. (arXiv:2203.00435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00435">
<div class="article-summary-box-inner">
<span><p>Fashion is one of the ways in which we show ourselves to the world. It is a
reflection of our personal decisions and one of the ways in which people
distinguish and represent themselves. In this paper, we focus on the fashion
design process and expand computer vision for fashion beyond its current focus
on western fashion. We discuss the history of Southern African se-Shweshwe
fabric fashion, the collection of a se-Shweshwe dataset, and the application of
sketch-to-design image generation for affordable fashion-design. The
application to fashion raises both technical questions of training with small
amounts of data, and also important questions for computer vision beyond
fairness, in particular ethical considerations on creating and employing
fashion datasets, and how computer vision supports cultural representation and
might avoid algorithmic cultural appropriation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boundary Corrected Multi-scale Fusion Network for Real-time Semantic Segmentation. (arXiv:2203.00436v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00436">
<div class="article-summary-box-inner">
<span><p>Image semantic segmentation aims at the pixel-level classification of images,
which has requirements for both accuracy and speed in practical application.
Existing semantic segmentation methods mainly rely on the high-resolution input
to achieve high accuracy and do not meet the requirements of inference time.
Although some methods focus on high-speed scene parsing with lightweight
architectures, they can not fully mine semantic features under low computation
with relatively low performance. To realize the real-time and high-precision
segmentation, we propose a new method named Boundary Corrected Multi-scale
Fusion Network, which uses the designed Low-resolution Multi-scale Fusion
Module to extract semantic information. Moreover, to deal with boundary errors
caused by low-resolution feature map fusion, we further design an additional
Boundary Corrected Loss to constrain overly smooth features. Extensive
experiments show that our method achieves a state-of-the-art balance of
accuracy and speed for the real-time semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realtime strategy for image data labelling using binary models and active sampling. (arXiv:2203.00439v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00439">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) and Deep Learning (DL) tasks primarily depend on data.
Most of the ML and DL applications involve supervised learning which requires
labelled data. In the initial phases of ML realm lack of data used to be a
problem, now we are in a new era of big data. The supervised ML algorithms
require data to be labelled and of good quality. Labelling task requires a
large amount of money and time investment. Data labelling require a skilled
person who will charge high for this task, consider the case of the medical
field or the data is in bulk that requires a lot of people assigned to label
it. The amount of data that is well enough for training needs to be known,
money and time can not be wasted to label the whole data. This paper mainly
aims to propose a strategy that helps in labelling the data along with oracle
in real-time. With balancing on model contribution for labelling is 89 and 81.1
for furniture type and intel scene image data sets respectively. Further with
balancing being kept off model contribution is found to be 83.47 and 78.71 for
furniture type and flower data sets respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridge the Gap between Supervised and Unsupervised Learning for Fine-Grained Classification. (arXiv:2203.00441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00441">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning technology has caught up with or even surpassed
supervised learning technology in general object classification (GOC) and
person re-identification (re-ID). However, it is found that the unsupervised
learning of fine-grained visual classification (FGVC) is more challenging than
GOC and person re-ID. In order to bridge the gap between unsupervised and
supervised learning for FGVC, we investigate the essential factors (including
feature extraction, clustering, and contrastive learning) for the performance
gap between supervised and unsupervised FGVC. Furthermore, we propose a simple,
effective, and practical method, termed as UFCL, to alleviate the gap. Three
key issues are concerned and improved: First, we introduce a robust and
powerful backbone, ResNet50-IBN, which has an ability of domain adaptation when
we transfer ImageNet pre-trained models to FGVC tasks. Next, we propose to
introduce HDBSCAN instead of DBSCAN to do clustering, which can generate better
clusters for adjacent categories with fewer hyper-parameters. Finally, we
propose a weighted feature agent and its updating mechanism to do contrastive
learning by using the pseudo labels with inevitable noise, which can improve
the optimization process of learning the parameters of the network. The
effectiveness of our UFCL is verified on CUB-200-2011, Oxford-Flowers,
Oxford-Pets, Stanford-Dogs, Stanford-Cars and FGVC-Aircraft datasets. Under the
unsupervised FGVC setting, we achieve state-of-the-art results, and analyze the
key factors and the important parameters to provide a practical guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technological evaluation of two AFIS systems. (arXiv:2203.00447v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00447">
<div class="article-summary-box-inner">
<span><p>This paper provides a technological evaluation of two Automatic Fingerprint
Identification Systems (AFIS) used in forensic applications. Both of them are
installed and working in Spanish police premises. The first one is a Printrak
AFIS 2000 system with a database of more than 450,000 fingerprints, while the
second one is a NEC AFIS 21 SAID NT-LEXS Release 2.4.4 with a database of more
than 15 million fingerprints. Our experiments reveal that although both systems
can manage inkless fingerprints, the latest one offers better experimental
results
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning based Prediction of MSI in Colorectal Cancer via Prediction of the Status of MMR Markers. (arXiv:2203.00449v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00449">
<div class="article-summary-box-inner">
<span><p>An accurate diagnosis and profiling of tumour are critical to the best
treatment choices for cancer patients. In addition to the cancer type and its
aggressiveness, molecular heterogeneity also plays a vital role in treatment
selection. MSI or MMR deficiency is one of the well-studied aberrations in
terms of molecular changes. Colorectal cancer patients with MMR deficiency
respond well to immunotherapy, hence assessment of the relevant molecular
markers can assist clinicians in making optimal treatment selections for
patients. Immunohistochemistry is one of the ways for identifying these
molecular changes which requires additional sections of tumour tissue.
Introduction of automated methods that can predict MSI or MMR status from a
target image without the need for additional sections can substantially reduce
the cost associated with it. In this work, we present our work on predicting
MSI status in a two-stage process using a single target slide either stained
with CK818 or H\&amp;E. First, we train a multi-headed convolutional neural network
model where each head is responsible for predicting one of the MMR protein
expressions. To this end, we perform registration of MMR slides to the target
slide as a pre-processing step. In the second stage, statistical features
computed from the MMR prediction maps are used for the final MSI prediction.
Our results demonstrate that MSI classification can be improved on
incorporating fine-grained MMR labels in comparison to the previous approaches
in which coarse labels (MSI/MSS) are utilised.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-Tailed Classification with Gradual Balanced Loss and Adaptive Feature Generation. (arXiv:2203.00452v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00452">
<div class="article-summary-box-inner">
<span><p>The real-world data distribution is essentially long-tailed, which poses
great challenge to the deep model. In this work, we propose a new method,
Gradual Balanced Loss and Adaptive Feature Generator (GLAG) to alleviate
imbalance. GLAG first learns a balanced and robust feature model with Gradual
Balanced Loss, then fixes the feature model and augments the under-represented
tail classes on the feature level with the knowledge from well-represented head
classes. And the generated samples are mixed up with real training samples
during training epochs. Gradual Balanced Loss is a general loss and it can
combine with different decoupled training methods to improve the original
performance. State-of-the-art results have been achieved on long-tail datasets
such as CIFAR100-LT, ImageNetLT, and iNaturalist, which demonstrates the
effectiveness of GLAG for long-tailed visual recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JOINED : Prior Guided Multi-task Learning for Joint Optic Disc/Cup Segmentation and Fovea Detection. (arXiv:2203.00461v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00461">
<div class="article-summary-box-inner">
<span><p>Fundus photography has been routinely used to document the presence and
severity of various retinal degenerative diseases such as age-related macula
degeneration, glaucoma, and diabetic retinopathy, for which the fovea, optic
disc (OD), and optic cup (OC) are important anatomical landmarks.
Identification of those anatomical landmarks is of great clinical importance.
However, the presence of lesions, drusen, and other abnormalities during
retinal degeneration severely complicates automatic landmark detection and
segmentation. Most existing works treat the identification of each landmark as
a single task and typically do not make use of any clinical prior information.
In this paper, we present a novel method, named JOINED, for prior guided
multi-task learning for joint OD/OC segmentation and fovea detection. An
auxiliary branch for distance prediction, in addition to a segmentation branch
and a detection branch, is constructed to effectively utilize the distance
information from each image pixel to landmarks of interest. Our proposed JOINED
pipeline consists of a coarse stage and a fine stage. At the coarse stage, we
obtain the OD/OC coarse segmentation and the heatmap localization of fovea
through a joint segmentation and detection module. Afterwards, we crop the
regions of interest for subsequent fine processing and use predictions obtained
at the coarse stage as additional information for better performance and faster
convergence. Experimental results reveal that our proposed JOINED outperforms
existing state-of-the-art approaches on the publicly-available GAMMA, PALM, and
REFUGE datasets of fundus images. Furthermore, JOINED ranked the 5th on the
OD/OC segmentation and fovea detection tasks in the GAMMA challenge hosted by
the MICCAI2021 workshop OMIA8.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compliance Challenges in Forensic Image Analysis Under the Artificial Intelligence Act. (arXiv:2203.00469v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00469">
<div class="article-summary-box-inner">
<span><p>In many applications of forensic image analysis, state-of-the-art results are
nowadays achieved with machine learning methods. However, concerns about their
reliability and opaqueness raise the question whether such methods can be used
in criminal investigations. So far, this question of legal compliance has
hardly been discussed, also because legal regulations for machine learning
methods were not defined explicitly. To this end, the European Commission
recently proposed the artificial intelligence (AI) act, a regulatory framework
for the trustworthy use of AI. Under the draft AI act, high-risk AI systems for
use in law enforcement are permitted but subject to compliance with mandatory
requirements. In this paper, we review why the use of machine learning in
forensic image analysis is classified as high-risk. We then summarize the
mandatory requirements for high-risk AI systems and discuss these requirements
in light of two forensic applications, license plate recognition and deep fake
detection. The goal of this paper is to raise awareness of the upcoming legal
requirements and to point out avenues for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Recurrent Fusion for Indoor Localization. (arXiv:2203.00510v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00510">
<div class="article-summary-box-inner">
<span><p>This paper considers indoor localization using multi-modal wireless signals
including Wi-Fi, inertial measurement unit (IMU), and ultra-wideband (UWB). By
formulating the localization as a multi-modal sequence regression problem, a
multi-stream recurrent fusion method is proposed to combine the current hidden
state of each modality in the context of recurrent neural networks while
accounting for the modality uncertainty which is directly learned from its own
immediate past states. The proposed method was evaluated on the large-scale
SPAWC2021 multi-modal localization dataset and compared with a wide range of
baseline methods including the trilateration method, traditional fingerprinting
methods, and convolution network-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Creativity Characterization of Generative Models via Group-based Subset Scanning. (arXiv:2203.00523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00523">
<div class="article-summary-box-inner">
<span><p>Deep generative models, such as Variational Autoencoders (VAEs) and
Generative Adversarial Networks (GANs), have been employed widely in
computational creativity research. However, such models discourage
out-of-distribution generation to avoid spurious sample generation, thereby
limiting their creativity. Thus, incorporating research on human creativity
into generative deep learning techniques presents an opportunity to make their
outputs more compelling and human-like. As we see the emergence of generative
models directed toward creativity research, a need for machine learning-based
surrogate metrics to characterize creative output from these models is
imperative. We propose group-based subset scanning to identify, quantify, and
characterize creative processes by detecting a subset of anomalous
node-activations in the hidden layers of the generative models. Our experiments
on the standard image benchmarks, and their "creatively generated" variants,
reveal that the proposed subset scores distribution is more useful for
detecting creative processes in the activation space rather than the pixel
space. Further, we found that creative samples generate larger subsets of
anomalies than normal or non-creative samples across datasets. The node
activations highlighted during the creative decoding process are different from
those responsible for the normal sample generation. Lastly, we assess if the
images from the subsets selected by our method were also found creative by
human evaluators, presenting a link between creativity perception in humans and
node activations within deep neural nets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards deep learning-powered IVF: A large public benchmark for morphokinetic parameter prediction. (arXiv:2203.00531v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00531">
<div class="article-summary-box-inner">
<span><p>An important limitation to the development of Artificial Intelligence
(AI)-based solutions for In Vitro Fertilization (IVF) is the absence of a
public reference benchmark to train and evaluate deep learning (DL) models. In
this work, we describe a fully annotated dataset of 756 videos of developing
embryos, for a total of 337k images. We applied ResNet, LSTM, and ResNet-3D
architectures to our dataset and demonstrate that they overperform algorithmic
approaches to automatically annotate stage development phases. Altogether, we
propose the first public benchmark that will allow the community to evaluate
morphokinetic models. This is the first step towards deep learning-powered IVF.
Of note, we propose highly detailed annotations with 16 different development
phases, including early cell division phases, but also late cell divisions,
phases after morulation, and very early phases, which have never been used
before. We postulate that this original approach will help improve the overall
performance of deep learning approaches on time-lapse videos of embryo
development, ultimately benefiting infertile patients with improved clinical
success rates (Code and data are available at
https://gitlab.univ-nantes.fr/E144069X/bench_mk_pred.git).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Descriptellation: Deep Learned Constellation Descriptors for SLAM. (arXiv:2203.00567v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00567">
<div class="article-summary-box-inner">
<span><p>Current global localization descriptors in Simultaneous Localization and
Mapping (SLAM) often fail under vast viewpoint or appearance changes. Adding
topological information of semantic objects into the descriptors ameliorates
the problem. However, hand-crafted topological descriptors extract limited
information and they are not robust to environmental noise, drastic perspective
changes, or object occlusion or misdetections. To solve this problem, we
formulate a learning-based approach by constructing constellations from
semantically meaningful objects and use Deep Graph Convolution Networks to map
the constellation representation to a descriptor. We demonstrate the
effectiveness of our Deep Learned Constellation Descriptor (Descriptellation)
on the Paris-Rue-Lille and IQmulus datasets. Although Descriptellation is
trained on randomly generated simulation datasets, it shows good generalization
abilities on real-world datasets. Descriptellation outperforms the PointNet and
handcrafted constellation descriptors for global localization, and shows
robustness against different types of noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a unified view of unsupervised non-local methods for image denoising: the NL-Ridge approach. (arXiv:2203.00570v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00570">
<div class="article-summary-box-inner">
<span><p>We propose a unified view of unsupervised non-local methods for image
denoising that linearily combine noisy image patches. The best methods,
established in different modeling and estimation frameworks, are two-step
algorithms. Leveraging Stein's unbiased risk estimate (SURE) for the first step
and the "internal adaptation", a concept borrowed from deep learning theory,
for the second one, we show that our NL-Ridge approach enables to reconcile
several patch aggregation methods for image denoising. In the second step, our
closed-form aggregation weights are computed through multivariate Ridge
regressions. Experiments on artificially noisy images demonstrate that NL-Ridge
may outperform well established state-of-the-art unsupervised denoisers such as
BM3D and NL-Bayes, as well as recent unsupervised deep learning methods, while
being simpler conceptually.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology. (arXiv:2203.00585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00585">
<div class="article-summary-box-inner">
<span><p>Tissue phenotyping is a fundamental task in learning objective
characterizations of histopathologic biomarkers within the tumor-immune
microenvironment in cancer pathology. However, whole-slide imaging (WSI) is a
complex computer vision in which: 1) WSIs have enormous image resolutions with
precludes large-scale pixel-level efforts in data curation, and 2) diversity of
morphological phenotypes results in inter- and intra-observer variability in
tissue labeling. To address these limitations, current efforts have proposed
using pretrained image encoders (transfer learning from ImageNet,
self-supervised pretraining) in extracting morphological features from
pathology, but have not been extensively validated. In this work, we conduct a
search for good representations in pathology by training a variety of
self-supervised models with validation on a variety of weakly-supervised and
patch-level tasks. Our key finding is in discovering that Vision Transformers
using DINO-based knowledge distillation are able to learn data-efficient and
interpretable features in histology images wherein the different attention
heads learn distinct morphological phenotypes. We make evaluation code and
pretrained weights publicly-available at:
https://github.com/Richarizardd/Self-Supervised-ViT-Path.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwitchHit: A Probabilistic, Complementarity-Based Switching System for Improved Visual Place Recognition in Changing Environments. (arXiv:2203.00591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00591">
<div class="article-summary-box-inner">
<span><p>Visual place recognition (VPR), a fundamental task in computer vision and
robotics, is the problem of identifying a place mainly based on visual
information. Viewpoint and appearance changes, such as due to weather and
seasonal variations, make this task challenging. Currently, there is no
universal VPR technique that can work in all types of environments, on a
variety of robotic platforms, and under a wide range of viewpoint and
appearance changes. Recent work has shown the potential of combining different
VPR methods intelligently by evaluating complementarity for some specific VPR
datasets to achieve better performance. This, however, requires ground truth
information (correct matches) which is not available when a robot is deployed
in a real-world scenario. Moreover, running multiple VPR techniques in parallel
may be prohibitive for resource-constrained embedded platforms. To overcome
these limitations, this paper presents a probabilistic complementarity based
switching VPR system, SwitchHit. Our proposed system consists of multiple VPR
techniques, however, it does not simply run all techniques at once, rather
predicts the probability of correct match for an incoming query image and
dynamically switches to another complementary technique if the probability of
correctly matching the query is below a certain threshold. This innovative use
of multiple VPR techniques allow our system to be more efficient and robust
than other combined VPR approaches employing brute force and running multiple
VPR techniques at once. Thus making it more suitable for resource constrained
embedded systems and achieving an overall superior performance from what any
individual VPR method in the system could have by achieved running
independently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A unified 3D framework for Organs at Risk Localization and Segmentation for Radiation Therapy Planning. (arXiv:2203.00624v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00624">
<div class="article-summary-box-inner">
<span><p>Automatic localization and segmentation of organs-at-risk (OAR) in CT are
essential pre-processing steps in medical image analysis tasks, such as
radiation therapy planning. For instance, the segmentation of OAR surrounding
tumors enables the maximization of radiation to the tumor area without
compromising the healthy tissues. However, the current medical workflow
requires manual delineation of OAR, which is prone to errors and is
annotator-dependent. In this work, we aim to introduce a unified 3D pipeline
for OAR localization-segmentation rather than novel localization or
segmentation architectures. To the best of our knowledge, our proposed
framework fully enables the exploitation of 3D context information inherent in
medical imaging. In the first step, a 3D multi-variate regression network
predicts organs' centroids and bounding boxes. Secondly, 3D organ-specific
segmentation networks are leveraged to generate a multi-organ segmentation map.
Our method achieved an overall Dice score of $0.9260\pm 0.18 \%$ on the
VISCERAL dataset containing CT scans with varying fields of view and multiple
organs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full RGB Just Noticeable Difference (JND) Modelling. (arXiv:2203.00629v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00629">
<div class="article-summary-box-inner">
<span><p>Just Noticeable Difference (JND) has many applications in multimedia signal
processing, especially for visual data processing up to date. It's generally
defined as the minimum visual content changes that the human can perspective,
which has been studied for decades. However, most of the existing methods only
focus on the luminance component of JND modelling and simply regard chrominance
components as scaled versions of luminance. In this paper, we propose a JND
model to generate the JND by taking the characteristics of full RGB channels
into account, termed as the RGB-JND. To this end, an RGB-JND-NET is proposed,
where the visual content in full RGB channels is used to extract features for
JND generation. To supervise the JND generation, an adaptive image quality
assessment combination (AIC) is developed. Besides, the RDB-JND-NET also takes
the visual attention into account by automatically mining the underlying
relationship between visual attention and the JND, which is further used to
constrain the JND spatial distribution. To the best of our knowledge, this is
the first work on careful investigation of JND modelling for full-color space.
Experimental results demonstrate that the RGB-JND-NET model outperforms the
relevant state-of-the-art JND models. Besides, the JND of the red and blue
channels are larger than that of the green one according to the experimental
results of the proposed model, which demonstrates that more changes can be
tolerated in the red and blue channels, in line with the well-known fact that
the human visual system is more sensitive to the green channel in comparison
with the red and blue ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Multi-Scale Learning For Outcome Prediction in 3D PET Images. (arXiv:2203.00641v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00641">
<div class="article-summary-box-inner">
<span><p>Background and Objectives: Predicting patient response to treatment and
survival in oncology is a prominent way towards precision medicine. To that
end, radiomics was proposed as a field of study where images are used instead
of invasive methods. The first step in radiomic analysis is the segmentation of
the lesion. However, this task is time consuming and can be physician
subjective. Automated tools based on supervised deep learning have made great
progress to assist physicians. However, they are data hungry, and annotated
data remains a major issue in the medical field where only a small subset of
annotated images is available. Methods: In this work, we propose a multi-task
learning framework to predict patient's survival and response. We show that the
encoder can leverage multiple tasks to extract meaningful and powerful features
that improve radiomics performance. We show also that subsidiary tasks serve as
an inductive bias so that the model can better generalize. Results: Our model
was tested and validated for treatment response and survival in lung and
esophageal cancers, with an area under the ROC curve of 77% and 71%
respectively, outperforming single task learning methods. Conclusions: We show
that, by using a multi-task learning approach, we can boost the performance of
radiomic analysis by extracting rich information of intratumoral and
peritumoral regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Autoencoders Without the Variation. (arXiv:2203.00645v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00645">
<div class="article-summary-box-inner">
<span><p>Variational autoencdoers (VAE) are a popular approach to generative
modelling. However, exploiting the capabilities of VAEs in practice can be
difficult. Recent work on regularised and entropic autoencoders have begun to
explore the potential, for generative modelling, of removing the variational
approach and returning to the classic deterministic autoencoder (DAE) with
additional novel regularisation methods. In this paper we empirically explore
the capability of DAEs for image generation without additional novel methods
and the effect of the implicit regularisation and smoothness of large networks.
We find that DAEs can be used successfully for image generation without
additional loss terms, and that many of the useful properties of VAEs can arise
implicitly from sufficiently large convolutional encoders and decoders when
trained on CIFAR-10 and CelebA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Networks. (arXiv:2203.00667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00667">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) are very popular frameworks for
generating high-quality data, and are immensely used in both the academia and
industry in many domains. Arguably, their most substantial impact has been in
the area of computer vision, where they achieve state-of-the-art image
generation. This chapter gives an introduction to GANs, by discussing their
principle mechanism and presenting some of their inherent problems during
training and evaluation. We focus on these three issues: (1) mode collapse, (2)
vanishing gradients, and (3) generation of low-quality images. We then list
some architecture-variant and loss-variant GANs that remedy the above
challenges. Lastly, we present two utilization examples of GANs for real-world
applications: Data augmentation and face images generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Person Re-Identification via Self-Supervised Batch Norm Test-Time Adaption. (arXiv:2203.00672v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00672">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the generalization problem of person
re-identification (re-id), whose major challenge is the distribution shift on
an unseen domain. As an important tool of regularizing the distribution, batch
normalization (BN) has been widely used in existing methods. However, they
neglect that BN is severely biased to the training domain and inevitably
suffers the performance drop if directly generalized without being updated. To
tackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel
re-id framework that applies the self-supervised strategy to update BN
parameters adaptively. Specifically, BNTA quickly explores the domain-aware
information within unlabeled target data before inference, and accordingly
modulates the feature distribution normalized by BN to adapt to the target
domain. This is accomplished by two designed self-supervised auxiliary tasks,
namely part positioning and part nearest neighbor matching, which help the
model mine the domain-aware information with respect to the structure and
identity of body parts, respectively. To demonstrate the effectiveness of our
method, we conduct extensive experiments on three re-id datasets and confirm
the superior performance to the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding. (arXiv:2203.00680v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00680">
<div class="article-summary-box-inner">
<span><p>Manual annotation of large-scale point cloud dataset for varying tasks such
as 3D object classification, segmentation and detection is often laborious
owing to the irregular structure of point clouds. Self-supervised learning,
which operates without any human labeling, is a promising approach to address
this issue. We observe in the real world that humans are capable of mapping the
visual concepts learnt from 2D images to understand the 3D world. Encouraged by
this insight, we propose CrossPoint, a simple cross-modal contrastive learning
approach to learn transferable 3D point cloud representations. It enables a
3D-2D correspondence of objects by maximizing agreement between point clouds
and the corresponding rendered 2D image in the invariant space, while
encouraging invariance to transformations in the point cloud modality. Our
joint training objective combines the feature correspondences within and across
modalities, thus ensembles a rich learning signal from both 3D point cloud and
2D image modalities in a self-supervised fashion. Experimental results show
that our approach outperforms the previous unsupervised learning methods on a
diverse range of downstream tasks including 3D object classification and
segmentation. Further, the ablation studies validate the potency of our
approach for a better point cloud understanding. Code and pretrained models are
available at <a href="http://github.com/MohamedAfham/CrossPoint.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't miss the Mismatch: Investigating the Objective Function Mismatch for Unsupervised Representation Learning. (arXiv:2009.02383v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.02383">
<div class="article-summary-box-inner">
<span><p>Finding general evaluation metrics for unsupervised representation learning
techniques is a challenging open research question, which recently has become
more and more necessary due to the increasing interest in unsupervised methods.
Even though these methods promise beneficial representation characteristics,
most approaches currently suffer from the objective function mismatch. This
mismatch states that the performance on a desired target task can decrease when
the unsupervised pretext task is learned too long - especially when both tasks
are ill-posed. In this work, we build upon the widely used linear evaluation
protocol and define new general evaluation metrics to quantitatively capture
the objective function mismatch and the more generic metrics mismatch. We
discuss the usability and stability of our protocols on a variety of pretext
and target tasks and study mismatches in a wide range of experiments. Thereby
we disclose dependencies of the objective function mismatch across several
pretext and target tasks with respect to the pretext model's representation
size, target model complexity, pretext and target augmentations as well as
pretext and target task types. In our experiments, we find that the objective
function mismatch reduces performance by ~0.1-5.0% for Cifar10, Cifar100 and
PCam in many setups, and up to ~25-59% in extreme cases for the 3dshapes
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Grassmannians for Dimensionality Reduction with Applications. (arXiv:2010.14589v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14589">
<div class="article-summary-box-inner">
<span><p>In the recent past, nested structures in Riemannian manifolds has been
studied in the context of dimensionality reduction as an alternative to the
popular principal geodesic analysis (PGA) technique, for example, the principal
nested spheres. In this paper, we propose a novel framework for constructing a
nested sequence of homogeneous Riemannian manifolds. Common examples of
homogeneous Riemannian manifolds include the $n$-sphere, the Stiefel manifold,
the Grassmann manifold and many others. In particular, we focus on applying the
proposed framework to the Grassmann manifold, giving rise to the nested
Grassmannians (NG). An important application in which Grassmann manifolds are
encountered is planar shape analysis. Specifically, each planar (2D) shape can
be represented as a point in the complex projective space which is a complex
Grass-mann manifold. Some salient features of our framework are: (i) it
explicitly exploits the geometry of the homogeneous Riemannian manifolds and
(ii) the nested lower-dimensional submanifolds need not be geodesic. With the
proposed NG structure, we develop algorithms for the supervised and
unsupervised dimensionality reduction problems respectively. The proposed
algorithms are compared with PGA via simulation studies and real data
experiments and are shown to achieve a higher ratio of expressed variance
compared to PGA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04293">
<div class="article-summary-box-inner">
<span><p>Humans are able to perceive, understand and reason about causal events.
Developing models with similar physical and causal understanding capabilities
is a long-standing goal of artificial intelligence. As a step towards this
direction, we introduce CRAFT, a new video question answering dataset that
requires causal reasoning about physical forces and object interactions. It
contains 58K video and question pairs that are generated from 10K videos from
20 different virtual environments, containing various objects in motion that
interact with each other and the scene. Two question categories in CRAFT
include previously studied descriptive and counterfactual questions.
Additionally, inspired by the Force Dynamics Theory in cognitive linguistics,
we introduce a new causal question category that involves understanding the
causal interactions between objects through notions like cause, enable, and
prevent. Our results show that even though the questions in CRAFT are easy for
humans, the tested baseline models, including existing state-of-the-art
methods, do not yet deal with the challenges posed in our benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-End Computer Vision Methodology for Quantitative Metallography. (arXiv:2104.11159v3 [cond-mat.mtrl-sci] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11159">
<div class="article-summary-box-inner">
<span><p>Metallography is crucial for a proper assessment of material's properties. It
involves mainly the investigation of spatial distribution of grains and the
occurrence and characteristics of inclusions or precipitates. This work
presents an holistic artificial intelligence model for Anomaly Detection that
automatically quantifies the degree of anomaly of impurities in alloys. We
suggest the following examination process: (1) Deep semantic segmentation is
performed on the inclusions (based on a suitable metallographic database of
alloys and corresponding tags of inclusions), producing inclusions masks that
are saved into a separated database. (2) Deep image inpainting is performed to
fill the removed inclusions parts, resulting in 'clean' metallographic images,
which contain the background of grains. (3) Grains' boundaries are marked using
deep semantic segmentation (based on another metallographic database of
alloys), producing boundaries that are ready for further inspection on the
distribution of grains' size. (4) Deep anomaly detection and pattern
recognition is performed on the inclusions masks to determine spatial, shape
and area anomaly detection of the inclusions. Finally, the system recommends to
an expert on areas of interests for further examination. The performance of the
model is presented and analyzed based on few representative cases. Although the
models presented here were developed for metallography analysis, most of them
can be generalized to a wider set of problems in which anomaly detection of
geometrical objects is desired. All models as well as the data-sets that were
created for this work, are publicly available at
https://github.com/Scientific-Computing-Lab-NRCN/MLography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation. (arXiv:2105.00402v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00402">
<div class="article-summary-box-inner">
<span><p>Colorectal cancer is among the most common malignancies and can develop from
high-risk colon polyps. Colonoscopy is an effective screening tool to detect
and remove polyps, especially in the case of precancerous lesions. However, the
missing rate in clinical practice is relatively high due to many factors. The
procedure could benefit greatly from using AI models for automatic polyp
segmentation, which provide valuable insights for improving colon polyp
detection. However, precise segmentation is still challenging due to variations
of polyps in size, shape, texture, and color. This paper proposes a novel
neural network architecture called AG-CUResNeSt, which enhances Coupled UNets
using the robust ResNeSt backbone and attention gates. The network is capable
of effectively combining multi-level features to yield accurate polyp
segmentation. Experimental results on five popular benchmark datasets show that
our proposed method achieves state-of-the-art accuracy compared to existing
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AttDLNet: Attention-based DL Network for 3D LiDAR Place Recognition. (arXiv:2106.09637v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09637">
<div class="article-summary-box-inner">
<span><p>LiDAR-based place recognition is one of the key components of SLAM and global
localization in autonomous vehicles and robotics applications. With the success
of DL approaches in learning useful information from 3D LiDARs, place
recognition has also benefited from this modality, which has led to higher
re-localization and loop-closure detection performance, particularly, in
environments with significant changing conditions. Despite the progress in this
field, the extraction of proper and efficient descriptors from 3D LiDAR data
that are invariant to changing conditions and orientation is still an unsolved
challenge. To address this problem, this work proposes a novel 3D LiDAR-based
deep learning network (named AttDLNet) that uses a range-based proxy
representation for point clouds and an attention network with stacked attention
layers to selectively focus on long-range context and inter-feature
relationships. The proposed network is trained and validated on the KITTI
dataset and an ablation study is presented to assess the novel attention
network. Results show that adding attention to the network improves
performance, leading to efficient loop closures, and outperforming an
established 3D LiDAR-based place recognition approach. From the ablation study,
results indicate that the middle encoder layers have the highest mean
performance, while deeper layers are more robust to orientation change. The
code is publicly available at https://github.com/Cybonic/AttDLNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Place recognition survey: An update on deep learning approaches. (arXiv:2106.10458v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10458">
<div class="article-summary-box-inner">
<span><p>Autonomous Vehicles (AV) are becoming more capable of navigating in complex
environments with dynamic and changing conditions. A key component that enables
these intelligent vehicles to overcome such conditions and become more
autonomous is the sophistication of the perception and localization systems. As
part of the localization system, place recognition has benefited from recent
developments in other perception tasks such as place categorization or object
recognition, namely with the emergence of deep learning (DL) frameworks. This
paper surveys recent approaches and methods used in place recognition,
particularly those based on deep learning. The contributions of this work are
twofold: surveying recent sensors such as 3D LiDARs and RADARs, applied in
place recognition; and categorizing the various DL-based place recognition
works into supervised, unsupervised, semi-supervised, parallel, and
hierarchical categories. First, this survey introduces key place recognition
concepts to contextualize the reader. Then, sensor characteristics are
addressed. This survey proceeds by elaborating on the various DL-based works,
presenting summaries for each framework. Some lessons learned from this survey
include: the importance of NetVLAD for supervised end-to-end learning; the
advantages of unsupervised approaches in place recognition, namely for
cross-domain applications; or the increasing tendency of recent works to seek,
not only for higher performance but also for higher efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Fusion Vision Transformer for Fine-Grained Visual Categorization. (arXiv:2107.02341v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02341">
<div class="article-summary-box-inner">
<span><p>The core for tackling the fine-grained visual categorization (FGVC) is to
learn subtle yet discriminative features. Most previous works achieve this by
explicitly selecting the discriminative parts or integrating the attention
mechanism via CNN-based approaches.However, these methods enhance the
computational complexity and make the modeldominated by the regions containing
the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA
performance on general image recognition tasks. Theself-attention mechanism
aggregates and weights the information from all patches to the classification
token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation
token in the deep layer pays more attention to the global information, lacking
the local and low-level features that are essential for FGVC. In this work, we
proposea novel pure transformer-based framework Feature Fusion Vision
Transformer (FFVT)where we aggregate the important tokens from each transformer
layer to compensate thelocal, low-level and middle-level information. We design
a novel token selection mod-ule called mutual attention weight selection (MAWS)
to guide the network effectively and efficiently towards selecting
discriminative tokens without introducing extra param-eters. We verify the
effectiveness of FFVT on three benchmarks where FFVT achieves the
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multispectral Vineyard Segmentation: A Deep Learning approach. (arXiv:2108.01200v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01200">
<div class="article-summary-box-inner">
<span><p>Digital agriculture has evolved significantly over the last few years due to
the technological developments in automation and computational intelligence
applied to the agricultural sector, including vineyards which are a relevant
crop in the Mediterranean region. In this work, a study is presented of
semantic segmentation for vine detection in real-world vineyards by exploring
state-of-the-art deep segmentation networks and conventional unsupervised
methods. Camera data have been collected on vineyards using an Unmanned Aerial
System (UAS) equipped with a dual imaging sensor payload, namely a
high-definition RGB camera and a five-band multispectral and thermal camera.
Extensive experiments using deep-segmentation networks and unsupervised methods
have been performed on multimodal datasets representing four distinct vineyards
located in the central region of Portugal. The reported results indicate that
SegNet, U-Net, and ModSegNet have equivalent overall performance in vine
segmentation. The results also show that multimodality slightly improves the
performance of vine segmentation, but the NIR spectrum alone generally is
sufficient on most of the datasets. Furthermore, results suggest that
high-definition RGB images produce equivalent or higher performance than any
lower resolution multispectral band combination. Lastly, Deep Learning (DL)
networks have higher overall performance than classical methods. The code and
dataset are publicly available at
https://github.com/Cybonic/DL_vineyard_segmentation_study.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset. (arXiv:2108.05080v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05080">
<div class="article-summary-box-inner">
<span><p>While the significant advancements have made in the generation of deepfakes
using deep learning technologies, its misuse is a well-known issue now.
Deepfakes can cause severe security and privacy issues as they can be used to
impersonate a person's identity in a video by replacing his/her face with
another person's face. Recently, a new problem of generating synthesized human
voice of a person is emerging, where AI-based deep learning models can
synthesize any person's voice requiring just a few seconds of audio. With the
emerging threat of impersonation attacks using deepfake audios and videos, a
new generation of deepfake detectors is needed to focus on both video and audio
collectively. To develop a competent deepfake detector, a large amount of
high-quality data is typically required to capture real-world (or practical)
scenarios. Existing deepfake datasets either contain deepfake videos or audios,
which are racially biased as well. As a result, it is critical to develop a
high-quality video and audio deepfake dataset that can be used to detect both
audio and video deepfakes simultaneously. To fill this gap, we propose a novel
Audio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake
videos but also respective synthesized lip-synced fake audios. We generate this
dataset using the most popular deepfake generation methods. We selected real
YouTube videos of celebrities with four ethnic backgrounds to develop a more
realistic multimodal dataset that addresses racial bias, and further help
develop multimodal deepfake detectors. We performed several experiments using
state-of-the-art detection methods to evaluate our deepfake dataset and
demonstrate the challenges and usefulness of our multimodal Audio-Video
deepfake dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ego4D: Around the World in 3,000 Hours of Egocentric Video. (arXiv:2110.07058v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07058">
<div class="article-summary-box-inner">
<span><p>We introduce Ego4D, a massive-scale egocentric video dataset and benchmark
suite. It offers 3,670 hours of daily-life activity video spanning hundreds of
scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique
camera wearers from 74 worldwide locations and 9 different countries. The
approach to collection is designed to uphold rigorous privacy and ethics
standards with consenting participants and robust de-identification procedures
where relevant. Ego4D dramatically expands the volume of diverse egocentric
video footage publicly available to the research community. Portions of the
video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo,
and/or synchronized videos from multiple egocentric cameras at the same event.
Furthermore, we present a host of new benchmark challenges centered around
understanding the first-person visual experience in the past (querying an
episodic memory), present (analyzing hand-object manipulation, audio-visual
conversation, and social interactions), and future (forecasting activities). By
publicly sharing this massive annotated dataset and benchmark suite, we aim to
push the frontier of first-person perception. Project page:
https://ego4d-data.org/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAIR: Data Augmented Invariant Regularization. (arXiv:2110.11205v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11205">
<div class="article-summary-box-inner">
<span><p>While deep learning through empirical risk minimization (ERM) has succeeded
at achieving human-level performance at a variety of complex tasks, ERM
generalizes poorly to distribution shift. This is partly explained by
overfitting to spurious features such as background in images or named entities
in natural language. Synthetic data augmentation followed by empirical risk
minimization (DA-ERM) is a simple and widely used solution to remedy this
problem. In addition, consistency regularization could be applied to further
promote model performance to be consistent on the augmented sample and the
original one. In this paper, we propose data augmented invariant regularization
(DAIR), a simple form of consistency regularization that is applied directly on
the loss function rather than intermediate features, making it widely
applicable regardless of network architecture or problem setup. We apply DAIR
to multiple real-world learning problems, namely robust regression, visual
question answering, robust deep neural network training, and neural
task-oriented dialog modeling. Our experiments show that DAIR consistently
outperforms ERM and DA-ERM with little marginal cost and sets new
state-of-the-art results in several benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Learning the Partial Permutation Matrix for Robust 3D Point Cloud Registration. (arXiv:2110.15250v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15250">
<div class="article-summary-box-inner">
<span><p>Even though considerable progress has been made in deep learning-based 3D
point cloud processing, how to obtain accurate correspondences for robust
registration remains a major challenge because existing hard assignment methods
cannot deal with outliers naturally. Alternatively, the soft matching-based
methods have been proposed to learn the matching probability rather than hard
assignment. However, in this paper, we prove that these methods have an
inherent ambiguity causing many deceptive correspondences. To address the above
challenges, we propose to learn a partial permutation matching matrix, which
does not assign corresponding points to outliers, and implements hard
assignment to prevent ambiguity. However, this proposal poses two new problems,
i.e., existing hard assignment algorithms can only solve a full rank
permutation matrix rather than a partial permutation matrix, and this desired
matrix is defined in the discrete space, which is non-differentiable. In
response, we design a dedicated soft-to-hard (S2H) matching procedure within
the registration pipeline consisting of two steps: solving the soft matching
matrix (S-step) and projecting this soft matrix to the partial permutation
matrix (H-step). Specifically, we augment the profit matrix before the hard
assignment to solve an augmented permutation matrix, which is cropped to
achieve the final partial permutation matrix. Moreover, to guarantee end-to-end
learning, we supervise the learned partial permutation matrix but propagate the
gradient to the soft matrix instead. Our S2H matching procedure can be easily
integrated with existing registration frameworks, which has been verified in
representative frameworks including DCP, RPMNet, and DGR. Extensive experiments
have validated our method, which creates a new state-of-the-art performance for
robust 3D point cloud registration. The code will be made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis. (arXiv:2111.03133v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03133">
<div class="article-summary-box-inner">
<span><p>Generating images that fit a given text description using machine learning
has improved greatly with the release of technologies such as the CLIP
image-text encoder model; however, current methods lack artistic control of the
style of image to be generated. We introduce StyleCLIPDraw which adds a style
loss to the CLIPDraw text-to-drawing synthesis model to allow artistic control
of the synthesized drawings in addition to control of the content via text.
Whereas performing decoupled style transfer on a generated image only affects
the texture, our proposed coupled approach is able to capture a style in both
texture and shape, suggesting that the style of the drawing is coupled with the
drawing process itself. More results and our code are available at
https://github.com/pschaldenbrand/StyleCLIPDraw
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction. (arXiv:2111.06636v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06636">
<div class="article-summary-box-inner">
<span><p>This work proposes a new computational framework for learning a structured
generative model for real-world datasets. In particular, we propose to learn a
closed-loop transcription between a multi-class multi-dimensional data
distribution and a linear discriminative representation (LDR) in the feature
space that consists of multiple independent multi-dimensional linear subspaces.
In particular, we argue that the optimal encoding and decoding mappings sought
can be formulated as the equilibrium point of a two-player minimax game between
the encoder and decoder. A natural utility function for this game is the
so-called rate reduction, a simple information-theoretic measure for distances
between mixtures of subspace-like Gaussians in the feature space. Our
formulation draws inspiration from closed-loop error feedback from control
systems and avoids expensive evaluating and minimizing approximated distances
between arbitrary distributions in either the data space or the feature space.
To a large extent, this new formulation unifies the concepts and benefits of
Auto-Encoding and GAN and naturally extends them to the settings of learning a
both discriminative and generative representation for multi-class and
multi-dimensional real-world data. Our extensive experiments on many benchmark
imagery datasets demonstrate tremendous potential of this new closed-loop
formulation: under fair comparison, visual quality of the learned decoder and
classification performance of the encoder is competitive and often better than
existing methods based on GAN, VAE, or a combination of both. Unlike existing
generative models, the so learned features of the multiple classes are
structured: different classes are explicitly mapped onto corresponding
independent principal subspaces in the feature space. Source code can be found
at https://github.com/Delay-Xili/LDR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attribute Artifacts Removal for Geometry-based Point Cloud Compression. (arXiv:2112.00560v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00560">
<div class="article-summary-box-inner">
<span><p>Geometry-based point cloud compression (G-PCC) can achieve remarkable
compression efficiency for point clouds. However, it still leads to serious
attribute compression artifacts, especially under low bitrate scenarios. In
this paper, we propose a Multi-Scale Graph Attention Network (MS-GAT) to remove
the artifacts of point cloud attributes compressed by G-PCC. We first construct
a graph based on point cloud geometry coordinates and then use the Chebyshev
graph convolutions to extract features of point cloud attributes. Considering
that one point may be correlated with points both near and far away from it, we
propose a multi-scale scheme to capture the short- and long-range correlations
between the current point and its neighboring and distant points. To address
the problem that various points may have different degrees of artifacts caused
by adaptive quantization, we introduce the quantization step per point as an
extra input to the proposed network. We also incorporate a weighted graph
attentional layer into the network to pay special attention to the points with
more attribute artifacts. To the best of our knowledge, this is the first
attribute artifacts removal method for G-PCC. We validate the effectiveness of
our method over various point clouds. Objective comparison results show that
our proposed method achieves an average of 9.74% BD-rate reduction compared
with Predlift and 10.13% BD-rate reduction compared with RAHT. Subjective
comparison results present that visual artifacts such as color shifting,
blurring, and quantization noise are reduced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Medical Image Segmentation via Cross Teaching between CNN and Transformer. (arXiv:2112.04894v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04894">
<div class="article-summary-box-inner">
<span><p>Recently, deep learning with Convolutional Neural Networks (CNNs) and
Transformers has shown encouraging results in fully supervised medical image
segmentation. However, it is still challenging for them to achieve good
performance with limited annotations for training. In this work, we present a
very simple yet efficient framework for semi-supervised medical image
segmentation by introducing the cross teaching between CNN and Transformer.
Specifically, we simplify the classical deep co-training from consistency
regularization to cross teaching, where the prediction of a network is used as
the pseudo label to supervise the other network directly end-to-end.
Considering the difference in learning paradigm between CNN and Transformer, we
introduce the Cross Teaching between CNN and Transformer rather than just using
CNNs. Experiments on a public benchmark show that our method outperforms eight
existing semi-supervised learning methods just with a simpler framework.
Notably, this work may be the first attempt to combine CNN and transformer for
semi-supervised medical image segmentation and achieve promising results on a
public benchmark. The code will be released at:
https://github.com/HiLab-git/SSL4MIS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseTact: Optical Tactile Sensor for Dense Shape Reconstruction. (arXiv:2201.01367v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01367">
<div class="article-summary-box-inner">
<span><p>Increasing the performance of tactile sensing in robots enables versatile,
in-hand manipulation. Vision-based tactile sensors have been widely used as
rich tactile feedback has been shown to be correlated with increased
performance in manipulation tasks. Existing tactile sensor solutions with high
resolution have limitations that include low accuracy, expensive components, or
lack of scalability. In this paper, an inexpensive, scalable, and compact
tactile sensor with high-resolution surface deformation modeling for surface
reconstruction of the 3D sensor surface is proposed. By measuring the image
from the fisheye camera, it is shown that the sensor can successfully estimate
the surface deformation in real-time (1.8ms) by using deep convolutional neural
networks. This sensor in its design and sensing abilities represents a
significant step toward better object in-hand localization, classification, and
surface estimation all enabled by high-resolution shape reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedMed-GAN: Federated Domain Translation on Unsupervised Cross-Modality Brain Image Synthesis. (arXiv:2201.08953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08953">
<div class="article-summary-box-inner">
<span><p>Utilizing multi-modal neuroimaging data has been proved to be effective to
investigate human cognitive activities and certain pathologies. However, it is
not practical to obtain the full set of paired neuroimaging data centrally
since the collection faces several constraints, e.g., high examination cost,
long acquisition time, and image corruption. In addition, these data are
dispersed into different medical institutions and thus cannot be aggregated for
centralized training considering the privacy issues. There is a clear need to
launch a federated learning and facilitate the integration of the dispersed
data from different institutions. In this paper, we propose a new benchmark for
federated domain translation on unsupervised brain image synthesis (termed as
FedMed-GAN) to bridge the gap between federated learning and medical GAN.
FedMed-GAN mitigates the mode collapse without sacrificing the performance of
generators, and is widely applied to different proportions of unpaired and
paired data with variation adaptation property. We treat the gradient penalties
by federally averaging algorithm and then leveraging differential privacy
gradient descent to regularize the training dynamics. A comprehensive
evaluation is provided for comparing FedMed-GAN and other centralized methods,
which shows the new state-of-the-art performance by our FedMed-GAN. Our code
has been released on the website: https://github.com/M-3LAB/FedMed-GAN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SafePicking: Learning Safe Object Extraction via Object-Level Mapping. (arXiv:2202.05832v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05832">
<div class="article-summary-box-inner">
<span><p>Robots need object-level scene understanding to manipulate objects while
reasoning about contact, support, and occlusion among objects. Given a pile of
objects, object recognition and reconstruction can identify the boundary of
object instances, giving important cues as to how the objects form and support
the pile. In this work, we present a system, SafePicking, that integrates
object-level mapping and learning-based motion planning to generate a motion
that safely extracts occluded target objects from a pile. Planning is done by
learning a deep Q-network that receives observations of predicted poses and a
depth-based heightmap to output a motion trajectory, trained to maximize a
safety metric reward. Our results show that the observation fusion of poses and
depth-sensing gives both better performance and robustness to the model. We
evaluate our methods using the YCB objects in both simulation and the real
world, achieving safe object extraction from piles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Functional Connectivity Based Classification of ADHD Using Different Atlases. (arXiv:2202.08953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08953">
<div class="article-summary-box-inner">
<span><p>These days, computational diagnosis strategies of neuropsychiatric disorders
are gaining attention day by day. It's critical to determine the brain's
functional connectivity based on Functional-Magnetic-Resonance-Imaging(fMRI) to
diagnose the disorder. It's known as a chronic disease, and millions of
children amass the symptoms of this disease, so there is much vacuum for the
researcher to formulate a model to improve the accuracy to diagnose ADHD
accurately. In this paper, we consider the functional connectivity of a brain
extracted using various time templates/Atlases. Local-Binary Encoding-Method
(LBEM) algorithm is utilized for feature extraction, while Hierarchical-
Extreme-Learning-Machine (HELM) is used to classify the extracted features. To
validate our approach, fMRI data of 143 normal and 100 ADHD affected children
is used for experimental purpose. Our experimental results are based on
comparing various Atlases given as CC400, CC200, and AAL. Our model achieves
high performance with CC400 as compared to other Atlases
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12154">
<div class="article-summary-box-inner">
<span><p>Trojan attacks on deep neural networks are both dangerous and surreptitious.
Over the past few years, Trojan attacks have advanced from using only a simple
trigger and targeting only one class to using many sophisticated triggers and
targeting multiple classes. However, Trojan defenses have not caught up with
this development. Most defense methods still make out-of-date assumptions about
Trojan triggers and target classes, thus, can be easily circumvented by modern
Trojan attacks. In this paper, we advocate general defenses that are effective
and robust against various Trojan attacks and propose two novel "filtering"
defenses with these characteristics called Variational Input Filtering (VIF)
and Adversarial Input Filtering (AIF). VIF and AIF leverage variational
inference and adversarial training respectively to purify all potential Trojan
triggers in the input at run time without making any assumption about their
numbers and forms. We further extend "filtering" to
"filtering-then-contrasting" - a new defense mechanism that helps avoid the
drop in classification accuracy on clean data caused by filtering. Extensive
experimental results show that our proposed defenses significantly outperform 4
well-known defenses in mitigating 5 different Trojan attacks including the two
state-of-the-art which defeat many strong defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence Calibration for Object Detection and Segmentation. (arXiv:2202.12785v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12785">
<div class="article-summary-box-inner">
<span><p>Calibrated confidence estimates obtained from neural networks are crucial,
particularly for safety-critical applications such as autonomous driving or
medical image diagnosis. However, although the task of confidence calibration
has been investigated on classification problems, thorough investigations on
object detection and segmentation problems are still missing. Therefore, we
focus on the investigation of confidence calibration for object detection and
segmentation models in this chapter. We introduce the concept of multivariate
confidence calibration that is an extension of well-known calibration methods
to the task of object detection and segmentation. This allows for an extended
confidence calibration that is also aware of additional features such as
bounding box/pixel position, shape information, etc. Furthermore, we extend the
expected calibration error (ECE) to measure miscalibration of object detection
and segmentation models. We examine several network architectures on MS COCO as
well as on Cityscapes and show that especially object detection as well as
instance segmentation models are intrinsically miscalibrated given the
introduced definition of calibration. Using our proposed calibration methods,
we have been able to improve calibration so that it also has a positive impact
on the quality of segmentation masks as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient End-to-End 3D Model Reconstruction based on Neural Architecture Search. (arXiv:2202.13313v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13313">
<div class="article-summary-box-inner">
<span><p>Using neural networks to represent 3D objects has become popular. However,
many previous works employ neural networks with fixed architecture and size to
represent different 3D objects, which lead to excessive network parameters for
simple objects and limited reconstruction accuracy for complex objects. For
each 3D model, it is desirable to have an end-to-end neural network with as few
parameters as possible to achieve high-fidelity reconstruction. In this paper,
we propose an efficient model reconstruction method utilizing neural
architecture search (NAS) and binary classification. Taking the number of
layers, the number of nodes in each layer, and the activation function of each
layer as the search space, a specific network architecture can be obtained
based on reinforcement learning technology. Furthermore, to get rid of the
traditional surface reconstruction algorithms (e.g., marching cube) used after
network inference, we complete the end-to-end network by classifying binary
voxels. Compared to other signed distance field (SDF) prediction or binary
classification networks, our method achieves significantly higher
reconstruction accuracy using fewer network parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Stacked Capsule Autoencoder with Hybrid Adversarial Training. (arXiv:2202.13755v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13755">
<div class="article-summary-box-inner">
<span><p>Capsule networks (CapsNets) are new neural networks that classify images
based on the spatial relationships of features. By analyzing the pose of
features and their relative positions, it is more capable to recognize images
after affine transformation. The stacked capsule autoencoder (SCAE) is a
state-of-the-art CapsNet, and achieved unsupervised classification of CapsNets
for the first time. However, the security vulnerabilities and the robustness of
the SCAE has rarely been explored. In this paper, we propose an evasion attack
against SCAE, where the attacker can generate adversarial perturbations based
on reducing the contribution of the object capsules in SCAE related to the
original category of the image. The adversarial perturbations are then applied
to the original images, and the perturbed images will be misclassified.
Furthermore, we propose a defense method called Hybrid Adversarial Training
(HAT) against such evasion attacks. HAT makes use of adversarial training and
adversarial distillation to achieve better robustness and stability. We
evaluate the defense method and the experimental results show that the refined
SCAE model can achieve 82.14% classification accuracy under evasion attack. The
source code is available at https://github.com/FrostbiteXSW/SCAE_Defense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Background Mixup Data Augmentation for Hand and Object-in-Contact Detection. (arXiv:2202.13941v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13941">
<div class="article-summary-box-inner">
<span><p>Detecting the positions of human hands and objects-in-contact (hand-object
detection) in each video frame is vital for understanding human activities from
videos. For training an object detector, a method called Mixup, which overlays
two training images to mitigate data bias, has been empirically shown to be
effective for data augmentation. However, in hand-object detection, mixing two
hand-manipulation images produces unintended biases, e.g., the concentration of
hands and objects in a specific region degrades the ability of the hand-object
detector to identify object boundaries. We propose a data-augmentation method
called Background Mixup that leverages data-mixing regularization while
reducing the unintended effects in hand-object detection. Instead of mixing two
images where a hand and an object in contact appear, we mix a target training
image with background images without hands and objects-in-contact extracted
from external image sources, and use the mixed images for training the
detector. Our experiments demonstrated that the proposed method can effectively
reduce false positives and improve the performance of hand-object detection in
both supervised and semi-supervised learning settings.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-02 23:07:38.228829501 UTC">2022-03-02 23:07:38 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>