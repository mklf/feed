<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-10T01:30:00Z">06-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Words are all you need? Capturing human sensory similarity with textual descriptors. (arXiv:2206.04105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04105">
<div class="article-summary-box-inner">
<span><p>Recent advances in multimodal training use textual descriptions to
significantly enhance machine understanding of images and videos. Yet, it
remains unclear to what extent language can fully capture sensory experiences
across different modalities. A well-established approach for characterizing
sensory experiences relies on similarity judgments, namely, the degree to which
people perceive two distinct stimuli as similar. We explore the relation
between human similarity judgments and language in a series of large-scale
behavioral studies ($N=1,823$ participants) across three modalities (images,
audio, and video) and two types of text descriptors: simple word tags and
free-text captions. In doing so, we introduce a novel adaptive pipeline for tag
mining that is both efficient and domain-general. We show that our prediction
pipeline based on text descriptors exhibits excellent performance, and we
compare it against a comprehensive array of 611 baseline models based on
vision-, audio-, and video-processing architectures. We further show that the
degree to which textual descriptors and models predict human similarity varies
across and within modalities. Taken together, these studies illustrate the
value of integrating machine learning and cognitive science approaches to
better understand the similarities and differences between human and machine
representations. We present an interactive visualization at
https://words-are-all-you-need.s3.amazonaws.com/index.html for exploring the
similarity between stimuli as experienced by humans and different methods
reported in the paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Text Normalization. (arXiv:2206.04137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04137">
<div class="article-summary-box-inner">
<span><p>Text-based adversarial attacks are becoming more commonplace and accessible
to general internet users. As these attacks proliferate, the need to address
the gap in model robustness becomes imminent. While retraining on adversarial
data may increase performance, there remains an additional class of
character-level attacks on which these models falter. Additionally, the process
to retrain a model is time and resource intensive, creating a need for a
lightweight, reusable defense. In this work, we propose the Adversarial Text
Normalizer, a novel method that restores baseline performance on attacked
content with low computational overhead. We evaluate the efficacy of the
normalizer on two problem areas prone to adversarial attacks, i.e. Hate Speech
and Natural Language Inference. We find that text normalization provides a
task-agnostic defense against character-level attacks that can be implemented
supplementary to adversarial retraining solutions, which are more suited for
semantic alterations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Key Event Detection from Massive Text Corpora. (arXiv:2206.04153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04153">
<div class="article-summary-box-inner">
<span><p>Automated event detection from news corpora is a crucial task towards mining
fast-evolving structured knowledge. As real-world events have different
granularities, from the top-level themes to key events and then to event
mentions corresponding to concrete actions, there are generally two lines of
research: (1) theme detection identifies from a news corpus major themes (e.g.,
"2019 Hong Kong Protests" vs. "2020 U.S. Presidential Election") that have very
distinct semantics; and (2) action extraction extracts from one document
mention-level actions (e.g., "the police hit the left arm of the protester")
that are too fine-grained for comprehending the event. In this paper, we
propose a new task, key event detection at the intermediate level, aiming to
detect from a news corpus key events (e.g., "HK Airport Protest on Aug.
12-14"), each happening at a particular time/location and focusing on the same
topic. This task can bridge event understanding and structuring and is
inherently challenging because of the thematic and temporal closeness of key
events and the scarcity of labeled data due to the fast-evolving nature of news
articles. To address these challenges, we develop an unsupervised key event
detection framework, EvMine, that (1) extracts temporally frequent peak phrases
using a novel ttf-itf score, (2) merges peak phrases into event-indicative
feature sets by detecting communities from our designed peak phrase graph that
captures document co-occurrences, semantic similarities, and temporal closeness
signals, and (3) iteratively retrieves documents related to each key event by
training a classifier with automatically generated pseudo labels from the
event-indicative feature sets and refining the detected key events using the
retrieved documents. Extensive experiments and case studies show EvMine
outperforms all the baseline methods and its ablations on two real-world news
corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved two-stage hate speech classification for twitter based on Deep Neural Networks. (arXiv:2206.04162v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04162">
<div class="article-summary-box-inner">
<span><p>Hate speech is a form of online harassment that involves the use of abusive
language, and it is commonly seen in social media posts. This sort of
harassment mainly focuses on specific group characteristics such as religion,
gender, ethnicity, etc and it has both societal and economic consequences
nowadays. The automatic detection of abusive language in text postings has
always been a difficult task, but it is lately receiving much interest from the
scientific community. This paper addresses the important problem of discerning
hateful content in social media. The model we propose in this work is an
extension of an existing approach based on LSTM neural network architectures,
which we appropriately enhanced and fine-tuned to detect certain forms of
hatred language, such as racism or sexism, in a short text. The most
significant enhancement is the conversion to a two-stage scheme consisting of
Recurrent Neural Network (RNN) classifiers. The output of all One-vs-Rest (OvR)
classifiers from the first stage are combined and used to train the second
stage classifier, which finally determines the type of harassment. Our study
includes a performance comparison of several proposed alternative methods for
the second stage evaluated on a public corpus of 16k tweets, followed by a
generalization study on another dataset. The reported results show the superior
classification quality of the proposed scheme in the task of hate speech
detection as compared to the current state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstraction not Memory: BERT and the English Article System. (arXiv:2206.04184v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04184">
<div class="article-summary-box-inner">
<span><p>Article prediction is a task that has long defied accurate linguistic
description. As such, this task is ideally suited to evaluate models on their
ability to emulate native-speaker intuition. To this end, we compare the
performance of native English speakers and pre-trained models on the task of
article prediction set up as a three way choice (a/an, the, zero). Our
experiments with BERT show that BERT outperforms humans on this task across all
articles. In particular, BERT is far superior to humans at detecting the zero
article, possibly because we insert them using rules that the deep neural model
can easily pick up. More interestingly, we find that BERT tends to agree more
with annotators than with the corpus when inter-annotator agreement is high but
switches to agreeing more with the corpus as inter-annotator agreement drops.
We contend that this alignment with annotators, despite being trained on the
corpus, suggests that BERT is not memorising article use, but captures a high
level generalisation of article use akin to human intuition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Question Generation for Personalized Feedback in Intelligent Tutoring Systems. (arXiv:2206.04187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04187">
<div class="article-summary-box-inner">
<span><p>Existing work on generating hints in Intelligent Tutoring Systems (ITS)
focuses mostly on manual and non-personalized feedback. In this work, we
explore automatically generated questions as personalized feedback in an ITS.
Our personalized feedback can pinpoint correct and incorrect or missing phrases
in student answers as well as guide them towards correct answer by asking a
question in natural language. Our approach combines cause-effect analysis to
break down student answers using text similarity-based NLP Transformer models
to identify correct and incorrect or missing parts. We train a few-shot Neural
Question Generation and Question Re-ranking models to show questions addressing
components missing in the student answers which steers students towards the
correct answer. Our model vastly outperforms both simple and strong baselines
in terms of student learning gains by 45% and 23% respectively when tested in a
real dialogue-based ITS. Finally, we show that our personalized corrective
feedback system has the potential to improve Generative Question Answering
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Folktales of Different Regions Using Topic Modeling and Clustering. (arXiv:2206.04221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04221">
<div class="article-summary-box-inner">
<span><p>This paper employs two major natural language processing techniques, topic
modeling and clustering, to find patterns in folktales and reveal cultural
relationships between regions. In particular, we used Latent Dirichlet
Allocation and BERTopic to extract the recurring elements as well as K-means
clustering to group folktales. Our paper tries to answer the question what are
the similarities and differences between folktales, and what do they say about
culture. Here we show that the common trends between folktales are family,
food, traditional gender roles, mythological figures, and animals. Also,
folktales topics differ based on geographical location with folktales found in
different regions having different animals and environment. We were not
surprised to find that religious figures and animals are some of the common
topics in all cultures. However, we were surprised that European and Asian
folktales were often paired together. Our results demonstrate the prevalence of
certain elements in cultures across the world. We anticipate our work to be a
resource to future research of folktales and an example of using natural
language processing to analyze documents in specific domains. Furthermore,
since we only analyzed the documents based on their topics, more work could be
done in analyzing the structure, sentiment, and the characters of these
folktales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crosslinguistic word order variation reflects evolutionary pressures of dependency and information locality. (arXiv:2206.04239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04239">
<div class="article-summary-box-inner">
<span><p>Languages vary considerably in syntactic structure. About 40% of the world's
languages have subject-verb-object order, and about 40% have
subject-object-verb order. Extensive work has sought to explain this word order
variation across languages. However, the existing approaches are not able to
explain coherently the frequency distribution and evolution of word order in
individual languages. We propose that variation in word order reflects
different ways of balancing competing pressures of dependency locality and
information locality, whereby languages favor placing elements together when
they are syntactically related or contextually informative about each other.
Using data from 80 languages in 17 language families and phylogenetic modeling,
we demonstrate that languages evolve to balance these pressures, such that word
order change is accompanied by change in the frequency distribution of the
syntactic structures which speakers communicate to maintain overall efficiency.
Variability in word order thus reflects different ways in which languages
resolve these evolutionary pressures. We identify relevant characteristics that
result from this joint optimization, particularly the frequency with which
subjects and objects are expressed together for the same verb. Our findings
suggest that syntactic structure and usage across languages co-adapt to support
efficient communication under limited cognitive resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLTS+: A New Chinese Long Text Summarization Dataset with Abstractive Summaries. (arXiv:2206.04253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04253">
<div class="article-summary-box-inner">
<span><p>The abstractive methods lack of creative ability is particularly a problem in
automatic text summarization. The summaries generated by models are mostly
extracted from the source articles. One of the main causes for this problem is
the lack of dataset with abstractiveness, especially for Chinese. In order to
solve this problem, we paraphrase the reference summaries in CLTS, the Chinese
Long Text Summarization dataset, correct errors of factual inconsistencies, and
propose the first Chinese Long Text Summarization dataset with a high level of
abstractiveness, CLTS+, which contains more than 180K article-summary pairs and
is available online. Additionally, we introduce an intrinsic metric based on
co-occurrence words to evaluate the dataset we constructed. We analyze the
extraction strategies used in CLTS+ summaries against other datasets to
quantify the abstractiveness and difficulty of our new data and train several
baselines on CLTS+ to verify the utility of it for improving the creative
ability of models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation. (arXiv:2206.04294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04294">
<div class="article-summary-box-inner">
<span><p>The speaker-follower models have proven to be effective in
vision-and-language navigation, where a speaker model is used to synthesize new
instructions to augment the training data for a follower navigation model.
However, in many of the previous methods, the generated instructions are not
directly trained to optimize the performance of the follower. In this paper, we
present \textsc{foam}, a \textsc{Fo}llower-\textsc{a}ware speaker
\textsc{M}odel that is constantly updated given the follower feedback, so that
the generated instructions can be more suitable to the current learning state
of the follower. Specifically, we optimize the speaker using a bi-level
optimization framework and obtain its training signals by evaluating the
follower on labeled data. Experimental results on the Room-to-Room and
Room-across-Room datasets demonstrate that our methods can outperform strong
baseline models across settings. Analyses also reveal that our generated
instructions are of higher quality than the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling Transformers with LEGO: a synthetic reasoning task. (arXiv:2206.04301v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04301">
<div class="article-summary-box-inner">
<span><p>We propose a synthetic task, LEGO (Learning Equality and Group Operations),
that encapsulates the problem of following a chain of reasoning, and we study
how the transformer architecture learns this task. We pay special attention to
data effects such as pretraining (on seemingly unrelated NLP tasks) and dataset
composition (e.g., differing chain length at training and test time), as well
as architectural variants such as weight-tied layers or adding convolutional
components. We study how the trained models eventually succeed at the task, and
in particular, we are able to understand (to some extent) some of the attention
heads as well as how the information flows in the network. Based on these
observations we propose a hypothesis that here pretraining helps merely due to
being a smart initialization rather than some deep knowledge stored in the
network. We also observe that in some data regime the trained transformer finds
"shortcut" solutions to follow the chain of reasoning, which impedes the
model's ability to generalize to simple variants of the main task, and moreover
we find that one can prevent such shortcut with appropriate architecture
modification or careful data preparation. Motivated by our findings, we begin
to explore the task of learning to execute C programs, where a convolutional
modification to transformers, namely adding convolutional structures in the
key/query/value maps, shows an encouraging edge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-based out-of-vocabulary word recovery for ASR systems in Indian languages. (arXiv:2206.04305v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04305">
<div class="article-summary-box-inner">
<span><p>Detecting and recovering out-of-vocabulary (OOV) words is always challenging
for Automatic Speech Recognition (ASR) systems. Many existing methods focus on
modeling OOV words by modifying acoustic and language models and integrating
context words cleverly into models. To train such complex models, we need a
large amount of data with context words, additional training time, and
increased model size. However, after getting the ASR transcription to recover
context-based OOV words, the post-processing method has not been explored much.
In this work, we propose a post-processing technique to improve the performance
of context-based OOV recovery. We created an acoustically boosted language
model with a sub-graph made at phone level with an OOV words list. We proposed
two methods to determine a suitable cost function to retrieve the OOV words
based on the context. The cost function is defined based on phonetic and
acoustic knowledge for matching and recovering the correct context words in the
decode. The effectiveness of the proposed cost function is evaluated at both
word-level and sentence-level. The evaluation results show that this approach
can recover an average of 50% context-based OOV words across multiple
categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic-Aware Evaluation and Transformer Methods for Topic-Controllable Summarization. (arXiv:2206.04317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04317">
<div class="article-summary-box-inner">
<span><p>Topic-controllable summarization is an emerging research area with a wide
range of potential applications. However, existing approaches suffer from
significant limitations. First, there is currently no established evaluation
metric for this task. Furthermore, existing methods built upon recurrent
architectures, which can significantly limit their performance compared to more
recent Transformer-based architectures, while they also require modifications
to the model's architecture for controlling the topic. In this work, we propose
a new topic-oriented evaluation measure to automatically evaluate the generated
summaries based on the topic affinity between the generated summary and the
desired topic. We also conducted a user study that validates the reliability of
this measure. Finally, we propose simple, yet powerful methods for
topic-controllable summarization either incorporating topic embeddings into the
model's architecture or employing control tokens to guide the summary
generation. Experimental results show that control tokens can achieve better
performance compared to more complicated embedding-based approaches while being
at the same time significantly faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Identification for Austronesian Languages. (arXiv:2206.04327v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04327">
<div class="article-summary-box-inner">
<span><p>This paper provides language identification models for low- and
under-resourced languages in the Pacific region with a focus on previously
unavailable Austronesian languages. Accurate language identification is an
important part of developing language resources. The approach taken in this
paper combines 29 Austronesian languages with 171 non-Austronesian languages to
create an evaluation set drawn from eight data sources. After evaluating six
approaches to language identification, we find that a classifier based on
skip-gram embeddings reaches a significantly higher performance than alternate
methods. We then systematically increase the number of non-Austronesian
languages in the model up to a total of 800 languages to evaluate whether an
increased language inventory leads to less precise predictions for the
Austronesian languages of interest. This evaluation finds that there is only a
minimal impact on accuracy caused by increasing the inventory of
non-Austronesian languages. Further experiments adapt these language
identification models for code-switching detection, achieving high accuracy
across all 29 languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Embedding Reliability in Low-Resource Settings Using Corpus Similarity Measures. (arXiv:2206.04330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04330">
<div class="article-summary-box-inner">
<span><p>This paper simulates a low-resource setting across 17 languages in order to
evaluate embedding similarity, stability, and reliability under different
conditions. The goal is to use corpus similarity measures before training to
predict properties of embeddings after training. The main contribution of the
paper is to show that it is possible to predict downstream embedding similarity
using upstream corpus similarity measures. This finding is then applied to
low-resource settings by modelling the reliability of embeddings created from
very limited training data. Results show that it is possible to estimate the
reliability of low-resource embeddings using corpus similarity measures that
remain robust on small amounts of data. These findings have significant
implications for the evaluation of truly low-resource languages in which such
systematic downstream validation methods are not possible because of data
limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Corpus Similarity Measures Remain Robust Across Diverse Languages. (arXiv:2206.04332v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04332">
<div class="article-summary-box-inner">
<span><p>This paper experiments with frequency-based corpus similarity measures across
39 languages using a register prediction task. The goal is to quantify (i) the
distance between different corpora from the same language and (ii) the
homogeneity of individual corpora. Both of these goals are essential for
measuring how well corpus-based linguistic analysis generalizes from one
dataset to another. The problem is that previous work has focused on
Indo-European languages, raising the question of whether these measures are
able to provide robust generalizations across diverse languages. This paper
uses a register prediction task to evaluate competing measures across 39
languages: how well are they able to distinguish between corpora representing
different contexts of production? Each experiment compares three corpora from a
single language, with the same three digital registers shared across all
languages: social media, web pages, and Wikipedia. Results show that measures
of corpus similarity retain their validity across different language families,
writing systems, and types of morphology. Further, the measures remain robust
when evaluated on out-of-domain corpora, when applied to low-resource
languages, and when applied to different sets of registers. These findings are
significant given our need to make generalizations across the rapidly
increasing number of corpora available for analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ancestor-to-Creole Transfer is Not a Walk in the Park. (arXiv:2206.04371v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04371">
<div class="article-summary-box-inner">
<span><p>We aim to learn language models for Creole languages for which large volumes
of data are not readily available, and therefore explore the potential transfer
from ancestor languages (the 'Ancestry Transfer Hypothesis'). We find that
standard transfer methods do not facilitate ancestry transfer. Surprisingly,
different from other non-Creole languages, a very distinct two-phase pattern
emerges for Creoles: As our training losses plateau, and language models begin
to overfit on their source languages, perplexity on the Creoles drop. We
explore if this compression phase can lead to practically useful language
models (the 'Ancestry Bottleneck Hypothesis'), but also falsify this. Moreover,
we show that Creoles even exhibit this two-phase pattern even when training on
random, unrelated languages. Thus Creoles seem to be typological outliers and
we speculate whether there is a link between the two observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dict-NMT: Bilingual Dictionary based NMT for Extremely Low Resource Languages. (arXiv:2206.04439v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04439">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation (NMT) models have been effective on large
bilingual datasets. However, the existing methods and techniques show that the
model's performance is highly dependent on the number of examples in training
data. For many languages, having such an amount of corpora is a far-fetched
dream. Taking inspiration from monolingual speakers exploring new languages
using bilingual dictionaries, we investigate the applicability of bilingual
dictionaries for languages with extremely low, or no bilingual corpus. In this
paper, we explore methods using bilingual dictionaries with an NMT model to
improve translations for extremely low resource languages. We extend this work
to multilingual systems, exhibiting zero-shot properties. We present a detailed
analysis of the effects of the quality of dictionaries, training dataset size,
language family, etc., on the translation quality. Results on multiple
low-resource test languages show a clear advantage of our bilingual
dictionary-based method over the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Encoder-Decoder Self-Supervised Pre-training for ASR. (arXiv:2206.04465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04465">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has shown tremendous success in various
speech-related downstream tasks, including Automatic Speech Recognition (ASR).
The output embeddings of the SSL model are treated as powerful short-time
representations of the speech signal. However, in the ASR task, the main
objective is to get the correct sequence of acoustic units, characters, or
byte-pair encodings (BPEs). Usually, encoder-decoder architecture works
exceptionally well for a sequence-to-sequence task like ASR. Therefore, in this
paper, we propose a new paradigm that exploits the power of a decoder during
self-supervised learning. We use Hidden Unit BERT (HuBERT) SSL framework to
compute the conventional masked prediction loss for the encoder. In addition,
we have introduced a decoder in the SSL framework and proposed a target
preparation strategy for the decoder. Finally, we use a multitask SSL setup
wherein we jointly optimize both the encoder and decoder losses. We hypothesize
that the presence of a decoder in the SSL model helps it learn an acoustic
unit-based language model, which might improve the performance of an ASR
downstream task. We compare our proposed SSL model with HuBERT and show up to
25% relative improvement in performance on ASR by finetuning on various
LibriSpeech subsets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SsciBERT: A Pre-trained Language Model for Social Science Texts. (arXiv:2206.04510v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04510">
<div class="article-summary-box-inner">
<span><p>The academic literature of social sciences is the literature that records
human civilization and studies human social problems. With the large-scale
growth of this literature, ways to quickly find existing research on relevant
issues have become an urgent demand for researchers. Previous studies, such as
SciBERT, have shown that pre-training using domain-specific texts can improve
the performance of natural language processing tasks in those fields. However,
there is no pre-trained language model for social sciences, so this paper
proposes a pre-trained model on many abstracts published in the Social Science
Citation Index (SSCI) journals. The models, which are available on Github
(https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent
performance on discipline classification and abstract structure-function
recognition tasks with the social sciences literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos. (arXiv:2206.04523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04523">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a neural end-to-end system for voice preserving,
lip-synchronous translation of videos. The system is designed to combine
multiple component models and produces a video of the original speaker speaking
in the target language that is lip-synchronous with the target speech, yet
maintains emphases in speech, voice characteristics, face video of the original
speaker. The pipeline starts with automatic speech recognition including
emphasis detection, followed by a translation model. The translated text is
then synthesized by a Text-to-Speech model that recreates the original emphases
mapped from the original sentence. The resulting synthetic voice is then mapped
back to the original speakers' voice using a voice conversion model. Finally,
to synchronize the lips of the speaker with the translated audio, a conditional
generative adversarial network-based model generates frames of adapted lip
movements with respect to the input face image as well as the output of the
voice conversion model. In the end, the system combines the generated video
with the converted audio to produce the final output. The result is a video of
a speaker speaking in another language without actually knowing it. To evaluate
our design, we present a user study of the complete system as well as separate
evaluations of the single components. Since there is no available dataset to
evaluate our whole system, we collect a test set and evaluate our system on
this test set. The results indicate that our system is able to generate
convincing videos of the original speaker speaking the target language while
preserving the original speaker's characteristics. The collected dataset will
be shared.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting End-to-End Speech-to-Text Translation From Scratch. (arXiv:2206.04571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04571">
<div class="article-summary-box-inner">
<span><p>End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining
its encoder and/or decoder using source transcripts via speech recognition or
text translation tasks, without which translation performance drops
substantially. However, transcripts are not always available, and how
significant such pretraining is for E2E ST has rarely been studied in the
literature. In this paper, we revisit this question and explore the extent to
which the quality of E2E ST trained on speech-translation pairs alone can be
improved. We reexamine several techniques proven beneficial to ST previously,
and offer a set of best practices that biases a Transformer-based E2E ST system
toward training from scratch. Besides, we propose parameterized distance
penalty to facilitate the modeling of locality in the self-attention model for
speech. On four benchmarks covering 23 languages, our experiments show that,
without using any transcripts or pretraining, the proposed system reaches and
even outperforms previous studies adopting pretraining, although the gap
remains in (extremely) low-resource settings. Finally, we discuss neural
acoustic feature modeling, where a neural model is designed to extract acoustic
features from raw speech signals directly, with the goal to simplify inductive
biases and add freedom to the model in describing speech. For the first time,
we demonstrate its feasibility and show encouraging results on ST tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Zero-shot Common Sense from Large Language Models for Robot 3D Scene Understanding. (arXiv:2206.04585v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04585">
<div class="article-summary-box-inner">
<span><p>Semantic 3D scene understanding is a problem of critical importance in
robotics. While significant advances have been made in simultaneous
localization and mapping algorithms, robots are still far from having the
common sense knowledge about household objects and their locations of an
average human. We introduce a novel method for leveraging common sense embedded
within large language models for labelling rooms given the objects contained
within. This algorithm has the added benefits of (i) requiring no task-specific
pre-training (operating entirely in the zero-shot regime) and (ii) generalizing
to arbitrary room and object labels, including previously-unseen ones -- both
of which are highly desirable traits in robotic scene understanding algorithms.
The proposed algorithm operates on 3D scene graphs produced by modern spatial
perception systems, and we hope it will pave the way to more generalizable and
scalable high-level 3D scene understanding for robotics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Leakage in Text Classification: A Data Extraction Approach. (arXiv:2206.04591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04591">
<div class="article-summary-box-inner">
<span><p>Recent work has demonstrated the successful extraction of training data from
generative language models. However, it is not evident whether such extraction
is feasible in text classification models since the training objective is to
predict the class label as opposed to next-word prediction. This poses an
interesting challenge and raises an important question regarding the privacy of
training data in text classification settings. Therefore, we study the
potential privacy leakage in the text classification domain by investigating
the problem of unintended memorization of training data that is not pertinent
to the learning task. We propose an algorithm to extract missing tokens of a
partial text by exploiting the likelihood of the class label provided by the
model. We test the effectiveness of our algorithm by inserting canaries into
the training set and attempting to extract tokens in these canaries
post-training. In our experiments, we demonstrate that successful extraction is
possible to some extent. This can also be used as an auditing strategy to
assess any potential unauthorized use of personal data without consent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04615">
<div class="article-summary-box-inner">
<span><p>Language models demonstrate both quantitative improvement and new qualitative
capabilities with increasing scale. Despite their potentially transformative
impact, these new capabilities are as yet poorly characterized. In order to
inform future research, prepare for disruptive new model capabilities, and
ameliorate socially harmful effects, it is vital that we understand the present
and near-future capabilities and limitations of language models. To address
this challenge, we introduce the Beyond the Imitation Game benchmark
(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442
authors across 132 institutions. Task topics are diverse, drawing problems from
linguistics, childhood development, math, common-sense reasoning, biology,
physics, social bias, software development, and beyond. BIG-bench focuses on
tasks that are believed to be beyond the capabilities of current language
models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense
transformer architectures, and Switch-style sparse transformers on BIG-bench,
across model sizes spanning millions to hundreds of billions of parameters. In
addition, a team of human expert raters performed all tasks in order to provide
a strong baseline. Findings include: model performance and calibration both
improve with scale, but are poor in absolute terms (and when compared with
rater performance); performance is remarkably similar across model classes,
though with benefits from sparsity; tasks that improve gradually and
predictably commonly involve a large knowledge or memorization component,
whereas tasks that exhibit "breakthrough" behavior at a critical scale often
involve multiple steps or components, or brittle metrics; social bias typically
increases with scale in settings with ambiguous context, but this can be
improved with prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factuality Enhanced Language Models for Open-Ended Text Generation. (arXiv:2206.04624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04624">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (LMs) are susceptible to generate text with
nonfactual information. In this work, we measure and improve the factual
accuracy of large-scale LMs for open-ended text generation. We design the
FactualityPrompts test set and metrics to measure the factuality of LM
generations. Based on that, we study the factual accuracy of LMs with parameter
sizes ranging from 126M to 530B. Interestingly, we find that larger LMs are
more factual than smaller ones, although a previous study suggests that larger
LMs can be less truthful in terms of misconceptions. In addition, popular
sampling algorithms (e.g., top-p) in open-ended text generation can harm the
factuality due to the "uniform randomness" introduced at every sampling step.
We propose the factual-nucleus sampling algorithm that dynamically adapts the
randomness to improve the factuality of generation while maintaining quality.
Furthermore, we analyze the inefficiencies of the standard training method in
learning correct associations between entities from factual text corpus (e.g.,
Wikipedia). We propose a factuality-enhanced training method that uses
TopicPrefix for better awareness of facts and sentence completion as the
training objective, which can vastly reduce the factual errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BigVGAN: A Universal Neural Vocoder with Large-Scale Training. (arXiv:2206.04658v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04658">
<div class="article-summary-box-inner">
<span><p>Despite recent progress in generative adversarial network(GAN)-based
vocoders, where the model generates raw waveform conditioned on mel
spectrogram, it is still challenging to synthesize high-fidelity audio for
numerous speakers across varied recording environments. In this work, we
present BigVGAN, a universal vocoder that generalizes well under various unseen
conditions in zero-shot setting. We introduce periodic nonlinearities and
anti-aliased representation into the generator, which brings the desired
inductive bias for waveform synthesis and significantly improves audio quality.
Based on our improved generator and the state-of-the-art discriminators, we
train our GAN vocoder at the largest scale up to 112M parameters, which is
unprecedented in the literature. In particular, we identify and address the
training instabilities specific to such scale, while maintaining high-fidelity
output without over-regularization. Our BigVGAN achieves the state-of-the-art
zero-shot performance for various out-of-distribution scenarios, including new
speakers, novel languages, singing voices, music and instrumental audio in
unseen (even noisy) recording environments. We will release our code and model
at: https://github.com/NVIDIA/BigVGAN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jewelry Shop Conversational Chatbot. (arXiv:2206.04659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04659">
<div class="article-summary-box-inner">
<span><p>Since the advent of chatbots in the commercial sector, they have been widely
employed in the customer service department. Typically, these commercial
chatbots are retrieval-based, so they are unable to respond to queries absent
in the provided dataset. On the contrary, generative chatbots try to create the
most appropriate response, but are mostly unable to create a smooth flow in the
customer-bot dialog. Since the client has few options left for continuing after
receiving a response, the dialog becomes short. Through our work, we try to
maximize the intelligence of a simple conversational agent so it can answer
unseen queries, and generate follow-up questions or remarks. We have built a
chatbot for a jewelry shop that finds the underlying objective of the
customer's query by finding similarity of the input to patterns in the corpus.
Our system features an audio input interface for clients, so they may speak to
it in natural language. After converting the audio to text, we trained the
model to extract the intent of the query, to find an appropriate response and
to speak to the client in a natural human voice. To gauge the system's
performance, we used performance metrics such as Recall, Precision and F1
score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaK-NER: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations. (arXiv:2109.05233v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05233">
<div class="article-summary-box-inner">
<span><p>State-of-the-art Named Entity Recognition(NER) models rely heavily on large
amountsof fully annotated training data. However, ac-cessible data are often
incompletely annotatedsince the annotators usually lack comprehen-sive
knowledge in the target domain. Normallythe unannotated tokens are regarded as
non-entities by default, while we underline thatthese tokens could either be
non-entities orpart of any entity. Here, we study NER mod-eling with incomplete
annotated data whereonly a fraction of the named entities are la-beled, and the
unlabeled tokens are equiva-lently multi-labeled by every possible label.Taking
multi-labeled tokens into account, thenumerous possible paths can distract the
train-ing model from the gold path (ground truthlabel sequence), and thus
hinders the learn-ing ability. In this paper, we propose AdaK-NER, named the
adaptive top-Kapproach, tohelp the model focus on a smaller feasible re-gion
where the gold path is more likely to belocated. We demonstrate the superiority
ofour approach through extensive experimentson both English and Chinese
datasets, aver-agely improving 2% in F-score on the CoNLL-2003 and over 10% on
two Chinese datasetscompared with the prior state-of-the-art works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Self-Supervised Automatic Post-Editing Data Generation Tool. (arXiv:2111.12284v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12284">
<div class="article-summary-box-inner">
<span><p>Data building for automatic post-editing (APE) requires extensive and
expert-level human effort, as it contains an elaborate process that involves
identifying errors in sentences and providing suitable revisions. Hence, we
develop a self-supervised data generation tool, deployable as a web
application, that minimizes human supervision and constructs personalized APE
data from a parallel corpus for several language pairs with English as the
target language. Data-centric APE research can be conducted using this tool,
involving many language pairs that have not been studied thus far owing to the
lack of suitable data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Attention Network for Stock Movements Prediction. (arXiv:2112.13593v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13593">
<div class="article-summary-box-inner">
<span><p>Stock prices move as piece-wise trending fluctuation rather than a purely
random walk. Traditionally, the prediction of future stock movements is based
on the historical trading record. Nowadays, with the development of social
media, many active participants in the market choose to publicize their
strategies, which provides a window to glimpse over the whole market's attitude
towards future movements by extracting the semantics behind social media.
However, social media contains conflicting information and cannot replace
historical records completely. In this work, we propose a multi-modality
attention network to reduce conflicts and integrate semantic and numeric
features to predict future stock movements comprehensively. Specifically, we
first extract semantic information from social media and estimate their
credibility based on posters' identity and public reputation. Then we
incorporate the semantic from online posts and numeric features from historical
records to make the trading strategy. Experimental results show that our
approach outperforms previous methods by a significant margin in both
prediction accuracy (61.20\%) and trading profits (9.13\%). It demonstrates
that our method improves the performance of stock movements prediction and
informs future research on multi-modality fusion towards stock prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Open Text Release 1: Public Domain News in 44 Languages. (arXiv:2201.05609v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05609">
<div class="article-summary-box-inner">
<span><p>We present Multilingual Open Text (MOT), a new multilingual corpus containing
text in 44 languages, many of which have limited existing text resources for
natural language processing. The first release of the corpus contains over 2.8
million news articles and an additional 1 million short snippets (photo
captions, video descriptions, etc.) published between 2001--2022 and collected
from Voice of America's news websites. We describe our process for collecting,
filtering, and processing the data. The source material is in the public
domain, our collection is licensed using a creative commons license (CC BY
4.0), and all software used to create the corpus is released under the MIT
License. The corpus will be regularly updated as additional documents are
published.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval. (arXiv:2201.12431v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12431">
<div class="article-summary-box-inner">
<span><p>Retrieval-based language models (R-LM) model the probability of natural
language text by combining a standard language model (LM) with examples
retrieved from an external datastore at test time. While effective, a major
bottleneck of using these models in practice is the computationally costly
datastore search, which can be performed as frequently as every time step. In
this paper, we present RetoMaton - retrieval automaton - which approximates the
datastore search, based on (1) saving pointers between consecutive datastore
entries, and (2) clustering of entries into "states". This effectively results
in a weighted finite automaton built on top of the datastore, instead of
representing the datastore as a flat list. The creation of the automaton is
unsupervised, and a RetoMaton can be constructed from any text collection:
either the original training corpus or from another domain. Traversing this
automaton at inference time, in parallel to the LM inference, reduces its
perplexity by up to 1.85, or alternatively saves up to 83% of the nearest
neighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting
perplexity. Our code and trained models are available at
https://github.com/neulab/retomaton .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16434">
<div class="article-summary-box-inner">
<span><p>We consider the problem of localizing a spatio-temporal tube in a video
corresponding to a given text query. This is a challenging task that requires
the joint and efficient modeling of temporal, spatial and multi-modal
interactions. To address this task, we propose TubeDETR, a transformer-based
architecture inspired by the recent success of such models for text-conditioned
object detection. Our model notably includes: (i) an efficient video and text
encoder that models spatial multi-modal interactions over sparsely sampled
frames and (ii) a space-time decoder that jointly performs spatio-temporal
localization. We demonstrate the advantage of our proposed components through
an extensive ablation study. We also evaluate our full approach on the
spatio-temporal video grounding task and demonstrate improvements over the
state of the art on the challenging VidSTG and HC-STVG benchmarks. Code and
trained models are publicly available at
https://antoyang.github.io/tubedetr.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREER: A Large-Scale Corpus for Relation Extraction and Entity Recognition. (arXiv:2204.12710v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12710">
<div class="article-summary-box-inner">
<span><p>We describe the design and use of the CREER dataset, a large corpus annotated
with rich English grammar and semantic attributes. The CREER dataset uses the
Stanford CoreNLP Annotator to capture rich language structures from Wikipedia
plain text. This dataset follows widely used linguistic and semantic
annotations so that it can be used for not only most natural language
processing tasks but also scaling the dataset. This large supervised dataset
can serve as the basis for improving the performance of NLP tasks in the
future. We publicize the dataset through the link:
https://140.116.82.111/share.cgi?ssid=000dOJ4
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Russian Texts Detoxification with Levenshtein Editing. (arXiv:2204.13638v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13638">
<div class="article-summary-box-inner">
<span><p>Text detoxification is a style transfer task of creating neutral versions of
toxic texts. In this paper, we use the concept of text editing to build a
two-step tagging-based detoxification model using a parallel corpus of Russian
texts. With this model, we achieved the best style transfer accuracy among all
models in the RUSSE Detox shared task, surpassing larger sequence-to-sequence
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance. (arXiv:2205.02293v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02293">
<div class="article-summary-box-inner">
<span><p>Human-translated text displays distinct features from naturally written text
in the same language. This phenomena, known as translationese, has been argued
to confound the machine translation (MT) evaluation. Yet, we find that existing
work on translationese neglects some important factors and the conclusions are
mostly correlational but not causal. In this work, we collect CausalMT, a
dataset where the MT training data are also labeled with the human translation
directions. We inspect two critical factors, the train-test direction match
(whether the human translation directions in the training and test sets are
aligned), and data-model direction match (whether the model learns in the same
direction as the human translation direction in the dataset). We show that
these two factors have a large causal effect on the MT performance, in addition
to the test-model direction mismatch highlighted by existing work on the impact
of translationese. In light of our findings, we provide a set of suggestions
for MT training and evaluation. Our code and data are at
https://github.com/EdisonNi-hku/CausalMT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Zero-Shot Reasoners. (arXiv:2205.11916v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11916">
<div class="article-summary-box-inner">
<span><p>Pretrained large language models (LLMs) are widely used in many sub-fields of
natural language processing (NLP) and generally known as excellent few-shot
learners with task-specific exemplars. Notably, chain of thought (CoT)
prompting, a recent technique for eliciting complex multi-step reasoning
through step-by-step answer examples, achieved the state-of-the-art
performances in arithmetics and symbolic reasoning, difficult system-2 tasks
that do not follow the standard scaling laws for LLMs. While these successes
are often attributed to LLMs' ability for few-shot learning, we show that LLMs
are decent zero-shot reasoners by simply adding "Let's think step by step"
before each answer. Experimental results demonstrate that our Zero-shot-CoT,
using the same single prompt template, significantly outperforms zero-shot LLM
performances on diverse benchmark reasoning tasks including arithmetics
(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin
Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled
Objects), without any hand-crafted few-shot examples, e.g. increasing the
accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with
175B parameter InstructGPT model, as well as similar magnitudes of improvements
with another off-the-shelf large model, 540B parameter PaLM. The versatility of
this single prompt across very diverse reasoning tasks hints at untapped and
understudied fundamental zero-shot capabilities of LLMs, suggesting high-level,
multi-task broad cognitive capabilities may be extracted by simple prompting.
We hope our work not only serves as the minimal strongest zero-shot baseline
for the challenging reasoning benchmarks, but also highlights the importance of
carefully exploring and analyzing the enormous zero-shot knowledge hidden
inside LLMs before crafting finetuning datasets or few-shot exemplars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14014">
<div class="article-summary-box-inner">
<span><p>Transformers have made progress in miscellaneous tasks, but suffer from
quadratic computational and memory complexities. Recent works propose sparse
Transformers with attention on sparse graphs to reduce complexity and remain
strong performance. While effective, the crucial parts of how dense a graph
needs to be to perform well are not fully explored. In this paper, we propose
Normalized Information Payload (NIP), a graph scoring function measuring
information transfer on graph, which provides an analysis tool for trade-offs
between performance and complexity. Guided by this theoretical analysis, we
present Hypercube Transformer, a sparse Transformer that models token
interactions in a hypercube and shows comparable or even better results with
vanilla Transformer while yielding $O(N\log N)$ complexity with sequence length
$N$. Experiments on tasks requiring various sequence lengths lay validation for
our graph function well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems. (arXiv:2205.15060v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15060">
<div class="article-summary-box-inner">
<span><p>In this paper, we present Duplex Conversation, a multi-turn, multimodal
spoken dialogue system that enables telephone-based agents to interact with
customers like a human. We use the concept of full-duplex in telecommunication
to demonstrate what a human-like interactive experience should be and how to
achieve smooth turn-taking through three subtasks: user state detection,
backchannel selection, and barge-in detection. Besides, we propose
semi-supervised learning with multimodal data augmentation to leverage
unlabeled data to increase model generalization. Experimental results on three
sub-tasks show that the proposed method achieves consistent improvements
compared with baselines. We deploy the Duplex Conversation to Alibaba
intelligent customer service and share lessons learned in production. Online
A/B experiments show that the proposed system can significantly reduce response
latency by 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation. (arXiv:2206.03354v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03354">
<div class="article-summary-box-inner">
<span><p>Vision-and-language tasks are gaining popularity in the research community,
but the focus is still mainly on English. We propose a pipeline that utilizes
English-only vision-language models to train a monolingual model for a target
language. We propose to extend OSCAR+, a model which leverages object tags as
anchor points for learning image-text alignments, to train on visual question
answering datasets in different languages. We propose a novel approach to
knowledge distillation to train the model in other languages using parallel
sentences. Compared to other models that use the target language in the
pretraining corpora, we can leverage an existing English model to transfer the
knowledge to the target language using significantly lesser resources. We also
release a large-scale visual question answering dataset in Japanese and Hindi
language. Though we restrict our work to visual question answering, our model
can be extended to any sequence-level classification task, and it can be
extended to other languages as well. This paper focuses on two languages for
the visual question answering task - Japanese and Hindi. Our pipeline
outperforms the current state-of-the-art models by a relative increase of 4.4%
and 13.4% respectively in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Prompting Towards Controllable Response Generation. (arXiv:2206.03931v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03931">
<div class="article-summary-box-inner">
<span><p>Much literature has shown that prompt-based learning is an efficient method
to make use of the large pre-trained language model. Recent works also exhibit
the possibility of steering a chatbot's output by plugging in an appropriate
prompt. Gradient-based methods are often used to perturb the prompts. However,
some language models are not even available to the public. In this work, we
first explored the combination of prompting and reinforcement learning (RL) to
steer models' generation without accessing any of the models' parameters.
Second, to reduce the training effort and enhance the generalizability to the
unseen task, we apply multi-task learning to make the model learn to generalize
to new tasks better. The experiment results show that our proposed method can
successfully control several state-of-the-art (SOTA) dialogue models without
accessing their parameters. Furthermore, the model demonstrates the strong
ability to quickly adapt to an unseen task in fewer steps than the baseline
model.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">An Improved Deep Convolutional Neural Network by Using Hybrid Optimization Algorithms to Detect and Classify Brain Tumor Using Augmented MRI Images. (arXiv:2206.04056v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04056">
<div class="article-summary-box-inner">
<span><p>Automated brain tumor detection is becoming a highly considerable medical
diagnosis research. In recent medical diagnoses, detection and classification
are highly considered to employ machine learning and deep learning techniques.
Nevertheless, the accuracy and performance of current models need to be
improved for suitable treatments. In this paper, an improvement in deep
convolutional learning is ensured by adopting enhanced optimization algorithms,
Thus, Deep Convolutional Neural Network (DCNN) based on improved Harris Hawks
Optimization (HHO), called G-HHO has been considered. This hybridization
features Grey Wolf Optimization (GWO) and HHO to give better results, limiting
the convergence rate and enhancing performance. Moreover, Otsu thresholding is
adopted to segment the tumor portion that emphasizes brain tumor detection.
Experimental studies are conducted to validate the performance of the suggested
method on a total number of 2073 augmented MRI images. The technique's
performance was ensured by comparing it with the nine existing algorithms on
huge augmented MRI images in terms of accuracy, precision, recall, f-measure,
execution time, and memory usage. The performance comparison shows that the
DCNN-G-HHO is much more successful than existing methods, especially on a
scoring accuracy of 97%. Additionally, the statistical performance analysis
indicates that the suggested approach is faster and utilizes less memory at
identifying and categorizing brain tumor cancers on the MR images. The
implementation of this validation is conducted on the Python platform. The
relevant codes for the proposed approach are available at:
https://github.com/bryarahassan/DCNN-G-HHO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRHDR: A Dual branch Residual Network for Multi-Bracket High Dynamic Range Imaging. (arXiv:2206.04124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04124">
<div class="article-summary-box-inner">
<span><p>We introduce DRHDR, a Dual branch Residual Convolutional Neural Network for
Multi-Bracket HDR Imaging. To address the challenges of fusing multiple
brackets from dynamic scenes, we propose an efficient dual branch network that
operates on two different resolutions. The full resolution branch uses a
Deformable Convolutional Block to align features and retain high-frequency
details. A low resolution branch with a Spatial Attention Block aims to attend
wanted areas from the non-reference brackets, and suppress displaced features
that could incur on ghosting artifacts. By using a dual branch approach we are
able to achieve high quality results while constraining the computational
resources required to estimate the HDR results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Self-supervised and Weight-preserving Neural Architecture Search. (arXiv:2206.04125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04125">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) algorithms save tremendous labor from human
experts. Recent advancements further reduce the computational overhead to an
affordable level. However, it is still cumbersome to deploy the NAS techniques
in real-world applications due to the fussy procedures and the supervised
learning paradigm. In this work, we propose the self-supervised and
weight-preserving neural architecture search (SSWP-NAS) as an extension of the
current NAS framework by allowing the self-supervision and retaining the
concomitant weights discovered during the search stage. As such, we simplify
the workflow of NAS to a one-stage and proxy-free procedure. Experiments show
that the architectures searched by the proposed framework achieve
state-of-the-art accuracy on CIFAR-10, CIFAR-100, and ImageNet datasets without
using manual labels. Moreover, we show that employing the concomitant weights
as initialization consistently outperforms the random initialization and the
two-stage weight pre-training method by a clear margin under semi-supervised
learning scenarios. Codes are publicly available at
https://github.com/LzVv123456/SSWP-NAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Receding Moving Object Segmentation in 3D LiDAR Data Using Sparse 4D Convolutions. (arXiv:2206.04129v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04129">
<div class="article-summary-box-inner">
<span><p>A key challenge for autonomous vehicles is to navigate in unseen dynamic
environments. Separating moving objects from static ones is essential for
navigation, pose estimation, and understanding how other traffic participants
are likely to move in the near future. In this work, we tackle the problem of
distinguishing 3D LiDAR points that belong to currently moving objects, like
walking pedestrians or driving cars, from points that are obtained from
non-moving objects, like walls but also parked cars. Our approach takes a
sequence of observed LiDAR scans and turns them into a voxelized sparse 4D
point cloud. We apply computationally efficient sparse 4D convolutions to
jointly extract spatial and temporal features and predict moving object
confidence scores for all points in the sequence. We develop a receding horizon
strategy that allows us to predict moving objects online and to refine
predictions on the go based on new observations. We use a binary Bayes filter
to recursively integrate new predictions of a scan resulting in more robust
estimation. We evaluate our approach on the SemanticKITTI moving object
segmentation challenge and show more accurate predictions than existing
methods. Since our approach only operates on the geometric information of point
clouds over time, it generalizes well to new, unseen environments, which we
evaluate on the Apollo dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Estimation of Speckle Statistics Parametric Images. (arXiv:2206.04145v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04145">
<div class="article-summary-box-inner">
<span><p>Quantitative Ultrasound (QUS) provides important information about the tissue
properties. QUS parametric image can be formed by dividing the envelope data
into small overlapping patches and computing different speckle statistics such
as parameters of the Nakagami and Homodyned K-distributions (HK-distribution).
The calculated QUS parametric images can be erroneous since only a few
independent samples are available inside the patches. Another challenge is that
the envelope samples inside the patch are assumed to come from the same
distribution, an assumption that is often violated given that the tissue is
usually not homogenous. In this paper, we propose a method based on
Convolutional Neural Networks (CNN) to estimate QUS parametric images without
patching. We construct a large dataset sampled from the HK-distribution, having
regions with random shapes and QUS parameter values. We then use a well-known
network to estimate QUS parameters in a multi-task learning fashion. Our
results confirm that the proposed method is able to reduce errors and improve
border definition in QUS parametric images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensembling Framework for Texture Extraction Techniques for Classification. (arXiv:2206.04158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04158">
<div class="article-summary-box-inner">
<span><p>In the past few years, texture-based classification problems have proven
their significance in many domains, from industrial inspection to
health-related applications. New techniques and CNN-based architectures have
been developed in recent years to solve texture-based classification problems.
The limitation of these approaches is that none of them claims to be the best
suited for all types of textures. Each technique has its advantage over a
specific texture type. To address this issue, we are proposing a framework that
combines existing techniques to extract texture features and displays better
results than the present ones. The proposed framework works well on the most of
the texture types, and in this framework, new techniques can also be added to
achieve better results than existing ones. We are also presenting the SOTA
results on FMD and KTH datasets by combining three existing techniques, using
the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CASS: Cross Architectural Self-Supervision for Medical Image Analysis. (arXiv:2206.04170v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04170">
<div class="article-summary-box-inner">
<span><p>Recent advances in Deep Learning and Computer Vision have alleviated many of
the bottlenecks, allowing algorithms to be label-free with better performance.
Specifically, Transformers provide a global perspective of the image, which
Convolutional Neural Networks (CNN) lack by design. Here we present
\textbf{C}ross \textbf{A}rchitectural - \textbf{S}elf \textbf{S}upervision , a
novel self-supervised learning approach which leverages transformers and CNN
simultaneously, while also being computationally accessible to general
practitioners via easily available cloud services. Compared to existing
state-of-the-art self-supervised learning approaches, we empirically show CASS
trained CNNs, and Transformers gained an average of 8.5\% with 100\% labelled
data, 7.3\% with 10\% labelled data, and 11.5\% with 1\% labelled data, across
three diverse datasets. Notably, one of the employed datasets included
histopathology slides of an autoimmune disease, a topic underrepresented in
Medical Imaging and has minimal data. In addition, our findings reveal that
CASS is twice as efficient as other state-of-the-art methods in terms of
training time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VN-Transformer: Rotation-Equivariant Attention for Vector Neurons. (arXiv:2206.04176v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04176">
<div class="article-summary-box-inner">
<span><p>Rotation equivariance is a desirable property in many practical applications
such as motion forecasting and 3D perception, where it can offer benefits like
sample efficiency, better generalization, and robustness to input
perturbations. Vector Neurons (VN) is a recently developed framework offering a
simple yet effective approach for deriving rotation-equivariant analogs of
standard machine learning operations by extending one-dimensional scalar
neurons to three-dimensional "vector neurons." We introduce a novel
"VN-Transformer" architecture to address several shortcomings of the current VN
models. Our contributions are: $(i)$ we derive a rotation-equivariant attention
mechanism which eliminates the need for the heavy feature preprocessing
required by the original Vector Neurons models; $(ii)$ we extend the VN
framework to support non-spatial attributes, expanding the applicability of
these models to real-world datasets; $(iii)$ we derive a rotation-equivariant
mechanism for multi-scale reduction of point-cloud resolution, greatly speeding
up inference and training; $(iv)$ we show that small tradeoffs in equivariance
($\epsilon$-approximate equivariance) can be used to obtain large improvements
in numerical stability and training robustness on accelerated hardware, and we
bound the propagation of equivariance violations in our models. Finally, we
apply our VN-Transformer to 3D shape classification and motion forecasting with
compelling results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCAMPS: Synthetics for Camera Measurement of Physiological Signals. (arXiv:2206.04197v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04197">
<div class="article-summary-box-inner">
<span><p>The use of cameras and computational algorithms for noninvasive, low-cost and
scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs
is very attractive. However, diverse data representing a range of environments,
body motions, illumination conditions and physiological states is laborious,
time consuming and expensive to obtain. Synthetic data have proven a valuable
tool in several areas of machine learning, yet are not widely available for
camera measurement of physiological states. Synthetic data offer "perfect"
labels (e.g., without noise and with precise synchronization), labels that may
not be possible to obtain otherwise (e.g., precise pixel level segmentation
maps) and provide a high degree of control over variation and diversity in the
dataset. We present SCAMPS, a dataset of synthetics containing 2,800 videos
(1.68M frames) with aligned cardiac and respiratory signals and facial action
intensities. The RGB frames are provided alongside segmentation maps. We
provide precise descriptive statistics about the underlying waveforms,
including inter-beat interval, heart rate variability, and pulse arrival time.
Finally, we present baseline results training on these synthetic data and
testing on real-world datasets to illustrate generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JNMR: Joint Non-linear Motion Regression for Video Frame Interpolation. (arXiv:2206.04231v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04231">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation (VFI) aims to generate predictive frames by warping
learnable motions from the bidirectional historical references. Most existing
works utilize spatio-temporal semantic information extractor to realize motion
estimation and interpolation modeling, not enough considering with the real
mechanistic rationality of generated middle motions. In this paper, we
reformulate VFI as a multi-variable non-linear (MNL) regression problem, and a
Joint Non-linear Motion Regression (JNMR) strategy is proposed to model
complicated motions of inter-frame. To establish the MNL regression, ConvLSTM
is adopted to construct the distribution of complete motions in temporal
dimension. The motion correlations between the target frame and multiple
reference frames can be regressed by the modeled distribution. Moreover, the
feature learning network is designed to optimize for the MNL regression
modeling. A coarse-to-fine synthesis enhancement module is further conducted to
learn visual dynamics at different resolutions through repetitive regression
and interpolation. Highly competitive experimental results on frame
interpolation show that the effectiveness and significant improvement compared
with state-of-the-art performance, and the robustness of complicated motion
estimation is improved by the MNL motion regression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cardiac Adipose Tissue Segmentation via Image-Level Annotations. (arXiv:2206.04238v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04238">
<div class="article-summary-box-inner">
<span><p>Automatically identifying the structural substrates underlying cardiac
abnormalities can potentially provide real-time guidance for interventional
procedures. With the knowledge of cardiac tissue substrates, the treatment of
complex arrhythmias such as atrial fibrillation and ventricular tachycardia can
be further optimized by detecting arrhythmia substrates to target for treatment
(i.e., adipose) and identifying critical structures to avoid. Optical coherence
tomography (OCT) is a real-time imaging modality that aids in addressing this
need. Existing approaches for cardiac image analysis mainly rely on fully
supervised learning techniques, which suffer from the drawback of workload on
labor-intensive annotation process of pixel-wise labeling. To lessen the need
for pixel-wise labeling, we develop a two-stage deep learning framework for
cardiac adipose tissue segmentation using image-level annotations on OCT images
of human cardiac substrates. In particular, we integrate class activation
mapping with superpixel segmentation to solve the sparse tissue seed challenge
raised in cardiac tissue segmentation. Our study bridges the gap between the
demand on automatic tissue analysis and the lack of high-quality pixel-wise
annotations. To the best of our knowledge, this is the first study that
attempts to address cardiac tissue segmentation on OCT images via weakly
supervised learning techniques. Within an in-vitro human cardiac OCT dataset,
we demonstrate that our weakly supervised approach on image-level annotations
achieves comparable performance as fully supervised methods trained on
pixel-wise annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OOD Augmentation May Be at Odds with Open-Set Recognition. (arXiv:2206.04242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04242">
<div class="article-summary-box-inner">
<span><p>Despite advances in image classification methods, detecting the samples not
belonging to the training classes is still a challenging problem. There has
been a burst of interest in this subject recently, which is called Open-Set
Recognition (OSR). In OSR, the goal is to achieve both the classification and
detecting out-of-distribution (OOD) samples. Several ideas have been proposed
to push the empirical result further through complicated techniques. We believe
that such complication is indeed not necessary. To this end, we have shown that
Maximum Softmax Probability (MSP), as the simplest baseline for OSR, applied on
Vision Transformers (ViTs) as the base classifier that is trained with non-OOD
augmentations can surprisingly outperform many recent methods. Non-OOD
augmentations are the ones that do not alter the data distribution by much. Our
results outperform state-of-the-art in CIFAR-10 datasets, and is also better
than most of the current methods in SVHN and MNIST. We show that training
augmentation has a significant effect on the performance of ViTs in the OSR
tasks, and while they should produce significant diversity in the augmented
samples, the generated sample OOD-ness must remain limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwinCheX: Multi-label classification on chest X-ray images with transformers. (arXiv:2206.04246v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04246">
<div class="article-summary-box-inner">
<span><p>According to the considerable growth in the avail of chest X-ray images in
diagnosing various diseases, as well as gathering extensive datasets, having an
automated diagnosis procedure using deep neural networks has occupied the minds
of experts. Most of the available methods in computer vision use a CNN backbone
to acquire high accuracy on the classification problems. Nevertheless, recent
researches show that transformers, established as the de facto method in NLP,
can also outperform many CNN-based models in vision. This paper proposes a
multi-label classification deep model based on the Swin Transformer as the
backbone to achieve state-of-the-art diagnosis classification. It leverages
Multi-Layer Perceptron, also known as MLP, for the head architecture. We
evaluate our model on one of the most widely-used and largest x-ray datasets
called "Chest X-ray14," which comprises more than 100,000 frontal/back-view
images from over 30,000 patients with 14 famous chest diseases. Our model has
been tested with several number of MLP layers for the head setting, each
achieves a competitive AUC score on all classes. Comprehensive experiments on
Chest X-ray14 have shown that a 3-layer head attains state-of-the-art
performance with an average AUC score of 0.810, compared to the former SOTA
average AUC of 0.799. We propose an experimental setup for the fair
benchmarking of existing methods, which could be used as a basis for the future
studies. Finally, we followed up our results by confirming that the proposed
method attends to the pathologically relevant areas of the chest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepVerge: Classification of Roadside Verge Biodiversity and Conservation Potential. (arXiv:2206.04271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04271">
<div class="article-summary-box-inner">
<span><p>Open space grassland is being increasingly farmed or built upon, leading to a
ramping up of conservation efforts targeting roadside verges. Approximately
half of all UK grassland species can be found along the country's 500,000 km of
roads, with some 91 species either threatened or near threatened. Careful
management of these "wildlife corridors" is therefore essential to preventing
species extinction and maintaining biodiversity in grassland habitats. Wildlife
trusts have often enlisted the support of volunteers to survey roadside verges
and identify new "Local Wildlife Sites" as areas of high conservation
potential. Using volunteer survey data from 3,900 km of roadside verges
alongside publicly available street-view imagery, we present DeepVerge; a deep
learning-based method that can automatically survey sections of roadside verges
by detecting the presence of positive indicator species. Using images and
ground truth survey data from the rural county of Lincolnshire, DeepVerge
achieved a mean accuracy of 88%. Such a method may be used by local authorities
to identify new local wildlife sites, and aid management and environmental
planning in line with legal and government policy obligations, saving thousands
of hours of manual labour.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STEM image analysis based on deep learning: identification of vacancy defects and polymorphs of ${MoS_2}$. (arXiv:2206.04272v1 [cond-mat.mes-hall])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04272">
<div class="article-summary-box-inner">
<span><p>Scanning transmission electron microscopy (STEM) is an indispensable tool for
atomic-resolution structural analysis for a wide range of materials. The
conventional analysis of STEM images is an extensive hands-on process, which
limits efficient handling of high-throughput data. Here we apply a fully
convolutional network (FCN) for identification of important structural features
of two-dimensional crystals. ResUNet, a type of FCN, is utilized in identifying
sulfur vacancies and polymorph types of ${MoS_2}$ from atomic resolution STEM
images. Efficient models are achieved based on training with simulated images
in the presence of different levels of noise, aberrations, and carbon
contamination. The accuracy of the FCN models toward extensive experimental
STEM images is comparable to that of careful hands-on analysis. Our work
provides a guideline on best practices to train a deep learning model for STEM
image analysis and demonstrates FCN's application for efficient processing of a
large volume of STEM data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Spatiotemporal Representation Learning for Longitudinally-consistent Neuroimage Analysis. (arXiv:2206.04281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04281">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised advances in medical computer vision exploit global and
local anatomical self-similarity for pretraining prior to downstream tasks such
as segmentation. However, current methods assume i.i.d. image acquisition,
which is invalid in clinical study designs where follow-up longitudinal scans
track subject-specific temporal changes. Further, existing self-supervised
methods for medically-relevant image-to-image architectures exploit only
spatial or temporal self-similarity and only do so via a loss applied at a
single image-scale, with naive multi-scale spatiotemporal extensions collapsing
to degenerate solutions. To these ends, this paper makes two contributions: (1)
It presents a local and multi-scale spatiotemporal representation learning
method for image-to-image architectures trained on longitudinal images. It
exploits the spatiotemporal self-similarity of learned multi-scale
intra-subject features for pretraining and develops several feature-wise
regularizations that avoid collapsed identity representations; (2) During
finetuning, it proposes a surprisingly simple self-supervised segmentation
consistency regularization to exploit intra-subject correlation. Benchmarked in
the one-shot segmentation setting, the proposed framework outperforms both
well-tuned randomly-initialized baselines and current self-supervised
techniques designed for both i.i.d. and longitudinal datasets. These
improvements are demonstrated across both longitudinal neurodegenerative adult
MRI and developing infant brain MRI and yield both higher performance and
longitudinal consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A No-Reference Deep Learning Quality Assessment Method for Super-resolution Images Based on Frequency Maps. (arXiv:2206.04289v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04289">
<div class="article-summary-box-inner">
<span><p>To support the application scenarios where high-resolution (HR) images are
urgently needed, various single image super-resolution (SISR) algorithms are
developed. However, SISR is an ill-posed inverse problem, which may bring
artifacts like texture shift, blur, etc. to the reconstructed images, thus it
is necessary to evaluate the quality of super-resolution images (SRIs). Note
that most existing image quality assessment (IQA) methods were developed for
synthetically distorted images, which may not work for SRIs since their
distortions are more diverse and complicated. Therefore, in this paper, we
propose a no-reference deep-learning image quality assessment method based on
frequency maps because the artifacts caused by SISR algorithms are quite
sensitive to frequency information. Specifically, we first obtain the
high-frequency map (HM) and low-frequency map (LM) of SRI by using Sobel
operator and piecewise smooth image approximation. Then, a two-stream network
is employed to extract the quality-aware features of both frequency maps.
Finally, the features are regressed into a single quality value using fully
connected layers. The experimental results show that our method outperforms all
compared IQA models on the selected three super-resolution quality assessment
(SRQA) databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstruct Face from Features Using GAN Generator as a Distribution Constraint. (arXiv:2206.04295v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04295">
<div class="article-summary-box-inner">
<span><p>Face recognition based on the deep convolutional neural networks (CNN) shows
superior accuracy performance attributed to the high discriminative features
extracted. Yet, the security and privacy of the extracted features from deep
learning models (deep features) have been often overlooked. This paper proposes
the reconstruction of face images from deep features without accessing the CNN
network configurations as a constrained optimization problem. Such optimization
minimizes the distance between the features extracted from the original face
image and the reconstructed face image. Instead of directly solving the
optimization problem in the image space, we innovatively reformulate the
problem by looking for a latent vector of a GAN generator, then use it to
generate the face image. The GAN generator serves as a dual role in this novel
framework, i.e., face distribution constraint of the optimization goal and a
face generator. On top of the novel optimization task, we also propose an
attack pipeline to impersonate the target user based on the generated face
image. Our results show that the generated face images can achieve a
state-of-the-art successful attack rate of 98.0\% on LFW under type-I attack @
FAR of 0.1\%. Our work sheds light on the biometric deployment to meet the
privacy-preserving and security policies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing. (arXiv:2206.04310v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04310">
<div class="article-summary-box-inner">
<span><p>Certified defenses such as randomized smoothing have shown promise towards
building reliable machine learning systems against $\ell_p$-norm bounded
attacks. However, existing methods are insufficient or unable to provably
defend against semantic transformations, especially those without closed-form
expressions (such as defocus blur and pixelate), which are more common in
practice and often unrestricted. To fill up this gap, we propose generalized
randomized smoothing (GSmooth), a unified theoretical framework for certifying
robustness against general semantic transformations via a novel dimension
augmentation strategy. Under the GSmooth framework, we present a scalable
algorithm that uses a surrogate image-to-image network to approximate the
complex transformation. The surrogate model provides a powerful tool for
studying the properties of semantic transformations and certifying robustness.
Experimental results on several datasets demonstrate the effectiveness of our
approach for robustness certification against multiple kinds of semantic
transformations and corruptions, which is not achievable by the alternative
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind Surveillance Image Quality Assessment via Deep Neural Network Combined with the Visual Saliency. (arXiv:2206.04318v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04318">
<div class="article-summary-box-inner">
<span><p>The intelligent video surveillance system (IVSS) can automatically analyze
the content of the surveillance image (SI) and reduce the burden of the manual
labour. However, the SIs may suffer quality degradations in the procedure of
acquisition, compression, and transmission, which makes IVSS hard to understand
the content of SIs. In this paper, we first conduct an example experiment (i.e.
the face detection task) to demonstrate that the quality of the SIs has a
crucial impact on the performance of the IVSS, and then propose a
saliency-based deep neural network for the blind quality assessment of the SIs,
which helps IVSS to filter the low-quality SIs and improve the detection and
recognition performance. Specifically, we first compute the saliency map of the
SI to select the most salient local region since the salient regions usually
contain rich semantic information for machine vision and thus have a great
impact on the overall quality of the SIs. Next, the convolutional neural
network (CNN) is adopted to extract quality-aware features for the whole image
and local region, which are then mapped into the global and local quality
scores through the fully connected (FC) network respectively. Finally, the
overall quality score is computed as the weighted sum of the global and local
quality scores. Experimental results on the SI quality database (SIQD) show
that the proposed method outperforms all compared state-of-the-art BIQA
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CFA: Coupled-hypersphere-based Feature Adaptation for Target-Oriented Anomaly Localization. (arXiv:2206.04325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04325">
<div class="article-summary-box-inner">
<span><p>For a long time, anomaly localization has been widely used in industries.
Previous studies focused on approximating the distribution of normal features
without adaptation to a target dataset. However, since anomaly localization
should precisely discriminate normal and abnormal features, the absence of
adaptation may make the normality of abnormal features overestimated. Thus, we
propose Coupled-hypersphere-based Feature Adaptation (CFA) which accomplishes
sophisticated anomaly localization using features adapted to the target
dataset. CFA consists of (1) a learnable patch descriptor that learns and
embeds target-oriented features and (2) scalable memory bank independent of the
size of the target dataset. And, CFA adopts transfer learning to increase the
normal feature density so that abnormal features can be clearly distinguished
by applying patch descriptor and memory bank to a pre-trained CNN. The proposed
method outperforms the previous methods quantitatively and qualitatively. For
example, it provides an AUROC score of 99.5% in anomaly detection and 98.5% in
anomaly localization of MVTec AD benchmark. In addition, this paper points out
the negative effects of biased features of pre-trained CNNs and emphasizes the
importance of the adaptation to the target dataset. The code is publicly
available at https://github.com/sungwool/CFA_for_anomaly_localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novel projection schemes for graph-based Light Field coding. (arXiv:2206.04328v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04328">
<div class="article-summary-box-inner">
<span><p>In Light Field compression, graph-based coding is powerful to exploit signal
redundancy along irregular shapes and obtains good energy compaction. However,
apart from high time complexity to process high dimensional graphs, their graph
construction method is highly sensitive to the accuracy of disparity
information between viewpoints. In real world Light Field or synthetic Light
Field generated by computer software, the use of disparity information for
super-rays projection might suffer from inaccuracy due to vignetting effect and
large disparity between views in the two types of Light Fields respectively.
This paper introduces two novel projection schemes resulting in less error in
disparity information, in which one projection scheme can also significantly
reduce time computation for both encoder and decoder. Experimental results show
projection quality of super-pixels across views can be considerably enhanced
using the proposals, along with rate-distortion performance when compared
against original projection scheme and HEVC-based or JPEG Pleno-based coding
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Modeling of Image and Label Statistics for Enhancing Model Generalizability of Medical Image Segmentation. (arXiv:2206.04336v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04336">
<div class="article-summary-box-inner">
<span><p>Although supervised deep-learning has achieved promising performance in
medical image segmentation, many methods cannot generalize well on unseen data,
limiting their real-world applicability. To address this problem, we propose a
deep learning-based Bayesian framework, which jointly models image and label
statistics, utilizing the domain-irrelevant contour of a medical image for
segmentation. Specifically, we first decompose an image into components of
contour and basis. Then, we model the expected label as a variable only related
to the contour. Finally, we develop a variational Bayesian framework to infer
the posterior distributions of these variables, including the contour, the
basis, and the label. The framework is implemented with neural networks, thus
is referred to as deep Bayesian segmentation. Results on the task of
cross-sequence cardiac MRI segmentation show that our method set a new state of
the art for model generalizability. Particularly, the BayeSeg model trained
with LGE MRI generalized well on T2 images and outperformed other models with
great margins, i.e., over 0.47 in terms of average Dice. Our code is available
at https://zmiclab.github.io/projects.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Asynchronous Events Encode Video. (arXiv:2206.04341v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04341">
<div class="article-summary-box-inner">
<span><p>As event-based sensing gains in popularity, theoretical understanding is
needed to harness this technology's potential. Instead of recording video by
capturing frames, event-based cameras have sensors that emit events when their
inputs change, thus encoding information in the timing of events. This creates
new challenges in establishing reconstruction guarantees and algorithms, but
also provides advantages over frame-based video. We use time encoding machines
to model event-based sensors: TEMs also encode their inputs by emitting events
characterized by their timing and reconstruction from time encodings is well
understood. We consider the case of time encoding bandlimited video and
demonstrate a dependence between spatial sensor density and overall spatial and
temporal resolution. Such a dependence does not occur in frame-based video,
where temporal resolution depends solely on the frame rate of the video and
spatial resolution depends solely on the pixel grid. However, this dependence
arises naturally in event-based video and allows oversampling in space to
provide better time resolution. As such, event-based vision encourages using
more sensors that emit fewer events over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep radiomic signature with immune cell markers predicts the survival of glioma patients. (arXiv:2206.04349v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04349">
<div class="article-summary-box-inner">
<span><p>Imaging biomarkers offer a non-invasive way to predict the response of
immunotherapy prior to treatment. In this work, we propose a novel type of deep
radiomic features (DRFs) computed from a convolutional neural network (CNN),
which capture tumor characteristics related to immune cell markers and overall
survival. Our study uses four MRI sequences (T1-weighted, T1-weighted
post-contrast, T2-weighted and FLAIR) with corresponding immune cell markers of
151 patients with brain tumor. The proposed method extracts a total of 180 DRFs
by aggregating the activation maps of a pre-trained 3D-CNN within labeled tumor
regions of MRI scans. These features offer a compact, yet powerful
representation of regional texture encoding tissue heterogeneity. A
comprehensive set of experiments is performed to assess the relationship
between the proposed DRFs and immune cell markers, and measure their
association with overall survival. Results show a high correlation between DRFs
and various markers, as well as significant differences between patients
grouped based on these markers. Moreover, combining DRFs, clinical features and
immune cell markers as input to a random forest classifier helps discriminate
between short and long survival outcomes, with AUC of 72\% and
p=2.36$\times$10$^{-5}$. These results demonstrate the usefulness of proposed
DRFs as non-invasive biomarker for predicting treatment response in patients
with brain tumors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Neural Network for Blind Visual Quality Assessment of 4K Content. (arXiv:2206.04363v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04363">
<div class="article-summary-box-inner">
<span><p>The 4K content can deliver a more immersive visual experience to consumers
due to the huge improvement of spatial resolution. However, existing blind
image quality assessment (BIQA) methods are not suitable for the original and
upscaled 4K contents due to the expanded resolution and specific distortions.
In this paper, we propose a deep learning-based BIQA model for 4K content,
which on one hand can recognize true and pseudo 4K content and on the other
hand can evaluate their perceptual visual quality. Considering the
characteristic that high spatial resolution can represent more abundant
high-frequency information, we first propose a Grey-level Co-occurrence Matrix
(GLCM) based texture complexity measure to select three representative image
patches from a 4K image, which can reduce the computational complexity and is
proven to be very effective for the overall quality prediction through
experiments. Then we extract different kinds of visual features from the
intermediate layers of the convolutional neural network (CNN) and integrate
them into the quality-aware feature representation. Finally, two multilayer
perception (MLP) networks are utilized to map the quality-aware features into
the class probability and the quality score for each patch respectively. The
overall quality index is obtained through the average pooling of patch results.
The proposed model is trained through the multi-task learning manner and we
introduce an uncertainty principle to balance the losses of the classification
and regression tasks. The experimental results show that the proposed model
outperforms all compared BIQA metrics on four 4K content quality assessment
databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CARLA-GeAR: a Dataset Generator for a Systematic Evaluation of Adversarial Robustness of Vision Models. (arXiv:2206.04365v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04365">
<div class="article-summary-box-inner">
<span><p>Adversarial examples represent a serious threat for deep neural networks in
several application domains and a huge amount of work has been produced to
investigate them and mitigate their effects. Nevertheless, no much work has
been devoted to the generation of datasets specifically designed to evaluate
the adversarial robustness of neural models. This paper presents CARLA-GeAR, a
tool for the automatic generation of photo-realistic synthetic datasets that
can be used for a systematic evaluation of the adversarial robustness of neural
models against physical adversarial patches, as well as for comparing the
performance of different adversarial defense/detection methods. The tool is
built on the CARLA simulator, using its Python API, and allows the generation
of datasets for several vision tasks in the context of autonomous driving. The
adversarial patches included in the generated datasets are attached to
billboards or the back of a truck and are crafted by using state-of-the-art
white-box attack strategies to maximize the prediction error of the model under
test. Finally, the paper presents an experimental study to evaluate the
performance of some defense methods against such attacks, showing how the
datasets generated with CARLA-GeAR might be used in future work as a benchmark
for adversarial defense in the real world. All the code and datasets used in
this paper are available at <a href="http://carlagear.retis.santannapisa.it.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncovering bias in the PlantVillage dataset. (arXiv:2206.04374v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04374">
<div class="article-summary-box-inner">
<span><p>We report our investigation on the use of the popular PlantVillage dataset
for training deep learning based plant disease detection models. We trained a
machine learning model using only 8 pixels from the PlantVillage image
backgrounds. The model achieved 49.0% accuracy on the held-out test set, well
above the random guessing accuracy of 2.6%. This result indicates that the
PlantVillage dataset contains noise correlated with the labels and deep
learning models can easily exploit this bias to make predictions. Possible
approaches to alleviate this problem are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STIP: A SpatioTemporal Information-Preserving and Perception-Augmented Model for High-Resolution Video Prediction. (arXiv:2206.04381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04381">
<div class="article-summary-box-inner">
<span><p>Although significant achievements have been achieved by recurrent neural
network (RNN) based video prediction methods, their performance in datasets
with high resolutions is still far from satisfactory because of the information
loss problem and the perception-insensitive mean square error (MSE) based loss
functions. In this paper, we propose a Spatiotemporal Information-Preserving
and Perception-Augmented Model (STIP) to solve the above two problems. To solve
the information loss problem, the proposed model aims to preserve the
spatiotemporal information for videos during the feature extraction and the
state transitions, respectively. Firstly, a Multi-Grained Spatiotemporal
Auto-Encoder (MGST-AE) is designed based on the X-Net structure. The proposed
MGST-AE can help the decoders recall multi-grained information from the
encoders in both the temporal and spatial domains. In this way, more
spatiotemporal information can be preserved during the feature extraction for
high-resolution videos. Secondly, a Spatiotemporal Gated Recurrent Unit (STGRU)
is designed based on the standard Gated Recurrent Unit (GRU) structure, which
can efficiently preserve spatiotemporal information during the state
transitions. The proposed STGRU can achieve more satisfactory performance with
a much lower computation load compared with the popular Long Short-Term (LSTM)
based predictive memories. Furthermore, to improve the traditional MSE loss
functions, a Learned Perceptual Loss (LP-loss) is further designed based on the
Generative Adversarial Networks (GANs), which can help obtain a satisfactory
trade-off between the objective quality and the perceptual quality.
Experimental results show that the proposed STIP can predict videos with more
satisfactory visual quality compared with a variety of state-of-the-art
methods. Source code has been available at
\url{https://github.com/ZhengChang467/STIPHR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. (arXiv:2206.04382v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04382">
<div class="article-summary-box-inner">
<span><p>We propose CLIP-Actor, a text-driven motion recommendation and neural mesh
stylization system for human mesh animation. CLIP-Actor animates a 3D human
mesh to conform to a text prompt by recommending a motion sequence and learning
mesh style attributes. Prior work fails to generate plausible results when the
artist-designed mesh content does not conform to the text from the beginning.
Instead, we build a text-driven human motion recommendation system by
leveraging a large-scale human motion dataset with language labels. Given a
natural language prompt, CLIP-Actor first suggests a human motion that conforms
to the prompt in a coarse-to-fine manner. Then, we propose a
synthesize-through-optimization method that detailizes and texturizes a
recommended mesh sequence in a disentangled way from the pose of each frame. It
allows the style attribute to conform to the prompt in a temporally-consistent
and pose-agnostic manner. The decoupled neural optimization also enables
spatio-temporal view augmentation from multi-frame human motion. We further
propose the mask-weighted embedding attention, which stabilizes the
optimization process by rejecting distracting renders containing scarce
foreground pixels. We demonstrate that CLIP-Actor produces plausible and
human-recognizable style 3D human mesh in motion with detailed geometry and
texture from a natural language prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depression Recognition using Remote Photoplethysmography from Facial Videos. (arXiv:2206.04399v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04399">
<div class="article-summary-box-inner">
<span><p>Depression is a mental illness that may be harmful to an individual's health.
The detection of mental health disorders in the early stages and a precise
diagnosis are critical to avoid social, physiological, or psychological side
effects. This work analyzes physiological signals to observe if different
depressive states have a noticeable impact on the blood volume pulse (BVP) and
the heart rate variability (HRV) response. Although typically, HRV features are
calculated from biosignals obtained with contact-based sensors such as
wearables, we propose instead a novel scheme that directly extracts them from
facial videos, just based on visual information, removing the need for any
contact-based device. Our solution is based on a pipeline that is able to
extract complete remote photoplethysmography signals (rPPG) in a fully
unsupervised manner. We use these rPPG signals to calculate over 60
statistical, geometrical, and physiological features that are further used to
train several machine learning regressors to recognize different levels of
depression. Experiments on two benchmark datasets indicate that this approach
offers comparable results to other audiovisual modalities based on voice or
facial expression, potentially complementing them. In addition, the results
achieved for the proposed method show promising and solid performance that
outperforms hand-engineered methods and is comparable to deep learning-based
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Local Shortest Path and Global Enhancement for Visible-Thermal Person Re-Identification. (arXiv:2206.04401v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04401">
<div class="article-summary-box-inner">
<span><p>In addition to considering the recognition difficulty caused by human posture
and occlusion, it is also necessary to solve the modal differences caused by
different imaging systems in the Visible-Thermal cross-modal person
re-identification (VT-ReID) task. In this paper,we propose the Cross-modal
Local Shortest Path and Global Enhancement (CM-LSP-GE) modules,a two-stream
network based on joint learning of local and global features. The core idea of
our paper is to use local feature alignment to solve occlusion problem, and to
solve modal difference by strengthening global feature. Firstly,
Attention-based two-stream ResNet network is designed to extract dual-modality
features and map to a unified feature space. Then, to solve the cross-modal
person pose and occlusion problems, the image are cut horizontally into several
equal parts to obtain local features and the shortest path in local features
between two graphs is used to achieve the fine-grained local feature alignment.
Thirdly, a batch normalization enhancement module applies global features to
enhance strategy, resulting in difference enhancement between different
classes. The multi granularity loss fusion strategy further improves the
performance of the algorithm. Finally, joint learning mechanism of local and
global features is used to improve cross-modal person re-identification
accuracy. The experimental results on two typical datasets show that our model
is obviously superior to the most state-of-the-art methods. Especially, on
SYSU-MM01 datasets, our model can achieve a gain of 2.89%and 7.96% in all
search term of Rank-1 and mAP. The source code will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VITA: Video Instance Segmentation via Object Token Association. (arXiv:2206.04403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04403">
<div class="article-summary-box-inner">
<span><p>We introduce a novel paradigm for offline Video Instance Segmentation (VIS),
based on the hypothesis that explicit object-oriented information can be a
strong clue for understanding the context of the entire sequence. To this end,
we propose VITA, a simple structure built on top of an off-the-shelf
Transformer-based image instance segmentation model. Specifically, we use an
image object detector as a means of distilling object-specific contexts into
object tokens. VITA accomplishes video-level understanding by associating
frame-level object tokens without using spatio-temporal backbone features. By
effectively building relationships between objects using the condensed
information, VITA achieves the state-of-the-art on VIS benchmarks with a
ResNet-50 backbone: 49.8 AP, 45.7 AP on YouTube-VIS 2019 &amp; 2021 and 19.6 AP on
OVIS. Moreover, thanks to its object token-based structure that is disjoint
from the backbone features, VITA shows several practical advantages that
previous offline VIS methods have not explored - handling long and
high-resolution videos with a common GPU and freezing a frame-level detector
trained on image domain. Code will be made available at
https://github.com/sukjunhwang/VITA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning of the Total Variation Flow. (arXiv:2206.04406v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04406">
<div class="article-summary-box-inner">
<span><p>The total variation (TV) flow generates a scale-space representation of an
image based on the TV functional. This gradient flow observes desirable
features for images such as sharp edges and enables spectral, scale, and
texture analysis. The standard numerical approach for TV flow requires solving
multiple non-smooth optimisation problems. Even with state-of-the-art convex
optimisation techniques, this is often prohibitively expensive and strongly
motivates the use of alternative, faster approaches. Inspired by and extending
the framework of physics-informed neural networks (PINNs), we propose the
TVflowNET, a neural network approach to compute the solution of the TV flow
given an initial image and a time instance. We significantly speed up the
computation time by more than one order of magnitude and show that the
TVflowNET approximates the TV flow solution with high fidelity. This is a
preliminary report, more details are to follow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Instance Learning for Digital Pathology: A Review on the State-of-the-Art, Limitations & Future Potential. (arXiv:2206.04425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04425">
<div class="article-summary-box-inner">
<span><p>Digital whole slides images contain an enormous amount of information
providing a strong motivation for the development of automated image analysis
tools. Particularly deep neural networks show high potential with respect to
various tasks in the field of digital pathology. However, a limitation is given
by the fact that typical deep learning algorithms require (manual) annotations
in addition to the large amounts of image data, to enable effective training.
Multiple instance learning exhibits a powerful tool for learning deep neural
networks in a scenario without fully annotated data. These methods are
particularly effective in this domain, due to the fact that labels for a
complete whole slide image are often captured routinely, whereas labels for
patches, regions or pixels are not. This potential already resulted in a
considerable number of publications, with the majority published in the last
three years. Besides the availability of data and a high motivation from the
medical perspective, the availability of powerful graphics processing units
exhibits an accelerator in this field. In this paper, we provide an overview of
widely and effectively used concepts of used deep multiple instance learning
approaches, recent advances and also critically discuss remaining challenges
and future potential.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-boosting of WNNM Image Denoising method by Directional Wavelet Packets. (arXiv:2206.04431v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04431">
<div class="article-summary-box-inner">
<span><p>The paper presents an image denoising scheme by combining a method that is
based on directional quasi-analytic wavelet packets (qWPs) with the
state-of-the-art Weighted Nuclear Norm Minimization (WNNM) denoising algorithm.
The qWP-based denoising method (qWPdn) consists of multiscale qWP transform of
the degraded image, application of adaptive localized soft thresholding to the
transform coefficients using the Bivariate Shrinkage methodology, and
restoration of the image from the thresholded coefficients from several
decomposition levels. The combined method consists of several iterations of
qWPdn and WNNM algorithms in a way that at each iteration the output from one
algorithm boosts the input to the other. The proposed methodology couples the
qWPdn capabilities to capture edges and fine texture patterns even in the
severely corrupted images with utilizing the non-local self-similarity in real
images that is inherent in the WNNM algorithm.
</p>
<p>Multiple experiments, which compared the proposed methodology with six
advanced denoising algorithms, including WNNM, confirmed that the combined
cross-boosting algorithm outperforms most of them in terms of both quantitative
measure and visual perception quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation Enhanced Lameness Detection in Dairy Cows from RGB and Depth Video. (arXiv:2206.04449v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04449">
<div class="article-summary-box-inner">
<span><p>Cow lameness is a severe condition that affects the life cycle and life
quality of dairy cows and results in considerable economic losses. Early
lameness detection helps farmers address illnesses early and avoid negative
effects caused by the degeneration of cows' condition. We collected a dataset
of short clips of cows passing through a hallway exiting a milking station and
annotated the degree of lameness of the cows. This paper explores the resulting
dataset and provides a detailed description of the data collection process.
Additionally, we proposed a lameness detection method that leverages
pre-trained neural networks to extract discriminative features from videos and
assign a binary score to each cow indicating its condition: "healthy" or
"lame." We improve this approach by forcing the model to focus on the structure
of the cow, which we achieve by substituting the RGB videos with binary
segmentation masks predicted with a trained segmentation model. This work aims
to encourage research and provide insights into the applicability of computer
vision models for cow lameness detection on farms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Draft-and-Revise: Effective Image Generation with Contextual RQ-Transformer. (arXiv:2206.04452v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04452">
<div class="article-summary-box-inner">
<span><p>Although autoregressive models have achieved promising results on image
generation, their unidirectional generation process prevents the resultant
images from fully reflecting global contexts. To address the issue, we propose
an effective image generation framework of Draft-and-Revise with Contextual
RQ-transformer to consider global contexts during the generation process. As a
generalized VQ-VAE, RQ-VAE first represents a high-resolution image as a
sequence of discrete code stacks. After code stacks in the sequence are
randomly masked, Contextual RQ-Transformer is trained to infill the masked code
stacks based on the unmasked contexts of the image. Then, Contextual
RQ-Transformer uses our two-phase decoding, Draft-and-Revise, and generates an
image, while exploiting the global contexts of the image during the generation
process. Specifically. in the draft phase, our model first focuses on
generating diverse images despite rather low quality. Then, in the revise
phase, the model iteratively improves the quality of images, while preserving
the global contexts of generated images. In experiments, our method achieves
state-of-the-art results on conditional image generation. We also validate that
the Draft-and-Revise decoding can achieve high performance by effectively
controlling the quality-diversity trade-off in image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Missing Link: Finding label relations across datasets. (arXiv:2206.04453v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04453">
<div class="article-summary-box-inner">
<span><p>Computer Vision is driven by the many datasets which can be used for training
or evaluating novel methods. However, each dataset has different set of class
labels, visual definition of classes, images following a specific distribution,
annotation protocols, etc. In this paper we explore the automatic discovery of
visual-semantic relations between labels across datasets. We want to understand
how the instances of a certain class in a dataset relate to the instances of
another class in another dataset. Are they in an identity, parent/child,
overlap relation? Or is there no link between them at all? To find relations
between labels across datasets, we propose methods based on language, on
vision, and on a combination of both. Our methods can effectively discover
label relations across datasets and the type of the relations. We use these
results for a deeper inspection on why instances relate, find missing aspects
of a class, and use our relations to create finer-grained annotations. We
conclude that label relations cannot be established by looking at the names of
classes alone, as they depend strongly on how each of the datasets was
constructed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDQ: Stochastic Differentiable Quantization with Mixed Precision. (arXiv:2206.04459v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04459">
<div class="article-summary-box-inner">
<span><p>In order to deploy deep models in a computationally efficient manner, model
quantization approaches have been frequently used. In addition, as new hardware
that supports mixed bitwidth arithmetic operations, recent research on mixed
precision quantization (MPQ) begins to fully leverage the capacity of
representation by searching optimized bitwidths for different layers and
modules in a network. However, previous studies mainly search the MPQ strategy
in a costly scheme using reinforcement learning, neural architecture search,
etc., or simply utilize partial prior knowledge for bitwidth assignment, which
might be biased and sub-optimal. In this work, we present a novel Stochastic
Differentiable Quantization (SDQ) method that can automatically learn the MPQ
strategy in a more flexible and globally-optimized space with smoother gradient
approximation. Particularly, Differentiable Bitwidth Parameters (DBPs) are
employed as the probability factors in stochastic quantization between adjacent
bitwidth choices. After the optimal MPQ strategy is acquired, we further train
our network with entropy-aware bin regularization and knowledge distillation.
We extensively evaluate our method for several networks on different hardware
(GPUs and FPGA) and datasets. SDQ outperforms all state-of-the-art mixed or
single precision quantization with a lower bitwidth and is even better than the
full-precision counterparts across various ResNet and MobileNet families,
demonstrating the effectiveness and superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BSM loss: A superior way in modeling aleatory uncertainty of fine_grained classification. (arXiv:2206.04479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04479">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence(AI)-assisted method had received much attention in
the risk field such as disease diagnosis. Different from the classification of
disease types, it is a fine-grained task to classify the medical images as
benign or malignant. However, most research only focuses on improving the
diagnostic accuracy and ignores the evaluation of model reliability, which
limits its clinical application. For clinical practice, calibration presents
major challenges in the low-data regime extremely for over-parametrized models
and inherent noises. In particular, we discovered that modeling data-dependent
uncertainty is more conducive to confidence calibrations. Compared with
test-time augmentation(TTA), we proposed a modified Bootstrapping loss(BS loss)
function with Mixup data augmentation strategy that can better calibrate
predictive uncertainty and capture data distribution transformation without
additional inference time. Our experiments indicated that BS loss with
Mixup(BSM) model can halve the Expected Calibration Error(ECE) compared to
standard data augmentation, deep ensemble and MC dropout. The correlation
between uncertainty and similarity of in-domain data is up to -0.4428 under the
BSM model. Additionally, the BSM model is able to perceive the semantic
distance of out-of-domain data, demonstrating high potential in real-world
clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">cycle text2face: cycle text-to-face gan via transformers. (arXiv:2206.04503v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04503">
<div class="article-summary-box-inner">
<span><p>Text-to-face is a subset of text-to-image that require more complex
architecture due to their more detailed production. In this paper, we present
an encoder-decoder model called Cycle Text2Face. Cycle Text2Face is a new
initiative in the encoder part, it uses a sentence transformer and GAN to
generate the image described by the text. The Cycle is completed by reproducing
the text of the face in the decoder part of the model. Evaluating the model
using the CelebA dataset, leads to better results than previous GAN-based
models. In measuring the quality of the generate face, in addition to
satisfying the human audience, we obtain an FID score of 3.458. This model,
with high-speed processing, provides quality face images in the short time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Human Pose Estimation via 3D Event Point Cloud. (arXiv:2206.04511v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04511">
<div class="article-summary-box-inner">
<span><p>Human Pose Estimation (HPE) based on RGB images has experienced a rapid
development benefiting from deep learning. However, event-based HPE has not
been fully studied, which remains great potential for applications in extreme
scenes and efficiency-critical conditions. In this paper, we are the first to
estimate 2D human pose directly from 3D event point cloud. We propose a novel
representation of events, the rasterized event point cloud, aggregating events
on the same position of a small time slice. It maintains the 3D features from
multiple statistical cues and significantly reduces memory consumption and
computation complexity, proved to be efficient in our work. We then leverage
the rasterized event point cloud as input to three different backbones,
PointNet, DGCNN, and Point Transformer, with two linear layer decoders to
predict the location of human keypoints. We find that based on our method,
PointNet achieves promising results with much faster speed, whereas Point
Transfomer reaches much higher accuracy, even close to previous
event-frame-based methods. A comprehensive set of results demonstrates that our
proposed method is consistently effective for these 3D backbone models in
event-driven human pose estimation. Our method based on PointNet with 2048
points input achieves 82.46mm in MPJPE3D on the DHP19 dataset, while only has a
latency of 12.29ms on an NVIDIA Jetson Xavier NX edge computing platform, which
is ideally suitable for real-time detection with event cameras. Code will be
made publicly at https://github.com/MasterHow/EventPointPose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAR Despeckling using a Denoising Diffusion Probabilistic Model. (arXiv:2206.04514v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04514">
<div class="article-summary-box-inner">
<span><p>Speckle is a multiplicative noise which affects all coherent imaging
modalities including Synthetic Aperture Radar (SAR) images. The presence of
speckle degrades the image quality and adversely affects the performance of SAR
image understanding applications such as automatic target recognition and
change detection. Thus, SAR despeckling is an important problem in remote
sensing. In this paper, we introduce SAR-DDPM, a denoising diffusion
probabilistic model for SAR despeckling. The proposed method comprises of a
Markov chain that transforms clean images to white Gaussian noise by repeatedly
adding random noise. The despeckled image is recovered by a reverse process
which iteratively predicts the added noise using a noise predictor which is
conditioned on the speckled image. In addition, we propose a new inference
strategy based on cycle spinning to improve the despeckling performance. Our
experiments on both synthetic and real SAR images demonstrate that the proposed
method achieves significant improvements in both quantitative and qualitative
results over the state-of-the-art despeckling methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos. (arXiv:2206.04523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04523">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a neural end-to-end system for voice preserving,
lip-synchronous translation of videos. The system is designed to combine
multiple component models and produces a video of the original speaker speaking
in the target language that is lip-synchronous with the target speech, yet
maintains emphases in speech, voice characteristics, face video of the original
speaker. The pipeline starts with automatic speech recognition including
emphasis detection, followed by a translation model. The translated text is
then synthesized by a Text-to-Speech model that recreates the original emphases
mapped from the original sentence. The resulting synthetic voice is then mapped
back to the original speakers' voice using a voice conversion model. Finally,
to synchronize the lips of the speaker with the translated audio, a conditional
generative adversarial network-based model generates frames of adapted lip
movements with respect to the input face image as well as the output of the
voice conversion model. In the end, the system combines the generated video
with the converted audio to produce the final output. The result is a video of
a speaker speaking in another language without actually knowing it. To evaluate
our design, we present a user study of the complete system as well as separate
evaluations of the single components. Since there is no available dataset to
evaluate our whole system, we collect a test set and evaluate our system on
this test set. The results indicate that our system is able to generate
convincing videos of the original speaker speaking the target language while
preserving the original speaker's characteristics. The collected dataset will
be shared.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DORA: Exploring outlier representations in Deep Neural Networks. (arXiv:2206.04530v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04530">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) draw their power from the representations they
learn. In recent years, however, researchers have found that DNNs, while being
incredibly effective in learning complex abstractions, also tend to be infected
with artifacts, such as biases, Clever Hanses (CH), or Backdoors, due to
spurious correlations inherent in the training data. So far, existing methods
for uncovering such artifactual and malicious behavior in trained models focus
on finding artifacts in the input data, which requires both availabilities of a
data set and human intervention. In this paper, we introduce DORA
(Data-agnOstic Representation Analysis): the first automatic data-agnostic
method for the detection of potentially infected representations in Deep Neural
Networks. We further show that contaminated representations found by DORA can
be used to detect infected samples in any given dataset. We qualitatively and
quantitatively evaluate the performance of our proposed method in both,
controlled toy scenarios, and in real-world settings, where we demonstrate the
benefit of DORA in safety-critical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04531">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks are being increasingly used in critical
systems, where ensuring their robustness and alignment is crucial. In this
context, the field of explainable artificial intelligence has proposed the
generation of high-level explanations through concept extraction. These methods
detect whether a concept is present in an image, but are incapable of locating
where. What is more, a fair comparison of approaches is difficult, as proper
validation procedures are missing. To fill these gaps, we propose a novel
method for automatic concept extraction and localization based on
representations obtained through the pixel-wise aggregations of activation maps
of CNNs. Further, we introduce a process for the validation of
concept-extraction techniques based on synthetic datasets with pixel-wise
annotations of their main components, reducing human intervention. Through
extensive experimentation on both synthetic and real-world datasets, our method
achieves better performance in comparison to state-of-the-art alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of COVID-19 in Chest X-ray Images Using Fusion of Deep Features and LightGBM. (arXiv:2206.04548v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04548">
<div class="article-summary-box-inner">
<span><p>The COVID-19 disease was first discovered in Wuhan, China, and spread quickly
worldwide. After the COVID-19 pandemic, many researchers have begun to identify
a way to diagnose the COVID-19 using chest X-ray images. The early diagnosis of
this disease can significantly impact the treatment process. In this article,
we propose a new technique that is faster and more accurate than the other
methods reported in the literature. The proposed method uses a combination of
DenseNet169 and MobileNet Deep Neural Networks to extract the features of the
patient's X-ray images. Using the univariate feature selection algorithm, we
refined the features for the most important ones. Then we applied the selected
features as input to the LightGBM (Light Gradient Boosting Machine) algorithm
for classification. To assess the effectiveness of the proposed method, the
ChestX-ray8 dataset, which includes 1125 X-ray images of the patient's chest,
was used. The proposed method achieved 98.54% and 91.11% accuracies in the
two-class (COVID-19, Healthy) and multi-class (COVID-19, Healthy, Pneumonia)
classification problems, respectively. It is worth mentioning that we have used
Gradient-weighted Class Activation Mapping (Grad-CAM) for further analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseFormer: Attention-based Depth Completion Network. (arXiv:2206.04557v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04557">
<div class="article-summary-box-inner">
<span><p>Most pipelines for Augmented and Virtual Reality estimate the ego-motion of
the camera by creating a map of sparse 3D landmarks. In this paper, we tackle
the problem of depth completion, that is, densifying this sparse 3D map using
RGB images as guidance. This remains a challenging problem due to the low
density, non-uniform and outlier-prone 3D landmarks produced by SfM and SLAM
pipelines. We introduce a transformer block, SparseFormer, that fuses 3D
landmarks with deep visual features to produce dense depth. The SparseFormer
has a global receptive field, making the module especially effective for depth
completion with low-density and non-uniform landmarks. To address the issue of
depth outliers among the 3D landmarks, we introduce a trainable refinement
module that filters outliers through attention between the sparse landmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BFS-Net: Weakly Supervised Cell Instance Segmentation from Bright-Field Microscopy Z-Stacks. (arXiv:2206.04558v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04558">
<div class="article-summary-box-inner">
<span><p>Despite its broad availability, volumetric information acquisition from
Bright-Field Microscopy (BFM) is inherently difficult due to the projective
nature of the acquisition process. We investigate the prediction of 3D cell
instances from a set of BFM Z-Stack images. We propose a novel two-stage weakly
supervised method for volumetric instance segmentation of cells which only
requires approximate cell centroids annotation. Created pseudo-labels are
thereby refined with a novel refinement loss with Z-stack guidance. The
evaluations show that our approach can generalize not only to BFM Z-Stack data,
but to other 3D cell imaging modalities. A comparison of our pipeline against
fully supervised methods indicates that the significant gain in reduced data
collection and labelling results in minor performance difference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer based Urdu Handwritten Text Optical Character Reader. (arXiv:2206.04575v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04575">
<div class="article-summary-box-inner">
<span><p>Extracting Handwritten text is one of the most important components of
digitizing information and making it available for large scale setting.
Handwriting Optical Character Reader (OCR) is a research problem in computer
vision and natural language processing computing, and a lot of work has been
done for English, but unfortunately, very little work has been done for low
resourced languages such as Urdu. Urdu language script is very difficult
because of its cursive nature and change of shape of characters based on it's
relative position, therefore, a need arises to propose a model which can
understand complex features and generalize it for every kind of handwriting
style. In this work, we propose a transformer based Urdu Handwritten text
extraction model. As transformers have been very successful in Natural Language
Understanding task, we explore them further to understand complex Urdu
Handwriting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer. (arXiv:2206.04584v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04584">
<div class="article-summary-box-inner">
<span><p>Learning Bird's Eye View (BEV) representation from surrounding-view cameras
is of great importance for autonomous driving. In this work, we propose a
Geometry-guided Kernel Transformer (GKT), a novel 2D-to-BEV representation
learning mechanism. GKT leverages the geometric priors to guide the transformer
to focus on discriminative regions and unfolds kernel features to generate BEV
representation. For fast inference, we further introduce a look-up table (LUT)
indexing method to get rid of the camera's calibrated parameters at runtime.
GKT can run at $72.3$ FPS on 3090 GPU / $45.6$ FPS on 2080ti GPU and is robust
to the camera deviation and the predefined BEV height. And GKT achieves the
state-of-the-art real-time segmentation results, i.e., 38.0 mIoU
(100m$\times$100m perception range at a 0.5m resolution) on the nuScenes val
set. Given the efficiency, effectiveness, and robustness, GKT has great
practical values in autopilot scenarios, especially for real-time running
systems. Code and models will be available at
\url{https://github.com/hustvl/GKT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GASP: Gated Attention For Saliency Prediction. (arXiv:2206.04590v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04590">
<div class="article-summary-box-inner">
<span><p>Saliency prediction refers to the computational task of modeling overt
attention. Social cues greatly influence our attention, consequently altering
our eye movements and behavior. To emphasize the efficacy of such features, we
present a neural model for integrating social cues and weighting their
influences. Our model consists of two stages. During the first stage, we detect
two social cues by following gaze, estimating gaze direction, and recognizing
affect. These features are then transformed into spatiotemporal maps through
image processing operations. The transformed representations are propagated to
the second stage (GASP) where we explore various techniques of late fusion for
integrating social cues and introduce two sub-networks for directing attention
to relevant stimuli. Our experiments indicate that fusion approaches achieve
better results for static integration methods, whereas non-fusion approaches
for which the influence of each modality is unknown, result in better outcomes
when coupled with recurrent models for dynamic saliency prediction. We show
that gaze direction and affective representations contribute a prediction to
ground-truth correspondence improvement of at least 5% compared to dynamic
saliency models without social cues. Furthermore, affective representations
improve GASP, supporting the necessity of considering affect-biased attention
in predicting saliency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AttX: Attentive Cross-Connections for Fusion of Wearable Signals in Emotion Recognition. (arXiv:2206.04625v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04625">
<div class="article-summary-box-inner">
<span><p>We propose cross-modal attentive connections, a new dynamic and effective
technique for multimodal representation learning from wearable data. Our
solution can be integrated into any stage of the pipeline, i.e., after any
convolutional layer or block, to create intermediate connections between
individual streams responsible for processing each modality. Additionally, our
method benefits from two properties. First, it can share information
uni-directionally (from one modality to the other) or bi-directionally. Second,
it can be integrated into multiple stages at the same time to further allow
network gradients to be exchanged in several touch-points. We perform extensive
experiments on three public multimodal wearable datasets, WESAD, SWELL-KW, and
CASE, and demonstrate that our method can effectively regulate and share
information between different modalities to learn better representations. Our
experiments further demonstrate that once integrated into simple CNN-based
multimodal solutions (2, 3, or 4 modalities), our method can result in superior
or competitive performance to state-of-the-art and outperform a variety of
baseline uni-modal and classical multimodal methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Entropy Regularization for Vision Transformers. (arXiv:2206.04636v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04636">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that the attention maps of Vision Transformers (VTs),
when trained with self-supervision, can contain a semantic segmentation
structure which does not spontaneously emerge when training is supervised. In
this paper, we explicitly encourage the emergence of this spatial clustering as
a form of training regularization, this way including a self-supervised pretext
task into the standard supervised learning. In more detail, we propose a VT
regularization method based on a spatial formulation of the information
entropy. By minimizing the proposed spatial entropy, we explicitly ask the VT
to produce spatially ordered attention maps, this way including an object-based
prior during training. Using extensive experiments, we show that the proposed
regularization approach is beneficial with different training scenarios,
datasets, downstream tasks and VT architectures. The code will be available
upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution. (arXiv:2206.04647v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04647">
<div class="article-summary-box-inner">
<span><p>Videos typically record the streaming and continuous visual data as discrete
consecutive frames. Since the storage cost is expensive for videos of high
fidelity, most of them are stored in a relatively low resolution and frame
rate. Recent works of Space-Time Video Super-Resolution (STVSR) are developed
to incorporate temporal interpolation and spatial super-resolution in a unified
framework. However, most of them only support a fixed up-sampling scale, which
limits their flexibility and applications. In this work, instead of following
the discrete representations, we propose Video Implicit Neural Representation
(VideoINR), and we show its applications for STVSR. The learned implicit neural
representation can be decoded to videos of arbitrary spatial resolution and
frame rate. We show that VideoINR achieves competitive performances with
state-of-the-art STVSR methods on common up-sampling scales and significantly
outperforms prior works on continuous and out-of-training-distribution scales.
Our project page is at <a href="http://zeyuan-chen.com/VideoINR/">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Layer-wise Image Vectorization. (arXiv:2206.04655v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04655">
<div class="article-summary-box-inner">
<span><p>Image rasterization is a mature technique in computer graphics, while image
vectorization, the reverse path of rasterization, remains a major challenge.
Recent advanced deep learning-based models achieve vectorization and semantic
interpolation of vector graphs and demonstrate a better topology of generating
new figures. However, deep models cannot be easily generalized to out-of-domain
testing data. The generated SVGs also contain complex and redundant shapes that
are not quite convenient for further editing. Specifically, the crucial
layer-wise topology and fundamental semantics in images are still not well
understood and thus not fully explored. In this work, we propose Layer-wise
Image Vectorization, namely LIVE, to convert raster images to SVGs and
simultaneously maintain its image topology. LIVE can generate compact SVG forms
with layer-wise structures that are semantically consistent with human
perspective. We progressively add new bezier paths and optimize these paths
with the layer-wise framework, newly designed loss functions, and
component-wise path initialization technique. Our experiments demonstrate that
LIVE presents more plausible vectorized forms than prior works and can be
generalized to new images. With the help of this newly learned topology, LIVE
initiates human editable SVGs for both designers and other downstream
applications. Codes are made available at
https://github.com/Picsart-AI-Research/LIVE-Layerwise-Image-Vectorization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Cues Lead to a Strong Multi-Object Tracker. (arXiv:2206.04656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04656">
<div class="article-summary-box-inner">
<span><p>For a long time, the most common paradigm in Multi-Object Tracking was
tracking-by-detection (TbD), where objects are first detected and then
associated over video frames. For association, most models resource to motion
and appearance cues. While still relying on these cues, recent approaches based
on, e.g., attention have shown an ever-increasing need for training data and
overall complex frameworks. We claim that 1) strong cues can be obtained from
little amounts of training data if some key design choices are applied, 2)
given these strong cues, standard Hungarian matching-based association is
enough to obtain impressive results. Our main insight is to identify key
components that allow a standard reidentification network to excel at
appearance-based tracking. We extensively analyze its failure cases and show
that a combination of our appearance features with a simple motion model leads
to strong tracking results. Our model achieves state-of-the-art performance on
MOT17 and MOT20 datasets outperforming previous state-of-the-art trackers by up
to 5.4pp in IDF1 and 4.4pp in HOTA. We will release the code and models after
the paper's acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiSparse: Disentangled Sparsification for Multitask Model Compression. (arXiv:2206.04662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04662">
<div class="article-summary-box-inner">
<span><p>Despite the popularity of Model Compression and Multitask Learning, how to
effectively compress a multitask model has been less thoroughly analyzed due to
the challenging entanglement of tasks in the parameter space. In this paper, we
propose DiSparse, a simple, effective, and first-of-its-kind multitask pruning
and sparse training scheme. We consider each task independently by
disentangling the importance measurement and take the unanimous decisions among
all tasks when performing parameter pruning and selection. Our experimental
results demonstrate superior performance on various configurations and settings
compared to popular sparse training and pruning methods. Besides the
effectiveness in compression, DiSparse also provides a powerful tool to the
multitask learning community. Surprisingly, we even observed better performance
than some dedicated multitask learning methods in several cases despite the
high model sparsity enforced by DiSparse. We analyzed the pruning masks
generated with DiSparse and observed strikingly similar sparse network
architecture identified by each task even before the training starts. We also
observe the existence of a "watershed" layer where the task relatedness sharply
drops, implying no benefits in continued parameters sharing. Our code and
models will be available at:
https://github.com/SHI-Labs/DiSparse-Multitask-Model-Compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Data Scaling in Masked Image Modeling. (arXiv:2206.04664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04664">
<div class="article-summary-box-inner">
<span><p>An important goal of self-supervised learning is to enable model pre-training
to benefit from almost unlimited data. However, one method that has recently
become popular, namely masked image modeling (MIM), is suspected to be unable
to benefit from larger data. In this work, we break this misconception through
extensive experiments, with data scales ranging from 10\% of ImageNet-1K to
full ImageNet-22K, model sizes ranging from 49 million to 1 billion, and
training lengths ranging from 125K iterations to 500K iterations. Our study
reveals that: (i) Masked image modeling is also demanding on larger data. We
observed that very large models got over-fitted with relatively small data;
(ii) The length of training matters. Large models trained with masked image
modeling can benefit from more data with longer training; (iii) The validation
loss in pre-training is a good indicator to measure how well the model performs
for fine-tuning on multiple tasks. This observation allows us to pre-evaluate
pre-trained models in advance without having to make costly trial-and-error
assessments of downstream tasks. We hope that our findings will advance the
understanding of masked image modeling in terms of scaling ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGConv: Adaptive Graph Convolution on 3D Point Clouds. (arXiv:2206.04665v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04665">
<div class="article-summary-box-inner">
<span><p>Convolution on 3D point clouds is widely researched yet far from perfect in
geometric deep learning. The traditional wisdom of convolution characterises
feature correspondences indistinguishably among 3D points, arising an intrinsic
limitation of poor distinctive feature learning. In this paper, we propose
Adaptive Graph Convolution (AGConv) for wide applications of point cloud
analysis. AGConv generates adaptive kernels for points according to their
dynamically learned features. Compared with the solution of using
fixed/isotropic kernels, AGConv improves the flexibility of point cloud
convolutions, effectively and precisely capturing the diverse relations between
points from different semantic parts. Unlike the popular attentional weight
schemes, AGConv implements the adaptiveness inside the convolution operation
instead of simply assigning different weights to the neighboring points.
Extensive evaluations clearly show that our method outperforms
state-of-the-arts of point cloud classification and segmentation on various
benchmark datasets.Meanwhile, AGConv can flexibly serve more point cloud
analysis approaches to boost their performance. To validate its flexibility and
effectiveness, we explore AGConv-based paradigms of completion, denoising,
upsampling, registration and circle extraction, which are comparable or even
superior to their competitors. Our code is available at
https://github.com/hrzhou2/AdaptConv-master.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extreme Masking for Learning Instance and Distributed Visual Representations. (arXiv:2206.04667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04667">
<div class="article-summary-box-inner">
<span><p>The paper presents a scalable approach for learning distributed
representations over individual tokens and a holistic instance representation
simultaneously. We use self-attention blocks to represent distributed tokens,
followed by cross-attention blocks to aggregate the holistic instance. The core
of the approach is the use of extremely large token masking (75%-90%) as the
data augmentation for supervision. Our model, named ExtreMA, follows the plain
BYOL approach where the instance representation from the unmasked subset is
trained to predict that from the intact input. Learning requires the model to
capture informative variations in an instance, instead of encouraging
invariances. The paper makes three contributions: 1) Random masking is a strong
and computationally efficient data augmentation for learning generalizable
attention representations. 2) With multiple sampling per instance, extreme
masking greatly speeds up learning and hungers for more data. 3) Distributed
representations can be learned from the instance supervision alone, unlike
per-token supervisions in masked modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GateHUB: Gated History Unit with Background Suppression for Online Action Detection. (arXiv:2206.04668v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04668">
<div class="article-summary-box-inner">
<span><p>Online action detection is the task of predicting the action as soon as it
happens in a streaming video. A major challenge is that the model does not have
access to the future and has to solely rely on the history, i.e., the frames
observed so far, to make predictions. It is therefore important to accentuate
parts of the history that are more informative to the prediction of the current
frame. We present GateHUB, Gated History Unit with Background Suppression, that
comprises a novel position-guided gated cross-attention mechanism to enhance or
suppress parts of the history as per how informative they are for current frame
prediction. GateHUB further proposes Future-augmented History (FaH) to make
history features more informative by using subsequently observed frames when
available. In a single unified framework, GateHUB integrates the transformer's
ability of long-range temporal modeling and the recurrent model's capacity to
selectively encode relevant information. GateHUB also introduces a background
suppression objective to further mitigate false positive background frames that
closely resemble the action frames. Extensive validation on three benchmark
datasets, THUMOS, TVSeries, and HDD, demonstrates that GateHUB significantly
outperforms all existing methods and is also more efficient than the existing
best work. Furthermore, a flow-free version of GateHUB is able to achieve
higher or close accuracy at 2.8x higher frame rate compared to all existing
methods that require both RGB and optical flow information for prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields. (arXiv:2206.04669v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04669">
<div class="article-summary-box-inner">
<span><p>Comprehensive 3D scene understanding, both geometrically and semantically, is
important for real-world applications such as robot perception. Most of the
existing work has focused on developing data-driven discriminative models for
scene understanding. This paper provides a new approach to scene understanding,
from a synthesis model perspective, by leveraging the recent progress on
implicit 3D representation and neural rendering. Building upon the great
success of Neural Radiance Fields (NeRFs), we introduce Scene-Property
Synthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic
RGB images from novel viewpoints, but also render various accurate scene
properties (e.g., appearance, geometry, and semantics). By doing so, we
facilitate addressing a variety of scene understanding tasks under a unified
framework, including semantic segmentation, surface normal estimation,
reshading, keypoint detection, and edge detection. Our SS-NeRF framework can be
a powerful tool for bridging generative learning and discriminative learning,
and thus be beneficial to the investigation of a wide range of interesting
problems, such as studying task relationships within a synthesis paradigm,
transferring knowledge to novel tasks, facilitating downstream discriminative
tasks as ways of data augmentation, and serving as auto-labeller for data
creation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies. (arXiv:2206.04670v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04670">
<div class="article-summary-box-inner">
<span><p>PointNet++ is one of the most influential neural architectures for point
cloud understanding. Although the accuracy of PointNet++ has been largely
surpassed by recent networks such as PointMLP and Point Transformer, we find
that a large portion of the performance gain is due to improved training
strategies, i.e. data augmentation and optimization techniques, and increased
model sizes rather than architectural innovations. Thus, the full potential of
PointNet++ has yet to be explored. In this work, we revisit the classical
PointNet++ through a systematic study of model training and scaling strategies,
and offer two major contributions. First, we propose a set of improved training
strategies that significantly improve PointNet++ performance. For example, we
show that, without any change in architecture, the overall accuracy (OA) of
PointNet++ on ScanObjectNN object classification can be raised from 77.9\% to
86.1\%, even outperforming state-of-the-art PointMLP. Second, we introduce an
inverted residual bottleneck design and separable MLPs into PointNet++ to
enable efficient and effective model scaling and propose PointNeXt, the next
version of PointNets. PointNeXt can be flexibly scaled up and outperforms
state-of-the-art methods on both 3D classification and segmentation tasks. For
classification, PointNeXt reaches an overall accuracy of $87.7\%$ on
ScanObjectNN, surpassing PointMLP by $2.3\%$, while being $10 \times$ faster in
inference. For semantic segmentation, PointNeXt establishes a new
state-of-the-art performance with $74.9\%$ mean IoU on S3DIS (6-fold
cross-validation), being superior to the recent Point Transformer. The code and
models are available at https://github.com/guochengqian/pointnext.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Challenges in Deep Stereo: the Booster Dataset. (arXiv:2206.04671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04671">
<div class="article-summary-box-inner">
<span><p>We present a novel high-resolution and challenging stereo dataset framing
indoor scenes annotated with dense and accurate ground-truth disparities.
Peculiar to our dataset is the presence of several specular and transparent
surfaces, i.e. the main causes of failures for state-of-the-art stereo
networks. Our acquisition pipeline leverages a novel deep space-time stereo
framework which allows for easy and accurate labeling with sub-pixel precision.
We release a total of 419 samples collected in 64 different scenes and
annotated with dense ground-truth disparities. Each sample include a
high-resolution pair (12 Mpx) as well as an unbalanced pair (Left: 12 Mpx,
Right: 1.1 Mpx). Additionally, we provide manually annotated material
segmentation masks and 15K unlabeled samples. We evaluate state-of-the-art deep
networks based on our dataset, highlighting their limitations in addressing the
open challenges in stereo and drawing hints for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Prompt Search. (arXiv:2206.04673v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04673">
<div class="article-summary-box-inner">
<span><p>The size of vision models has grown exponentially over the last few years,
especially after the emergence of Vision Transformer. This has motivated the
development of parameter-efficient tuning methods, such as learning adapter
layers or visual prompt tokens, which allow a tiny portion of model parameters
to be trained whereas the vast majority obtained from pre-training are frozen.
However, designing a proper tuning method is non-trivial: one might need to try
out a lengthy list of design choices, not to mention that each downstream
dataset often requires custom designs. In this paper, we view the existing
parameter-efficient tuning methods as "prompt modules" and propose Neural
prOmpt seArcH (NOAH), a novel approach that learns, for large vision models,
the optimal design of prompt modules through a neural architecture search
algorithm, specifically for each downstream dataset. By conducting extensive
experiments on over 20 vision datasets, we demonstrate that NOAH (i) is
superior to individual prompt modules, (ii) has a good few-shot learning
ability, and (iii) is domain-generalizable. The code and models are available
at https://github.com/Davidzhangyuanhan/NOAH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs. (arXiv:2206.04674v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04674">
<div class="article-summary-box-inner">
<span><p>To build an artificial neural network like the biological intelligence
system, recent works have unified numerous tasks into a generalist model, which
can process various tasks with shared parameters and do not have any
task-specific modules. While generalist models achieve promising results on
various benchmarks, they have performance degradation on some tasks compared
with task-specialized models. In this work, we find that interference among
different tasks and modalities is the main factor to this phenomenon. To
mitigate such interference, we introduce the Conditional Mixture-of-Experts
(Conditional MoEs) to generalist models. Routing strategies under different
levels of conditions are proposed to take both the training/inference cost and
generalization ability into account. By incorporating the proposed Conditional
MoEs, the recently proposed generalist model Uni-Perceiver can effectively
mitigate the interference across tasks and modalities, and achieves
state-of-the-art results on a series of downstream tasks via prompt tuning on
1% of downstream data. Moreover, the introduction of Conditional MoEs still
holds the generalization ability of generalist models to conduct zero-shot
inference on new tasks, e.g., video-text retrieval and video caption. Code and
pre-trained generalist models shall be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indoor Depth Completion with Boundary Consistency and Self-Attention. (arXiv:1908.08344v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.08344">
<div class="article-summary-box-inner">
<span><p>Depth estimation features are helpful for 3D recognition. Commodity-grade
depth cameras are able to capture depth and color image in real-time. However,
glossy, transparent or distant surface cannot be scanned properly by the
sensor. As a result, enhancement and restoration from sensing depth is an
important task. Depth completion aims at filling the holes that sensors fail to
detect, which is still a complex task for machine to learn. Traditional
hand-tuned methods have reached their limits, while neural network based
methods tend to copy and interpolate the output from surrounding depth values.
This leads to blurred boundaries, and structures of the depth map are lost.
Consequently, our main work is to design an end-to-end network improving
completion depth maps while maintaining edge clarity. We utilize self-attention
mechanism, previously used in image inpainting fields, to extract more useful
information in each layer of convolution so that the complete depth map is
enhanced. In addition, we propose boundary consistency concept to enhance the
depth map quality and structure. Experimental results validate the
effectiveness of our self-attention and boundary consistency schema, which
outperforms previous state-of-the-art depth completion work on Matterport3D
dataset. Our code is publicly available at
https://github.com/tsunghan-wu/Depth-Completion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blacklight: Scalable Defense for Neural Networks against Query-Based Black-Box Attacks. (arXiv:2006.14042v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14042">
<div class="article-summary-box-inner">
<span><p>Deep learning systems are known to be vulnerable to adversarial examples. In
particular, query-based black-box attacks do not require knowledge of the deep
learning model, but can compute adversarial examples over the network by
submitting queries and inspecting returns. Recent work largely improves the
efficiency of those attacks, demonstrating their practicality on today's
ML-as-a-service platforms.
</p>
<p>We propose Blacklight, a new defense against query-based black-box
adversarial attacks. The fundamental insight driving our design is that, to
compute adversarial examples, these attacks perform iterative optimization over
the network, producing image queries highly similar in the input space.
Blacklight detects query-based black-box attacks by detecting highly similar
queries, using an efficient similarity engine operating on probabilistic
content fingerprints. We evaluate Blacklight against eight state-of-the-art
attacks, across a variety of models and image classification tasks. Blacklight
identifies them all, often after only a handful of queries. By rejecting all
detected queries, Blacklight prevents any attack to complete, even when
attackers persist to submit queries after account ban or query rejection.
Blacklight is also robust against several powerful countermeasures, including
an optimal black-box attack that approximates white-box attacks in efficiency.
Finally, we illustrate how Blacklight generalizes to other domains like text
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks in Highly Accelerated MRI. (arXiv:2008.06029v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.06029">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has shown great promise due to its capability to
train deep learning MRI reconstruction methods without fully-sampled data.
Current self-supervised learning methods for physics-guided reconstruction
networks split acquired undersampled data into two disjoint sets, where one is
used for data consistency (DC) in the unrolled network and the other to define
the training loss. In this study, we propose an improved self-supervised
learning strategy that more efficiently uses the acquired data to train a
physics-guided reconstruction network without a database of fully-sampled data.
The proposed multi-mask self-supervised learning via data undersampling (SSDU)
applies a hold-out masking operation on acquired measurements to split it into
multiple pairs of disjoint sets for each training sample, while using one of
these pairs for DC units and the other for defining loss, thereby more
efficiently using the undersampled data. Multi-mask SSDU is applied on
fully-sampled 3D knee and prospectively undersampled 3D brain MRI datasets, for
various acceleration rates and patterns, and compared to CG-SENSE and
single-mask SSDU DL-MRI, as well as supervised DL-MRI when fully-sampled data
is available. Results on knee MRI show that the proposed multi-mask SSDU
outperforms SSDU and performs closely with supervised DL-MRI. A clinical reader
study further ranks the multi-mask SSDU higher than supervised DL-MRI in terms
of SNR and aliasing artifacts. Results on brain MRI show that multi-mask SSDU
achieves better reconstruction quality compared to SSDU. Reader study
demonstrates that multi-mask SSDU at R=8 significantly improves reconstruction
compared to single-mask SSDU at R=8, as well as CG-SENSE at R=2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoising Diffusion Implicit Models. (arXiv:2010.02502v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02502">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models (DDPMs) have achieved high quality
image generation without adversarial training, yet they require simulating a
Markov chain for many steps to produce a sample. To accelerate sampling, we
present denoising diffusion implicit models (DDIMs), a more efficient class of
iterative implicit probabilistic models with the same training procedure as
DDPMs. In DDPMs, the generative process is defined as the reverse of a
Markovian diffusion process. We construct a class of non-Markovian diffusion
processes that lead to the same training objective, but whose reverse process
can be much faster to sample from. We empirically demonstrate that DDIMs can
produce high quality samples $10 \times$ to $50 \times$ faster in terms of
wall-clock time compared to DDPMs, allow us to trade off computation for sample
quality, and can perform semantically meaningful image interpolation directly
in the latent space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Clinical Decision Support Systems in Medical Imaging using Cycle-Consistent Activation Maximization. (arXiv:2010.05759v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05759">
<div class="article-summary-box-inner">
<span><p>Clinical decision support using deep neural networks has become a topic of
steadily growing interest. While recent work has repeatedly demonstrated that
deep learning offers major advantages for medical image classification over
traditional methods, clinicians are often hesitant to adopt the technology
because its underlying decision-making process is considered to be
intransparent and difficult to comprehend. In recent years, this has been
addressed by a variety of approaches that have successfully contributed to
providing deeper insight. Most notably, additive feature attribution methods
are able to propagate decisions back into the input space by creating a
saliency map which allows the practitioner to "see what the network sees."
However, the quality of the generated maps can become poor and the images noisy
if only limited data is available - a typical scenario in clinical contexts. We
propose a novel decision explanation scheme based on CycleGAN activation
maximization which generates high-quality visualizations of classifier
decisions even in smaller data sets. We conducted a user study in which we
evaluated our method on the LIDC dataset for lung lesion malignancy
classification, the BreastMNIST dataset for ultrasound image breast cancer
detection, as well as two subsets of the CIFAR-10 dataset for RBG image object
recognition. Within this user study, our method clearly outperformed existing
approaches on the medical imaging datasets and ranked second in the natural
image setting. With our approach we make a significant contribution towards a
better understanding of clinical decision support systems based on deep neural
networks and thus aim to foster overall clinical acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DUT: Learning Video Stabilization by Simply Watching Unstable Videos. (arXiv:2011.14574v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14574">
<div class="article-summary-box-inner">
<span><p>Previous deep learning-based video stabilizers require a large scale of
paired unstable and stable videos for training, which are difficult to collect.
Traditional trajectory-based stabilizers, on the other hand, divide the task
into several sub-tasks and tackle them subsequently, which are fragile in
textureless and occluded regions regarding the usage of hand-crafted features.
In this paper, we attempt to tackle the video stabilization problem in a deep
unsupervised learning manner, which borrows the divide-and-conquer idea from
traditional stabilizers while leveraging the representation power of DNNs to
handle the challenges in real-world scenarios. Technically, DUT is composed of
a trajectory estimation stage and a trajectory smoothing stage. In the
trajectory estimation stage, we first estimate the motion of keypoints,
initialize and refine the motion of grids via a novel multi-homography
estimation strategy and a motion refinement network, respectively, and get the
grid-based trajectories via temporal association. In the trajectory smoothing
stage, we devise a novel network to predict dynamic smoothing kernels for
trajectory smoothing, which can well adapt to trajectories with different
dynamic patterns. We exploit the spatial and temporal coherence of keypoints
and grid vertices to formulate the training objectives, resulting in an
unsupervised training scheme. Experiment results on public benchmarks show that
DUT outperforms state-of-the-art methods both qualitatively and quantitatively.
The source code is available at https://github.com/Annbless/DUTCode.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Unsupervised Meta-Learning via the Characteristics of Few-Shot Tasks. (arXiv:2011.14663v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14663">
<div class="article-summary-box-inner">
<span><p>Meta-learning has become a practical approach towards few-shot image
classification, where "a strategy to learn a classifier" is meta-learned on
labeled base classes and can be applied to tasks with novel classes. We remove
the requirement of base class labels and learn generalizable embeddings via
Unsupervised Meta-Learning (UML). Specifically, episodes of tasks are
constructed with data augmentations from unlabeled base classes during
meta-training, and we apply embedding-based classifiers to novel tasks with
labeled few-shot examples during meta-test. We observe two elements play
important roles in UML, i.e., the way to sample tasks and measure similarities
between instances. Thus we obtain a strong baseline with two simple
modifications -- a sufficient sampling strategy constructing multiple tasks per
episode efficiently together with a semi-normalized similarity. We then take
advantage of the characteristics of tasks from two directions to get further
improvements. First, synthesized confusing instances are incorporated to help
extract more discriminative embeddings. Second, we utilize an additional
task-specific embedding transformation as an auxiliary component during
meta-training to promote the generalization ability of the pre-adapted
embeddings. Experiments on few-shot learning benchmarks verify that our
approaches outperform previous UML methods and achieve comparable or even
better performance than its supervised variants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification. (arXiv:2103.04053v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04053">
<div class="article-summary-box-inner">
<span><p>Real-world large-scale medical image analysis (MIA) datasets have three
challenges: 1) they contain noisy-labelled samples that affect training
convergence and generalisation, 2) they usually have an imbalanced distribution
of samples per class, and 3) they normally comprise a multi-label problem,
where samples can have multiple diagnoses. Current approaches are commonly
trained to solve a subset of those problems, but we are unaware of methods that
address the three problems simultaneously. In this paper, we propose a new
training module called Non-Volatile Unbiased Memory (NVUM), which
non-volatility stores running average of model logits for a new regularization
loss on noisy multi-label problem. We further unbias the classification
prediction in NVUM update for imbalanced learning problem. We run extensive
experiments to evaluate NVUM on new benchmarks proposed by this paper, where
training is performed on noisy multi-label imbalanced chest X-ray (CXR)
training sets, formed by Chest-Xray14 and CheXpert, and the testing is
performed on the clean multi-label CXR datasets OpenI and PadChest. Our method
outperforms previous state-of-the-art CXR classifiers and previous methods that
can deal with noisy labels on all evaluations. Our code is available at
https://github.com/FBLADL/NVUM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Face Obfuscation in ImageNet. (arXiv:2103.06191v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06191">
<div class="article-summary-box-inner">
<span><p>Face obfuscation (blurring, mosaicing, etc.) has been shown to be effective
for privacy protection; nevertheless, object recognition research typically
assumes access to complete, unobfuscated images. In this paper, we explore the
effects of face obfuscation on the popular ImageNet challenge visual
recognition benchmark. Most categories in the ImageNet challenge are not people
categories; however, many incidental people appear in the images, and their
privacy is a concern. We first annotate faces in the dataset. Then we
demonstrate that face obfuscation has minimal impact on the accuracy of
recognition models. Concretely, we benchmark multiple deep neural networks on
obfuscated images and observe that the overall recognition accuracy drops only
slightly (&lt;= 1.0%). Further, we experiment with transfer learning to 4
downstream tasks (object recognition, scene recognition, face attribute
classification, and object detection) and show that features learned on
obfuscated images are equally transferable. Our work demonstrates the
feasibility of privacy-aware visual recognition, improves the highly-used
ImageNet challenge benchmark, and suggests an important path for future visual
datasets. Data and code are available at
https://github.com/princetonvisualai/imagenet-face-obfuscation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Hierarchical Games for Image Explanations. (arXiv:2104.06164v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06164">
<div class="article-summary-box-inner">
<span><p>As modern complex neural networks keep breaking records and solving harder
problems, their predictions also become less and less intelligible. The current
lack of interpretability often undermines the deployment of accurate machine
learning tools in sensitive settings. In this work, we present a model-agnostic
explanation method for image classification based on a hierarchical extension
of Shapley coefficients--Hierarchical Shap (h-Shap)--that resolves some of the
limitations of current approaches. Unlike other Shapley-based explanation
methods, h-Shap is scalable and can be computed without the need of
approximation. Under certain distributional assumptions, such as those common
in multiple instance learning, h-Shap retrieves the exact Shapley coefficients
with an exponential improvement in computational complexity. We compare our
hierarchical approach with popular Shapley-based and non-Shapley-based methods
on a synthetic dataset, a medical imaging scenario, and a general computer
vision problem, showing that h-Shap outperforms the state of the art in both
accuracy and runtime. Code and experiments are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks on Self-Supervised Learning. (arXiv:2105.10123v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10123">
<div class="article-summary-box-inner">
<span><p>Large-scale unlabeled data has spurred recent progress in self-supervised
learning methods that learn rich visual representations. State-of-the-art
self-supervised methods for learning representations from images (e.g., MoCo,
BYOL, MSF) use an inductive bias that random augmentations (e.g., random crops)
of an image should produce similar embeddings. We show that such methods are
vulnerable to backdoor attacks - where an attacker poisons a small part of the
unlabeled data by adding a trigger (image patch chosen by the attacker) to the
images. The model performance is good on clean test images, but the attacker
can manipulate the decision of the model by showing the trigger at test time.
Backdoor attacks have been studied extensively in supervised learning and to
the best of our knowledge, we are the first to study them for self-supervised
learning. Backdoor attacks are more practical in self-supervised learning,
since the use of large unlabeled data makes data inspection to remove poisons
prohibitive. We show that in our targeted attack, the attacker can produce many
false positives for the target category by using the trigger at test time. We
also propose a defense method based on knowledge distillation that succeeds in
neutralizing the attack. Our code is available here:
https://github.com/UMBCvision/SSL-Backdoor .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Deformation Estimation via Multi-Objective Optimization. (arXiv:2106.04139v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04139">
<div class="article-summary-box-inner">
<span><p>The free-form deformation model can represent a wide range of non-rigid
deformations by manipulating a control point lattice over the image. However,
due to a large number of parameters, it is challenging to fit the free-form
deformation model directly to the deformed image for deformation estimation
because of the complexity of the fitness landscape. In this paper, we cast the
registration task as a multi-objective optimization problem (MOP) according to
the fact that regions affected by each control point overlap with each other.
Specifically, by partitioning the template image into several regions and
measuring the similarity of each region independently, multiple objectives are
built and deformation estimation can thus be realized by solving the MOP with
off-the-shelf multi-objective evolutionary algorithms (MOEAs). In addition, a
coarse-to-fine strategy is realized by image pyramid combined with control
point mesh subdivision. Specifically, the optimized candidate solutions of the
current image level are inherited by the next level, which increases the
ability to deal with large deformation. Also, a post-processing procedure is
proposed to generate a single output utilizing the Pareto optimal solutions.
Comparative experiments on both synthetic and real-world images show the
effectiveness and usefulness of our deformation estimation method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Counterfactual Visual Explanations With Overdetermination. (arXiv:2106.14556v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14556">
<div class="article-summary-box-inner">
<span><p>A novel explainable AI method called CLEAR Image is introduced in this paper.
CLEAR Image is based on the view that a satisfactory explanation should be
contrastive, counterfactual and measurable. CLEAR Image explains an image's
classification probability by contrasting the image with a corresponding image
generated automatically via adversarial learning. This enables both salient
segmentation and perturbations that faithfully determine each segment's
importance. CLEAR Image was successfully applied to a medical imaging case
study where it outperformed methods such as Grad-CAM and LIME by an average of
27% using a novel pointing game metric. CLEAR Image excels in identifying cases
of "causal overdetermination" where there are multiple patches in an image, any
one of which is sufficient by itself to cause the classification probability to
be close to one.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalization and Robustness Implications in Object-Centric Learning. (arXiv:2107.00637v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00637">
<div class="article-summary-box-inner">
<span><p>The idea behind object-centric representation learning is that natural scenes
can better be modeled as compositions of objects and their relations as opposed
to distributed representations. This inductive bias can be injected into neural
networks to potentially improve systematic generalization and performance of
downstream tasks in scenes with multiple objects. In this paper, we train
state-of-the-art unsupervised models on five common multi-object datasets and
evaluate segmentation metrics and downstream object property prediction. In
addition, we study generalization and robustness by investigating the settings
where either a single object is out of distribution -- e.g., having an unseen
color, texture, or shape -- or global properties of the scene are altered --
e.g., by occlusions, cropping, or increasing the number of objects. From our
experimental study, we find object-centric representations to be useful for
downstream tasks and generally robust to most distribution shifts affecting
objects. However, when the distribution shift affects the input in a less
structured manner, robustness in terms of segmentation and downstream task
performance may vary significantly across models and distribution shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08543">
<div class="article-summary-box-inner">
<span><p>Domain shift is one of the most salient challenges in medical computer
vision. Due to immense variability in scanners' parameters and imaging
protocols, even images obtained from the same person and the same scanner could
differ significantly. We address variability in computed tomography (CT) images
caused by different convolution kernels used in the reconstruction process, the
critical domain shift factor in CT. The choice of a convolution kernel affects
pixels' granularity, image smoothness, and noise level. We analyze a dataset of
paired CT images, where smooth and sharp images were reconstructed from the
same sinograms with different kernels, thus providing identical anatomy but
different style. Though identical predictions are desired, we show that the
consistency, measured as the average Dice between predictions on pairs, is just
0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and
surprisingly efficient approach to augment CT images in sinogram space
emulating reconstruction with different kernels. We apply the proposed method
in a zero-shot domain adaptation setup and show that the consistency boosts
from 0.54 to 0.92 outperforming other augmentation approaches. Neither specific
preparation of source domain data nor target domain data is required, so our
publicly released FBPAug can be used as a plug-and-play module for zero-shot
domain adaptation in any CT-based task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11769">
<div class="article-summary-box-inner">
<span><p>Despite the success of deep learning on supervised point cloud semantic
segmentation, obtaining large-scale point-by-point manual annotations is still
a significant challenge. To reduce the huge annotation burden, we propose a
Region-based and Diversity-aware Active Learning (ReDAL), a general framework
for many deep learning approaches, aiming to automatically select only
informative and diverse sub-scene regions for label acquisition. Observing that
only a small portion of annotated regions are sufficient for 3D scene
understanding with deep learning, we use softmax entropy, color discontinuity,
and structural complexity to measure the information of sub-scene regions. A
diversity-aware selection algorithm is also developed to avoid redundant
annotations resulting from selecting informative but similar regions in a
querying batch. Extensive experiments show that our method highly outperforms
previous active learning strategies, and we achieve the performance of 90%
fully supervised learning, while less than 15% and 5% annotations are required
on S3DIS and SemanticKITTI datasets, respectively. Our code is publicly
available at https://github.com/tsunghan-wu/ReDAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Monocular Depth Estimation in Highly Complex Environments. (arXiv:2107.13137v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13137">
<div class="article-summary-box-inner">
<span><p>With the development of computational intelligence algorithms, unsupervised
monocular depth and pose estimation framework, which is driven by warped
photometric consistency, has shown great performance in the daytime scenario.
While in some challenging environments, like night and rainy night, the
essential photometric consistency hypothesis is untenable because of the
complex lighting and reflection, so that the above unsupervised framework
cannot be directly applied to these complex scenarios. In this paper, we
investigate the problem of unsupervised monocular depth estimation in highly
complex scenarios and address this challenging problem by adopting an image
transfer-based domain adaptation framework. We adapt the depth model trained on
day-time scenarios to be applicable to night-time scenarios, and constraints on
both feature space and output space promote the framework to learn the key
features for depth decoding. Meanwhile, we further tackle the effects of
unstable image transfer quality on domain adaptation, and an image adaptation
approach is proposed to evaluate the quality of transferred images and
re-weight the corresponding losses, so as to improve the performance of the
adapted depth model. Extensive experiments show the effectiveness of the
proposed unsupervised framework in estimating the dense depth map from highly
complex images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NPBDREG: Uncertainty Assessment in Diffeomorphic Brain MRI Registration using a Non-parametric Bayesian Deep-Learning Based Approach. (arXiv:2108.06771v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06771">
<div class="article-summary-box-inner">
<span><p>Quantification of uncertainty in deep-neural-networks (DNN) based image
registration algorithms plays a critical role in the deployment of image
registration algorithms for clinical applications such as surgical planning,
intraoperative guidance, and longitudinal monitoring of disease progression or
treatment efficacy as well as in research-oriented processing pipelines.
Currently available approaches for uncertainty estimation in DNN-based image
registration algorithms may result in sub-optimal clinical decision making due
to potentially inaccurate estimation of the uncertainty of the registration
stems for the assumed parametric distribution of the registration latent space.
We introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty
estimation in DNN-based deformable image registration by combining an Adam
optimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the
underlying posterior distribution through posterior sampling. Thus, it has the
potential to provide uncertainty estimates that are highly correlated with the
presence of out of distribution data. We demonstrated the added-value of
NPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on
brain MRI image registration using $390$ image pairs from four publicly
available databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a
better correlation of the predicted uncertainty with out-of-distribution data
($r&gt;0.95$ vs. $r&lt;0.5$) as well as a 7.3%improvement in the registration
accuracy (Dice score, $0.74$ vs. $0.69$, $p \ll 0.01$), and 18% improvement in
registration smoothness (percentage of folds in the deformation field, 0.014
vs. 0.017, $p \ll 0.01$). Finally, NPBDREG demonstrated a better generalization
capability for data corrupted by a mixed structure noise (Dice score of $0.73$
vs. $0.69$, $p \ll 0.01$) compared to the baseline PrVXM approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEDIC: A Multi-Task Learning Dataset for Disaster Image Classification. (arXiv:2108.12828v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12828">
<div class="article-summary-box-inner">
<span><p>Recent research in disaster informatics demonstrates a practical and
important use case of artificial intelligence to save human lives and suffering
during natural disasters based on social media contents (text and images).
While notable progress has been made using texts, research on exploiting the
images remains relatively under-explored. To advance image-based approaches, we
propose MEDIC (Available at: https://crisisnlp.qcri.org/medic/index.html),
which is the largest social media image classification dataset for humanitarian
response consisting of 71,198 images to address four different tasks in a
multi-task learning setup. This is the first dataset of its kind: social media
images, disaster response, and multi-task learning research. An important
property of this dataset is its high potential to facilitate research on
multi-task learning, which recently receives much interest from the machine
learning community and has shown remarkable results in terms of memory,
inference speed, performance, and generalization capability. Therefore, the
proposed dataset is an important resource for advancing image-based disaster
management and multi-task machine learning research. We experiment with
different deep learning architectures and report promising results, which are
above the majority baselines for all tasks. Along with the dataset, we also
release all relevant scripts (https://github.com/firojalam/medic).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization for Medical Image Segmentation via Hierarchical Consistency Regularization. (arXiv:2109.05742v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05742">
<div class="article-summary-box-inner">
<span><p>Modern deep neural networks struggle to transfer knowledge and generalize
across diverse domains when deployed to real-world applications. Currently,
domain generalization (DG) is introduced to learn a universal representation
from multiple domains to improve the network generalization ability on unseen
domains. However, previous DG methods only focus on the data-level consistency
scheme without considering the synergistic regularization among different
consistency schemes. In this paper, we present a novel Hierarchical Consistency
framework for Domain Generalization (HCDG) by integrating Extrinsic Consistency
and Intrinsic Consistency synergistically. Particularly, for the Extrinsic
Consistency, we leverage the knowledge across multiple source domains to
enforce data-level consistency. To better enhance such consistency, we design a
novel Amplitude Gaussian-mixing strategy into Fourier-based data augmentation
called DomainUp. For the Intrinsic Consistency, we perform task-level
consistency for the same instance under the dual-task scenario. We evaluate the
proposed HCDG framework on two medical image segmentation tasks, i.e., optic
cup/disc segmentation on fundus images and prostate MRI segmentation. Extensive
experimental results manifest the effectiveness and versatility of our HCDG
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Fast Adversarial Training with Learnable Adversarial Initialization. (arXiv:2110.05007v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05007">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) has been demonstrated to be effective in improving
model robustness by leveraging adversarial examples for training. However, most
AT methods are in face of expensive time and computational cost for calculating
gradients at multiple steps in generating adversarial examples. To boost
training efficiency, fast gradient sign method (FGSM) is adopted in fast AT
methods by calculating gradient only once. Unfortunately, the robustness is far
from satisfactory. One reason may arise from the initialization fashion.
Existing fast AT generally uses a random sample-agnostic initialization, which
facilitates the efficiency yet hinders a further robustness improvement. Up to
now, the initialization in fast AT is still not extensively explored. In this
paper, we boost fast AT with a sample-dependent adversarial initialization,
i.e., an output from a generative network conditioned on a benign image and its
gradient information from the target network. As the generative network and the
target network are optimized jointly in the training phase, the former can
adaptively generate an effective initialization with respect to the latter,
which motivates gradually improved robustness. Experimental evaluations on four
benchmark databases demonstrate the superiority of our proposed method over
state-of-the-art fast AT methods, as well as comparable robustness to advanced
multi-step AT methods. The code is released at
https://github.com//jiaxiaojunQAQ//FGSM-SDI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash. (arXiv:2111.06628v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06628">
<div class="article-summary-box-inner">
<span><p>Apple recently revealed its deep perceptual hashing system NeuralHash to
detect child sexual abuse material (CSAM) on user devices before files are
uploaded to its iCloud service. Public criticism quickly arose regarding the
protection of user privacy and the system's reliability. In this paper, we
present the first comprehensive empirical analysis of deep perceptual hashing
based on NeuralHash. Specifically, we show that current deep perceptual hashing
may not be robust. An adversary can manipulate the hash values by applying
slight changes in images, either induced by gradient-based approaches or simply
by performing standard image transformations, forcing or preventing hash
collisions. Such attacks permit malicious actors easily to exploit the
detection system: from hiding abusive material to framing innocent users,
everything is possible. Moreover, using the hash values, inferences can still
be made about the data stored on user devices. In our view, based on our
results, deep perceptual hashing in its current form is generally not ready for
robust client-side scanning and should not be used from a privacy perspective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Educated Warm Start For Deep Image Prior-Based Micro CT Reconstruction. (arXiv:2111.11926v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11926">
<div class="article-summary-box-inner">
<span><p>Deep image prior (DIP) was recently introduced as an effective unsupervised
approach for image restoration tasks. DIP represents the image to be recovered
as the output of a deep convolutional neural network, and learns the network's
parameters such that the output matches the corrupted observation. Despite its
impressive reconstructive properties, the approach is slow when compared to
supervisedly learned, or traditional reconstruction techniques. To address the
computational challenge, we bestow DIP with a two-stage learning paradigm: (i)
perform a supervised pretraining of the network on a simulated dataset; (ii)
fine-tune the network's parameters to adapt to the target reconstruction task.
We provide a thorough empirical analysis to shed insights into the impacts of
pretraining in the context of image reconstruction. We showcase that
pretraining considerably speeds up and stabilizes the subsequent reconstruction
task from real-measured 2D and 3D micro computed tomography data of biological
specimens. The code and additional experimental materials are available at
https://educateddip.github.io/docs.educated_deep_image_prior/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FogAdapt: Self-Supervised Domain Adaptation for Semantic Segmentation of Foggy Images. (arXiv:2201.02588v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02588">
<div class="article-summary-box-inner">
<span><p>This paper presents FogAdapt, a novel approach for domain adaptation of
semantic segmentation for dense foggy scenes. Although significant research has
been directed to reduce the domain shift in semantic segmentation, adaptation
to scenes with adverse weather conditions remains an open question. Large
variations in the visibility of the scene due to weather conditions, such as
fog, smog, and haze, exacerbate the domain shift, thus making unsupervised
adaptation in such scenarios challenging. We propose a self-entropy and
multi-scale information augmented self-supervised domain adaptation method
(FogAdapt) to minimize the domain shift in foggy scenes segmentation. Supported
by the empirical evidence that an increase in fog density results in high
self-entropy for segmentation probabilities, we introduce a self-entropy based
loss function to guide the adaptation method. Furthermore, inferences obtained
at different image scales are combined and weighted by the uncertainty to
generate scale-invariant pseudo-labels for the target domain. These
scale-invariant pseudo-labels are robust to visibility and scale variations. We
evaluate the proposed model on real clear-weather scenes to real foggy scenes
adaptation and synthetic non-foggy images to real foggy scenes adaptation
scenarios. Our experiments demonstrate that FogAdapt significantly outperforms
the current state-of-the-art in semantic segmentation of foggy images.
Specifically, by considering the standard settings compared to state-of-the-art
(SOTA) methods, FogAdapt gains 3.8% on Foggy Zurich, 6.0% on Foggy
Driving-dense, and 3.6% on Foggy Driving in mIoU when adapted from Cityscapes
to Foggy Zurich.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The CLEAR Benchmark: Continual LEArning on Real-World Imagery. (arXiv:2201.06289v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06289">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) is widely regarded as crucial challenge for lifelong
AI. However, existing CL benchmarks, e.g. Permuted-MNIST and Split-CIFAR, make
use of artificial temporal variation and do not align with or generalize to the
real-world. In this paper, we introduce CLEAR, the first continual image
classification benchmark dataset with a natural temporal evolution of visual
concepts in the real world that spans a decade (2004-2014). We build CLEAR from
existing large-scale image collections (YFCC100M) through a novel and scalable
low-cost approach to visio-linguistic dataset curation. Our pipeline makes use
of pretrained vision-language models (e.g. CLIP) to interactively build labeled
datasets, which are further validated with crowd-sourcing to remove errors and
even inappropriate images (hidden in original YFCC100M). The major strength of
CLEAR over prior CL benchmarks is the smooth temporal evolution of visual
concepts with real-world imagery, including both high-quality labeled data
along with abundant unlabeled samples per time period for continual
semi-supervised learning. We find that a simple unsupervised pre-training step
can already boost state-of-the-art CL algorithms that only utilize
fully-supervised data. Our analysis also reveals that mainstream CL evaluation
protocols that train and test on iid data artificially inflate performance of
CL system. To address this, we propose novel "streaming" protocols for CL that
always test on the (near) future. Interestingly, streaming protocols (a) can
simplify dataset curation since today's testset can be repurposed for
tomorrow's trainset and (b) can produce more generalizable models with more
accurate estimates of performance since all labeled data from each time-period
is used for both training and testing (unlike classic iid train-test splits).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks. (arXiv:2201.12179v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12179">
<div class="article-summary-box-inner">
<span><p>Model inversion attacks (MIAs) aim to create synthetic images that reflect
the class-wise characteristics from a target classifier's private training data
by exploiting the model's learned knowledge. Previous research has developed
generative MIAs that use generative adversarial networks (GANs) as image priors
tailored to a specific target model. This makes the attacks time- and
resource-consuming, inflexible, and susceptible to distributional shifts
between datasets. To overcome these drawbacks, we present Plug &amp; Play Attacks,
which relax the dependency between the target model and image prior, and enable
the use of a single GAN to attack a wide range of targets, requiring only minor
adjustments to the attack. Moreover, we show that powerful MIAs are possible
even with publicly available pre-trained GANs and under strong distributional
shifts, for which previous approaches fail to produce meaningful results. Our
extensive evaluation confirms the improved robustness and flexibility of Plug &amp;
Play Attacks and their ability to create high-quality images revealing
sensitive class characteristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADG-Pose: Automated Dataset Generation for Real-World Human Pose Estimation. (arXiv:2202.00753v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00753">
<div class="article-summary-box-inner">
<span><p>Recent advancements in computer vision have seen a rise in the prominence of
applications using neural networks to understand human poses. However, while
accuracy has been steadily increasing on State-of-the-Art datasets, these
datasets often do not address the challenges seen in real-world applications.
These challenges are dealing with people distant from the camera, people in
crowds, and heavily occluded people. As a result, many real-world applications
have trained on data that does not reflect the data present in deployment,
leading to significant underperformance. This article presents ADG-Pose, a
method for automatically generating datasets for real-world human pose
estimation. These datasets can be customized to determine person distances,
crowdedness, and occlusion distributions. Models trained with our method are
able to perform in the presence of these challenges where those trained on
other datasets fail. Using ADG-Pose, end-to-end accuracy for real-world
skeleton-based action recognition sees a 20% increase on scenes with moderate
distance and occlusion levels, and a 4X increase on distant scenes where other
models failed to perform better than random.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computing Multiple Image Reconstructions with a Single Hypernetwork. (arXiv:2202.11009v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11009">
<div class="article-summary-box-inner">
<span><p>Deep learning based techniques achieve state-of-the-art results in a wide
range of image reconstruction tasks like compressed sensing. These methods
almost always have hyperparameters, such as the weight coefficients that
balance the different terms in the optimized loss function. The typical
approach is to train the model for a hyperparameter setting determined with
some empirical or theoretical justification. Thus, at inference time, the model
can only compute reconstructions corresponding to the pre-determined
hyperparameter values. In this work, we present a hypernetwork-based approach,
called HyperRecon, to train reconstruction models that are agnostic to
hyperparameter settings. At inference time, HyperRecon can efficiently produce
diverse reconstructions, which would each correspond to different
hyperparameter values. In this framework, the user is empowered to select the
most useful output(s) based on their own judgement. We demonstrate our method
in compressed sensing, super-resolution and denoising tasks, using two
large-scale and publicly-available MRI datasets. Our code is available at
https://github.com/alanqrwang/hyperrecon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors. (arXiv:2203.01825v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01825">
<div class="article-summary-box-inner">
<span><p>Transfer learning is a standard technique to transfer knowledge from one
domain to another. For applications in medical imaging, transfer from ImageNet
has become the de-facto approach, despite differences in the tasks and image
characteristics between the domains. However, it is unclear what factors
determine whether - and to what extent - transfer learning to the medical
domain is useful. The long-standing assumption that features from the source
domain get reused has recently been called into question. Through a series of
experiments on several medical image benchmark datasets, we explore the
relationship between transfer learning, data size, the capacity and inductive
bias of the model, as well as the distance between the source and target
domain. Our findings suggest that transfer learning is beneficial in most
cases, and we characterize the important role feature reuse plays in its
success.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Promoted Supervision for Few-Shot Transformer. (arXiv:2203.07057v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07057">
<div class="article-summary-box-inner">
<span><p>The few-shot learning ability of vision transformers (ViTs) is rarely
investigated though heavily desired. In this work, we empirically find that
with the same few-shot learning frameworks, \eg~Meta-Baseline, replacing the
widely used CNN feature extractor with a ViT model often severely impairs
few-shot classification performance. Moreover, our empirical study shows that
in the absence of inductive bias, ViTs often learn the low-qualified token
dependencies under few-shot learning regime where only a few labeled training
data are available, which largely contributes to the above performance
degradation. To alleviate this issue, for the first time, we propose a simple
yet effective few-shot training framework for ViTs, namely Self-promoted
sUpervisioN (SUN). Specifically, besides the conventional global supervision
for global semantic learning SUN further pretrains the ViT on the few-shot
learning dataset and then uses it to generate individual location-specific
supervision for guiding each patch token. This location-specific supervision
tells the ViT which patch tokens are similar or dissimilar and thus accelerates
token dependency learning. Moreover, it models the local semantics in each
patch token to improve the object grounding and recognition capability which
helps learn generalizable patterns. To improve the quality of location-specific
supervision, we further propose two techniques:~1) background patch filtration
to filtrate background patches out and assign them into an extra background
class; and 2) spatial-consistent augmentation to introduce sufficient diversity
for data augmentation while keeping the accuracy of the generated local
supervisions. Experimental results show that SUN using ViTs significantly
surpasses other few-shot learning frameworks with ViTs and is the first one
that achieves higher performance than those CNN state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of Demonstrations for Social Navigation. (arXiv:2203.15041v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15041">
<div class="article-summary-box-inner">
<span><p>Social navigation is the capability of an autonomous agent, such as a robot,
to navigate in a 'socially compliant' manner in the presence of other
intelligent agents such as humans. With the emergence of autonomously
navigating mobile robots in human populated environments (e.g., domestic
service robots in homes and restaurants and food delivery robots on public
sidewalks), incorporating socially compliant navigation behaviors on these
robots becomes critical to ensuring safe and comfortable human robot
coexistence. To address this challenge, imitation learning is a promising
framework, since it is easier for humans to demonstrate the task of social
navigation rather than to formulate reward functions that accurately capture
the complex multi objective setting of social navigation. The use of imitation
learning and inverse reinforcement learning to social navigation for mobile
robots, however, is currently hindered by a lack of large scale datasets that
capture socially compliant robot navigation demonstrations in the wild. To fill
this gap, we introduce Socially CompliAnt Navigation Dataset (SCAND) a large
scale, first person view dataset of socially compliant navigation
demonstrations. Our dataset contains 8.7 hours, 138 trajectories, 25 miles of
socially compliant, human teleoperated driving demonstrations that comprises
multi modal data streams including 3D lidar, joystick commands, odometry,
visual and inertial information, collected on two morphologically different
mobile robots a Boston Dynamics Spot and a Clearpath Jackal by four different
human demonstrators in both indoor and outdoor environments. We additionally
perform preliminary analysis and validation through real world robot
experiments and show that navigation policies learned by imitation learning on
SCAND generate socially compliant behaviors
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16434">
<div class="article-summary-box-inner">
<span><p>We consider the problem of localizing a spatio-temporal tube in a video
corresponding to a given text query. This is a challenging task that requires
the joint and efficient modeling of temporal, spatial and multi-modal
interactions. To address this task, we propose TubeDETR, a transformer-based
architecture inspired by the recent success of such models for text-conditioned
object detection. Our model notably includes: (i) an efficient video and text
encoder that models spatial multi-modal interactions over sparsely sampled
frames and (ii) a space-time decoder that jointly performs spatio-temporal
localization. We demonstrate the advantage of our proposed components through
an extensive ablation study. We also evaluate our full approach on the
spatio-temporal video grounding task and demonstrate improvements over the
state of the art on the challenging VidSTG and HC-STVG benchmarks. Code and
trained models are publicly available at
https://antoyang.github.io/tubedetr.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16616">
<div class="article-summary-box-inner">
<span><p>Knowledge-based entity prediction (KEP) is a novel task that aims to improve
machine perception in autonomous systems. KEP leverages relational knowledge
from heterogeneous sources in predicting potentially unrecognized entities. In
this paper, we provide a formal definition of KEP as a knowledge completion
task. Three potential solutions are then introduced, which employ several
machine learning and data mining techniques. Finally, the applicability of KEP
is demonstrated on two autonomous systems from different domains; namely,
autonomous driving and smart manufacturing. We argue that in complex real-world
systems, the use of KEP would significantly improve machine perception while
pushing the current technology one step closer to achieving full autonomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization. (arXiv:2205.07547v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07547">
<div class="article-summary-box-inner">
<span><p>One noted issue of vector-quantized variational autoencoder (VQ-VAE) is that
the learned discrete representation uses only a fraction of the full capacity
of the codebook, also known as codebook collapse. We hypothesize that the
training scheme of VQ-VAE, which involves some carefully designed heuristics,
underlies this issue. In this paper, we propose a new training scheme that
extends the standard VAE via novel stochastic dequantization and quantization,
called stochastically quantized variational autoencoder (SQ-VAE). In SQ-VAE, we
observe a trend that the quantization is stochastic at the initial stage of the
training but gradually converges toward a deterministic quantization, which we
call self-annealing. Our experiments show that SQ-VAE improves codebook
utilization without using common heuristics. Furthermore, we empirically show
that SQ-VAE is superior to VAE and VQ-VAE in vision- and speech-related tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields. (arXiv:2205.09351v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09351">
<div class="article-summary-box-inner">
<span><p>Neural scene representations, such as Neural Radiance Fields (NeRF), are
based on training a multilayer perceptron (MLP) using a set of color images
with known poses. An increasing number of devices now produce RGB-D(color +
depth) information, which has been shown to be very important for a wide range
of tasks. Therefore, the aim of this paper is to investigate what improvements
can be made to these promising implicit representations by incorporating depth
information with the color images. In particular, the recently proposed
Mip-NeRF approach, which uses conical frustums instead of rays for volume
rendering, allows one to account for the varying area of a pixel with distance
from the camera center. The proposed method additionally models depth
uncertainty. This allows to address major limitations of NeRF-based approaches
including improving the accuracy of geometry, reduced artifacts, faster
training time, and shortened prediction time. Experiments are performed on
well-known benchmark scenes, and comparisons show improved accuracy in scene
geometry and photometric reconstruction, while reducing the training time by 3
- 5 times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions. (arXiv:2205.10218v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10218">
<div class="article-summary-box-inner">
<span><p>Generalization across different environments with the same tasks is critical
for successful applications of visual reinforcement learning (RL) in real
scenarios. However, visual distractions -- which are common in real scenes --
from high-dimensional observations can be hurtful to the learned
representations in visual RL, thus degrading the performance of generalization.
To tackle this problem, we propose a novel approach, namely Characteristic
Reward Sequence Prediction (CRESP), to extract the task-relevant information by
learning reward sequence distributions (RSDs), as the reward signals are
task-relevant in RL and invariant to visual distractions. Specifically, to
effectively capture the task-relevant information via RSDs, CRESP introduces an
auxiliary task -- that is, predicting the characteristic functions of RSDs --
to learn task-relevant representations, because we can well approximate the
high-dimensional distributions by leveraging the corresponding characteristic
functions. Experiments demonstrate that CRESP significantly improves the
performance of generalization on unseen environments, outperforming several
state-of-the-arts on DeepMind Control tasks with different visual distractions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01612">
<div class="article-summary-box-inner">
<span><p>We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python
library of eXplainable AI (XAI), which offers omni-way explainable AI
capabilities and various interpretable machine learning techniques to address
the pain points of understanding and interpreting the decisions made by machine
learning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library
that makes explainable AI easy for data scientists, ML researchers and
practitioners who need explanation for various types of data, models and
explanation methods at different stages of ML process (data exploration,
feature engineering, model development, evaluation, and decision-making, etc).
In particular, our library includes a rich family of explanation methods
integrated in a unified interface, which supports multiple data types (tabular
data, images, texts, time-series), multiple types of ML models (traditional ML
in Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of
diverse explanation methods including "model-specific" and "model-agnostic"
ones (such as feature-attribution explanation, counterfactual explanation,
gradient-based explanation, etc). For practitioners, the library provides an
easy-to-use unified interface to generate the explanations for their
applications by only writing a few lines of codes, and also a GUI dashboard for
visualization of different explanations for more insights about decisions. In
this technical report, we present OmniXAI's design principles, system
architectures, and major functionalities, and also demonstrate several example
use cases across different types of data, tasks, and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01714">
<div class="article-summary-box-inner">
<span><p>Large text-guided diffusion models, such as DALLE-2, are able to generate
stunning photorealistic images given natural language descriptions. While such
models are highly flexible, they struggle to understand the composition of
certain concepts, such as confusing the attributes of different objects or
relations between objects. In this paper, we propose an alternative structured
approach for compositional generation using diffusion models. An image is
generated by composing a set of diffusion models, with each of them modeling a
certain component of the image. To do this, we interpret diffusion models as
energy-based models in which the data distributions defined by the energy
functions may be explicitly combined. The proposed method can generate scenes
at test time that are substantially more complex than those seen in training,
composing sentence descriptions, object relations, human facial attributes, and
even generalizing to new combinations that are rarely seen in the real world.
We further illustrate how our approach may be used to compose pre-trained
text-guided diffusion models and generate photorealistic images containing all
the details described in the input descriptions, including the binding of
certain object attributes that have been shown difficult for DALLE-2. These
results point to the effectiveness of the proposed method in promoting
structured generalization for visual generation. Project page:
https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator. (arXiv:2206.02284v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02284">
<div class="article-summary-box-inner">
<span><p>Understanding the underlying relationship between tongue and oropharyngeal
muscle deformation seen in tagged-MRI and intelligible speech plays an
important role in advancing speech motor control theories and treatment of
speech related-disorders. Because of their heterogeneous representations,
however, direct mapping between the two modalities -- i.e., two-dimensional
(mid-sagittal slice) plus time tagged-MRI sequence and its corresponding
one-dimensional waveform -- is not straightforward. Instead, we resort to
two-dimensional spectrograms as an intermediate representation, which contains
both pitch and resonance, from which to develop an end-to-end deep learning
framework to translate from a sequence of tagged-MRI to its corresponding audio
waveform with limited dataset size.~Our framework is based on a novel fully
convolutional asymmetry translator with guidance of a self residual attention
strategy to specifically exploit the moving muscular structures during
speech.~In addition, we leverage a pairwise correlation of the samples with the
same utterances with a latent space representation disentanglement
strategy.~Furthermore, we incorporate an adversarial training approach with
generative adversarial networks to offer improved realism on our generated
spectrograms.~Our experimental results, carried out with a total of 63
tagged-MRI sequences alongside speech acoustics, showed that our framework
enabled the generation of clear audio waveforms from a sequence of tagged-MRI,
surpassing competing methods. Thus, our framework provides the great potential
to help better understand the relationship between the two modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-training. (arXiv:2206.02288v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02288">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) has been vastly explored to alleviate
domain shifts between source and target domains, by applying a well-performed
model in an unlabeled target domain via supervision of a labeled source domain.
Recent literature, however, has indicated that the performance is still far
from satisfactory in the presence of significant domain shifts. Nonetheless,
delineating a few target samples is usually manageable and particularly
worthwhile, due to the substantial performance gain. Inspired by this, we aim
to develop semi-supervised domain adaptation (SSDA) for medical image
segmentation, which is largely underexplored. We, thus, propose to exploit both
labeled source and target domain data, in addition to unlabeled target data in
a unified manner. Specifically, we present a novel asymmetric co-training (ACT)
framework to integrate these subsets and avoid the domination of the source
domain data. Following a divide-and-conquer strategy, we explicitly decouple
the label supervisions in SSDA into two asymmetric sub-tasks, including
semi-supervised learning (SSL) and UDA, and leverage different knowledge from
two segmentors to take into account the distinction between the source and
target label supervisions. The knowledge learned in the two modules is then
adaptively integrated with ACT, by iteratively teaching each other, based on
the confidence-aware pseudo-label. In addition, pseudo label noise is
well-controlled with an exponential MixUp decay scheme for smooth propagation.
Experiments on cross-modality brain tumor MRI segmentation tasks using the
BraTS18 database showed, even with limited labeled target samples, ACT yielded
marked improvements over UDA and state-of-the-art SSDA methods and approached
an "upper bound" of supervised joint training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVC-Net: Multi-scale V-Net with Conditional Random Fields for Brain Extraction. (arXiv:2206.02837v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02837">
<div class="article-summary-box-inner">
<span><p>Brain extraction is one of the first steps of pre-processing 3D brain MRI
data. It is a prerequisite for any forthcoming brain imaging analyses. However,
it is not a simple segmentation problem due to the complex structure of the
brain and human head. Although multiple solutions have been proposed in the
literature, we are still far from having truly robust methods. While previous
methods have used machine learning with structural/geometric priors, with the
development of deep learning in computer vision tasks, there has been an
increase in proposed convolutional neural network architectures for this
semantic segmentation task. Yet, most models focus on improving the training
data and loss functions with little change in the architecture. In this paper,
we propose a novel architecture we call EVC-Net. EVC-Net adds lower scale
inputs on each encoder block. This enhances the multi-scale scheme of the V-Net
architecture, hence increasing the efficiency of the model. Conditional Random
Fields, a popular approach for image segmentation before the deep learning era,
are re-introduced here as an additional step for refining the network's output
to capture fine-grained results in segmentation. We compare our model to
state-of-the-art methods such as HD-BET, Synthstrip and brainy. Results show
that even with limited training resources, EVC-Net achieves higher Dice
Coefficient and Jaccard Index along with lower surface distance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation. (arXiv:2206.03354v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03354">
<div class="article-summary-box-inner">
<span><p>Vision-and-language tasks are gaining popularity in the research community,
but the focus is still mainly on English. We propose a pipeline that utilizes
English-only vision-language models to train a monolingual model for a target
language. We propose to extend OSCAR+, a model which leverages object tags as
anchor points for learning image-text alignments, to train on visual question
answering datasets in different languages. We propose a novel approach to
knowledge distillation to train the model in other languages using parallel
sentences. Compared to other models that use the target language in the
pretraining corpora, we can leverage an existing English model to transfer the
knowledge to the target language using significantly lesser resources. We also
release a large-scale visual question answering dataset in Japanese and Hindi
language. Though we restrict our work to visual question answering, our model
can be extended to any sequence-level classification task, and it can be
extended to other languages as well. This paper focuses on two languages for
the visual question answering task - Japanese and Hindi. Our pipeline
outperforms the current state-of-the-art models by a relative increase of 4.4%
and 13.4% respectively in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Long Videos of Dynamic Scenes. (arXiv:2206.03429v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03429">
<div class="article-summary-box-inner">
<span><p>We present a video generation model that accurately reproduces object motion,
changes in camera viewpoint, and new content that arises over time. Existing
video generation methods often fail to produce new content as a function of
time while maintaining consistencies expected in real environments, such as
plausible dynamics and object persistence. A common failure case is for content
to never change due to over-reliance on inductive biases to provide temporal
consistency, such as a single latent code that dictates content for the entire
video. On the other extreme, without long-term consistency, generated videos
may morph unrealistically between different scenes. To address these
limitations, we prioritize the time axis by redesigning the temporal latent
representation and learning long-term consistency from data by training on
longer videos. To this end, we leverage a two-phase training strategy, where we
separately train using longer videos at a low resolution and shorter videos at
a high resolution. To evaluate the capabilities of our model, we introduce two
new benchmark datasets with explicit focus on long-term temporal dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity. (arXiv:2206.03544v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03544">
<div class="article-summary-box-inner">
<span><p>Reconstructing natural videos from fMRI brain recordings is very challenging,
for two main reasons: (i) As fMRI data acquisition is difficult, we only have a
limited amount of supervised samples, which is not enough to cover the huge
space of natural videos; and (ii) The temporal resolution of fMRI recordings is
much lower than the frame rate of natural videos. In this paper, we propose a
self-supervised approach for natural-movie reconstruction. By employing
cycle-consistency over Encoding-Decoding natural videos, we can: (i) exploit
the full framerate of the training videos, and not be limited only to clips
that correspond to fMRI recordings; (ii) exploit massive amounts of external
natural videos which the subjects never saw inside the fMRI machine. These
enable increasing the applicable training data by several orders of magnitude,
introducing natural video priors to the decoding network, as well as temporal
coherence. Our approach significantly outperforms competing methods, since
those train only on the limited supervised data. We further introduce a new and
simple temporal prior of natural videos, which - when folded into our fMRI
decoder further - allows us to reconstruct videos at a higher frame-rate (HFR)
of up to x8 of the original fMRI sample rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hypernetwork-based Personalized Federated Learning for Multi-Institutional CT Imaging. (arXiv:2206.03709v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03709">
<div class="article-summary-box-inner">
<span><p>Computed tomography (CT) is of great importance in clinical practice due to
its powerful ability to provide patients' anatomical information without any
invasive inspection, but its potential radiation risk is raising people's
concerns. Deep learning-based methods are considered promising in CT
reconstruction, but these network models are usually trained with the measured
data obtained from specific scanning protocol and need to centralizedly collect
large amounts of data, which will lead to serious data domain shift, and
privacy concerns. To relieve these problems, in this paper, we propose a
hypernetwork-based federated learning method for personalized CT imaging,
dubbed as HyperFed. The basic assumption of HyperFed is that the optimization
problem for each institution can be divided into two parts: the local data
adaption problem and the global CT imaging problem, which are implemented by an
institution-specific hypernetwork and a global-sharing imaging network,
respectively. The purpose of global-sharing imaging network is to learn stable
and effective common features from different institutions. The
institution-specific hypernetwork is carefully designed to obtain
hyperparameters to condition the global-sharing imaging network for
personalized local CT reconstruction. Experiments show that HyperFed achieves
competitive performance in CT reconstruction compared with several other
state-of-the-art methods. It is believed as a promising direction to improve CT
imaging quality and achieve personalized demands of different institutions or
scanners without privacy data sharing. The codes will be released at
https://github.com/Zi-YuanYang/HyperFed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03826">
<div class="article-summary-box-inner">
<span><p>For unsupervised pretraining, mask-reconstruction pretraining (MRP)
approaches randomly mask input patches and then reconstruct pixels or semantic
features of these masked patches via an auto-encoder. Then for a downstream
task, supervised fine-tuning the pretrained encoder remarkably surpasses the
conventional supervised learning (SL) trained from scratch. However, it is
still unclear 1) how MRP performs semantic learning in the pretraining phase
and 2) why it helps in downstream tasks. To solve these problems, we
theoretically show that on an auto-encoder of a two/one-layered convolution
encoder/decoder, MRP can capture all discriminative semantics in the
pretraining dataset, and accordingly show its provable improvement over SL on
the classification downstream task. Specifically, we assume that pretraining
dataset contains multi-view samples of ratio $1-\mu$ and single-view samples of
ratio $\mu$, where multi/single-view samples has multiple/single discriminative
semantics. Then for pretraining, we prove that 1) the convolution kernels of
the MRP encoder captures all discriminative semantics in the pretraining data;
and 2) a convolution kernel captures at most one semantic. Accordingly, in the
downstream supervised fine-tuning, most semantics would be captured and
different semantics would not be fused together. This helps the downstream
fine-tuned network to easily establish the relation between kernels and
semantic class labels. In this way, the fine-tuned encoder in MRP provably
achieves zero test error with high probability for both multi-view and
single-view test data. In contrast, as proved by~[3], conventional SL can only
obtain a test accuracy between around $0.5\mu$ for single-view test data. These
results together explain the benefits of MRP in downstream tasks. Experimental
results testify to multi-view data assumptions and our theoretical
implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays. (arXiv:2206.03935v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03935">
<div class="article-summary-box-inner">
<span><p>Chest X-ray (CXR) is the most typical radiological exam for diagnosis of
various diseases. Due to the expensive and time-consuming annotations,
detecting anomalies in CXRs in an unsupervised fashion is very promising.
However, almost all of the existing methods consider anomaly detection as a
One-Class Classification (OCC) problem. They model the distribution of only
known normal images during training and identify the samples not conforming to
normal profile as anomalies in the testing phase. A large number of unlabeled
images containing anomalies are thus ignored in the training phase, although
they are easy to obtain in clinical practice. In this paper, we propose a novel
strategy, Dual-distribution Discrepancy for Anomaly Detection (DDAD), utilizing
both known normal images and unlabeled images. The proposed method consists of
two modules, denoted as A and B. During training, module A takes both known
normal and unlabeled images as inputs, capturing anomalous features from
unlabeled images in some way, while module B models the distribution of only
known normal images. Subsequently, the inter-discrepancy between modules A and
B, and intra-discrepancy inside module B are designed as anomaly scores to
indicate anomalies. Experiments on three CXR datasets demonstrate that the
proposed DDAD achieves consistent, significant gains and outperforms
state-of-the-art methods. Code is available at
https://github.com/caiyu6666/DDAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Score-based Generative Models for High-Resolution Image Synthesis. (arXiv:2206.04029v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04029">
<div class="article-summary-box-inner">
<span><p>Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. The key idea is to produce high-quality images by
recurrently adding Gaussian noises and gradients to a Gaussian sample until
converging to the target distribution, a.k.a. the diffusion sampling. To ensure
stability of convergence in sampling and generation quality, however, this
sequential sampling process has to take a small step size and many sampling
iterations (e.g., 2000). Several acceleration methods have been proposed with
focus on low-resolution generation. In this work, we consider the acceleration
of high-resolution generation with SGMs, a more challenging yet more important
problem. We prove theoretically that this slow convergence drawback is
primarily due to the ignorance of the target distribution. Further, we
introduce a novel Target Distribution Aware Sampling (TDAS) method by
leveraging the structural priors in space and frequency domains. Extensive
experiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can
consistently accelerate state-of-the-art SGMs, particularly on more challenging
high resolution (1024x1024) image generation tasks by up to 18.4x, whilst
largely maintaining the synthesis quality. With fewer sampling iterations, TDAS
can still generate good quality images. In contrast, the existing methods
degrade drastically or even fails completely
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Dictionary Learning for Anomaly Detection. (arXiv:2003.00293v2 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.00293">
<div class="article-summary-box-inner">
<span><p>We investigate the possibilities of employing dictionary learning to address
the requirements of most anomaly detection applications, such as absence of
supervision, online formulations, low false positive rates. We present new
results of our recent semi-supervised online algorithm, TODDLeR, on a
anti-money laundering application. We also introduce a novel unsupervised
method of using the performance of the learning algorithm as indication of the
nature of the samples.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-12 23:07:47.388607435 UTC">2022-06-12 23:07:47 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>