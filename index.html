<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-30T01:30:00Z">03-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Knowledge Graph Completion with Self-Supervised Adaptive Graph Alignment. (arXiv:2203.14987v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14987">
<div class="article-summary-box-inner">
<span><p>Predicting missing facts in a knowledge graph (KG) is crucial as modern KGs
are far from complete. Due to labor-intensive human labeling, this phenomenon
deteriorates when handling knowledge represented in various languages. In this
paper, we explore multilingual KG completion, which leverages limited seed
alignment as a bridge, to embrace the collective knowledge from multiple
languages. However, language alignment used in prior works is still not fully
exploited: (1) alignment pairs are treated equally to maximally push parallel
entities to be close, which ignores KG capacity inconsistency; (2) seed
alignment is scarce and new alignment identification is usually in a noisily
unsupervised manner. To tackle these issues, we propose a novel self-supervised
adaptive graph alignment (SS-AGA) method. Specifically, SS-AGA fuses all KGs as
a whole graph by regarding alignment as a new edge type. As such, information
propagation and noise influence across KGs can be adaptively controlled via
relation-aware attention weights. Meanwhile, SS-AGA features a new pair
generator that dynamically captures potential alignment pairs in a
self-supervised paradigm. Extensive experiments on both the public multilingual
DBPedia KG and newly-created industrial multilingual E-commerce KG empirically
demonstrate the effectiveness of SS-AG
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing in context: Improving cosine similarity measures with a metric tensor. (arXiv:2203.14996v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14996">
<div class="article-summary-box-inner">
<span><p>Cosine similarity is a widely used measure of the relatedness of pre-trained
word embeddings, trained on a language modeling goal. Datasets such as
WordSim-353 and SimLex-999 rate how similar words are according to human
annotators, and as such are often used to evaluate the performance of language
models. Thus, any improvement on the word similarity task requires an improved
word representation. In this paper, we propose instead the use of an extended
cosine similarity measure to improve performance on that task, with gains in
interpretability. We explore the hypothesis that this approach is particularly
useful if the word-similarity pairs share the same context, for which distinct
contextualized similarity measures can be learned. We first use the dataset of
Richie et al. (2020) to learn contextualized metrics and compare the results
with the baseline values obtained using the standard cosine similarity measure,
which consistently shows improvement. We also train a contextualized similarity
measure for both SimLex-999 and WordSim-353, comparing the results with the
corresponding baselines, and using these datasets as independent test sets for
the all-context similarity measure learned on the contextualized dataset,
obtaining positive results for a number of tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15081">
<div class="article-summary-box-inner">
<span><p>We present a method for visually-grounded spoken term discovery. After
training either a HuBERT or wav2vec2.0 model to associate spoken captions with
natural images, we show that powerful word segmentation and clustering
capability emerges within the model's self-attention heads. Our experiments
reveal that this ability is not present to nearly the same extent in the base
HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a
crucial component of the word discovery capability we observe. We also evaluate
our method on the Buckeye word segmentation and ZeroSpeech spoken term
discovery tasks, where we outperform all currently published methods on several
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Named Entity Recognition. (arXiv:2203.15101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15101">
<div class="article-summary-box-inner">
<span><p>We present an analysis of the performance of Federated Learning in a
paradigmatic natural-language processing task: Named-Entity Recognition (NER).
For our evaluation, we use the language-independent CoNLL-2003 dataset as our
benchmark dataset and a Bi-LSTM-CRF model as our benchmark NER model. We show
that federated training reaches almost the same performance as the centralized
model, though with some performance degradation as the learning environments
become more heterogeneous. We also show the convergence rate of federated
models for NER. Finally, we discuss existing challenges of Federated Learning
for NLP applications that can foster future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Well-Composed Text is Half Done! Composition Sampling for Diverse Conditional Generation. (arXiv:2203.15108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15108">
<div class="article-summary-box-inner">
<span><p>We propose Composition Sampling, a simple but effective method to generate
diverse outputs for conditional generation of higher quality compared to
previous stochastic decoding strategies. It builds on recently proposed
plan-based neural generation models (Narayan et al, 2021) that are trained to
first create a composition of the output and then generate by conditioning on
it and the input. Our approach avoids text degeneration by first sampling a
composition in the form of an entity chain and then using beam search to
generate the best possible text grounded to this entity chain. Experiments on
summarization (CNN/DailyMail and XSum) and question generation (SQuAD), using
existing and newly proposed automatic metrics together with human-based
evaluation, demonstrate that Composition Sampling is currently the best
available decoding strategy for generating diverse meaningful outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15125">
<div class="article-summary-box-inner">
<span><p>Natural language-based communication with mobile devices and home appliances
is becoming increasingly popular and has the potential to become natural for
communicating with mobile robots in the future. Towards this goal, we
investigate cross-modal text-to-point-cloud localization that will allow us to
specify, for example, a vehicle pick-up or goods delivery location. In
particular, we propose Text2Pos, a cross-modal localization module that learns
to align textual descriptions with localization cues in a coarse- to-fine
manner. Given a point cloud of the environment, Text2Pos locates a position
that is specified via a natural language-based description of the immediate
surroundings. To train Text2Pos and study its performance, we construct
KITTI360Pose, the first dataset for this task based on the recently introduced
KITTI360 dataset. Our experiments show that we can localize 65% of textual
queries within 15m distance to query locations for top-10 retrieved locations.
This is a starting point that we hope will spark future developments towards
language-based navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Filler Word Detection and Classification: A Dataset and Benchmark. (arXiv:2203.15135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15135">
<div class="article-summary-box-inner">
<span><p>Filler words such as `uh' or `um' are sounds or words people use to signal
they are pausing to think. Finding and removing filler words from recordings is
a common and tedious task in media editing. Automatically detecting and
classifying filler words could greatly aid in this task, but few studies have
been published on this problem. A key reason is the absence of a dataset with
annotated filler words for training and evaluation. In this work, we present a
novel speech dataset, PodcastFillers, with 35K annotated filler words and 50K
annotations of other sounds that commonly occur in podcasts such as breaths,
laughter, and word repetitions. We propose a pipeline that leverages VAD and
ASR to detect filler candidates and a classifier to distinguish between filler
word types. We evaluate our proposed pipeline on PodcastFillers, compare to
several baselines, and present a detailed ablation study. In particular, we
evaluate the importance of using ASR and how it compares to a
transcription-free approach resembling keyword spotting. We show that our
pipeline obtains state-of-the-art results, and that leveraging ASR strongly
outperforms a keyword spotting approach. We make PodcastFillers publicly
available, and hope our work serves as a benchmark for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-AI Collaboration Enables More Empathic Conversations in Text-based Peer-to-Peer Mental Health Support. (arXiv:2203.15144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15144">
<div class="article-summary-box-inner">
<span><p>Advances in artificial intelligence (AI) are enabling systems that augment
and collaborate with humans to perform simple, mechanistic tasks like
scheduling meetings and grammar-checking text. However, such Human-AI
collaboration poses challenges for more complex, creative tasks, such as
carrying out empathic conversations, due to difficulties of AI systems in
understanding complex human emotions and the open-ended nature of these tasks.
Here, we focus on peer-to-peer mental health support, a setting in which
empathy is critical for success, and examine how AI can collaborate with humans
to facilitate peer empathy during textual, online supportive conversations. We
develop Hailey, an AI-in-the-loop agent that provides just-in-time feedback to
help participants who provide support (peer supporters) respond more
empathically to those seeking help (support seekers). We evaluate Hailey in a
non-clinical randomized controlled trial with real-world peer supporters on
TalkLife (N=300), a large online peer-to-peer support platform. We show that
our Human-AI collaboration approach leads to a 19.60% increase in
conversational empathy between peers overall. Furthermore, we find a larger
38.88% increase in empathy within the subsample of peer supporters who
self-identify as experiencing difficulty providing support. We systematically
analyze the Human-AI collaboration patterns and find that peer supporters are
able to use the AI feedback both directly and indirectly without becoming
overly reliant on AI while reporting improved self-efficacy post-feedback. Our
findings demonstrate the potential of feedback-driven, AI-in-the-loop writing
systems to empower humans in open-ended, social, creative tasks such as
empathic conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separate What You Describe: Language-Queried Audio Source Separation. (arXiv:2203.15147v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15147">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the task of language-queried audio source
separation (LASS), which aims to separate a target source from an audio mixture
based on a natural language query of the target source (e.g., "a man tells a
joke followed by people laughing"). A unique challenge in LASS is associated
with the complexity of natural language description and its relation with the
audio sources. To address this issue, we proposed LASS-Net, an end-to-end
neural network that is learned to jointly process acoustic and linguistic
information, and separate the target source that is consistent with the
language query from an audio mixture. We evaluate the performance of our
proposed system with a dataset created from the AudioCaps dataset. Experimental
results show that LASS-Net achieves considerable improvements over baseline
methods. Furthermore, we observe that LASS-Net achieves promising
generalization results when using diverse human-annotated descriptions as
queries, indicating its potential use in real-world scenarios. The separated
audio samples and source code are available at
https://liuxubo717.github.io/LASS-demopage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Evaluation Dataset for Legal Word Embedding: A Case Study On Chinese Codex. (arXiv:2203.15173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15173">
<div class="article-summary-box-inner">
<span><p>Word embedding is a modern distributed word representations approach widely
used in many natural language processing tasks. Converting the vocabulary in a
legal document into a word embedding model facilitates subjecting legal
documents to machine learning, deep learning, and other algorithms and
subsequently performing the downstream tasks of natural language processing
vis-\`a-vis, for instance, document classification, contract review, and
machine translation. The most common and practical approach of accuracy
evaluation with the word embedding model uses a benchmark set with linguistic
rules or the relationship between words to perform analogy reasoning via
algebraic calculation. This paper proposes establishing a 1,134 Legal
Analogical Reasoning Questions Set (LARQS) from the 2,388 Chinese Codex corpus
using five kinds of legal relations, which are then used to evaluate the
accuracy of the Chinese word embedding model. Moreover, we discovered that
legal relations might be ubiquitous in the word embedding model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalization of Deep Neural Network Acoustic Models with Length Perturbation and N-best Based Label Smoothing. (arXiv:2203.15176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15176">
<div class="article-summary-box-inner">
<span><p>We introduce two techniques, length perturbation and n-best based label
smoothing, to improve generalization of deep neural network (DNN) acoustic
models for automatic speech recognition (ASR). Length perturbation is a data
augmentation algorithm that randomly drops and inserts frames of an utterance
to alter the length of the speech feature sequence. N-best based label
smoothing randomly injects noise to ground truth labels during training in
order to avoid overfitting, where the noisy labels are generated from n-best
hypotheses. We evaluate these two techniques extensively on the 300-hour
Switchboard (SWB300) dataset and an in-house 500-hour Japanese (JPN500) dataset
using recurrent neural network transducer (RNNT) acoustic models for ASR. We
show that both techniques improve the generalization of RNNT models
individually and they can also be complementary. In particular, they yield good
improvements over a strong SWB300 baseline and give state-of-art performance on
SWB300 using RNNT models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizations of Complex Sequences of Family-Infant Vocalizations Using Bag-of-Audio-Words Approach Based on Wav2vec 2.0 Features. (arXiv:2203.15183v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15183">
<div class="article-summary-box-inner">
<span><p>In the U.S., approximately 15-17% of children 2-8 years of age are estimated
to have at least one diagnosed mental, behavioral or developmental disorder.
However, such disorders often go undiagnosed, and the ability to evaluate and
treat disorders in the first years of life is limited. To analyze infant
developmental changes, previous studies have shown advanced ML models excel at
classifying infant and/or parent vocalizations collected using cell phone,
video, or audio-only recording device like LENA. In this study, we pilot test
the audio component of a new infant wearable multi-modal device that we have
developed called LittleBeats (LB). LB audio pipeline is advanced in that it
provides reliable labels for both speaker diarization and vocalization
classification tasks, compared with other platforms that only record audio
and/or provide speaker diarization labels. We leverage wav2vec 2.0 to obtain
superior and more nuanced results with the LB family audio stream. We use a
bag-of-audio-words method with wav2vec 2.0 features to create high-level
visualizations to understand family-infant vocalization interactions. We
demonstrate that our high-quality visualizations capture major types of family
vocalization interactions, in categories indicative of mental, behavioral, and
developmental health, for both labeled and unlabeled LB audio.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shifted Chunk Encoder for Transformer Based Streaming End-to-End ASR. (arXiv:2203.15206v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15206">
<div class="article-summary-box-inner">
<span><p>Currently, there are mainly three Transformer encoder based streaming End to
End (E2E) Automatic Speech Recognition (ASR) approaches, namely time-restricted
methods, chunk-wise methods, and memory based methods. However, all of them
have some limitations in aspects of global context modeling, linear
computational complexity, and model parallelism. In this work, we aim to build
a single model to achieve the benefits of all the three aspects for streaming
E2E ASR. Particularly, we propose to use a shifted chunk mechanism instead of
the conventional chunk mechanism for streaming Transformer and Conformer. This
shifted chunk mechanism can significantly enhance modeling power through
allowing chunk self-attention to capture global context across local chunks,
while keeping linear computational complexity and parallel trainable. We name
the Shifted Chunk Transformer and Conformer as SChunk-Transofromer and
SChunk-Conformer, respectively. And we verify their performance on the widely
used AISHELL-1 benckmark. Experiments show that the SChunk-Transformer and
SChunk-Conformer achieve CER 6.43% and 5.77%, respectively. That surpasses the
existing chunk-wise and memory based methods by a large margin, and is
competitive even compared with the state-of-the-art time-restricted methods
which have quadratic computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of EEG frequency bands for Envisioned Speech Recognition. (arXiv:2203.15250v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15250">
<div class="article-summary-box-inner">
<span><p>The use of Automatic speech recognition (ASR) interfaces have become
increasingly popular in daily life for use in interaction and control of
electronic devices. The interfaces currently being used are not feasible for a
variety of users such as those suffering from a speech disorder, locked-in
syndrome, paralysis or people with utmost privacy requirements. In such cases,
an interface that can identify envisioned speech using electroencephalogram
(EEG) signals can be of great benefit. Various works targeting this problem
have been done in the past. However, there has been limited work in identifying
the frequency bands ($\delta, \theta, \alpha, \beta, \gamma$) of the EEG signal
that contribute towards envisioned speech recognition. Therefore, in this work,
we aim to analyze the significance of different EEG frequency bands and signals
obtained from different lobes of the brain and their contribution towards
recognizing envisioned speech. Signals obtained from different lobes and
bandpass filtered for different frequency bands are fed to a spatio-temporal
deep learning architecture with Convolutional Neural Network (CNN) and Long
Short-Term Memory (LSTM). The performance is evaluated on a publicly available
dataset comprising of three classification tasks - digit, character and images.
We obtain a classification accuracy of $85.93\%$, $87.27\%$ and $87.51\%$ for
the three tasks respectively. The code for the implementation has been made
available at https://github.com/ayushayt/ImaginedSpeechRecognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying Syntax$\unicode{x2013}$Prosody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis. (arXiv:2203.15276v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15276">
<div class="article-summary-box-inner">
<span><p>End-to-end text-to-speech synthesis (TTS), which generates speech sounds
directly from strings of texts or phonemes, has improved the quality of speech
synthesis over the conventional TTS. However, most previous studies have been
evaluated based on subjective naturalness and have not objectively examined
whether they can reproduce pitch patterns of phonological phenomena such as
downstep, rhythmic boost, and initial lowering that reflect syntactic
structures in Japanese. These phenomena can be linguistically explained by
phonological constraints and the syntax$\unicode{x2013}$prosody mapping
hypothesis (SPMH), which assumes projections from syntactic structures to
phonological hierarchy. Although some experiments in psycholinguistics have
verified the validity of the SPMH, it is crucial to investigate whether it can
be implemented in TTS. To synthesize linguistic phenomena involving syntactic
or phonological constraints, we propose a model using phonological symbols
based on the SPMH and prosodic well-formedness constraints. Experimental
results showed that the proposed method synthesized similar pitch patterns to
those reported in linguistics experiments for the phenomena of initial lowering
and rhythmic boost. The proposed model efficiently synthesizes phonological
phenomena in the test data that were not explicitly included in the training
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation. (arXiv:2203.15319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15319">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation (NMT) has reached a level of maturity to be
recognized as the premier method for the translation between different
languages and aroused interest in different research areas, including software
engineering. A key step to validate the robustness of the NMT models consists
in evaluating the performance of the models on adversarial inputs, i.e., inputs
obtained from the original ones by adding small amounts of perturbation.
However, when dealing with the specific task of the code generation (i.e., the
generation of code starting from a description in natural language), it has not
yet been defined an approach to validate the robustness of the NMT models. In
this work, we address the problem by identifying a set of perturbations and
metrics tailored for the robustness assessment of such models. We present a
preliminary experimental evaluation, showing what type of perturbations affect
the model the most and deriving useful insights for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noise-robust Speech Recognition with 10 Minutes Unparalleled In-domain Data. (arXiv:2203.15321v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15321">
<div class="article-summary-box-inner">
<span><p>Noise-robust speech recognition systems require large amounts of training
data including noisy speech data and corresponding transcripts to achieve
state-of-the-art performances in face of various practical environments.
However, such plenty of in-domain data is not always available in the real-life
world. In this paper, we propose a generative adversarial network to simulate
noisy spectrum from the clean spectrum (Simu-GAN), where only 10 minutes of
unparalleled in-domain noisy speech data is required as labels. Furthermore, we
also propose a dual-path speech recognition system to improve the robustness of
the system under noisy conditions. Experimental results show that the proposed
speech recognition system achieves 7.3% absolute improvement with simulated
noisy data by Simu-GAN over the best baseline in terms of word error rate
(WER).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Persian Relation Extraction Models by Data Augmentation. (arXiv:2203.15323v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15323">
<div class="article-summary-box-inner">
<span><p>Relation extraction that is the task of predicting semantic relation type
between entities in a sentence or document is an important task in natural
language processing. Although there are many researches and datasets for
English, Persian suffers from sufficient researches and comprehensive datasets.
The only available Persian dataset for this task is PERLEX, which is a Persian
expert-translated version of the SemEval-2010-Task-8 dataset. In this paper, we
present our augmented dataset and the results and findings of our system,
participated in the Persian relation Extraction shared task of NSURL 2021
workshop. We use PERLEX as the base dataset and enhance it by applying some
text preprocessing steps and by increasing its size via data augmentation
techniques to improve the generalization and robustness of applied models. We
then employ two different models including ParsBERT and multilingual BERT for
relation extraction on the augmented PERLEX dataset. Our best model obtained
64.67% of Macro-F1 on the test phase of the contest and it achieved 83.68% of
Macro-F1 on the test set of PERLEX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LDKP: A Dataset for Identifying Keyphrases from Long Scientific Documents. (arXiv:2203.15349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15349">
<div class="article-summary-box-inner">
<span><p>Identifying keyphrases (KPs) from text documents is a fundamental task in
natural language processing and information retrieval. Vast majority of the
benchmark datasets for this task are from the scientific domain containing only
the document title and abstract information. This limits keyphrase extraction
(KPE) and keyphrase generation (KPG) algorithms to identify keyphrases from
human-written summaries that are often very short (approx 8 sentences). This
presents three challenges for real-world applications: human-written summaries
are unavailable for most documents, the documents are almost always long, and a
high percentage of KPs are directly found beyond the limited context of title
and abstract. Therefore, we release two extensive corpora mapping KPs of ~1.3M
and ~100K scientific articles with their fully extracted text and additional
metadata including publication venue, year, author, field of study, and
citations for facilitating research on this real-world problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Inefficiency of Language Models in Scholarly Retrieval: An Experimental Walk-through. (arXiv:2203.15364v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15364">
<div class="article-summary-box-inner">
<span><p>Language models are increasingly becoming popular in AI-powered scientific IR
systems. This paper evaluates popular scientific language models in handling
(i) short-query texts and (ii) textual neighbors. Our experiments showcase the
inability to retrieve relevant documents for a short-query text even under the
most relaxed conditions. Additionally, we leverage textual neighbors, generated
by small perturbations to the original text, to demonstrate that not all
perturbations lead to close neighbors in the embedding space. Further, an
exhaustive categorization yields several classes of orthographically and
semantically related, partially related, and completely unrelated neighbors.
Retrieval performance turns out to be more influenced by the surface form
rather than the semantics of the text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Short-Term Word-Learning in a Dynamically Changing Environment. (arXiv:2203.15404v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15404">
<div class="article-summary-box-inner">
<span><p>Neural sequence-to-sequence automatic speech recognition (ASR) systems are in
principle open vocabulary systems, when using appropriate modeling units. In
practice, however, they often fail to recognize words not seen during training,
e.g., named entities, numbers or technical terms. To alleviate this problem,
Huber et al. proposed to supplement an end-to-end ASR system with a word/phrase
memory and a mechanism to access this memory to recognize the words and phrases
correctly. In this paper we study, a) methods to acquire important words for
this memory dynamically and, b) the trade-off between improvement in
recognition accuracy of new words and the potential danger of false alarms for
those added words. We demonstrate significant improvements in the detection
rate of new words with only a minor increase in false alarms (F1 score 0.30
$\rightarrow$ 0.80), when using an appropriate number of new words. In
addition, we show that important keywords can be extracted from supporting
documents and used effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality Assurance of Generative Dialog Models in an Evolving Conversational Agent Used for Swedish Language Practice. (arXiv:2203.15414v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15414">
<div class="article-summary-box-inner">
<span><p>Due to the migration megatrend, efficient and effective second-language
acquisition is vital. One proposed solution involves AI-enabled conversational
agents for person-centered interactive language practice. We present results
from ongoing action research targeting quality assurance of proprietary
generative dialog models trained for virtual job interviews. The action team
elicited a set of 38 requirements for which we designed corresponding automated
test cases for 15 of particular interest to the evolving solution. Our results
show that six of the test case designs can detect meaningful differences
between candidate models. While quality assurance of natural language
processing applications is complex, we provide initial steps toward an
automated framework for machine learning model selection in the context of an
evolving conversational agent. Future work will focus on model selection in an
MLOps setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic properties of English nominal pluralization: Insights from word embeddings. (arXiv:2203.15424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15424">
<div class="article-summary-box-inner">
<span><p>Semantic differentiation of nominal pluralization is grammaticalized in many
languages. For example, plural markers may only be relevant for human nouns.
English does not appear to make such distinctions. Using distributional
semantics, we show that English nominal pluralization exhibits semantic
clusters. For instance, pluralization of fruit words is more similar to one
another and less similar to pluralization of other semantic classes. Therefore,
reduction of the meaning shift in plural formation to the addition of an
abstract plural meaning is too simplistic. A semantically informed method,
called CosClassAvg, is introduced that outperforms pluralization methods in
distributional semantics which assume plural formation amounts to the addition
of a fixed plural vector. In comparison with our approach, a method from
compositional distributional semantics, called FRACSS, predicted plural vectors
that were more similar to the corpus-extracted plural vectors in terms of
direction but not vector length. A modeling study reveals that the observed
difference between the two predicted semantic spaces by CosClassAvg and FRACSS
carries over to how well a computational model of the listener can understand
previously unencountered plural forms. Mappings from word forms, represented
with triphone vectors, to predicted semantic vectors are more productive when
CosClassAvg-generated semantic vectors are employed as gold standard vectors
instead of FRACSS-generated vectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit. (arXiv:2203.15455v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15455">
<div class="article-summary-box-inner">
<span><p>Recently, we made available WeNet, a production-oriented end-to-end speech
recognition toolkit, which introduces a unified two-pass (U2) framework and a
built-in runtime to address the streaming and non-streaming decoding modes in a
single model. To further improve ASR performance and facilitate various
production requirements, in this paper, we present WeNet 2.0 with four
important updates. (1) We propose U2++, a unified two-pass framework with
bidirectional attention decoders, which includes the future contextual
information by a right-to-left attention decoder to improve the representative
ability of the shared encoder and the performance during the rescoring stage.
(2) We introduce an n-gram based language model and a WFST-based decoder into
WeNet 2.0, promoting the use of rich text data in production scenarios. (3) We
design a unified contextual biasing framework, which leverages user-specific
context (e.g., contact lists) to provide rapid adaptation ability for
production and improves ASR accuracy in both with-LM and without-LM scenarios.
(4) We design a unified IO to support large-scale data for effective model
training. In summary, the brand-new WeNet 2.0 achieves up to 10\% relative
recognition performance improvement over the original WeNet on various corpora
and makes available several important production-oriented features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation. (arXiv:2203.15479v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15479">
<div class="article-summary-box-inner">
<span><p>Speech segmentation, which splits long speech into short segments, is
essential for speech translation (ST). Popular VAD tools like WebRTC VAD have
generally relied on pause-based segmentation. Unfortunately, pauses in speech
do not necessarily match sentence boundaries, and sentences can be connected by
a very short pause that is difficult to detect by VAD. In this study, we
propose a speech segmentation method using a binary classification model
trained using a segmented bilingual speech corpus. We also propose a hybrid
method that combines VAD and the above speech segmentation method. Experimental
results revealed that the proposed method is more suitable for cascade and
end-to-end ST systems than conventional segmentation methods. The hybrid
approach further improved the translation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representing `how you say' with `what you say': English corpus of focused speech and text reflecting corresponding implications. (arXiv:2203.15483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15483">
<div class="article-summary-box-inner">
<span><p>In speech communication, how something is said (paralinguistic information)
is as crucial as what is said (linguistic information). As a type of
paralinguistic information, English speech uses sentence stress, the heaviest
prominence within a sentence, to convey emphasis. While different placements of
sentence stress communicate different emphatic implications, current speech
translation systems return the same translations if the utterances are
linguistically identical, losing paralinguistic information. Concentrating on
focus, a type of emphasis, we propose mapping paralinguistic information into
the linguistic domain within the source language using lexical and grammatical
devices. This method enables us to translate the paraphrased text
representations instead of the transcription of the original speech and obtain
translations that preserve paralinguistic information. As a first step, we
present the collection of an English corpus containing speech that differed in
the placement of focus along with the corresponding text, which was designed to
reflect the implied meaning of the speech. Also, analyses of our corpus
demonstrated that mapping of focus from the paralinguistic domain into the
linguistic domain involved various lexical and grammatical methods. The data
and insights from our analysis will further advance research into
paralinguistic translation. The corpus will be published via LDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Audio-text Representation for Automated Audio Captioning with Contrastive Learning. (arXiv:2203.15526v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15526">
<div class="article-summary-box-inner">
<span><p>Automated Audio captioning (AAC) is a cross-modal task that generates natural
language to describe the content of input audio. Most prior works usually
extract single-modality acoustic features and are therefore sub-optimal for the
cross-modal decoding task. In this work, we propose a novel AAC system called
CLIP-AAC to learn interactive cross-modality representation with both acoustic
and textual information. Specifically, the proposed CLIP-AAC introduces an
audio-head and a text-head in the pre-trained encoder to extract audio-text
information. Furthermore, we also apply contrastive learning to narrow the
domain difference by learning the correspondence between the audio signal and
its paired captions. Experimental results show that the proposed CLIP-AAC
approach surpasses the best baseline by a significant margin on the Clotho
dataset in terms of NLP evaluation metrics. The ablation study indicates that
both the pre-trained model and contrastive learning contribute to the
performance gain of the AAC model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Compute-Optimal Large Language Models. (arXiv:2203.15556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15556">
<div class="article-summary-box-inner">
<span><p>We investigate the optimal model size and number of tokens for training a
transformer language model under a given compute budget. We find that current
large language models are significantly undertrained, a consequence of the
recent focus on scaling language models whilst keeping the amount of training
data constant. By training over \nummodels language models ranging from 70
million to over 16 billion parameters on 5 to 500 billion tokens, we find that
for compute-optimal training, the model size and the number of training tokens
should be scaled equally: for every doubling of model size the number of
training tokens should also be doubled. We test this hypothesis by training a
predicted compute-optimal model, \chinchilla, that uses the same compute budget
as \gopher but with 70B parameters and 4$\times$ more more data. \chinchilla
uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B),
Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of
downstream evaluation tasks. This also means that \chinchilla uses
substantially less compute for fine-tuning and inference, greatly facilitating
downstream usage. As a highlight, \chinchilla reaches a state-of-the-art
average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\%
improvement over \gopher.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset for Speech Emotion Recognition in Greek Theatrical Plays. (arXiv:2203.15568v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15568">
<div class="article-summary-box-inner">
<span><p>Machine learning methodologies can be adopted in cultural applications and
propose new ways to distribute or even present the cultural content to the
public. For instance, speech analytics can be adopted to automatically generate
subtitles in theatrical plays, in order to (among other purposes) help people
with hearing loss. Apart from a typical speech-to-text transcription with
Automatic Speech Recognition (ASR), Speech Emotion Recognition (SER) can be
used to automatically predict the underlying emotional content of speech
dialogues in theatrical plays, and thus to provide a deeper understanding how
the actors utter their lines. However, real-world datasets from theatrical
plays are not available in the literature. In this work we present GreThE, the
Greek Theatrical Emotion dataset, a new publicly available data collection for
speech emotion recognition in Greek theatrical plays. The dataset contains
utterances from various actors and plays, along with respective valence and
arousal annotations. Towards this end, multiple annotators have been asked to
provide their input for each speech recording and inter-annotator agreement is
taken into account in the final ground truth generation. In addition, we
discuss the results of some indicative experiments that have been conducted
with machine and deep learning frameworks, using the dataset, along with some
widely used databases in the field of speech emotion recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subspace-based Representation and Learning for Phonotactic Spoken Language Recognition. (arXiv:2203.15576v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15576">
<div class="article-summary-box-inner">
<span><p>Phonotactic constraints can be employed to distinguish languages by
representing a speech utterance as a multinomial distribution or phone events.
In the present study, we propose a new learning mechanism based on
subspace-based representation, which can extract concealed phonotactic
structures from utterances, for language verification and dialect/accent
identification. The framework mainly involves two successive parts. The first
part involves subspace construction. Specifically, it decodes each utterance
into a sequence of vectors filled with phone-posteriors and transforms the
vector sequence into a linear orthogonal subspace based on low-rank matrix
factorization or dynamic linear modeling. The second part involves subspace
learning based on kernel machines, such as support vector machines and the
newly developed subspace-based neural networks (SNNs). The input layer of SNNs
is specifically designed for the sample represented by subspaces. The topology
ensures that the same output can be derived from identical subspaces by
modifying the conventional feed-forward pass to fit the mathematical definition
of subspace similarity. Evaluated on the "General LR" test of NIST LRE 2007,
the proposed method achieved up to 52%, 46%, 56%, and 27% relative reductions
in equal error rates over the sequence-based PPR-LM, PPR-VSM, and PPR-IVEC
methods and the lattice-based PPR-LM method, respectively. Furthermore, on the
dialect/accent identification task of NIST LRE 2009, the SNN-based system
performed better than the aforementioned four baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heuristic-based Inter-training to Improve Few-shot Multi-perspective Dialog Summarization. (arXiv:2203.15590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15590">
<div class="article-summary-box-inner">
<span><p>Many organizations require their customer-care agents to manually summarize
their conversations with customers. These summaries are vital for decision
making purposes of the organizations. The perspective of the summary that is
required to be created depends on the application of the summaries. With this
work, we study the multi-perspective summarization of customer-care
conversations between support agents and customers. We observe that there are
different heuristics that are associated with summaries of different
perspectives, and explore these heuristics to create weak-labeled data for
intermediate training of the models before fine-tuning with scarce human
annotated summaries. Most importantly, we show that our approach supports
models to generate multi-perspective summaries with a very small amount of
annotated data. For example, our approach achieves 94\% of the performance
(Rouge-2) of a model trained with the original data, by training only with 7\%
of the original data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Earnings-22: A Practical Benchmark for Accents in the Wild. (arXiv:2203.15591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15591">
<div class="article-summary-box-inner">
<span><p>Modern automatic speech recognition (ASR) systems have achieved superhuman
Word Error Rate (WER) on many common corpora despite lacking adequate
performance on speech in the wild. Beyond that, there is a lack of real-world,
accented corpora to properly benchmark academic and commercial models. To
ensure this type of speech is represented in ASR benchmarking, we present
Earnings-22, a 125 file, 119 hour corpus of English-language earnings calls
gathered from global companies. We run a comparison across 4 commercial models
showing the variation in performance when taking country of origin into
consideration. Looking at hypothesis transcriptions, we explore errors common
to all ASR systems tested. By examining Individual Word Error Rate (IWER), we
find that key speech features impact model performance more for certain accents
than others. Earnings-22 provides a free-to-use benchmark of real-world,
accented audio to bridge academic and industrial research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Improving Selective Prediction Ability of NLP Systems. (arXiv:2008.09371v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09371">
<div class="article-summary-box-inner">
<span><p>It's better to say "I can't answer" than to answer incorrectly. This
selective prediction ability is crucial for NLP systems to be reliably deployed
in real-world applications. Prior work has shown that existing selective
prediction techniques fail to perform well, especially in the out-of-domain
setting. In this work, we propose a method that improves probability estimates
of models by calibrating them using prediction confidence and difficulty score
of instances. Using these two signals, we first annotate held-out instances and
then train a calibrator to predict the likelihood of correctness of the model's
prediction. We instantiate our method with Natural Language Inference (NLI) and
Duplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and
Out-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the
representations learned by our calibrator result in an improvement of (15.81%,
5.64%) and (6.19%, 13.9%) over MaxProb -- a selective prediction baseline -- on
NLI and DD tasks respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10407">
<div class="article-summary-box-inner">
<span><p>The ability to quickly learn from a small quantity oftraining data widens the
range of machine learning applications. In this paper, we propose a
data-efficient image captioning model, VisualGPT, which leverages the
linguistic knowledge from a large pretrained language model(LM). A crucial
challenge is to balance between the use of visual information in the image and
prior linguistic knowledge acquired from pretraining. We designed a novel
self-resurrecting encoder-decoder attention mechanism to quickly adapt the
pretrained LM as the language decoder ona small amount of in-domain training
data. The proposed self-resurrecting activation unit produces sparse
activations but has reduced susceptibility to zero gradients. We train the
proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual
Captions training data. Under these conditions, we outperform the best baseline
model by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual
Captions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,
a medical report generation dataset. To the best of our knowledge, this is the
first work that improves data efficiency of image captioning by utilizing LM
pretrained on unimodal data. Our code is available at:
https://github.com/Vision-CAIR/VisualGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Transformers. (arXiv:2103.01209v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01209">
<div class="article-summary-box-inner">
<span><p>We introduce the GANformer, a novel and efficient type of transformer, and
explore it for the task of visual generative modeling. The network employs a
bipartite structure that enables long-range interactions across the image,
while maintaining computation of linear efficiency, that can readily scale to
high-resolution synthesis. It iteratively propagates information from a set of
latent variables to the evolving visual features and vice versa, to support the
refinement of each in light of the other and encourage the emergence of
compositional representations of objects and scenes. In contrast to the classic
transformer architecture, it utilizes multiplicative integration that allows
flexible region-based modulation, and can thus be seen as a generalization of
the successful StyleGAN network. We demonstrate the model's strength and
robustness through a careful evaluation over a range of datasets, from
simulated multi-object environments to rich real-world indoor and outdoor
scenes, showing it achieves state-of-the-art results in terms of image quality
and diversity, while enjoying fast learning and better data-efficiency. Further
qualitative and quantitative experiments offer us an insight into the model's
inner workings, revealing improved interpretability and stronger
disentanglement, and illustrating the benefits and efficacy of our approach. An
implementation of the model is available at
https://github.com/dorarad/gansformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Coreference Resolution Models through Active Learning. (arXiv:2104.07611v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07611">
<div class="article-summary-box-inner">
<span><p>Neural coreference resolution models trained on one dataset may not transfer
to new, low-resource domains. Active learning mitigates this problem by
sampling a small subset of data for annotators to label. While active learning
is well-defined for classification tasks, its application to coreference
resolution is neither well-defined nor fully understood. This paper explores
how to actively label coreference, examining sources of model uncertainty and
document reading costs. We compare uncertainty sampling strategies and their
advantages through thorough error analysis. In both synthetic and human
experiments, labeling spans within the same document is more effective than
annotating spans across documents. The findings contribute to a more realistic
development of coreference resolution models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Greedy-layer Pruning: Speeding up Transformer Models for Natural Language Processing. (arXiv:2105.14839v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14839">
<div class="article-summary-box-inner">
<span><p>Fine-tuning transformer models after unsupervised pre-training reaches a very
high performance on many different natural language processing tasks.
Unfortunately, transformers suffer from long inference times which greatly
increases costs in production. One possible solution is to use knowledge
distillation, which solves this problem by transferring information from large
teacher models to smaller student models. Knowledge distillation maintains high
performance and reaches high compression rates, nevertheless, the size of the
student model is fixed after pre-training and can not be changed individually
for a given downstream task and use-case to reach a desired performance/speedup
ratio. Another solution to reduce the size of models in a much more
fine-grained and computationally cheaper fashion is to prune layers after the
pre-training. The price to pay is that the performance of layer-wise pruning
algorithms is not on par with state-of-the-art knowledge distillation methods.
In this paper, Greedy-layer pruning is introduced to (1) outperform current
state-of-the-art for layer-wise pruning, (2) close the performance gap when
compared to knowledge distillation, while (3) providing a method to adapt the
model size dynamically to reach a desired performance/speedup tradeoff without
the need of additional pre-training phases. Our source code is available on
https://github.com/deepopinion/greedy-layer-pruning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emphasis control for parallel neural TTS. (arXiv:2110.03012v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03012">
<div class="article-summary-box-inner">
<span><p>Recent parallel neural text-to-speech (TTS) synthesis methods are able to
generate speech with high fidelity while maintaining high performance. However,
these systems often lack control over the output prosody, thus restricting the
semantic information conveyable for a given text. This paper proposes a
hierarchical parallel neural TTS system for prosodic emphasis control by
learning a latent space that directly corresponds to a change in emphasis.
Three candidate features for the latent space are compared: 1) Variance of
pitch and duration within words in a sentence, 2) Wavelet-based feature
computed from pitch, energy, and duration, and 3) Learned combination of the
two aforementioned approaches. At inference time, word-level prosodic emphasis
is achieved by increasing the feature values of the latent space for the given
words. Experiments show that all the proposed methods are able to achieve the
perception of increased emphasis with little loss in overall quality. Moreover,
emphasized utterances were preferred in a pairwise comparison test over the
non-emphasized utterances, indicating promise for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled dimensionality reduction for noise-robust speaker diarisation. (arXiv:2110.03380v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03380">
<div class="article-summary-box-inner">
<span><p>The objective of this work is to train noise-robust speaker embeddings
adapted for speaker diarisation. Speaker embeddings play a crucial role in the
performance of diarisation systems, but they often capture spurious information
such as noise and reverberation, adversely affecting performance. Our previous
work has proposed an auto-encoder-based dimensionality reduction module to help
remove the redundant information. However, they do not explicitly separate such
information and have also been found to be sensitive to hyper-parameter values.
To this end, we propose two contributions to overcome these issues: (i) a novel
dimensionality reduction framework that can disentangle spurious information
from the speaker embeddings; (ii) the use of a speech/non-speech indicator to
prevent the speaker code from representing the background noise. Through a
range of experiments conducted on four different datasets, our approach
consistently demonstrates the state-of-the-art performance among models without
system fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction. (arXiv:2110.06651v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06651">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction (KPE) automatically extracts phrases in a document that
provide a concise summary of the core content, which benefits downstream
information retrieval and NLP tasks. Previous state-of-the-art (SOTA) methods
select candidate keyphrases based on the similarity between learned
representations of the candidates and the document. They suffer performance
degradation on long documents due to discrepancy between sequence lengths which
causes mismatch between representations of keyphrase candidates and the
document. In this work, we propose a novel unsupervised embedding-based KPE
approach, Masked Document Embedding Rank (MDERank), to address this problem by
leveraging a mask strategy and ranking candidates by the similarity between
embeddings of the source document and the masked document. We further develop a
KPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised
contrastive learning method, which is more compatible to MDERank than vanilla
BERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the
proposed MDERank outperforms state-of-the-art unsupervised KPE approach by
average 1.80 $F1@15$ improvement. MDERank further benefits from KPEBERT and
overall achieves average 3.53 $F1@15$ improvement over the SOTA SIFRank. Our
code is available at \url{https://github.com/LinhanZ/mderank}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Start to Finish: Latency Reduction Strategies for Incremental Speech Synthesis in Simultaneous Speech-to-Speech Translation. (arXiv:2110.08214v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08214">
<div class="article-summary-box-inner">
<span><p>Speech-to-speech translation (S2ST) converts input speech to speech in
another language. A challenge of delivering S2ST in real time is the
accumulated delay between the translation and speech synthesis modules. While
recently incremental text-to-speech (iTTS) models have shown large quality
improvements, they typically require additional future text inputs to reach
optimal performance. In this work, we minimize the initial waiting time of iTTS
by adapting the upstream speech translator to generate high-quality pseudo
lookahead for the speech synthesizer. After mitigating the initial delay, we
demonstrate that the duration of synthesized speech also plays a crucial role
on latency. We formalize this as a latency metric and then present a simple yet
effective duration-scaling approach for latency reduction. Our approaches
consistently reduce latency by 0.2-0.5 second without sacrificing speech
translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Case-based Reasoning for Better Generalization in Textual Reinforcement Learning. (arXiv:2110.08470v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08470">
<div class="article-summary-box-inner">
<span><p>Text-based games (TBG) have emerged as promising environments for driving
research in grounded language understanding and studying problems like
generalization and sample efficiency. Several deep reinforcement learning (RL)
methods with varying architectures and learning schemes have been proposed for
TBGs. However, these methods fail to generalize efficiently, especially under
distributional shifts. In a departure from deep RL approaches, in this paper,
we propose a general method inspired by case-based reasoning to train agents
and generalize out of the training distribution. The case-based reasoner
collects instances of positive experiences from the agent's interaction with
the world in the past and later reuses the collected experiences to act
efficiently. The method can be applied in conjunction with any existing
on-policy neural agent in the literature for TBGs. Our experiments show that
the proposed approach consistently improves existing methods, obtains good
out-of-distribution generalization, and achieves new state-of-the-art results
on widely used environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Dialogue Response Generation. (arXiv:2110.08515v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08515">
<div class="article-summary-box-inner">
<span><p>Responsing with image has been recognized as an important capability for an
intelligent conversational agent. Yet existing works only focus on exploring
the multimodal dialogue models which depend on retrieval-based methods, but
neglecting generation methods. To fill in the gaps, we first present a
multimodal dialogue generation model, which takes the dialogue history as
input, then generates a textual sequence or an image as response. Learning such
a model often requires multimodal dialogues containing both texts and images
which are difficult to obtain. Motivated by the challenge in practice, we
consider multimodal dialogue generation under a natural assumption that only
limited training examples are available. In such a low-resource setting, we
devise a novel conversational agent, Divter, in order to isolate parameters
that depend on multimodal dialogues from the entire generation model. By this
means, the major part of the model can be learned from a large number of
text-only dialogues and text-image pairs respectively, then the whole
parameters can be well fitted using the limited training examples. Extensive
experiments demonstrate our method achieves state-of-the-art results in both
automatic and human evaluation, and can generate informative text and
high-resolution image responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Private Language Model Adaptation for Speech Recognition. (arXiv:2110.10026v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10026">
<div class="article-summary-box-inner">
<span><p>Speech model adaptation is crucial to handle the discrepancy between
server-side proxy training data and actual data received on local devices of
users. With the use of federated learning (FL), we introduce an efficient
approach on continuously adapting neural network language models (NNLMs) on
private devices with applications on automatic speech recognition (ASR). To
address the potential speech transcription errors in the on-device training
corpus, we perform empirical studies on comparing various strategies of
leveraging token-level confidence scores to improve the NNLM quality in the FL
settings. Experiments show that compared with no model adaptation, the proposed
method achieves relative 2.6% and 10.8% word error rate (WER) reductions on two
speech evaluation datasets, respectively. We also provide analysis in
evaluating privacy guarantees of our presented procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Captioner: Inducing Content-Style Separation in Vision-and-Language Model Training. (arXiv:2111.12727v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12727">
<div class="article-summary-box-inner">
<span><p>While captioning models have obtained compelling results in describing
natural images, there is a growing effort to increase their capability of
dealing with real-world concepts. In this paper, we address the task of
generating fluent descriptions by training on a non-uniform combination of data
sources, containing both human- and automatically-collected captions. To this
end, we propose a model which induces a separation between content and
descriptive style through the incorporation of stylistic parameters and
keywords extracted from large-scale multi-modal models as pivotal data. In
terms of visual features, our model avoids the need of object detectors and
employs grid-like features together with a single objective of prompt language
modeling. Experimentally, we consistently outperform existing methods in terms
of caption quality and capability of describing out-of-domain concepts.
Finally, our model obtains a new state of the art on both COCO and nocaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection. (arXiv:2111.14592v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14592">
<div class="article-summary-box-inner">
<span><p>Pre-trained models have proved to be powerful in enhancing task-oriented
dialog systems. However, current pre-training methods mainly focus on enhancing
dialog understanding and generation tasks while neglecting the exploitation of
dialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog
model that explicitly learns dialog policy from limited labeled dialogs and
large-scale unlabeled dialog corpora via semi-supervised learning.
Specifically, we introduce a dialog act prediction task for policy optimization
during pre-training and employ a consistency regularization term to refine the
learned representation with the help of unlabeled dialogs. We also implement a
gating mechanism to weigh suitable unlabeled dialog samples. Empirical results
show that GALAXY substantially improves the performance of task-oriented dialog
systems, and achieves new state-of-the-art results on benchmark datasets:
In-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores
by 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a
stronger few-shot ability than existing models under various low-resource
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Facial Representation Learning in a Visual-Linguistic Manner. (arXiv:2112.03109v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03109">
<div class="article-summary-box-inner">
<span><p>How to learn a universal facial representation that boosts all face analysis
tasks? This paper takes one step toward this goal. In this paper, we study the
transfer performance of pre-trained models on face analysis tasks and introduce
a framework, called FaRL, for general Facial Representation Learning in a
visual-linguistic manner. On one hand, the framework involves a contrastive
loss to learn high-level semantic meaning from image-text pairs. On the other
hand, we propose exploring low-level information simultaneously to further
enhance the face representation, by adding a masked image modeling. We perform
pre-training on LAION-FACE, a dataset containing large amount of face
image-text pairs, and evaluate the representation capability on multiple
downstream tasks. We show that FaRL achieves better transfer performance
compared with previous pre-trained models. We also verify its superiority in
the low-data regime. More importantly, our model surpasses the state-of-the-art
methods on face analysis tasks including face parsing and face alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PM-MMUT: Boosted Phone-mask Data Augmentation using Multi-Modeling Unit Training for Phonetic-Reduction-Robust E2E Speech Recognition. (arXiv:2112.06721v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06721">
<div class="article-summary-box-inner">
<span><p>Consonant and vowel reduction are often encountered in speech, which might
cause performance degradation in automatic speech recognition (ASR). Our
recently proposed learning strategy based on masking, Phone Masking Training
(PMT), alleviates the impact of such phenomenon in Uyghur ASR. Although PMT
achieves remarkably improvements, there still exists room for further gains due
to the granularity mismatch between the masking unit of PMT (phoneme) and the
modeling unit (word-piece). To boost the performance of PMT, we propose
multi-modeling unit training (MMUT) architecture fusion with PMT (PM-MMUT). The
idea of MMUT framework is to split the Encoder into two parts including
acoustic feature sequences to phoneme-level representation (AF-to-PLR) and
phoneme-level representation to word-piece-level representation (PLR-to-WPLR).
It allows AF-to-PLR to be optimized by an intermediate phoneme-based CTC loss
to learn the rich phoneme-level context information brought by PMT.
Experimental results on Uyghur ASR show that the proposed approaches outperform
obviously the pure PMT. We also conduct experiments on the 960-hour Librispeech
benchmark using ESPnet1, which achieves about 10% relative WER reduction on all
the test set without LM fusion comparing with the latest official ESPnet1
pre-trained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems. (arXiv:2112.08718v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08718">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems have found their use in numerous
industrial applications in very diverse domains creating a need to adapt to new
domains with small memory and deployment overhead. In this work, we introduce
domain prompts, a methodology that involves training a small number of domain
embedding parameters to prime a Transformer-based Language Model (LM) to a
particular domain. Using this domain-adapted LM for rescoring ASR hypotheses
can achieve 7-13% WER reduction for a new domain with just 1000 unlabeled
textual domain-specific sentences and a handful of additional parameters. Our
method can match or even beat the performance of models fully fine-tuned
towards a particular domain with only 0.02% of the parameters. Given the
parameter efficiency and negligible deployment overhead, our experiments
showcase that such a method is an ideal choice for on-the-fly adaptation of LMs
used in ASR systems to progressively scale it to new domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. (arXiv:2202.01279v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01279">
<div class="article-summary-box-inner">
<span><p>PromptSource is a system for creating, sharing, and using natural language
prompts. Prompts are functions that map an example from a dataset to a natural
language input and target output. Using prompts to train and query language
models is an emerging area in NLP that requires new tools that let users
develop and refine these prompts collaboratively. PromptSource addresses the
emergent challenges in this new setting with (1) a templating language for
defining data-linked prompts, (2) an interface that lets users quickly iterate
on prompt development by observing outputs of their prompts on many examples,
and (3) a community-driven set of guidelines for contributing new prompts to a
common pool. Over 2,000 prompts for roughly 170 datasets are already available
in PromptSource. PromptSource is available at
https://github.com/bigscience-workshop/promptsource.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02113">
<div class="article-summary-box-inner">
<span><p>Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHAS: Approaching optimal Segmentation for End-to-End Speech Translation. (arXiv:2202.04774v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04774">
<div class="article-summary-box-inner">
<span><p>Speech translation models are unable to directly process long audios, like
TED talks, which have to be split into shorter segments. Speech translation
datasets provide manual segmentations of the audios, which are not available in
real-world scenarios, and existing segmentation methods usually significantly
reduce translation quality at inference time. To bridge the gap between the
manual segmentation of training and the automatic one at inference, we propose
Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively
learn the optimal segmentation from any manually segmented speech corpus.
First, we train a classifier to identify the included frames in a segmentation,
using speech representations from a pre-trained wav2vec 2.0. The optimal
splitting points are then found by a probabilistic Divide-and-Conquer algorithm
that progressively splits at the frame of lowest probability until all segments
are below a pre-specified length. Experiments on MuST-C and mTEDx show that the
translation of the segments produced by our method approaches the quality of
the manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of
the manual segmentation's BLEU score, compared to the 87-93% of the best
existing methods. Our method is additionally generalizable to different domains
and achieves high zero-shot performance in unseen languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
derived from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps of the semantic features
between the textual question and visual answer, existing methods adopting
visual span predictor perform poorly in the TAGV task. To bridge these gaps, we
propose a visual-prompt text span localizing (VPTSL) method, which introduces
the timestamped subtitles as a passage to perform the text span localization
for the input text question, and prompts the visual highlight features into the
pre-trained language model (PLM) for enhancing the joint semantic
representations. Specifically, the context query attention is utilized to
perform cross-modal interaction between the extracted textual and visual
features. Then, the highlight features are obtained through the video-text
highlighting for the visual prompt. To alleviate semantic differences between
textual and visual features, we design the text span predictor by encoding the
question, the subtitles, and the prompted visual highlight features with the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the visual answer. Extensive experiments on the medical instructional
dataset, namely MedVidQA, show that the proposed VPTSL outperforms the
state-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,
which demonstrates the effectiveness of the proposed visual prompt and the text
span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Feasibility Study of Answer-Agnostic Question Generation for Education. (arXiv:2203.08685v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08685">
<div class="article-summary-box-inner">
<span><p>We conduct a feasibility study into the applicability of answer-agnostic
question generation models to textbook passages. We show that a significant
portion of errors in such systems arise from asking irrelevant or
uninterpretable questions and that such errors can be ameliorated by providing
summarized input. We find that giving these models human-written summaries
instead of the original text results in a significant increase in acceptability
of generated questions (33% $\rightarrow$ 83%) as determined by expert
annotators. We also find that, in the absence of human-written summaries,
automatic summarization can serve as a good middle ground.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Factually Grounded Content Transfer with Factual Ablation. (arXiv:2203.10133v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10133">
<div class="article-summary-box-inner">
<span><p>Despite recent success, large neural models often generate factually
incorrect text. Compounding this is the lack of a standard automatic evaluation
for factuality--it cannot be meaningfully improved if it cannot be measured.
Grounded generation promises a path to solving both of these problems: models
draw on a reliable external document (grounding) for factual information,
simplifying the challenge of factuality. Measuring factuality is also
simplified--to factual consistency, testing whether the generation agrees with
the grounding, rather than all facts. Yet, without a standard automatic metric
for factual consistency, factually grounded generation remains an open problem.
</p>
<p>We study this problem for content transfer, in which generations extend a
prompt, using information from factual grounding. Particularly, this domain
allows us to introduce the notion of factual ablation for automatically
measuring factual consistency: this captures the intuition that the model
should be less likely to produce an output given a less relevant grounding
document. In practice, we measure this by presenting a model with two grounding
documents, and the model should prefer to use the more factually relevant one.
We contribute two evaluation sets to measure this. Applying our new evaluation,
we propose multiple novel methods improving over strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriching Unsupervised User Embedding via Medical Concepts. (arXiv:2203.10627v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10627">
<div class="article-summary-box-inner">
<span><p>Clinical notes in Electronic Health Records (EHR) present rich documented
information of patients to inference phenotype for disease diagnosis and study
patient characteristics for cohort selection. Unsupervised user embedding aims
to encode patients into fixed-length vectors without human supervisions.
Medical concepts extracted from the clinical notes contain rich connections
between patients and their clinical categories. However, existing unsupervised
approaches of user embeddings from clinical notes do not explicitly incorporate
medical concepts. In this study, we propose a concept-aware unsupervised user
embedding that jointly leverages text documents and medical concepts from two
clinical corpora, MIMIC-III and Diabetes. We evaluate user embeddings on both
extrinsic and intrinsic tasks, including phenotype classification, in-hospital
mortality prediction, patient retrieval, and patient relatedness. Experiments
on the two clinical corpora show our approach exceeds unsupervised baselines,
and incorporating medical concepts can significantly improve the baseline
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12667">
<div class="article-summary-box-inner">
<span><p>A long-term goal of AI research is to build intelligent agents that can
communicate with humans in natural language, perceive the environment, and
perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental
and interdisciplinary research topic towards this goal, and receives increasing
attention from natural language processing, computer vision, robotics, and
machine learning communities. In this paper, we review contemporary studies in
the emerging field of VLN, covering tasks, evaluation metrics, methods, etc.
Through structured analysis of current progress and challenges, we highlight
the limitations of current VLN and opportunities for future work. This paper
serves as a thorough reference for the VLN research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo Label Is Better Than Human Label. (arXiv:2203.12668v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12668">
<div class="article-summary-box-inner">
<span><p>State-of-the-art automatic speech recognition (ASR) systems are trained with
tens of thousands of hours of labeled speech data. Human transcription is
expensive and time consuming. Factors such as the quality and consistency of
the transcription can greatly affect the performance of the ASR models trained
with these data. In this paper, we show that we can train a strong teacher
model to produce high quality pseudo labels by utilizing recent self-supervised
and semi-supervised learning techniques. Specifically, we use JUST (Joint
Unsupervised/Supervised Training) and iterative noisy student teacher training
to train a 600 million parameter bi-directional teacher model. This model
achieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively
better than a baseline. We further show that by using this strong teacher model
to generate high-quality pseudo labels for training, we can achieve 13.6%
relative WER reduction (5.9% to 5.1%) for a streaming model compared to using
human labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-text Retrieval in Context. (arXiv:2203.13645v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13645">
<div class="article-summary-box-inner">
<span><p>Audio-text retrieval based on natural language descriptions is a challenging
task. It involves learning cross-modality alignments between long sequences
under inadequate data conditions. In this work, we investigate several audio
features as well as sequence aggregation methods for better audio-text
alignment. Moreover, through a qualitative analysis we observe that semantic
mapping is more important than temporal relations in contextual retrieval.
Using pre-trained audio features and a descriptor-based aggregation method, we
build our contextual audio-text retrieval system. Specifically, we utilize
PANNs features pre-trained on a large sound event dataset and NetRVLAD pooling,
which directly works with averaged descriptors. Experiments are conducted on
the AudioCaps and CLOTHO datasets, and results are compared with the previous
state-of-the-art system. With our proposed system, a significant improvement
has been achieved on bidirectional audio-text retrieval, on all metrics
including recall, median and mean rank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MQDD: Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain. (arXiv:2203.14093v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14093">
<div class="article-summary-box-inner">
<span><p>This work proposes a new pipeline for leveraging data collected on the Stack
Overflow website for pre-training a multimodal model for searching duplicates
on question answering websites. Our multimodal model is trained on question
descriptions and source codes in multiple programming languages. We design two
new learning objectives to improve duplicate detection capabilities. The result
of this work is a mature, fine-tuned Multimodal Question Duplicity Detection
(MQDD) model, ready to be integrated into a Stack Overflow search system, where
it can help users find answers for already answered questions. Alongside the
MQDD model, we release two datasets related to the software engineering domain.
The first Stack Overflow Dataset (SOD) represents a massive corpus of paired
questions and answers. The second Stack Overflow Duplicity Dataset (SODD)
contains data for training duplicate detection models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Simultaneous Speech Translation. (arXiv:2203.14835v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14835">
<div class="article-summary-box-inner">
<span><p>Applications designed for simultaneous speech translation during events such
as conferences or meetings need to balance quality and lag while displaying
translated text to deliver a good user experience. One common approach to
building online spoken language translation systems is by leveraging models
built for offline speech translation. Based on a technique to adapt end-to-end
monolingual models, we investigate multilingual models and different
architectures (end-to-end and cascade) on the ability to perform online speech
translation. On the multilingual TEDx corpus, we show that the approach
generalizes to different architectures. We see similar gains in latency
reduction (40% relative) across languages and architectures. However, the
end-to-end architecture leads to smaller translation quality losses after
adapting to the online model. Furthermore, the approach even scales to
zero-shot directions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to segment fetal brain tissue from noisy annotations. (arXiv:2203.14962v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14962">
<div class="article-summary-box-inner">
<span><p>Automatic fetal brain tissue segmentation can enhance the quantitative
assessment of brain development at this critical stage. Deep learning methods
represent the state of the art in medical image segmentation and have also
achieved impressive results in brain segmentation. However, effective training
of a deep learning model to perform this task requires a large number of
training images to represent the rapid development of the transient fetal brain
structures. On the other hand, manual multi-label segmentation of a large
number of 3D images is prohibitive. To address this challenge, we segmented 272
training images, covering 19-39 gestational weeks, using an automatic
multi-atlas segmentation strategy based on deformable registration and
probabilistic atlas fusion, and manually corrected large errors in those
segmentations. Since this process generated a large training dataset with noisy
segmentations, we developed a novel label smoothing procedure and a loss
function to train a deep learning model with smoothed noisy segmentations. Our
proposed methods properly account for the uncertainty in tissue boundaries. We
evaluated our method on 23 manually-segmented test images of a separate set of
fetuses. Results show that our method achieves an average Dice similarity
coefficient of 0.893 and 0.916 for the transient structures of younger and
older fetuses, respectively. Our method generated results that were
significantly more accurate than several state-of-the-art methods including
nnU-Net that achieved the closest results to our method. Our trained model can
serve as a valuable tool to enhance the accuracy and reproducibility of fetal
brain analysis in MRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TL-GAN: Improving Traffic Light Recognition via Data Synthesis for Autonomous Driving. (arXiv:2203.15006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15006">
<div class="article-summary-box-inner">
<span><p>Traffic light recognition, as a critical component of the perception module
of self-driving vehicles, plays a vital role in the intelligent transportation
systems. The prevalent deep learning based traffic light recognition methods
heavily hinge on the large quantity and rich diversity of training data.
However, it is quite challenging to collect data in various rare scenarios such
as flashing, blackout or extreme weather, thus resulting in the imbalanced
distribution of training data and consequently the degraded performance in
recognizing rare classes. In this paper, we seek to improve traffic light
recognition by leveraging data synthesis. Inspired by the generative
adversarial networks (GANs), we propose a novel traffic light generation
approach TL-GAN to synthesize the data of rare classes to improve traffic light
recognition for autonomous driving. TL-GAN disentangles traffic light sequence
generation into image synthesis and sequence assembling. In the image synthesis
stage, our approach enables conditional generation to allow full control of the
color of the generated traffic light images. In the sequence assembling stage,
we design the style mixing and adaptive template to synthesize realistic and
diverse traffic light sequences. Extensive experiments show that the proposed
TL-GAN renders remarkable improvement over the baseline without using the
generated data, leading to the state-of-the-art performance in comparison with
the competing algorithms that are used for general image synthesis and data
imbalance tackling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Registering Explicit to Implicit: Towards High-Fidelity Garment mesh Reconstruction from Single Images. (arXiv:2203.15007v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15007">
<div class="article-summary-box-inner">
<span><p>Fueled by the power of deep learning techniques and implicit shape learning,
recent advances in single-image human digitalization have reached unprecedented
accuracy and could recover fine-grained surface details such as garment
wrinkles. However, a common problem for the implicit-based methods is that they
cannot produce separated and topology-consistent mesh for each garment piece,
which is crucial for the current 3D content creation pipeline. To address this
issue, we proposed a novel geometry inference framework ReEF that reconstructs
topology-consistent layered garment mesh by registering the explicit garment
template to the whole-body implicit fields predicted from single images.
Experiments demonstrate that our method notably outperforms its counterparts on
single-image layered garment reconstruction and could bring high-quality
digital assets for further content creation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Interactive Learning-based ovarian cancer segmentation of H&E-stained whole slide images to study morphological patterns of BRCA mutation. (arXiv:2203.15015v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15015">
<div class="article-summary-box-inner">
<span><p>Deep learning has been widely used to analyze digitized hematoxylin and eosin
(H&amp;E)-stained histopathology whole slide images. Automated cancer segmentation
using deep learning can be used to diagnose malignancy and to find novel
morphological patterns to predict molecular subtypes. To train pixel-wise
cancer segmentation models, manual annotation from pathologists is generally a
bottleneck due to its time-consuming nature. In this paper, we propose Deep
Interactive Learning with a pretrained segmentation model from a different
cancer type to reduce manual annotation time. Instead of annotating all pixels
from cancer and non-cancer regions on giga-pixel whole slide images, an
iterative process of annotating mislabeled regions from a segmentation model
and training/finetuning the model with the additional annotation can reduce the
time. Especially, employing a pretrained segmentation model can further reduce
the time than starting annotation from scratch. We trained an accurate ovarian
cancer segmentation model with a pretrained breast segmentation model by 3.5
hours of manual annotation which achieved intersection-over-union of 0.74,
recall of 0.86, and precision of 0.84. With automatically extracted high-grade
serous ovarian cancer patches, we attempted to train another deep learning
model to predict BRCA mutation. The segmentation model and code have been
released at https://github.com/MSKCC-Computational-Pathology/DMMN-ovary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Object Detection with Fully Cross-Transformer. (arXiv:2203.15021v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15021">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection (FSOD), with the aim to detect novel objects using
very few training examples, has recently attracted great research interest in
the community. Metric-learning based methods have been demonstrated to be
effective for this task using a two-branch based siamese network, and calculate
the similarity between image regions and few-shot examples for detection.
However, in previous works, the interaction between the two branches is only
restricted in the detection head, while leaving the remaining hundreds of
layers for separate feature extraction. Inspired by the recent work on vision
transformers and vision-language transformers, we propose a novel Fully
Cross-Transformer based model (FCT) for FSOD by incorporating cross-transformer
into both the feature backbone and detection head. The asymmetric-batched
cross-attention is proposed to aggregate the key information from the two
branches with different batch sizes. Our model can improve the few-shot
similarity learning between the two branches by introducing the multi-level
interactions. Comprehensive experiments on both PASCAL VOC and MSCOCO FSOD
benchmarks demonstrate the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A systematic review and meta-analysis of Digital Elevation Model (DEM) fusion: pre-processing, methods and applications. (arXiv:2203.15026v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15026">
<div class="article-summary-box-inner">
<span><p>The remote sensing community has identified data fusion as one of the key
challenging topics of the 21st century. The subject of image fusion in
two-dimensional (2D) space has been covered in several published reviews.
However, the special case of 2.5D/3D Digital Elevation Model (DEM) fusion has
not been addressed till date. DEM fusion is a key application of data fusion in
remote sensing. It takes advantage of the complementary characteristics of
multi-source DEMs to deliver a more complete, accurate and reliable elevation
dataset. Although several methods for fusing DEMs have been developed, the
absence of a well-rounded review has limited their proliferation among
researchers and end-users. It is often required to combine knowledge from
multiple studies to inform a holistic perspective and guide further research.
In response, this paper provides a systematic review of DEM fusion: the
pre-processing workflow, methods and applications, enhanced with a
meta-analysis. Through the discussion and comparative analysis, unresolved
challenges and open issues were identified, and future directions for research
were proposed. This review is a timely solution and an invaluable source of
information for researchers within the fields of remote sensing and spatial
information science, and the data fusion community at large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of Demonstrations for Social Navigation. (arXiv:2203.15041v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15041">
<div class="article-summary-box-inner">
<span><p>Social navigation is the capability of an autonomous agent, such as a robot,
to navigate in a 'socially compliant' manner in the presence of other
intelligent agents such as humans. With the emergence of autonomously
navigating mobile robots in human populated environments (e.g., domestic
service robots in homes and restaurants and food delivery robots on public
sidewalks), incorporating socially compliant navigation behaviors on these
robots becomes critical to ensuring safe and comfortable human robot
coexistence. To address this challenge, imitation learning is a promising
framework, since it is easier for humans to demonstrate the task of social
navigation rather than to formulate reward functions that accurately capture
the complex multi objective setting of social navigation. The use of imitation
learning and inverse reinforcement learning to social navigation for mobile
robots, however, is currently hindered by a lack of large scale datasets that
capture socially compliant robot navigation demonstrations in the wild. To fill
this gap, we introduce Socially CompliAnt Navigation Dataset (SCAND) a large
scale, first person view dataset of socially compliant navigation
demonstrations. Our dataset contains 8.7 hours, 138 trajectories, 25 miles of
socially compliant, human teleoperated driving demonstrations that comprises
multi modal data streams including 3D lidar, joystick commands, odometry,
visual and inertial information, collected on two morphologically different
mobile robots a Boston Dynamics Spot and a Clearpath Jackal by four different
human demonstrators in both indoor and outdoor environments. We additionally
perform preliminary analysis and validation through real world robot
experiments and show that navigation policies learned by imitation learning on
SCAND generate socially compliant behaviors
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A distribution-dependent Mumford-Shah model for unsupervised hyperspectral image segmentation. (arXiv:2203.15058v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15058">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images provide a rich representation of the underlying spectrum
for each pixel, allowing for a pixel-wise classification/segmentation into
different classes. As the acquisition of labeled training data is very
time-consuming, unsupervised methods become crucial in hyperspectral image
analysis. The spectral variability and noise in hyperspectral data make this
task very challenging and define special requirements for such methods.
</p>
<p>Here, we present a novel unsupervised hyperspectral segmentation framework.
It starts with a denoising and dimensionality reduction step by the
well-established Minimum Noise Fraction (MNF) transform. Then, the Mumford-Shah
(MS) segmentation functional is applied to segment the data. We equipped the MS
functional with a novel robust distribution-dependent indicator function
designed to handle the characteristic challenges of hyperspectral data. To
optimize our objective function with respect to the parameters for which no
closed form solution is available, we propose an efficient fixed point
iteration scheme. Numerical experiments on four public benchmark datasets show
that our method produces competitive results, which outperform two
state-of-the-art methods substantially on three of these datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Technique using a Sequence of Follow Up X-Rays for Disease classification. (arXiv:2203.15060v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15060">
<div class="article-summary-box-inner">
<span><p>The ability to predict lung and heart based diseases using deep learning
techniques is central to many researchers, particularly in the medical field
around the world. In this paper, we present a unique outlook of a very familiar
problem of disease classification using X-rays. We present a hypothesis that
X-rays of patients included with the follow up history of their most recent
three chest X-ray images would perform better in disease classification in
comparison to one chest X-ray image input using an internal CNN to perform
feature extraction. We have discovered that our generic deep learning
architecture which we propose for solving this problem performs well with 3
input X ray images provided per sample for each patient. In this paper, we have
also established that without additional layers before the output
classification, the CNN models will improve the performance of predicting the
disease labels for each patient. We have provided our results in ROC curves and
AUROC scores. We define a fresh approach of collecting three X-ray images for
training deep learning models, which we have concluded has clearly improved the
performance of the models. We have shown that ResNet, in general, has a better
result than any other CNN model used in the feature extraction phase. With our
original approach to data pre-processing, image training, and pre-trained
models, we believe that the current research will assist many medical
institutions around the world, and this will improve the prediction of
patients' symptoms and diagnose them with more accurate cure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cycle-Consistent Counterfactuals by Latent Transformations. (arXiv:2203.15064v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15064">
<div class="article-summary-box-inner">
<span><p>CounterFactual (CF) visual explanations try to find images similar to the
query image that change the decision of a vision system to a specified outcome.
Existing methods either require inference-time optimization or joint training
with a generative adversarial model which makes them time-consuming and
difficult to use in practice. We propose a novel approach, Cycle-Consistent
Counterfactuals by Latent Transformations (C3LT), which learns a latent
transformation that automatically generates visual CFs by steering in the
latent space of generative models. Our method uses cycle consistency between
the query and CF latent representations which helps our training to find better
solutions. C3LT can be easily plugged into any state-of-the-art pretrained
generative network. This enables our method to generate high-quality and
interpretable CF images at high resolution such as those in ImageNet. In
addition to several established metrics for evaluating CF explanations, we
introduce a novel metric tailored to assess the quality of the generated CF
examples and validate the effectiveness of our method on an extensive set of
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepShadow: Neural Shape from Shadow. (arXiv:2203.15065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15065">
<div class="article-summary-box-inner">
<span><p>This paper presents DeepShadow, a one-shot method for recovering the depth
map and surface normals from photometric stereo shadow maps. Previous works
that try to recover the surface normals from photometric stereo images treat
cast shadows as a disturbance. We show that the self and cast shadows not only
do not disturb 3D reconstruction, but can be used alone, as a strong learning
signal, to recover the depth map and surface normals. We demonstrate that 3D
reconstruction from shadows can even outperform shape-from-shading in certain
cases. To the best of our knowledge, our method is the first to reconstruct 3D
shape-from-shadows using neural networks. The method does not require any
pre-training or expensive labeled data, and is optimized during inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Verification Bypass. (arXiv:2203.15068v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15068">
<div class="article-summary-box-inner">
<span><p>Face verification systems aim to validate the claimed identity using feature
vectors and distance metrics. However, no attempt has been made to bypass such
a system using generated images that are constrained by the same feature
vectors. In this work, we train StarGAN v2 to generate diverse images based on
a human user, that have similar feature vectors yet qualitatively look
different. We then demonstrate a proof of concept on a custom face verification
system and verify our claims by demonstrating the same proof of concept in a
black box setting on dating applications that utilize similar face verification
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Motion Correction Via Iterative Nonlinear Optimization and Animation. (arXiv:2203.15072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15072">
<div class="article-summary-box-inner">
<span><p>Here, we present an end-to-end method to create 2D animation for a goalkeeper
attempting to block a penalty kick, and then correct that motion using an
iterative nonlinear optimization scheme. The input is a raw video that is fed
into pose and object detection networks to find the skeleton of the goalkeeper
and the ball. The output is a set of key frames of the skeleton associated with
the corrected motion so that if the goalkeeper missed the ball, the animation
will show then successfully deflecting it. Our method is robust enough correct
different kinds of mistakes the goalkeeper can make, such as not lunging far
enough or jumping to the incorrect side. Our method is also meant to be
semantically similar to the goalkeeper's original motion, which helps keep our
animation grounded with respect to actual human behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neurosymbolic hybrid approach to driver collision warning. (arXiv:2203.15076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15076">
<div class="article-summary-box-inner">
<span><p>There are two main algorithmic approaches to autonomous driving systems: (1)
An end-to-end system in which a single deep neural network learns to map
sensory input directly into appropriate warning and driving responses. (2) A
mediated hybrid recognition system in which a system is created by combining
independent modules that detect each semantic feature. While some researchers
believe that deep learning can solve any problem, others believe that a more
engineered and symbolic approach is needed to cope with complex environments
with less data. Deep learning alone has achieved state-of-the-art results in
many areas, from complex gameplay to predicting protein structures. In
particular, in image classification and recognition, deep learning models have
achieved accuracies as high as humans. But sometimes it can be very difficult
to debug if the deep learning model doesn't work. Deep learning models can be
vulnerable and are very sensitive to changes in data distribution.
Generalization can be problematic. It's usually hard to prove why it works or
doesn't. Deep learning models can also be vulnerable to adversarial attacks.
Here, we combine deep learning-based object recognition and tracking with an
adaptive neurosymbolic network agent, called the Non-Axiomatic Reasoning System
(NARS), that can adapt to its environment by building concepts based on
perceptual sequences. We achieved an improved intersection-over-union (IOU)
object recognition performance of 0.65 in the adaptive retraining model
compared to IOU 0.31 in the COCO data pre-trained model. We improved the object
detection limits using RADAR sensors in a simulated environment, and
demonstrated the weaving car detection capability by combining deep
learning-based object detection and tracking with a neurosymbolic model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CD-Net: Histopathology Representation Learning using Pyramidal Context-Detail Network. (arXiv:2203.15078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15078">
<div class="article-summary-box-inner">
<span><p>Extracting rich phenotype information, such as cell density and arrangement,
from whole slide histology images (WSIs), requires analysis of large field of
view, i.e more contexual information. This can be achieved through analyzing
the digital slides at lower resolution. A potential drawback is missing out on
details present at a higher resolution. To jointly leverage complementary
information from multiple resolutions, we present a novel transformer based
Pyramidal Context-Detail Network (CD-Net). CD-Net exploits the WSI pyramidal
structure through co-training of proposed Context and Detail Modules, which
operate on inputs from multiple resolutions. The residual connections between
the modules enable the joint training paradigm while learning self-supervised
representation for WSIs. The efficacy of CD-Net is demonstrated in classifying
Lung Adenocarcinoma from Squamous cell carcinoma.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative, Deep Synthetic Aperture Sonar Image Segmentation. (arXiv:2203.15082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15082">
<div class="article-summary-box-inner">
<span><p>Synthetic aperture sonar (SAS) systems produce high-resolution images of the
seabed environment. Moreover, deep learning has demonstrated superior ability
in finding robust features for automating imagery analysis. However, the
success of deep learning is conditioned on having lots of labeled training
data, but obtaining generous pixel-level annotations of SAS imagery is often
practically infeasible. This challenge has thus far limited the adoption of
deep learning methods for SAS segmentation. Algorithms exist to segment SAS
imagery in an unsupervised manner, but they lack the benefit of
state-of-the-art learning methods and the results present significant room for
improvement. In view of the above, we propose a new iterative algorithm for
unsupervised SAS image segmentation combining superpixel formation, deep
learning, and traditional clustering methods. We call our method Iterative Deep
Unsupervised Segmentation (IDUS). IDUS is an unsupervised learning framework
that can be divided into four main steps: 1) A deep network estimates class
assignments. 2) Low-level image features from the deep network are clustered
into superpixels. 3) Superpixels are clustered into class assignments (which we
call pseudo-labels) using $k$-means. 4) Resulting pseudo-labels are used for
loss backpropagation of the deep network prediction. These four steps are
performed iteratively until convergence. A comparison of IDUS to current
state-of-the-art methods on a realistic benchmark dataset for SAS image
segmentation demonstrates the benefits of our proposal even as the IDUS incurs
a much lower computational burden during inference (actual labeling of a test
image). Finally, we also develop a semi-supervised (SS) extension of IDUS
called IDSS and demonstrate experimentally that it can further enhance
performance while outperforming supervised alternatives that exploit the same
labeled training imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval. (arXiv:2203.15086v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15086">
<div class="article-summary-box-inner">
<span><p>In text-video retrieval, the objective is to learn a cross-modal similarity
function between a text and a video that ranks relevant text-video pairs higher
than irrelevant pairs. However, videos inherently express a much wider gamut of
information than texts. Instead, texts often capture sub-regions of entire
videos and are most semantically similar to certain frames within videos.
Therefore, for a given text, a retrieval model should focus on the text's most
semantically similar video sub-regions to make a more relevant comparison. Yet,
most existing works aggregate entire videos without directly considering text.
Common text-agnostic aggregations schemes include mean-pooling or
self-attention over the frames, but these are likely to encode misleading
visual information not described in the given text. To address this, we propose
a cross-modal attention model called X-Pool that reasons between a text and the
frames of a video. Our core mechanism is a scaled dot product attention for a
text to attend to its most semantically similar frames. We then generate an
aggregated video representation conditioned on the text's attention weights
over the frames. We evaluate our method on three benchmark datasets of MSR-VTT,
MSVD and LSMDC, achieving new state-of-the-art results by up to 12% in relative
improvement in Recall@1. Our findings thereby highlight the importance of joint
text-video reasoning to extract important visual cues according to text. Full
code and demo can be found at: https://layer6ai-labs.github.io/xpool/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Optical Flow, Depth, and Scene Flow without Real-World Labels. (arXiv:2203.15089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15089">
<div class="article-summary-box-inner">
<span><p>Self-supervised monocular depth estimation enables robots to learn 3D
perception from raw video streams. This scalable approach leverages projective
geometry and ego-motion to learn via view synthesis, assuming the world is
mostly static. Dynamic scenes, which are common in autonomous driving and
human-robot interaction, violate this assumption. Therefore, they require
modeling dynamic objects explicitly, for instance via estimating pixel-wise 3D
motion, i.e. scene flow. However, the simultaneous self-supervised learning of
depth and scene flow is ill-posed, as there are infinitely many combinations
that result in the same 3D point. In this paper we propose DRAFT, a new method
capable of jointly learning depth, optical flow, and scene flow by combining
synthetic data with geometric self-supervision. Building upon the RAFT
architecture, we learn optical flow as an intermediate task to bootstrap depth
and scene flow learning via triangulation. Our algorithm also leverages
temporal and geometric consistency losses across tasks to improve multi-task
learning. Our DRAFT architecture simultaneously establishes a new state of the
art in all three tasks in the self-supervised monocular setting on the standard
KITTI benchmark. Project page: https://sites.google.com/tri.global/draft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">New pyramidal hybrid textural and deep features based automatic skin cancer classification model: Ensemble DarkNet and textural feature extractor. (arXiv:2203.15090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15090">
<div class="article-summary-box-inner">
<span><p>Background: Skin cancer is one of the widely seen cancer worldwide and
automatic classification of skin cancer can be benefited dermatology clinics
for an accurate diagnosis. Hence, a machine learning-based automatic skin
cancer detection model must be developed. Material and Method: This research
interests to overcome automatic skin cancer detection problem. A colored skin
cancer image dataset is used. This dataset contains 3297 images with two
classes. An automatic multilevel textural and deep features-based model is
presented. Multilevel fuse feature generation using discrete wavelet transform
(DWT), local phase quantization (LPQ), local binary pattern (LBP), pre-trained
DarkNet19, and DarkNet53 are utilized to generate features of the skin cancer
images, top 1000 features are selected threshold value-based neighborhood
component analysis (NCA). The chosen top 1000 features are classified using the
10-fold cross-validation technique. Results: To obtain results, ten-fold
cross-validation is used and 91.54% classification accuracy results are
obtained by using the recommended pyramidal hybrid feature generator and NCA
selector-based model. Further, various training and testing separation ratios
(90:10, 80:20, 70:30, 60:40, 50:50) are used and the maximum classification
rate is calculated as 95.74% using the 90:10 separation ratio. Conclusions: The
findings and accuracies calculated are denoted that this model can be used in
dermatology and pathology clinics to simplify the skin cancer detection process
and help physicians.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding out-of-distribution accuracies through quantifying difficulty of test samples. (arXiv:2203.15100v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15100">
<div class="article-summary-box-inner">
<span><p>Existing works show that although modern neural networks achieve remarkable
generalization performance on the in-distribution (ID) dataset, the accuracy
drops significantly on the out-of-distribution (OOD) datasets
\cite{recht2018cifar, recht2019imagenet}. To understand why a variety of models
consistently make more mistakes in the OOD datasets, we propose a new metric to
quantify the difficulty of the test images (either ID or OOD) that depends on
the interaction of the training dataset and the model. In particular, we
introduce \textit{confusion score} as a label-free measure of image difficulty
which quantifies the amount of disagreement on a given test image based on the
class conditional probabilities estimated by an ensemble of trained models.
Using the confusion score, we investigate CIFAR-10 and its OOD derivatives.
Next, by partitioning test and OOD datasets via their confusion scores, we
predict the relationship between ID and OOD accuracies for various
architectures. This allows us to obtain an estimator of the OOD accuracy of a
given model only using ID test labels. Our observations indicate that the
biggest contribution to the accuracy drop comes from images with high confusion
scores. Upon further inspection, we report on the nature of the misclassified
images grouped by their confusion scores: \textit{(i)} images with high
confusion scores contain \textit{weak spurious correlations} that appear in
multiple classes in the training data and lack clear \textit{class-specific
features}, and \textit{(ii)} images with low confusion scores exhibit spurious
correlations that belong to another class, namely \textit{class-specific
spurious correlations}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Semantic Segmentation: A Prototype View. (arXiv:2203.15102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15102">
<div class="article-summary-box-inner">
<span><p>Prevalent semantic segmentation solutions, despite their different network
designs (FCN based or attention based) and mask decoding strategies (parametric
softmax based or pixel-query based), can be placed in one category, by
considering the softmax weights or query vectors as learnable class prototypes.
In light of this prototype view, this study uncovers several limitations of
such parametric segmentation regime, and proposes a nonparametric alternative
based on non-learnable prototypes. Instead of prior methods learning a single
weight/query vector for each class in a fully parametric manner, our model
represents each class as a set of non-learnable prototypes, relying solely on
the mean features of several training pixels within that class. The dense
prediction is thus achieved by nonparametric nearest prototype retrieving. This
allows our model to directly shape the pixel embedding space, by optimizing the
arrangement between embedded pixels and anchored prototypes. It is able to
handle arbitrary number of classes with a constant amount of learnable
parameters. We empirically show that, with FCN based and attention based
segmentation models (i.e., HRNet, Swin, SegFormer) and backbones (i.e., ResNet,
HRNet, Swin, MiT), our nonparametric framework yields compelling results over
several datasets (i.e., ADE20K, Cityscapes, COCO-Stuff), and performs well in
the large-vocabulary situation. We expect this work will provoke a rethink of
the current de facto semantic segmentation model design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiDAR Snowfall Simulation for Robust 3D Object Detection. (arXiv:2203.15118v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15118">
<div class="article-summary-box-inner">
<span><p>3D object detection is a central task for applications such as autonomous
driving, in which the system needs to localize and classify surrounding traffic
agents, even in the presence of adverse weather. In this paper, we address the
problem of LiDAR-based 3D object detection under snowfall. Due to the
difficulty of collecting and annotating training data in this setting, we
propose a physically based method to simulate the effect of snowfall on real
clear-weather LiDAR point clouds. Our method samples snow particles in 2D space
for each LiDAR line and uses the induced geometry to modify the measurement for
each LiDAR beam accordingly. Moreover, as snowfall often causes wetness on the
ground, we also simulate ground wetness on LiDAR point clouds. We use our
simulation to generate partially synthetic snowy LiDAR data and leverage these
data for training 3D object detection models that are robust to snowfall. We
conduct an extensive evaluation using several state-of-the-art 3D object
detection methods and show that our simulation consistently yields significant
performance gains on the real snowy STF dataset compared to clear-weather
baselines and competing simulation approaches, while not sacrificing
performance in clear weather. Our code is available at
www.github.com/SysCV/LiDAR_snow_sim.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Odometry for RGB-D Cameras. (arXiv:2203.15119v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15119">
<div class="article-summary-box-inner">
<span><p>Visual odometry is the process of estimating the position and orientation of
a camera by analyzing the images associated to it. This paper develops a quick
and accurate approach to visual odometry of a moving RGB-D camera navigating on
a static environment. The proposed algorithm uses SURF (Speeded Up Robust
Features) as feature extractor, RANSAC (Random Sample Consensus) to filter the
results and Minimum Mean Square to estimate the rigid transformation of six
parameters between successive video frames. Data from a Kinect camera were used
in the tests. The results show that this approach is feasible and promising,
surpassing in performance the algorithms ICP (Interactive Closest Point) and
SfM (Structure from Motion) in tests using a publicly available dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15125">
<div class="article-summary-box-inner">
<span><p>Natural language-based communication with mobile devices and home appliances
is becoming increasingly popular and has the potential to become natural for
communicating with mobile robots in the future. Towards this goal, we
investigate cross-modal text-to-point-cloud localization that will allow us to
specify, for example, a vehicle pick-up or goods delivery location. In
particular, we propose Text2Pos, a cross-modal localization module that learns
to align textual descriptions with localization cues in a coarse- to-fine
manner. Given a point cloud of the environment, Text2Pos locates a position
that is specified via a natural language-based description of the immediate
surroundings. To train Text2Pos and study its performance, we construct
KITTI360Pose, the first dataset for this task based on the recently introduced
KITTI360 dataset. Our experiments show that we can localize 65% of textual
queries within 15m distance to query locations for top-10 retrieved locations.
This is a starting point that we hope will spark future developments towards
language-based navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LocalBins: Improving Depth Estimation by Learning Local Distributions. (arXiv:2203.15132v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15132">
<div class="article-summary-box-inner">
<span><p>We propose a novel architecture for depth estimation from a single image. The
architecture itself is based on the popular encoder-decoder architecture that
is frequently used as a starting point for all dense regression tasks. We build
on AdaBins which estimates a global distribution of depth values for the input
image and evolve the architecture in two ways. First, instead of predicting
global depth distributions, we predict depth distributions of local
neighborhoods at every pixel. Second, instead of predicting depth distributions
only towards the end of the decoder, we involve all layers of the decoder. We
call this new architecture LocalBins. Our results demonstrate a clear
improvement over the state-of-the-art in all metrics on the NYU-Depth V2
dataset. Code and pretrained models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards End-to-End Unified Scene Text Detection and Layout Analysis. (arXiv:2203.15143v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15143">
<div class="article-summary-box-inner">
<span><p>Scene text detection and document layout analysis have long been treated as
two separate tasks in different image domains. In this paper, we bring them
together and introduce the task of unified scene text detection and layout
analysis. The first hierarchical scene text dataset is introduced to enable
this novel research task. We also propose a novel method that is able to
simultaneously detect scene text and form text clusters in a unified way.
Comprehensive experiments show that our unified model achieves better
performance than multiple well-designed baseline methods. Additionally, this
model achieves state-of-the-art results on multiple scene text detection
datasets without the need of complex post-processing. Dataset and code:
https://github.com/google-research-datasets/hiertext.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Synthesize Volumetric Meshes from Vision-based Tactile Imprints. (arXiv:2203.15155v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15155">
<div class="article-summary-box-inner">
<span><p>Vision-based tactile sensors typically utilize a deformable elastomer and a
camera mounted above to provide high-resolution image observations of contacts.
Obtaining accurate volumetric meshes for the deformed elastomer can provide
direct contact information and benefit robotic grasping and manipulation. This
paper focuses on learning to synthesize the volumetric mesh of the elastomer
based on the image imprints acquired from vision-based tactile sensors.
Synthetic image-mesh pairs and real-world images are gathered from 3D finite
element methods (FEM) and physical sensors, respectively. A graph neural
network (GNN) is introduced to learn the image-to-mesh mappings with supervised
learning. A self-supervised adaptation method and image augmentation techniques
are proposed to transfer networks from simulation to reality, from primitive
contacts to unseen contacts, and from one sensor to another. Using these
learned and adapted networks, our proposed method can accurately reconstruct
the deformation of the real-world tactile sensor elastomer in various domains,
as indicated by the quantitative and qualitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Aspects of Zero-Shot Learning. (arXiv:2203.15158v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15158">
<div class="article-summary-box-inner">
<span><p>One of important areas of machine learning research is zero-shot learning. It
is applied when properly labeled training data set is not available. A number
of zero-shot algorithms have been proposed and experimented with. However, none
of them seems to be the "overall winner". In situations like this, it may be
possible to develop a meta-classifier that would combine "best aspects" of
individual classifiers and outperform all of them. In this context, the goal of
this contribution is twofold. First, multiple state-of-the-art zero-shot
learning methods are compared for standard benchmark datasets. Second, multiple
meta-classifiers are suggested and experimentally compared (for the same
datasets).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI. (arXiv:2203.15163v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15163">
<div class="article-summary-box-inner">
<span><p>Prostate cancer is the second leading cause of cancer death among men in the
United States. The diagnosis of prostate MRI often relies on the accurate
prostate zonal segmentation. However, state-of-the-art automatic segmentation
methods often fail to produce well-contained volumetric segmentation of the
prostate zones since certain slices of prostate MRI, such as base and apex
slices, are harder to segment than other slices. This difficulty can be
overcome by accounting for the cross-slice relationship of adjacent slices, but
current methods do not fully learn and exploit such relationships. In this
paper, we propose a novel cross-slice attention mechanism, which we use in a
Transformer module to systematically learn the cross-slice relationship at
different scales. The module can be utilized in any existing learning-based
segmentation framework with skip connections. Experiments show that our
cross-slice attention is able to capture the cross-slice information in
prostate zonal segmentation and improve the performance of current
state-of-the-art methods. Our method significantly improves segmentation
accuracy in the peripheral zone, such that the segmentation results are
consistent across all the prostate slices (apex, mid-gland, and base).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Light Field Depth Estimation Using Epipolar Plane Images. (arXiv:2203.15171v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15171">
<div class="article-summary-box-inner">
<span><p>Exploiting light field data makes it possible to obtain dense and accurate
depth map. However, synthetic scenes with limited disparity range cannot
contain the diversity of real scenes. By training in synthetic data, current
learning-based methods do not perform well in real scenes. In this paper, we
propose a self-supervised learning framework for light field depth estimation.
Different from the existing end-to-end training methods using disparity label
per pixel, our approach implements network training by estimating EPI disparity
shift after refocusing, which extends the disparity range of epipolar lines. To
reduce the sensitivity of EPI to noise, we propose a new input mode called
EPI-Stack, which stacks EPIs in the view dimension. This method is less
sensitive to noise scenes than traditional input mode and improves the
efficiency of estimation. Compared with other state-of-the-art methods, the
proposed method can also obtain higher quality results in real-world scenarios,
especially in the complex occlusion and depth discontinuity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15174">
<div class="article-summary-box-inner">
<span><p>Conventional self-supervised monocular depth prediction methods are based on
a static environment assumption, which leads to accuracy degradation in dynamic
scenes due to the mismatch and occlusion problems introduced by object motions.
Existing dynamic-object-focused methods only partially solved the mismatch
problem at the training loss level. In this paper, we accordingly propose a
novel multi-frame monocular depth prediction method to solve these problems at
both the prediction and supervision loss levels. Our method, called
DynamicDepth, is a new framework trained via a self-supervised cycle consistent
learning scheme. A Dynamic Object Motion Disentanglement (DOMD) module is
proposed to disentangle object motions to solve the mismatch problem. Moreover,
novel occlusion-aware Cost Volume and Re-projection Loss are designed to
alleviate the occlusion effects of object motions. Extensive analyses and
experiments on the Cityscapes and KITTI datasets show that our method
significantly outperforms the state-of-the-art monocular depth prediction
methods, especially in the areas of dynamic objects. Our code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Transformer Tracker for Object Tracking. (arXiv:2203.15175v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15175">
<div class="article-summary-box-inner">
<span><p>As an important area in computer vision, object tracking has formed two
separate communities that respectively study Single Object Tracking (SOT) and
Multiple Object Tracking (MOT). However, current methods in one tracking
scenario are not easily adapted to the other due to the divergent training
datasets and tracking objects of both tasks. Although UniTrack
\cite{wang2021different} demonstrates that a shared appearance model with
multiple heads can be used to tackle individual tracking tasks, it fails to
exploit the large-scale tracking datasets for training and performs poorly on
single object tracking. In this work, we present the Unified Transformer
Tracker (UTT) to address tracking problems in different scenarios with one
paradigm. A track transformer is developed in our UTT to track the target in
both SOT and MOT. The correlation between the target and tracking frame
features is exploited to localize the target. We demonstrate that both SOT and
MOT tasks can be solved within this framework. The model can be simultaneously
end-to-end trained by alternatively optimizing the SOT and MOT objectives on
the datasets of individual tasks. Extensive experiments are conducted on
several benchmarks with a unified model trained on SOT and MOT datasets. Code
will be available at https://github.com/Flowerfan/Trackron.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Min-Max Similarity: A Contrastive Learning Based Semi-Supervised Learning Network for Surgical Tools Segmentation. (arXiv:2203.15177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15177">
<div class="article-summary-box-inner">
<span><p>Segmentation of images is a popular topic in medical AI. This is mainly due
to the difficulty to obtain a significant number of pixel-level annotated data
to train a neural network. To address this issue, we proposed a semi-supervised
segmentation network based on contrastive learning. In contrast to the previous
state-of-the-art, we introduce a contrastive learning form of dual-view
training by employing classifiers and projectors to build all-negative, and
positive and negative feature pairs respectively to formulate the learning
problem as solving min-max similarity problem. The all-negative pairs are used
to supervise the networks learning from different views and make sure to
capture general features, and the consistency of unlabeled predictions is
measured by pixel-wise contrastive loss between positive and negative pairs. To
quantitative and qualitative evaluate our proposed method, we test it on two
public endoscopy surgical tool segmentation datasets and one cochlear implant
surgery dataset which we manually annotate the cochlear implant in surgical
videos. The segmentation performance (dice coefficients) indicates that our
proposed method outperforms state-of-the-art semi-supervised and fully
supervised segmentation algorithms consistently. The code is publicly available
at: https://github.com/AngeLouCN/Min_Max_Similarity
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-term Visual Map Sparsification with Heterogeneous GNN. (arXiv:2203.15182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15182">
<div class="article-summary-box-inner">
<span><p>We address the problem of map sparsification for long-term visual
localization. For map sparsification, a commonly employed assumption is that
the pre-build map and the later captured localization query are consistent.
However, this assumption can be easily violated in the dynamic world.
Additionally, the map size grows as new data accumulate through time, causing
large data overhead in the long term. In this paper, we aim to overcome the
environmental changes and reduce the map size at the same time by selecting
points that are valuable to future localization. Inspired by the recent
progress in Graph Neural Network(GNN), we propose the first work that models
SfM maps as heterogeneous graphs and predicts 3D point importance scores with a
GNN, which enables us to directly exploit the rich information in the SfM map
graph. Two novel supervisions are proposed: 1) a data-fitting term for
selecting valuable points to future localization based on training queries; 2)
a K-Cover term for selecting sparse points with full map coverage. The
experiments show that our method selected map points on stable and widely
visible structures and outperformed baselines in localization performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization. (arXiv:2203.15187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15187">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised temporal action localization aims to recognize and localize
action segments in untrimmed videos given only video-level action labels for
training. Without the boundary information of action segments, existing methods
mostly rely on multiple instance learning (MIL), where the predictions of
unlabeled instances (i.e., video snippets) are supervised by classifying
labeled bags (i.e., untrimmed videos). However, this formulation typically
treats snippets in a video as independent instances, ignoring the underlying
temporal structures within and across action segments. To address this problem,
we propose \system, a novel WTAL framework that enables explicit, action-aware
segment modeling beyond standard MIL-based methods. Our framework entails three
segment-centric components: (i) dynamic segment sampling for compensating the
contribution of short actions; (ii) intra- and inter-segment attention for
modeling action dynamics and capturing temporal dependencies; (iii) pseudo
instance-level supervision for improving action boundary prediction.
Furthermore, a multi-step refinement strategy is proposed to progressively
improve action proposals along the model training process. Extensive
experiments on THUMOS-14 and ActivityNet-v1.3 demonstrate the effectiveness of
our approach, establishing new state of the art on both datasets. The code and
models are publicly available at~\url{https://github.com/boheumd/ASM-Loc}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse to Fine: Image Restoration Boosted by Multi-Scale Low-Rank Tensor Completion. (arXiv:2203.15189v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15189">
<div class="article-summary-box-inner">
<span><p>Existing low-rank tensor completion (LRTC) approaches aim at restoring a
partially observed tensor by imposing a global low-rank constraint on the
underlying completed tensor. However, such a global rank assumption suffers the
trade-off between restoring the originally details-lacking parts and neglecting
the potentially complex objects, making the completion performance
unsatisfactory on both sides. To address this problem, we propose a novel and
practical strategy for image restoration that restores the partially observed
tensor in a coarse-to-fine (C2F) manner, which gets rid of such trade-off by
searching proper local ranks for both low- and high-rank parts. Extensive
experiments are conducted to demonstrate the superiority of the proposed C2F
scheme. The codes are available at: https://github.com/RuiLin0212/C2FLRTC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow. (arXiv:2203.15190v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15190">
<div class="article-summary-box-inner">
<span><p>Reconstructing 3D shape from a single 2D image is a challenging task, which
needs to estimate the detailed 3D structures based on the semantic attributes
from 2D image. So far, most of the previous methods still struggle to extract
semantic attributes for 3D reconstruction task. Since the semantic attributes
of a single image are usually implicit and entangled with each other, it is
still challenging to reconstruct 3D shape with detailed semantic structures
represented by the input image. To address this problem, we propose 3DAttriFlow
to disentangle and extract semantic attributes through different semantic
levels in the input images. These disentangled semantic attributes will be
integrated into the 3D shape reconstruction process, which can provide definite
guidance to the reconstruction of specific attribute on 3D shape. As a result,
the 3D decoder can explicitly capture high-level semantic features at the
bottom of the network, and utilize low-level features at the top of the
network, which allows to reconstruct more accurate 3D shapes. Note that the
explicit disentangling is learned without extra labels, where the only
supervision used in our training is the input image and its corresponding 3D
shape. Our comprehensive experiments on ShapeNet dataset demonstrate that
3DAttriFlow outperforms the state-of-the-art shape reconstruction methods, and
we also validate its generalization ability on shape completion task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnoDFDNet: A Deep Feature Difference Network for Anomaly Detection. (arXiv:2203.15195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15195">
<div class="article-summary-box-inner">
<span><p>This paper proposed a novel anomaly detection (AD) approach of High-speed
Train images based on convolutional neural networks and the Vision Transformer.
Different from previous AD works, in which anomalies are identified with a
single image using classification, segmentation, or object detection methods,
the proposed method detects abnormal difference between two images taken at
different times of the same region. In other words, we cast anomaly detection
problem with a single image into a difference detection problem with two
images. The core idea of the proposed method is that the 'anomaly' usually
represents an abnormal state instead of a specific object, and this state
should be identified by a pair of images. In addition, we introduced a deep
feature difference AD network (AnoDFDNet) which sufficiently explored the
potential of the Vision Transformer and convolutional neural networks. To
verify the effectiveness of the proposed AnoDFDNet, we collected three
datasets, a difference dataset (Diff Dataset), a foreign body dataset (FB
Dataset), and an oil leakage dataset (OL Dataset). Experimental results on
above datasets demonstrate the superiority of proposed method. Source code are
available at https://github.com/wangle53/AnoDFDNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light Field Depth Estimation Based on Stitched-EPI. (arXiv:2203.15201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15201">
<div class="article-summary-box-inner">
<span><p>Depth estimation is one of the most essential problems for light field
applications. In EPI-based methods, the slope computation usually suffers low
accuracy due to the discretization error and low angular resolution. In
addition, recent methods work well in most regions but often struggle with
blurry edges over occluded regions and ambiguity over texture-less regions. To
address these challenging issues, we first propose the stitched-EPI and
half-stitched-EPI algorithms for non-occluded and occluded regions,
respectively. The algorithms improve slope computation by shifting and
concatenating lines in different EPIs but related to the same point in 3D
scene, while the half-stitched-EPI only uses non-occluded part of lines.
Combined with the joint photo-consistency cost proposed by us, the more
accurate and robust depth map can be obtained in both occluded and non-occluded
regions. Furthermore, to improve the depth estimation in texture-less regions,
we propose a depth propagation strategy that determines their depth from the
edge to interior, from accurate regions to coarse regions. Experimental and
ablation results demonstrate that the proposed method achieves accurate and
robust depth maps in all regions effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimT: Handling Open-set Noise for Domain Adaptive Semantic Segmentation. (arXiv:2203.15202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15202">
<div class="article-summary-box-inner">
<span><p>This paper studies a practical domain adaptive (DA) semantic segmentation
problem where only pseudo-labeled target data is accessible through a black-box
model. Due to the domain gap and label shift between two domains,
pseudo-labeled target data contains mixed closed-set and open-set label noises.
In this paper, we propose a simplex noise transition matrix (SimT) to model the
mixed noise distributions in DA semantic segmentation and formulate the problem
as estimation of SimT. By exploiting computational geometry analysis and
properties of segmentation, we design three complementary regularizers, i.e.
volume regularization, anchor guidance, convex guarantee, to approximate the
true SimT. Specifically, volume regularization minimizes the volume of simplex
formed by rows of the non-square SimT, which ensures outputs of segmentation
model to fit into the ground truth label distribution. To compensate for the
lack of open-set knowledge, anchor guidance and convex guarantee are devised to
facilitate the modeling of open-set noise distribution and enhance the
discriminative feature learning among closed-set and open-set classes. The
estimated SimT is further utilized to correct noise issues in pseudo labels and
promote the generalization ability of segmentation model on target domain data.
Extensive experimental results demonstrate that the proposed SimT can be
flexibly plugged into existing DA methods to boost the performance. The source
code is available at \url{https://github.com/CityU-AIM-Group/SimT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Periocular Biometrics and its Relevance to Partially Masked Faces: A Survey. (arXiv:2203.15203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15203">
<div class="article-summary-box-inner">
<span><p>The performance of face recognition systems can be negatively impacted in the
presence of masks and other types of facial coverings that have become
prevalent due to the COVID-19 pandemic. In such cases, the periocular region of
the human face becomes an important biometric cue. In this article, we present
a detailed review of periocular biometrics. We first examine the various face
and periocular techniques specially designed to recognize humans wearing a face
mask. Then, we review different aspects of periocular biometrics: (a) the
anatomical cues present in the periocular region useful for recognition, (b)
the various feature extraction and matching techniques developed, (c)
recognition across different spectra, (d) fusion with other biometric
modalities (face or iris), (e) recognition on mobile devices, (f) its
usefulness in other applications, (g) periocular datasets, and (h) competitions
organized for evaluating the efficacy of this biometric modality. Finally, we
discuss various challenges and future directions in the field of periocular
biometrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPAct: Self-supervised Privacy Preservation for Action Recognition. (arXiv:2203.15205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15205">
<div class="article-summary-box-inner">
<span><p>Visual private information leakage is an emerging key issue for the fast
growing applications of video understanding like activity recognition. Existing
approaches for mitigating privacy leakage in action recognition require privacy
labels along with the action labels from the video dataset. However, annotating
frames of video dataset for privacy labels is not feasible. Recent developments
of self-supervised learning (SSL) have unleashed the untapped potential of the
unlabeled data. For the first time, we present a novel training framework which
removes privacy information from input video in a self-supervised manner
without requiring privacy labels. Our training framework consists of three main
components: anonymization function, self-supervised privacy removal branch, and
action recognition branch. We train our framework using a minimax optimization
strategy to minimize the action recognition cost function and maximize the
privacy cost function through a contrastive self-supervised loss. Employing
existing protocols of known-action and privacy attributes, our framework
achieves a competitive action-privacy trade-off to the existing
state-of-the-art supervised methods. In addition, we introduce a new protocol
to evaluate the generalization of learned the anonymization function to
novel-action and privacy attributes and show that our self-supervised framework
outperforms existing supervised methods. Code available at:
https://github.com/DAVEISHAN/SPAct
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing Few-Shot NAS with Gradient Matching. (arXiv:2203.15207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15207">
<div class="article-summary-box-inner">
<span><p>Efficient performance estimation of architectures drawn from large search
spaces is essential to Neural Architecture Search. One-Shot methods tackle this
challenge by training one supernet to approximate the performance of every
architecture in the search space via weight-sharing, thereby drastically
reducing the search cost. However, due to coupled optimization between child
architectures caused by weight-sharing, One-Shot supernet's performance
estimation could be inaccurate, leading to degraded search outcomes. To address
this issue, Few-Shot NAS reduces the level of weight-sharing by splitting the
One-Shot supernet into multiple separated sub-supernets via edge-wise
(layer-wise) exhaustive partitioning. Since each partition of the supernet is
not equally important, it necessitates the design of a more effective splitting
criterion. In this work, we propose a gradient matching score (GM) that
leverages gradient information at the shared weight for making informed
splitting decisions. Intuitively, gradients from different child models can be
used to identify whether they agree on how to update the shared modules, and
subsequently to decide if they should share the same weight. Compared with
exhaustive partitioning, the proposed criterion significantly reduces the
branching factor per edge. This allows us to split more edges (layers) for a
given budget, resulting in substantially improved performance as NAS search
spaces usually include dozens of edges (layers). Extensive empirical
evaluations of the proposed method on a wide range of search spaces
(NASBench-201, DARTS, MobileNet Space), datasets (cifar10, cifar100, ImageNet)
and search algorithms (DARTS, SNAS, RSPS, ProxylessNAS, OFA) demonstrate that
it significantly outperforms its Few-Shot counterparts while surpassing
previous comparable methods in terms of the accuracy of derived architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification. (arXiv:2203.15210v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15210">
<div class="article-summary-box-inner">
<span><p>To learn camera-view invariant features for person Re-IDentification (Re-ID),
the cross-camera image pairs of each person play an important role. However,
such cross-view training samples could be unavailable under the ISolated Camera
Supervised (ISCS) setting, e.g., a surveillance system deployed across distant
scenes.To handle this challenging problem, a new pipeline is introduced by
synthesizing the cross-camera samples in the feature space for model training.
Specifically, the feature encoder and generator are end-to-end optimized under
a novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint
learning procedure raises concern on the stability of generative model
training. Therefore, a new feature generator, $\sigma$-Regularized Conditional
Variational Autoencoder ($\sigma$-Reg.~CVAE), is proposed with theoretical and
experimental analysis on its robustness. Extensive experiments on two ISCS
person Re-ID datasets demonstrate the superiority of our CCSFG to the
competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affine Medical Image Registration with Coarse-to-Fine Vision Transformer. (arXiv:2203.15216v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15216">
<div class="article-summary-box-inner">
<span><p>Affine registration is indispensable in a comprehensive medical image
registration pipeline. However, only a few studies focus on fast and robust
affine registration algorithms. Most of these studies utilize convolutional
neural networks (CNNs) to learn joint affine and non-parametric registration,
while the standalone performance of the affine subnetwork is less explored.
Moreover, existing CNN-based affine registration approaches focus either on the
local misalignment or the global orientation and position of the input to
predict the affine transformation matrix, which are sensitive to spatial
initialization and exhibit limited generalizability apart from the training
dataset. In this paper, we present a fast and robust learning-based algorithm,
Coarse-to-Fine Vision Transformer (C2FViT), for 3D affine medical image
registration. Our method naturally leverages the global connectivity and
locality of the convolutional vision transformer and the multi-resolution
strategy to learn the global affine registration. We evaluate our method on 3D
brain atlas registration and template-matching normalization. Comprehensive
results demonstrate that our method is superior to the existing CNNs-based
affine registration methods in terms of registration accuracy, robustness and
generalizability while preserving the runtime advantage of the learning-based
methods. The source code is available at https://github.com/cwmok/C2FViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection. (arXiv:2203.15221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15221">
<div class="article-summary-box-inner">
<span><p>Recently, transformer-based methods have achieved promising progresses in
object detection, as they can eliminate the post-processes like NMS and enrich
the deep representations. However, these methods cannot well cope with scene
text due to its extreme variance of scales and aspect ratios. In this paper, we
present a simple yet effective transformer-based architecture for scene text
detection. Different from previous approaches that learn robust deep
representations of scene text in a holistic manner, our method performs scene
text detection based on a few representative features, which avoids the
disturbance by background and reduces the computational cost. Specifically, we
first select a few representative features at all scales that are highly
relevant to foreground text. Then, we adopt a transformer for modeling the
relationship of the sampled features, which effectively divides them into
reasonable groups. As each feature group corresponds to a text instance, its
bounding box can be easily obtained without any post-processing operation.
Using the basic feature pyramid network for feature extraction, our method
consistently achieves state-of-the-art results on several popular datasets for
scene text detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation. (arXiv:2203.15224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15224">
<div class="article-summary-box-inner">
<span><p>Large-scale training data with high-quality annotations is critical for
training semantic and instance segmentation models. Unfortunately, pixel-wise
annotation is labor-intensive and costly, raising the demand for more efficient
labeling strategies. In this work, we present a novel 3D-to-2D label transfer
method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and
instance labels from easy-to-obtain coarse 3D bounding primitives. Our method
utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D
semantic cues transferred from existing datasets. We demonstrate that this
combination allows for improved geometry guided by semantic information,
enabling rendering of accurate semantic maps across multiple views.
Furthermore, this fusion process resolves label ambiguity of the coarse 3D
annotations and filters noise in the 2D predictions. By inferring in 3D space
and rendering to 2D labels, our 2D semantic and instance labels are multi-view
consistent by design. Experimental results show that Panoptic NeRF outperforms
existing semantic and instance label transfer methods in terms of accuracy and
multi-view consistency on challenging urban scenes of the KITTI-360 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation. (arXiv:2203.15227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15227">
<div class="article-summary-box-inner">
<span><p>Multi-frame human pose estimation has long been a compelling and fundamental
problem in computer vision. This task is challenging due to fast motion and
pose occlusion that frequently occur in videos. State-of-the-art methods strive
to incorporate additional visual evidences from neighboring frames (supporting
frames) to facilitate the pose estimation of the current frame (key frame). One
aspect that has been obviated so far, is the fact that current methods directly
aggregate unaligned contexts across frames. The spatial-misalignment between
pose features of the current frame and neighboring frames might lead to
unsatisfactory results. More importantly, existing approaches build upon the
straightforward pose estimation loss, which unfortunately cannot constrain the
network to fully leverage useful information from neighboring frames. To tackle
these problems, we present a novel hierarchical alignment framework, which
leverages coarse-to-fine deformations to progressively update a neighboring
frame to align with the current frame at the feature level. We further propose
to explicitly supervise the knowledge extraction from neighboring frames,
guaranteeing that useful complementary cues are extracted. To achieve this
goal, we theoretically analyzed the mutual information between the frames and
arrived at a loss that maximizes the task-relevant mutual information. These
allow us to rank No.1 in the Multi-frame Person Pose Estimation Challenge on
benchmark dataset PoseTrack2017, and obtain state-of-the-art performance on
benchmarks Sub-JHMDB and Pose-Track2018. Our code is released at
https://github. com/Pose-Group/FAMI-Pose, hoping that it will be useful to the
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHOP: A Deep Learning Based Pipeline for near Real-Time Detection of Small Handheld Objects Present in Blurry Video. (arXiv:2203.15228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15228">
<div class="article-summary-box-inner">
<span><p>While prior works have investigated and developed computational models
capable of object detection, models still struggle to reliably interpret images
with motion blur and small objects. Moreover, none of these models are
specifically designed for handheld object detection. In this work, we present
SHOP (Small Handheld Object Pipeline), a pipeline that reliably and efficiently
interprets blurry images containing handheld objects. The specific models used
in each stage of the pipeline are flexible and can be changed based on
performance requirements. First, images are deblurred and then run through a
pose detection system where areas-of-interest are proposed around the hands of
any people present. Next, object detection is performed on the images by a
single-stage object detector. Finally, the proposed areas-of-interest are used
to filter out low confidence detections. Testing on a handheld subset of
Microsoft Common Objects in Context (MS COCO) demonstrates that this 3 stage
process results in a 70 percent decrease in false positives while only reducing
true positives by 17 percent in its strongest configuration. We also present a
subset of MS COCO consisting solely of handheld objects that can be used to
continue the development of handheld object detection methods.
https://github.com/spider-sense/SHOP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge Detection and Deep Learning Based SETI Signal Classification Method. (arXiv:2203.15229v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15229">
<div class="article-summary-box-inner">
<span><p>Scientists at the Berkeley SETI Research Center are Searching for
Extraterrestrial Intelligence (SETI) by a new signal detection method that
converts radio signals into spectrograms through Fourier transforms and
classifies signals represented by two-dimensional time-frequency spectrums,
which successfully converts a signal classification problem into an image
classification task. In view of the negative impact of background noises on the
accuracy of spectrograms classification, a new method is introduced in this
paper. After Gaussian convolution smoothing the signals, edge detection
functions are applied to detect the edge of the signals and enhance the outline
of the signals, then the processed spectrograms are used to train the deep
neural network to compare the classification accuracy of various image
classification networks. The results show that the proposed method can
effectively improve the classification accuracy of SETI spectrums.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Query Transfer Attacks on Context-Aware Object Detectors. (arXiv:2203.15230v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15230">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks perturb images such that a deep neural network produces
incorrect classification results. A promising approach to defend against
adversarial attacks on natural multi-object scenes is to impose a
context-consistency check, wherein, if the detected objects are not consistent
with an appropriately defined context, then an attack is suspected. Stronger
attacks are needed to fool such context-aware detectors. We present the first
approach for generating context-consistent adversarial attacks that can evade
the context-consistency check of black-box object detectors operating on
complex, natural scenes. Unlike many black-box attacks that perform repeated
attempts and open themselves to detection, we assume a "zero-query" setting,
where the attacker has no knowledge of the classification decisions of the
victim system. First, we derive multiple attack plans that assign incorrect
labels to victim objects in a context-consistent manner. Then we design and use
a novel data structure that we call the perturbation success probability
matrix, which enables us to filter the attack plans and choose the one most
likely to succeed. This final attack plan is implemented using a
perturbation-bounded adversarial attack algorithm. We compare our zero-query
attack against a few-query scheme that repeatedly checks if the victim system
is fooled. We also compare against state-of-the-art context-agnostic attacks.
Against a context-aware defense, the fooling rate of our zero-query approach is
significantly higher than context-agnostic approaches and higher than that
achievable with up to three rounds of the few-query scheme.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoPoly: Predicting a Polygonal Mesh Construction Sequence from a Silhouette Image. (arXiv:2203.15233v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15233">
<div class="article-summary-box-inner">
<span><p>Polygonal modeling is a core task of content creation in Computer Graphics.
The complexity of modeling, in terms of the number and the order of operations
and time required to execute them makes it challenging to learn and execute.
Our goal is to automatically derive a polygonal modeling sequence for a given
target. Then, one can learn polygonal modeling by observing the resulting
sequence and also expedite the modeling process by starting from the
auto-generated result. As a starting point for building a system for 3D
modeling in the future, we tackle the 2D shape modeling problem and present
AutoPoly, a hybrid method that generates a polygonal mesh construction sequence
from a silhouette image. The key idea of our method is the use of the Monte
Carlo tree search (MCTS) algorithm and differentiable rendering to separately
predict sequential topological actions and geometric actions. Our hybrid method
can alter topology, whereas the recently proposed inverse shape estimation
methods using differentiable rendering can only handle a fixed topology. Our
novel reward function encourages MCTS to select topological actions that lead
to a simpler shape without self-intersection. We further designed two deep
learning-based methods to improve the expansion and simulation steps in the
MCTS search process: an $n$-step "future action prediction" network (nFAP-Net)
to generate candidates for potential topological actions, and a shape warping
network (WarpNet) to predict polygonal shapes given the predicted rendered
images and topological actions. We demonstrate the efficiency of our method on
2D polygonal shapes of multiple man-made object categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equivariance Allows Handling Multiple Nuisance Variables When Analyzing Pooled Neuroimaging Datasets. (arXiv:2203.15234v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15234">
<div class="article-summary-box-inner">
<span><p>Pooling multiple neuroimaging datasets across institutions often enables
improvements in statistical power when evaluating associations (e.g., between
risk factors and disease outcomes) that may otherwise be too weak to detect.
When there is only a {\em single} source of variability (e.g., different
scanners), domain adaptation and matching the distributions of representations
may suffice in many scenarios. But in the presence of {\em more than one}
nuisance variable which concurrently influence the measurements, pooling
datasets poses unique challenges, e.g., variations in the data can come from
both the acquisition method as well as the demographics of participants
(gender, age). Invariant representation learning, by itself, is ill-suited to
fully model the data generation process. In this paper, we show how bringing
recent results on equivariant representation learning (for studying symmetries
in neural networks) instantiated on structured spaces together with simple use
of classical results on causal inference provides an effective practical
solution. In particular, we demonstrate how our model allows dealing with more
than one nuisance variable under some assumptions and can enable analysis of
pooled scientific datasets in scenarios that would otherwise entail removing a
large portion of the samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian. (arXiv:2203.15235v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15235">
<div class="article-summary-box-inner">
<span><p>We propose a framework that can deform an object in a 2D image as it exists
in 3D space. Most existing methods for 3D-aware image manipulation are limited
to (1) only changing the global scene information or depth, or (2) manipulating
an object of specific categories. In this paper, we present a 3D-aware image
deformation method with minimal restrictions on shape category and deformation
type. While our framework leverages 2D-to-3D reconstruction, we argue that
reconstruction is not sufficient for realistic deformations due to the
vulnerability to topological errors. Thus, we propose to take a supervised
learning-based approach to predict the shape Laplacian of the underlying volume
of a 3D reconstruction represented as a point cloud. Given the deformation
energy calculated using the predicted shape Laplacian and user-defined
deformation handles (e.g., keypoints), we obtain bounded biharmonic weights to
model plausible handle-based image deformation. In the experiments, we present
our results of deforming 2D character and clothed human images. We also
quantitatively show that our approach can produce more accurate deformation
weights compared to alternative methods (i.e., mesh reconstruction and point
cloud Laplacian methods).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Image-to-Image Translation using Latent Space Mapping. (arXiv:2203.15241v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15241">
<div class="article-summary-box-inner">
<span><p>Recent image-to-image translation works have been transferred from supervised
to unsupervised settings due to the expensive cost of capturing or labeling
large amounts of paired data. However, current unsupervised methods using the
cycle-consistency constraint may not find the desired mapping, especially for
difficult translation tasks. On the other hand, a small number of paired data
are usually accessible. We therefore introduce a general framework for
semi-supervised image translation. Unlike previous works, our main idea is to
learn the translation over the latent feature space instead of the image space.
Thanks to the low dimensional feature space, it is easier to find the desired
mapping function, resulting in improved quality of translation results as well
as the stability of the translation model. Empirically we show that using
feature translation generates better results, even using a few bits of paired
data. Experimental comparisons with state-of-the-art approaches demonstrate the
effectiveness of the proposed framework on a variety of challenging
image-to-image translation tasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Image Transformers using Learnable Memory. (arXiv:2203.15243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15243">
<div class="article-summary-box-inner">
<span><p>In this paper we propose augmenting Vision Transformer models with learnable
memory tokens. Our approach allows the model to adapt to new tasks, using few
parameters, while optionally preserving its capabilities on previously learned
tasks. At each layer we introduce a set of learnable embedding vectors that
provide contextual information useful for specific datasets. We call these
"memory tokens". We show that augmenting a model with just a handful of such
tokens per layer significantly improves accuracy when compared to conventional
head-only fine-tuning, and performs only slightly below the significantly more
expensive full fine-tuning. We then propose an attention-masking approach that
enables extension to new downstream tasks, with a computation reuse. In this
setup in addition to being parameters efficient, models can execute both old
and new tasks as a part of single inference at a small incremental cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients. (arXiv:2203.15245v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15245">
<div class="article-summary-box-inner">
<span><p>Deep neural networks for 3D point cloud classification, such as PointNet,
have been demonstrated to be vulnerable to adversarial attacks. Current
adversarial defenders often learn to denoise the (attacked) point clouds by
reconstruction, and then feed them to the classifiers as input. In contrast to
the literature, we propose a family of robust structured declarative
classifiers for point cloud classification, where the internal constrained
optimization mechanism can effectively defend adversarial attacks through
implicit gradients. Such classifiers can be formulated using a bilevel
optimization framework. We further propose an effective and efficient
instantiation of our approach, namely, Lattice Point Classifier (LPC), based on
structured sparse coding in the permutohedral lattice and 2D convolutional
neural networks (CNNs) that is end-to-end trainable. We demonstrate
state-of-the-art robust point cloud classification performance on ModelNet40
and ScanNet under seven different attackers. For instance, we achieve 89.51%
and 83.16% test accuracy on each dataset under the recent JGBA attacker that
outperforms DUP-Net and IF-Defense with PointNet by ~70%. Demo code is
available at https://zhang-vislab.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Intra- and Inter-Video Relation for Surgical Semantic Scene Segmentation. (arXiv:2203.15251v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15251">
<div class="article-summary-box-inner">
<span><p>Automatic surgical scene segmentation is fundamental for facilitating
cognitive intelligence in the modern operating theatre. Previous works rely on
conventional aggregation modules (e.g., dilated convolution, convolutional
LSTM), which only make use of the local context. In this paper, we propose a
novel framework STswinCL that explores the complementary intra- and inter-video
relations to boost segmentation performance, by progressively capturing the
global context. We firstly develop a hierarchy Transformer to capture
intra-video relation that includes richer spatial and temporal cues from
neighbor pixels and previous frames. A joint space-time window shift scheme is
proposed to efficiently aggregate these two cues into each pixel embedding.
Then, we explore inter-video relation via pixel-to-pixel contrastive learning,
which well structures the global embedding space. A multi-source contrast
training objective is developed to group the pixel embeddings across videos
with the ground-truth guidance, which is crucial for learning the global
property of the whole data. We extensively validate our approach on two public
surgical video benchmarks, including EndoVis18 Challenge and CaDIS dataset.
Experimental results demonstrate the promising performance of our method, which
consistently exceeds previous state-of-the-art approaches. Code will be
available at https://github.com/YuemingJin/STswinCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identification and classification of exfoliated graphene flakes from microscopy images using a hierarchical deep convolutional neural network. (arXiv:2203.15252v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15252">
<div class="article-summary-box-inner">
<span><p>Identification of the mechanically exfoliated graphene flakes and
classification of the thickness is important in the nanomanufacturing of
next-generation materials and devices that overcome the bottleneck of Moore's
Law. Currently, identification and classification of exfoliated graphene flakes
are conducted by human via inspecting the optical microscope images. The
existing state-of-the-art automatic identification by machine learning is not
able to accommodate images with different backgrounds while different
backgrounds are unavoidable in experiments. This paper presents a deep learning
method to automatically identify and classify the thickness of exfoliated
graphene flakes on Si/SiO2 substrates from optical microscope images with
various settings and background colors. The presented method uses a
hierarchical deep convolutional neural network that is capable of learning new
images while preserving the knowledge from previous images. The deep learning
model was trained and used to classify exfoliated graphene flakes into
monolayer (1L), bi-layer (2L), tri-layer (3L), four-to-six-layer (4-6L),
seven-to-ten-layer (7-10L), and bulk categories. Compared with existing machine
learning methods, the presented method possesses high accuracy and efficiency
as well as robustness to the backgrounds and resolutions of images. The results
indicated that our deep learning model has accuracy as high as 99% in
identifying and classifying exfoliated graphene flakes. This research will shed
light on scaled-up manufacturing and characterization of graphene for advanced
materials and devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Reflectance Capture with a Deep Gated Mixture-of-Experts. (arXiv:2203.15258v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15258">
<div class="article-summary-box-inner">
<span><p>We present a novel framework to efficiently acquire near-planar anisotropic
reflectance in a pixel-independent fashion, using a deep gated
mixtureof-experts. While existing work employs a unified network to handle all
possible input, our network automatically learns to condition on the input for
enhanced reconstruction. We train a gating module to select one out of a number
of specialized decoders for reflectance reconstruction, based on photometric
measurements, essentially trading generality for quality. A common, pre-trained
latent transform module is also appended to each decoder, to offset the burden
of the increased number of decoders. In addition, the illumination conditions
during acquisition can be jointly optimized. The effectiveness of our framework
is validated on a wide variety of challenging samples using a near-field
lightstage. Compared with the state-of-the-art technique, our results are
improved at the same input bandwidth, and our bandwidth can be reduced to about
1/3 for equal-quality results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eigencontours: Novel Contour Descriptors Based on Low-Rank Approximation. (arXiv:2203.15259v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15259">
<div class="article-summary-box-inner">
<span><p>Novel contour descriptors, called eigencontours, based on low-rank
approximation are proposed in this paper. First, we construct a contour matrix
containing all object boundaries in a training set. Second, we decompose the
contour matrix into eigencontours via the best rank-M approximation. Third, we
represent an object boundary by a linear combination of the M eigencontours. We
also incorporate the eigencontours into an instance segmentation framework.
Experimental results demonstrate that the proposed eigencontours can represent
object boundaries more effectively and more efficiently than existing
descriptors in a low-dimensional space. Furthermore, the proposed algorithm
yields meaningful performances on instance segmentation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Multi-Class Tiny-Object Detection. (arXiv:2203.15266v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15266">
<div class="article-summary-box-inner">
<span><p>Annotating tens or hundreds of tiny objects in a given image is laborious yet
crucial for a multitude of Computer Vision tasks. Such imagery typically
contains objects from various categories, yet the multi-class interactive
annotation setting for the detection task has thus far been unexplored. To
address these needs, we propose a novel interactive annotation method for
multiple instances of tiny objects from multiple classes, based on a few
point-based user inputs. Our approach, C3Det, relates the full image context
with annotator inputs in a local and global manner via late-fusion and
feature-correlation, respectively. We perform experiments on the Tiny-DOTA and
LCell datasets using both two-stage and one-stage object detection
architectures to verify the efficacy of our approach. Our approach outperforms
existing approaches in interactive annotation, achieving higher mAP with fewer
clicks. Furthermore, we validate the annotation efficiency of our approach in a
user study where it is shown to be 2.85x faster and yield only 0.36x task load
(NASA-TLX, lower is better) compared to manual annotation. The code is
available at
https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformers in Medical Computer Vision -- A Contemplative Retrospection. (arXiv:2203.15269v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15269">
<div class="article-summary-box-inner">
<span><p>Recent escalation in the field of computer vision underpins a huddle of
algorithms with the magnificent potential to unravel the information contained
within images. These computer vision algorithms are being practised in medical
image analysis and are transfiguring the perception and interpretation of
Imaging data. Among these algorithms, Vision Transformers are evolved as one of
the most contemporary and dominant architectures that are being used in the
field of computer vision. These are immensely utilized by a plenty of
researchers to perform new as well as former experiments. Here, in this article
we investigate the intersection of Vision Transformers and Medical images and
proffered an overview of various ViTs based frameworks that are being used by
different researchers in order to decipher the obstacles in Medical Computer
Vision. We surveyed the application of Vision transformers in different areas
of medical computer vision such as image-based disease classification,
anatomical structure segmentation, registration, region-based lesion Detection,
captioning, report generation, reconstruction using multiple medical imaging
modalities that greatly assist in medical diagnosis and hence treatment
process. Along with this, we also demystify several imaging modalities used in
Medical Computer Vision. Moreover, to get more insight and deeper
understanding, self-attention mechanism of transformers is also explained
briefly. Conclusively, we also put some light on available data sets, adopted
methodology, their performance measures, challenges and their solutions in form
of discussion. We hope that this review article will open future directions for
researchers in medical computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAT: Mask-Aware Transformer for Large Hole Image Inpainting. (arXiv:2203.15270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15270">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown the importance of modeling long-range interactions
in the inpainting problem. To achieve this goal, existing approaches exploit
either standalone attention techniques or transformers, but usually under a low
resolution in consideration of computational cost. In this paper, we present a
novel transformer-based model for large hole inpainting, which unifies the
merits of transformers and convolutions to efficiently process high-resolution
images. We carefully design each component of our framework to guarantee the
high fidelity and diversity of recovered images. Specifically, we customize an
inpainting-oriented transformer block, where the attention module aggregates
non-local information only from partial valid tokens, indicated by a dynamic
mask. Extensive experiments demonstrate the state-of-the-art performance of the
new model on multiple benchmark datasets. Code is released at
https://github.com/fenglinglwb/MAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Image based Navigation Architecture to Mitigate the need of precise Localization in Mobile Robots. (arXiv:2203.15272v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15272">
<div class="article-summary-box-inner">
<span><p>Traditional simultaneous localization and mapping (SLAM) methods focus on
improvement in the robot's localization under environment and sensor
uncertainty. This paper, however, focuses on mitigating the need for exact
localization of a mobile robot to pursue autonomous navigation using a sparse
set of images. The proposed method consists of a model architecture - RoomNet,
for unsupervised learning resulting in a coarse identification of the
environment and a separate local navigation policy for local identification and
navigation. The former learns and predicts the scene based on the short term
image sequences seen by the robot along with the transition image scenarios
using long term image sequences. The latter uses sparse image matching to
characterise the similarity of frames achieved vis-a-vis the frames viewed by
the robot during the mapping and training stage. A sparse graph of the image
sequence is created which is then used to carry out robust navigation purely on
the basis of visual goals. The proposed approach is evaluated on two robots in
a test environment and demonstrates the ability to navigate in dynamic
environments where landmarks are obscured and classical localization methods
fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Line Detection Using Mirror Attention and Comparative Ranking and Matching. (arXiv:2203.15285v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15285">
<div class="article-summary-box-inner">
<span><p>A novel algorithm to detect semantic lines is proposed in this paper. We
develop three networks: detection network with mirror attention (D-Net) and
comparative ranking and matching networks (R-Net and M-Net). D-Net extracts
semantic lines by exploiting rich contextual information. To this end, we
design the mirror attention module. Then, through pairwise comparisons of
extracted semantic lines, we iteratively select the most semantic line and
remove redundant ones overlapping with the selected one. For the pairwise
comparisons, we develop R-Net and M-Net in the Siamese architecture.
Experiments demonstrate that the proposed algorithm outperforms the
conventional semantic line detector significantly. Moreover, we apply the
proposed algorithm to detect two important kinds of semantic lines
successfully: dominant parallel lines and reflection symmetry axes. Our codes
are available at https://github.com/dongkwonjin/Semantic-Line-DRM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Adaptation for Self-Supervised 3D Human Pose Estimation. (arXiv:2203.15293v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15293">
<div class="article-summary-box-inner">
<span><p>The advances in monocular 3D human pose estimation are dominated by
supervised techniques that require large-scale 2D/3D pose annotations. Such
methods often behave erratically in the absence of any provision to discard
unfamiliar out-of-distribution data. To this end, we cast the 3D human pose
learning as an unsupervised domain adaptation problem. We introduce MRP-Net
that constitutes a common deep network backbone with two output heads
subscribing to two diverse configurations; a) model-free joint localization and
b) model-based parametric regression. Such a design allows us to derive
suitable measures to quantify prediction uncertainty at both pose and joint
level granularity. While supervising only on labeled synthetic samples, the
adaptation process aims to minimize the uncertainty for the unlabeled target
images while maximizing the same for an extreme out-of-distribution dataset
(backgrounds). Alongside synthetic-to-real 3D pose adaptation, the
joint-uncertainties allow expanding the adaptation to work on in-the-wild
images even in the presence of occlusion and truncation scenarios. We present a
comprehensive evaluation of the proposed approach and demonstrate
state-of-the-art performance on benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kernel Modulation: A Parameter-Efficient Method for Training Convolutional Neural Networks. (arXiv:2203.15297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15297">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks, particularly Convolutional Neural Networks (ConvNets),
have achieved incredible success in many vision tasks, but they usually require
millions of parameters for good accuracy performance. With increasing
applications that use ConvNets, updating hundreds of networks for multiple
tasks on an embedded device can be costly in terms of memory, bandwidth, and
energy. Approaches to reduce this cost include model compression and
parameter-efficient models that adapt a subset of network layers for each new
task. This work proposes a novel parameter-efficient kernel modulation (KM)
method that adapts all parameters of a base network instead of a subset of
layers. KM uses lightweight task-specialized kernel modulators that require
only an additional 1.4% of the base network parameters. With multiple tasks,
only the task-specialized KM weights are communicated and stored on the
end-user device. We applied this method in training ConvNets for Transfer
Learning and Meta-Learning scenarios. Our results show that KM delivers up to
9% higher accuracy than other parameter-efficient methods on the Transfer
Learning benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eigenlanes: Data-Driven Lane Descriptors for Structurally Diverse Lanes. (arXiv:2203.15302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15302">
<div class="article-summary-box-inner">
<span><p>A novel algorithm to detect road lanes in the eigenlane space is proposed in
this paper. First, we introduce the notion of eigenlanes, which are data-driven
descriptors for structurally diverse lanes, including curved, as well as
straight, lanes. To obtain eigenlanes, we perform the best rank-M approximation
of a lane matrix containing all lanes in a training set. Second, we generate a
set of lane candidates by clustering the training lanes in the eigenlane space.
Third, using the lane candidates, we determine an optimal set of lanes by
developing an anchor-based detection network, called SIIC-Net. Experimental
results demonstrate that the proposed algorithm provides excellent detection
performance for structurally diverse lanes. Our codes are available at
https://github.com/dongkwonjin/Eigenlanes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-based Point Cloud Registration for 6D Object Pose Estimation in the Real World. (arXiv:2203.15309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15309">
<div class="article-summary-box-inner">
<span><p>In this work, we tackle the task of estimating the 6D pose of an object from
point cloud data. While recent learning-based approaches to addressing this
task have shown great success on synthetic datasets, we have observed them to
fail in the presence of real-world data. We thus analyze the causes of these
failures, which we trace back to the difference between the feature
distributions of the source and target point clouds, and the sensitivity of the
widely-used SVD-based loss function to the range of rotation between the two
point clouds. We address the first challenge by introducing a new normalization
strategy, Match Normalization, and the second via the use of a loss function
based on the negative log likelihood of point correspondences. Our two
contributions are general and can be applied to many existing learning-based 3D
object registration frameworks, which we illustrate by implementing them in two
of them, DCP and IDAM. Our experiments on the real-scene TUD-L, LINEMOD and
Occluded-LINEMOD datasets evidence the benefits of our strategies. They allow
for the first time learning-based 3D object registration methods to achieve
meaningful results on real-world data. We therefore expect them to be key to
the future development of point cloud registration methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Routing Transformer for Zero-Shot Learning. (arXiv:2203.15310v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15310">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) aims to learn models that can recognize unseen image
semantics based on the training of data with seen semantics. Recent studies
either leverage the global image features or mine discriminative local patch
features to associate the extracted visual features to the semantic attributes.
However, due to the lack of the necessary top-down guidance and semantic
alignment for ensuring the model attending to the real attribute-correlation
regions, these methods still encounter a significant semantic gap between the
visual modality and the attribute modality, which makes their prediction on
unseen semantics unreliable. To solve this problem, this paper establishes a
novel transformer encoder-decoder model, called hybrid routing transformer
(HRT). In HRT encoder, we embed an active attention, which is constructed by
both the bottom-up and the top-down dynamic routing pathways to generate the
attribute-aligned visual feature. While in HRT decoder, we use static routing
to calculate the correlation among the attribute-aligned visual features, the
corresponding attribute semantics, and the class attribute vectors to generate
the final class label predictions. This design makes the presented transformer
model a hybrid of 1) top-down and bottom-up attention pathways and 2) dynamic
and static routing pathways. Comprehensive experiments on three widely-used
benchmark datasets, namely CUB, SUN, and AWA2, are conducted. The obtained
experimental results demonstrate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-N-Out Generative Learning for Dense Unsupervised Video Segmentation. (arXiv:2203.15312v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15312">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on the unsupervised Video Object Segmentation (VOS)
task which learns visual correspondence from unlabeled videos. Previous methods
are mainly based on the contrastive learning paradigm, which optimize either in
pixel level or image level and show unsatisfactory scalability. Image-level
optimization learns pixel-wise information implicitly therefore is sub-optimal
for such dense prediction task, while pixel-level optimization ignores the
high-level semantic scope for capturing object deformation. To complementarily
learn these two levels of information in an unified framework, we propose the
In-aNd-Out (INO) generative learning from a purely generative perspective,
which captures both high-level and fine-grained semantics by leveraging the
structural superiority of Vision Transformer (ViT) and achieves better
scalability. Specifically, the in-generative learning recovers the corrupted
parts of an image via inferring its fine-grained semantic structure, while the
out-generative learning captures high-level semantics by imagining the global
information of an image given only random fragments. To better discover the
temporal information, we additionally force the inter-frame consistency from
both feature level and affinity matrix level. Extensive experiments on
DAVIS-2017 val and YouTube-VOS 2018 val show that our INO outperforms previous
state-of-the-art methods by significant margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality High-Frequency Transformer for MR Image Super-Resolution. (arXiv:2203.15314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15314">
<div class="article-summary-box-inner">
<span><p>Improving the resolution of magnetic resonance (MR) image data is critical to
computer-aided diagnosis and brain function analysis. Higher resolution helps
to capture more detailed content, but typically induces to lower
signal-to-noise ratio and longer scanning time. To this end, MR image
super-resolution has become a widely-interested topic in recent times. Existing
works establish extensive deep models with the conventional architectures based
on convolutional neural networks (CNN). In this work, to further advance this
research field, we make an early effort to build a Transformer-based MR image
super-resolution framework, with careful designs on exploring valuable domain
prior knowledge. Specifically, we consider two-fold domain priors including the
high-frequency structure prior and the inter-modality context prior, and
establish a novel Transformer architecture, called Cross-modality
high-frequency Transformer (Cohf-T), to introduce such priors into
super-resolving the low-resolution (LR) MR images. Comprehensive experiments on
two datasets indicate that Cohf-T achieves new state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agreement or Disagreement in Noise-tolerant Mutual Learning?. (arXiv:2203.15317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15317">
<div class="article-summary-box-inner">
<span><p>Deep learning has made many remarkable achievements in many fields but
suffers from noisy labels in datasets. The state-of-the-art learning with noisy
label method Co-teaching and Co-teaching+ confronts the noisy label by
mutual-information between dual-network. However, the dual network always tends
to convergent which would weaken the dual-network mechanism to resist the noisy
labels. In this paper, we proposed a noise-tolerant framework named MLC in an
end-to-end manner. It adjusts the dual-network with divergent regularization to
ensure the effectiveness of the mechanism. In addition, we correct the label
distribution according to the agreement between dual-networks. The proposed
method can utilize the noisy data to improve the accuracy, generalization, and
robustness of the network. We test the proposed method on the simulate noisy
dataset MNIST, CIFAR-10, and the real-world noisy dataset Clothing1M. The
experimental result shows that our method outperforms the previous
state-of-the-art method. Besides, our method is network-free thus it is
applicable to many tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dressing in the Wild by Watching Dance Videos. (arXiv:2203.15320v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15320">
<div class="article-summary-box-inner">
<span><p>While significant progress has been made in garment transfer, one of the most
applicable directions of human-centric image generation, existing works
overlook the in-the-wild imagery, presenting severe garment-person misalignment
as well as noticeable degradation in fine texture details. This paper,
therefore, attends to virtual try-on in real-world scenes and brings essential
improvements in authenticity and naturalness especially for loose garment
(e.g., skirts, formal dresses), challenging poses (e.g., cross arms, bent
legs), and cluttered backgrounds. Specifically, we find that the pixel flow
excels at handling loose garments whereas the vertex flow is preferred for hard
poses, and by combining their advantages we propose a novel generative network
called wFlow that can effectively push up garment transfer to in-the-wild
context. Moreover, former approaches require paired images for training.
Instead, we cut down the laboriousness by working on a newly constructed
large-scale video dataset named Dance50k with self-supervised cross-frame
training and an online cycle optimization. The proposed Dance50k can boost
real-world virtual dressing by covering a wide variety of garments under
dancing poses. Extensive experiments demonstrate the superiority of our wFlow
in generating realistic garment transfer results for in-the-wild images without
resorting to expensive paired datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Single Image Dehazing Based on Consistent and Contrast-Assisted Reconstruction. (arXiv:2203.15325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15325">
<div class="article-summary-box-inner">
<span><p>Single image dehazing as a fundamental low-level vision task, is essential
for the development of robust intelligent surveillance system. In this paper,
we make an early effort to consider dehazing robustness under variational haze
density, which is a realistic while under-studied problem in the research filed
of singe image dehazing. To properly address this problem, we propose a novel
density-variational learning framework to improve the robustness of the image
dehzing model assisted by a variety of negative hazy images, to better deal
with various complex hazy scenarios. Specifically, the dehazing network is
optimized under the consistency-regularized framework with the proposed
Contrast-Assisted Reconstruction Loss (CARL). The CARL can fully exploit the
negative information to facilitate the traditional positive-orient dehazing
objective function, by squeezing the dehazed image to its clean target from
different directions. Meanwhile, the consistency regularization keeps
consistent outputs given multi-level hazy images, thus improving the model
robustness. Extensive experimental results on two synthetic and three
real-world datasets demonstrate that our method significantly surpasses the
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15331">
<div class="article-summary-box-inner">
<span><p>Currently, many theoretical as well as practically relevant questions towards
the transferability and robustness of Convolutional Neural Networks (CNNs)
remain unsolved. While ongoing research efforts are engaging these problems
from various angles, in most computer vision related cases these approaches can
be generalized to investigations of the effects of distribution shifts in image
data. In this context, we propose to study the shifts in the learned weights of
trained CNN models. Here we focus on the properties of the distributions of
dominantly used 3x3 convolution filter kernels. We collected and publicly
provide a dataset with over 1.4 billion filters from hundreds of trained CNNs,
using a wide range of datasets, architectures, and vision tasks. In a first use
case of the proposed dataset, we can show highly relevant properties of many
publicly available pre-trained models for practical applications: I) We analyze
distribution shifts (or the lack thereof) between trained filters along
different axes of meta-parameters, like visual category of the dataset, task,
architecture, or layer depth. Based on these results, we conclude that model
pre-training can succeed on arbitrary datasets if they meet size and variance
conditions. II) We show that many pre-trained models contain degenerated
filters which make them less robust and less suitable for fine-tuning on target
applications.
</p>
<p>Data &amp; Project website: https://github.com/paulgavrikov/cnn-filter-db
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced Multimodal Learning via On-the-fly Gradient Modulation. (arXiv:2203.15332v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15332">
<div class="article-summary-box-inner">
<span><p>Multimodal learning helps to comprehensively understand the world, by
integrating different senses. Accordingly, multiple input modalities are
expected to boost model performance, but we actually find that they are not
fully exploited even when the multimodal model outperforms its uni-modal
counterpart. Specifically, in this paper we point out that existing multimodal
discriminative models, in which uniform objective is designed for all
modalities, could remain under-optimized uni-modal representations, caused by
another dominated modality in some scenarios, e.g., sound in blowing wind
event, vision in drawing picture event, etc. To alleviate this optimization
imbalance, we propose on-the-fly gradient modulation to adaptively control the
optimization of each modality, via monitoring the discrepancy of their
contribution towards the learning objective. Further, an extra Gaussian noise
that changes dynamically is introduced to avoid possible generalization drop
caused by gradient modulation. As a result, we achieve considerable improvement
over common fusion methods on different multimodal tasks, and this simple
strategy can also boost existing multimodal methods, which illustrates its
efficacy and versatility. The source code is available at
\url{https://github.com/GeWu-Lab/OGM-GE_CVPR2022}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnyFace: Free-style Text-to-Face Synthesis and Manipulation. (arXiv:2203.15334v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15334">
<div class="article-summary-box-inner">
<span><p>Existing text-to-image synthesis methods generally are only applicable to
words in the training dataset. However, human faces are so variable to be
described with limited words. So this paper proposes the first free-style
text-to-face method namely AnyFace enabling much wider open world applications
such as metaverse, social media, cosmetics, forensics, etc. AnyFace has a novel
two-stream framework for face image synthesis and manipulation given arbitrary
descriptions of the human face. Specifically, one stream performs text-to-face
generation and the other conducts face image reconstruction. Facial text and
image features are extracted using the CLIP (Contrastive Language-Image
Pre-training) encoders. And a collaborative Cross Modal Distillation (CMD)
module is designed to align the linguistic and visual features across these two
streams. Furthermore, a Diverse Triplet Loss (DT loss) is developed to model
fine-grained features and improve facial diversity. Extensive experiments on
Multi-modal CelebA-HQ and CelebAText-HQ demonstrate significant advantages of
AnyFace over state-of-the-art methods. AnyFace can achieve high-quality,
high-resolution, and high-diversity face synthesis and manipulation results
without any constraints on the number and content of input captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Compressed Video Representation Learning for Generic Event Boundary Detection. (arXiv:2203.15336v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15336">
<div class="article-summary-box-inner">
<span><p>Generic event boundary detection aims to localize the generic, taxonomy-free
event boundaries that segment videos into chunks. Existing methods typically
require video frames to be decoded before feeding into the network, which
demands considerable computational power and storage space. To that end, we
propose a new end-to-end compressed video representation learning for event
boundary detection that leverages the rich information in the compressed
domain, i.e., RGB, motion vectors, residuals, and the internal group of
pictures (GOP) structure, without fully decoding the video. Specifically, we
first use the ConvNets to extract features of the I-frames in the GOPs. After
that, a light-weight spatial-channel compressed encoder is designed to compute
the feature representations of the P-frames based on the motion vectors,
residuals and representations of their dependent I-frames. A temporal
contrastive module is proposed to determine the event boundaries of video
sequences. To remedy the ambiguities of annotations and speed up the training
process, we use the Gaussian kernel to preprocess the ground-truth event
boundaries. Extensive experiments conducted on the Kinetics-GEBD dataset
demonstrate that the proposed method achieves comparable results to the
state-of-the-art methods with $4.5\times$ faster running speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infrared and Visible Image Fusion via Interactive Compensatory Attention Adversarial Learning. (arXiv:2203.15337v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15337">
<div class="article-summary-box-inner">
<span><p>The existing generative adversarial fusion methods generally concatenate
source images and extract local features through convolution operation, without
considering their global characteristics, which tends to produce an unbalanced
result and is biased towards the infrared image or visible image. Toward this
end, we propose a novel end-to-end mode based on generative adversarial
training to achieve better fusion balance, termed as \textit{interactive
compensatory attention fusion network} (ICAFusion). In particular, in the
generator, we construct a multi-level encoder-decoder network with a triple
path, and adopt infrared and visible paths to provide additional intensity and
gradient information. Moreover, we develop interactive and compensatory
attention modules to communicate their pathwise information, and model their
long-range dependencies to generate attention maps, which can more focus on
infrared target perception and visible detail characterization, and further
increase the representation power for feature extraction and feature
reconstruction. In addition, dual discriminators are designed to identify the
similar distribution between fused result and source images, and the generator
is optimized to produce a more balanced result. Extensive experiments
illustrate that our ICAFusion obtains superior fusion performance and better
generalization ability, which precedes other advanced methods in the subjective
visual description and objective metric evaluation. Our codes will be public at
\url{https://github.com/Zhishe-Wang/ICAFusion}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-specific Inconsistency Alignment for Domain Adaptive Object Detection. (arXiv:2203.15345v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15345">
<div class="article-summary-box-inner">
<span><p>Detectors trained with massive labeled data often exhibit dramatic
performance degradation in some particular scenarios with data distribution
gap. To alleviate this problem of domain shift, conventional wisdom typically
concentrates solely on reducing the discrepancy between the source and target
domains via attached domain classifiers, yet ignoring the difficulty of such
transferable features in coping with both classification and localization
subtasks in object detection. To address this issue, in this paper, we propose
Task-specific Inconsistency Alignment (TIA), by developing a new alignment
mechanism in separate task spaces, improving the performance of the detector on
both subtasks. Specifically, we add a set of auxiliary predictors for both
classification and localization branches, and exploit their behavioral
inconsistencies as finer-grained domain-specific measures. Then, we devise
task-specific losses to align such cross-domain disagreement of both subtasks.
By optimizing them individually, we are able to well approximate the category-
and boundary-wise discrepancies in each task space, and therefore narrow them
in a decoupled manner. TIA demonstrates superior results on various scenarios
to the previous state-of-the-art methods. It is also observed that both the
classification and localization capabilities of the detector are sufficiently
strengthened, further demonstrating the effectiveness of our TIA method. Code
and trained models are publicly available at https://github.com/MCG-NJU/TIA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harmonizing Pathological and Normal Pixels for Pseudo-healthy Synthesis. (arXiv:2203.15347v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15347">
<div class="article-summary-box-inner">
<span><p>Synthesizing a subject-specific pathology-free image from a pathological
image is valuable for algorithm development and clinical practice. In recent
years, several approaches based on the Generative Adversarial Network (GAN)
have achieved promising results in pseudo-healthy synthesis. However, the
discriminator (i.e., a classifier) in the GAN cannot accurately identify
lesions and further hampers from generating admirable pseudo-healthy images. To
address this problem, we present a new type of discriminator, the segmentor, to
accurately locate the lesions and improve the visual quality of pseudo-healthy
images. Then, we apply the generated images into medical image enhancement and
utilize the enhanced results to cope with the low contrast problem existing in
medical image segmentation. Furthermore, a reliable metric is proposed by
utilizing two attributes of label noise to measure the health of synthetic
images. Comprehensive experiments on the T2 modality of BraTS demonstrate that
the proposed method substantially outperforms the state-of-the-art methods. The
method achieves better performance than the existing methods with only 30\% of
the training data. The effectiveness of the proposed method is also
demonstrated on the LiTS and the T1 modality of BraTS. The code and the
pre-trained model of this study are publicly available at
https://github.com/Au3C2/Generator-Versus-Segmentor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Transformer Based Model for Image Captioning. (arXiv:2203.15350v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15350">
<div class="article-summary-box-inner">
<span><p>CNN-LSTM based architectures have played an important role in image
captioning, but limited by the training efficiency and expression ability,
researchers began to explore the CNN-Transformer based models and achieved
great success. Meanwhile, almost all recent works adopt Faster R-CNN as the
backbone encoder to extract region-level features from given images. However,
Faster R-CNN needs a pre-training on an additional dataset, which divides the
image captioning task into two stages and limits its potential applications. In
this paper, we build a pure Transformer-based model, which integrates image
captioning into one stage and realizes end-to-end training. Firstly, we adopt
SwinTransformer to replace Faster R-CNN as the backbone encoder to extract
grid-level features from given images; Then, referring to Transformer, we build
a refining encoder and a decoder. The refining encoder refines the grid
features by capturing the intra-relationship between them, and the decoder
decodes the refined features into captions word by word. Furthermore, in order
to increase the interaction between multi-modal (vision and language) features
to enhance the modeling capability, we calculate the mean pooling of grid
features as the global feature, then introduce it into refining encoder to
refine with grid features together, and add a pre-fusion process of refined
global feature and generated words in decoder. To validate the effectiveness of
our proposed model, we conduct experiments on MSCOCO dataset. The experimental
results compared to existing published works demonstrate that our model
achieves new state-of-the-art performances of 138.2% (single model) and 141.0%
(ensemble of 4 models) CIDEr scores on `Karpathy' offline test split and 136.0%
(c5) and 138.3% (c40) CIDEr scores on the official online test server. Trained
models and source code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIOD: Single Instance Annotated Per Category Per Image for Object Detection. (arXiv:2203.15353v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15353">
<div class="article-summary-box-inner">
<span><p>Object detection under imperfect data receives great attention recently.
Weakly supervised object detection (WSOD) suffers from severe localization
issues due to the lack of instance-level annotation, while semi-supervised
object detection (SSOD) remains challenging led by the inter-image discrepancy
between labeled and unlabeled data. In this study, we propose the Single
Instance annotated Object Detection (SIOD), requiring only one instance
annotation for each existing category in an image. Degraded from inter-task
(WSOD) or inter-image (SSOD) discrepancies to the intra-image discrepancy, SIOD
provides more reliable and rich prior knowledge for mining the rest of
unlabeled instances and trades off the annotation cost and performance. Under
the SIOD setting, we propose a simple yet effective framework, termed
Dual-Mining (DMiner), which consists of a Similarity-based Pseudo Label
Generating module (SPLG) and a Pixel-level Group Contrastive Learning module
(PGCL). SPLG firstly mines latent instances from feature representation space
to alleviate the annotation missing problem. To avoid being misled by
inaccurate pseudo labels, we propose PGCL to boost the tolerance to false
pseudo labels. Extensive experiments on MS COCO verify the feasibility of the
SIOD setting and the superiority of the proposed method, which obtains
consistent and significant improvements compared to baseline methods and
achieves comparable results with fully supervised object detection (FSOD)
methods with only 40% instances annotated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production. (arXiv:2203.15354v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15354">
<div class="article-summary-box-inner">
<span><p>Sign languages are visual languages, with vocabularies as rich as their
spoken language counterparts. However, current deep-learning based Sign
Language Production (SLP) models produce under-articulated skeleton pose
sequences from constrained vocabularies and this limits applicability. To be
understandable and accepted by the deaf, an automatic SLP system must be able
to generate co-articulated photo-realistic signing sequences for large domains
of discourse.
</p>
<p>In this work, we tackle large-scale SLP by learning to co-articulate between
dictionary signs, a method capable of producing smooth signing while scaling to
unconstrained domains of discourse. To learn sign co-articulation, we propose a
novel Frame Selection Network (FS-Net) that improves the temporal alignment of
interpolated dictionary signs to continuous signing sequences. Additionally, we
propose SignGAN, a pose-conditioned human synthesis model that produces
photo-realistic sign language videos direct from skeleton pose. We propose a
novel keypoint-based loss function which improves the quality of synthesized
hand images.
</p>
<p>We evaluate our SLP model on the large-scale meineDGS (mDGS) corpus,
conducting extensive user evaluation showing our FS-Net approach improves
co-articulation of interpolated dictionary signs. Additionally, we show that
SignGAN significantly outperforms all baseline methods for quantitative
metrics, human perceptual studies and native deaf signer comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15355">
<div class="article-summary-box-inner">
<span><p>Learning under a continuously changing data distribution with incorrect
labels is a desirable real-world problem yet challenging. A large body of
continual learning (CL) methods, however, assumes data streams with clean
labels, and online learning scenarios under noisy data streams are yet
underexplored. We consider a more practical CL task setup of an online learning
from blurry data stream with corrupted labels, where existing CL methods
struggle. To address the task, we first argue the importance of both diversity
and purity of examples in the episodic memory of continual learning models. To
balance diversity and purity in the episodic memory, we propose a novel
strategy to manage and use the memory by a unified approach of label noise
aware diverse sampling and robust learning with semi-supervised learning. Our
empirical validations on four real-world or synthetic noise datasets (CIFAR10
and 100, mini-WebVision, and Food-101N) exhibit that our method significantly
outperforms prior arts in this realistic and challenging continual learning
scenario. Code and data splits are available in
https://github.com/clovaai/puridiver.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Collaborative Learning for Long-Tailed Visual Recognition. (arXiv:2203.15359v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15359">
<div class="article-summary-box-inner">
<span><p>The networks trained on the long-tailed dataset vary remarkably, despite the
same training settings, which shows the great uncertainty in long-tailed
learning. To alleviate the uncertainty, we propose a Nested Collaborative
Learning (NCL), which tackles the problem by collaboratively learning multiple
experts together. NCL consists of two core components, namely Nested Individual
Learning (NIL) and Nested Balanced Online Distillation (NBOD), which focus on
the individual supervised learning for each single expert and the knowledge
transferring among multiple experts, respectively. To learn representations
more thoroughly, both NIL and NBOD are formulated in a nested way, in which the
learning is conducted on not just all categories from a full perspective but
some hard categories from a partial perspective. Regarding the learning in the
partial perspective, we specifically select the negative categories with high
predicted scores as the hard categories by using a proposed Hard Category
Mining (HCM). In the NCL, the learning from two perspectives is nested, highly
related and complementary, and helps the network to capture not only global and
robust features but also meticulous distinguishing ability. Moreover,
self-supervision is further utilized for feature enhancement. Extensive
experiments manifest the superiority of our method with outperforming the
state-of-the-art whether by using a single model or an ensemble.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Image Representation Learning with Geometric Set Consistency. (arXiv:2203.15361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15361">
<div class="article-summary-box-inner">
<span><p>We propose a method for self-supervised image representation learning under
the guidance of 3D geometric consistency. Our intuition is that 3D geometric
consistency priors such as smooth regions and surface discontinuities may imply
consistent semantics or object boundaries, and can act as strong cues to guide
the learning of 2D image representations without semantic labels. Specifically,
we introduce 3D geometric consistency into a contrastive learning framework to
enforce the feature consistency within image views. We propose to use geometric
consistency sets as constraints and adapt the InfoNCE loss accordingly. We show
that our learned image representations are general. By fine-tuning our
pre-trained representations for various 2D image-based downstream tasks,
including semantic segmentation, object detection, and instance segmentation on
real-world indoor scene datasets, we achieve superior performance compared with
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Invariant Siamese Attention Mask for Small Object Change Detection via Everyday Indoor Robot Navigation. (arXiv:2203.15362v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15362">
<div class="article-summary-box-inner">
<span><p>The problem of image change detection via everyday indoor robot navigation is
explored from a novel perspective of the self-attention technique. Detecting
semantically non-distinctive and visually small changes remains a key challenge
in the robotics community. Intuitively, these small non-distinctive changes may
be better handled by the recent paradigm of the attention mechanism, which is
the basic idea of this work. However, existing self-attention models require
significant retraining cost per domain, so it is not directly applicable to
robotics applications. We propose a new self-attention technique with an
ability of unsupervised on-the-fly domain adaptation, which introduces an
attention mask into the intermediate layer of an image change detection model,
without modifying the input and output layers of the model. Experiments, in
which an indoor robot aims to detect visually small changes in everyday
navigation, demonstrate that our attention technique significantly boosts the
state-of-the-art image change detection model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face segmentation: A comparison between visible and thermal images. (arXiv:2203.15366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15366">
<div class="article-summary-box-inner">
<span><p>Face segmentation is a first step for face biometric systems. In this paper
we present a face segmentation algorithm for thermographic images. This
algorithm is compared with the classic Viola and Jones algorithm used for
visible images. Experimental results reveal that, when segmenting a
multispectral (visible and thermal) face database, the proposed algorithm is
more than 10 times faster, while the accuracy of face segmentation in thermal
images is higher than in case of Viola-Jones
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mc-BEiT: Multi-choice Discretization for Image BERT Pre-training. (arXiv:2203.15371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15371">
<div class="article-summary-box-inner">
<span><p>Image BERT pre-training with masked image modeling (MIM) becomes a popular
practice to cope with self-supervised representation learning. A seminal work,
BEiT, casts MIM as a classification task with a visual vocabulary, tokenizing
the continuous visual signals into discrete vision tokens using a pre-learned
dVAE. Despite a feasible solution, the improper discretization hinders further
improvements of image pre-training. Since image discretization has no
ground-truth answers, we believe that the masked patch should not be assigned
with a unique token id even if a better tokenizer can be obtained. In this
work, we introduce an improved BERT-style image pre-training method, namely
mc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice
training objectives. Specifically, the multi-choice supervision for the masked
image patches is formed by the soft probability vectors of the discrete token
ids, which are predicted by the off-the-shelf image tokenizer and further
refined by high-level inter-patch perceptions resorting to the observation that
similar patches should share their choices. Extensive experiments on
classification, segmentation, and detection tasks demonstrate the superiority
of our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning
accuracy on ImageNet-1K classification, 51.2% mIOU on ADE20K semantic
segmentation, 51.2% AP^b and 44.3% AP^m of object detection and instance
segmentation on COCO, outperforming the competitive counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Style-aware Discriminator for Controllable Image Translation. (arXiv:2203.15375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15375">
<div class="article-summary-box-inner">
<span><p>Current image-to-image translations do not control the output domain beyond
the classes used during training, nor do they interpolate between different
domains well, leading to implausible results. This limitation largely arises
because labels do not consider the semantic distance. To mitigate such
problems, we propose a style-aware discriminator that acts as a critic as well
as a style encoder to provide conditions. The style-aware discriminator learns
a controllable style space using prototype-based self-supervised learning and
simultaneously guides the generator. Experiments on multiple datasets verify
that the proposed model outperforms current state-of-the-art image-to-image
translation methods. In contrast with current methods, the proposed approach
supports various applications, including style interpolation, content
transplantation, and local image translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SepViT: Separable Vision Transformer. (arXiv:2203.15380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15380">
<div class="article-summary-box-inner">
<span><p>Vision Transformers have witnessed prevailing success in a series of vision
tasks. However, they often require enormous amount of computations to achieve
high performance, which is burdensome to deploy on resource-constrained
devices. To address these issues, we draw lessons from depthwise separable
convolution and imitate its ideology to design the Separable Vision
Transformer, abbreviated as SepViT. SepViT helps to carry out the information
interaction within and among the windows via a depthwise separable
self-attention. The novel window token embedding and grouped self-attention are
employed to model the attention relationship among windows with negligible
computational cost and capture a long-range visual dependencies of multiple
windows, respectively. Extensive experiments on various benchmark tasks
demonstrate SepViT can achieve state-of-the-art results in terms of trade-off
between accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy
on ImageNet-1K classification while decreasing the latency by 40%, compared to
the ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream
vision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic
segmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box
AP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification. (arXiv:2203.15381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15381">
<div class="article-summary-box-inner">
<span><p>Most methods tackle zero-shot video classification by aligning
visual-semantic representations within seen classes, which limits
generalization to unseen classes. To enhance model generalizability, this paper
presents an end-to-end framework that preserves alignment and uniformity
properties for representations on both seen and unseen classes. Specifically,
we formulate a supervised contrastive loss to simultaneously align
visual-semantic features (i.e., alignment) and encourage the learned features
to distribute uniformly (i.e., uniformity). Unlike existing methods that only
consider the alignment, we propose uniformity to preserve maximal-info of
existing features, which improves the probability that unobserved features fall
around observed data. Further, we synthesize features of unseen classes by
proposing a class generator that interpolates and extrapolates the features of
seen classes. Besides, we introduce two metrics, closeness and dispersion, to
quantify the two properties and serve as new measurements of model
generalizability. Experiments show that our method significantly outperforms
SoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51. Code is
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category Guided Attention Network for Brain Tumor Segmentation in MRI. (arXiv:2203.15383v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15383">
<div class="article-summary-box-inner">
<span><p>Objective: Magnetic resonance imaging (MRI) has been widely used for the
analysis and diagnosis of brain diseases. Accurate and automatic brain tumor
segmentation is of paramount importance for radiation treatment. However, low
tissue contrast in tumor regions makes it a challenging task.Approach: We
propose a novel segmentation network named Category Guided Attention U-Net (CGA
U-Net). In this model, we design a Supervised Attention Module (SAM) based on
the attention mechanism, which can capture more accurate and stable long-range
dependency in feature maps without introducing much computational cost.
Moreover, we propose an intra-class update approach to reconstruct feature maps
by aggregating pixels of the same category. Main results: Experimental results
on the BraTS 2019 datasets show that the proposed method outperformers the
state-of-the-art algorithms in both segmentation performance and computational
complexity. Significance: The CGA U-Net can effectively capture the global
semantic information in the MRI image by using the SAM module, while
significantly reducing the computational cost. Code is available at
https://github.com/delugewalker/CGA-U-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Hybrid Network: Inducting Scattering Features. (arXiv:2203.15392v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15392">
<div class="article-summary-box-inner">
<span><p>Recent work showed that hybrid networks, which combine predefined and learnt
filters within a single architecture, are more amenable to theoretical analysis
and less prone to overfitting in data-limited scenarios. However, their
performance has yet to prove competitive against the conventional counterparts
when sufficient amounts of training data are available. In an attempt to
address this core limitation of current hybrid networks, we introduce an
Efficient Hybrid Network (E-HybridNet). We show that it is the first scattering
based approach that consistently outperforms its conventional counterparts on a
diverse range of datasets. It is achieved with a novel inductive architecture
that embeds scattering features into the network flow using Hybrid Fusion
Blocks. We also demonstrate that the proposed design inherits the key property
of prior hybrid networks -- an effective generalisation in data-limited
scenarios. Our approach successfully combines the best of the two worlds:
flexibility and power of learnt features and stability and predictability of
scattering representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Societal Bias Amplification in Image Captioning. (arXiv:2203.15395v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15395">
<div class="article-summary-box-inner">
<span><p>We study societal bias amplification in image captioning. Image captioning
models have been shown to perpetuate gender and racial biases, however, metrics
to measure, quantify, and evaluate the societal bias in captions are not yet
standardized. We provide a comprehensive study on the strengths and limitations
of each metric, and propose LIC, a metric to study captioning bias
amplification. We argue that, for image captioning, it is not enough to focus
on the correct prediction of the protected attribute, and the whole context
should be taken into account. We conduct extensive evaluation on traditional
and state-of-the-art image captioning models, and surprisingly find that, by
only focusing on the protected attribute prediction, bias mitigation models are
unexpectedly amplifying bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Face Video Compression using Multiple Views. (arXiv:2203.15401v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15401">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep generative models led to the development of neural
face video compression codecs that use an order of magnitude less bandwidth
than engineered codecs. These neural codecs reconstruct the current frame by
warping a source frame and using a generative model to compensate for
imperfections in the warped source frame. Thereby, the warp is encoded and
transmitted using a small number of keypoints rather than a dense flow field,
which leads to massive savings compared to traditional codecs. However, by
relying on a single source frame only, these methods lead to inaccurate
reconstructions (e.g. one side of the head becomes unoccluded when turning the
head and has to be synthesized). Here, we aim to tackle this issue by relying
on multiple source frames (views of the face) and present encouraging results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransGAN: a Transductive Adversarial Model for Novelty Detection. (arXiv:2203.15406v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15406">
<div class="article-summary-box-inner">
<span><p>Novelty detection, a widely studied problem in machine learning, is the
problem of detecting a novel class of data that has not been previously
observed. A common setting for novelty detection is inductive whereby only
examples of the negative class are available during training time. Transductive
novelty detection on the other hand has only witnessed a recent surge in
interest, it not only makes use of the negative class during training but also
incorporates the (unlabeled) test set to detect novel examples. Several studies
have emerged under the transductive setting umbrella that have demonstrated its
advantage over its inductive counterpart. Depending on the assumptions about
the data, these methods go by different names (e.g. transductive novelty
detection, semi-supervised novelty detection, positive-unlabeled learning,
out-of-distribution detection). With the use of generative adversarial networks
(GAN), a segment of those studies have adopted a transductive setup in order to
learn how to generate examples of the novel class. In this study, we propose
TransGAN, a transductive generative adversarial network that attempts to learn
how to generate image examples from both the novel and negative classes by
using a mixture of two Gaussians in the latent space. It achieves that by
incorporating an adversarial autoencoder with a GAN network, the ability to
generate examples of novel data points offers not only a visual representation
of novelties, but also overcomes the hurdle faced by many inductive methods of
how to tune the model hyperparameters at the decision rule level. Our model has
shown superior performance over state-of-the-art inductive and transductive
methods. Our study is fully reproducible with the code available publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoCoMet: Smart Neural Architecture Search via Co-Regulated Shaping Reinforcement. (arXiv:2203.15408v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15408">
<div class="article-summary-box-inner">
<span><p>Designing suitable deep model architectures, for AI-driven on-device apps and
features, at par with rapidly evolving mobile hardware and increasingly complex
target scenarios is a difficult task. Though Neural Architecture Search
(NAS/AutoML) has made this easier by shifting paradigm from extensive manual
effort to automated architecture learning from data, yet it has major
limitations, leading to critical bottlenecks in the context of mobile devices,
including model-hardware fidelity, prohibitive search times and deviation from
primary target objective(s). Thus, we propose AutoCoMet that can learn the most
suitable DNN architecture optimized for varied types of device hardware and
task contexts, ~ 3x faster. Our novel co-regulated shaping reinforcement
controller together with the high fidelity hardware meta-behavior predictor
produces a smart, fast NAS framework that adapts to context via a generalized
formalism for any kind of multi-criteria optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Reinforcement Learning for Data-Driven Adaptive Scanning in Ptychography. (arXiv:2203.15413v1 [physics.comp-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15413">
<div class="article-summary-box-inner">
<span><p>We present a method that lowers the dose required for a ptychographic
reconstruction by adaptively scanning the specimen, thereby providing the
required spatial information redundancy in the regions of highest importance.
The proposed method is built upon a deep learning model that is trained by
reinforcement learning (RL), using prior knowledge of the specimen structure
from training data sets. We show that equivalent low-dose experiments using
adaptive scanning outperform conventional ptychography experiments in terms of
reconstruction resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-term Video Frame Interpolation via Feature Propagation. (arXiv:2203.15427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15427">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation (VFI) works generally predict intermediate frame(s)
by first estimating the motion between inputs and then warping the inputs to
the target time with the estimated motion. This approach, however, is not
optimal when the temporal distance between the input sequence increases as
existing motion estimation modules cannot effectively handle large motions.
Hence, VFI works perform well for small frame gaps and perform poorly as the
frame gap increases. In this work, we propose a novel framework to address this
problem. We argue that when there is a large gap between inputs, instead of
estimating imprecise motion that will eventually lead to inaccurate
interpolation, we can safely propagate from one side of the input up to a
reliable time frame using the other input as a reference. Then, the rest of the
intermediate frames can be interpolated using standard approaches as the
temporal gap is now narrowed. To this end, we propose a propagation network
(PNet) by extending the classic feature-level forecasting with a novel
motion-to-feature approach. To be thorough, we adopt a simple interpolation
model along with PNet as our full model and design a simple procedure to train
the full model in an end-to-end manner. Experimental results on several
benchmark datasets confirm the effectiveness of our method for long-term VFI
compared to state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clean Implicit 3D Structure from Noisy 2D STEM Images. (arXiv:2203.15434v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15434">
<div class="article-summary-box-inner">
<span><p>Scanning Transmission Electron Microscopes (STEMs) acquire 2D images of a 3D
sample on the scale of individual cell components. Unfortunately, these 2D
images can be too noisy to be fused into a useful 3D structure and facilitating
good denoisers is challenging due to the lack of clean-noisy pairs.
Additionally, representing a detailed 3D structure can be difficult even for
clean data when using regular 3D grids. Addressing these two limitations, we
suggest a differentiable image formation model for STEM, allowing to learn a
joint model of 2D sensor noise in STEM together with an implicit 3D model. We
show, that the combination of these models are able to successfully disentangle
3D signal and noise without supervision and outperform at the same time several
baselines on synthetic and real data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Information Based Anomaly Detection for a Multi-Scene UAV Aerial Videos. (arXiv:2203.15437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15437">
<div class="article-summary-box-inner">
<span><p>UAV based surveillance is gaining much interest worldwide due to its
extensive applications in monitoring wildlife, urban planning, disaster
management, campus security, etc. These videos are analyzed for
strange/odd/anomalous patterns which are essential aspects of surveillance. But
manual analysis of these videos is tedious and laborious. Hence, the
development of computer-aided systems for the analysis of UAV based
surveillance videos is crucial. Despite this interest, in literature, several
computer aided systems are developed focusing only on CCTV based surveillance
videos. These methods are designed for single scene scenarios and lack
contextual knowledge which is required for multi-scene scenarios. Furthermore,
the lack of standard UAV based anomaly detection datasets limits the
development of these systems. In this regard, the present work aims at the
development of a Computer Aided Decision support system to analyse UAV based
surveillance videos. A new UAV based multi-scene anomaly detection dataset is
developed with frame-level annotations for the development of computer aided
systems. It holistically uses contextual, temporal and appearance features for
accurate detection of anomalies. Furthermore, a new inference strategy is
proposed that utilizes few anomalous samples along with normal samples to
identify better decision boundaries. The proposed method is extensively
evaluated on the UAV based anomaly detection dataset and performed
competitively with respect to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eventor: An Efficient Event-Based Monocular Multi-View Stereo Accelerator on FPGA Platform. (arXiv:2203.15439v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15439">
<div class="article-summary-box-inner">
<span><p>Event cameras are bio-inspired vision sensors that asynchronously represent
pixel-level brightness changes as event streams. Event-based monocular
multi-view stereo (EMVS) is a technique that exploits the event streams to
estimate semi-dense 3D structure with known trajectory. It is a critical task
for event-based monocular SLAM. However, the required intensive computation
workloads make it challenging for real-time deployment on embedded platforms.
In this paper, Eventor is proposed as a fast and efficient EMVS accelerator by
realizing the most critical and time-consuming stages including event
back-projection and volumetric ray-counting on FPGA. Highly paralleled and
fully pipelined processing elements are specially designed via FPGA and
integrated with the embedded ARM as a heterogeneous system to improve the
throughput and reduce the memory footprint. Meanwhile, the EMVS algorithm is
reformulated to a more hardware-friendly manner by rescheduling, approximate
computing and hybrid data quantization. Evaluation results on DAVIS dataset
show that Eventor achieves up to $24\times$ improvement in energy efficiency
compared with Intel i5 CPU platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnShadowNet: Illumination Critic Guided Contrastive Learning For Shadow Removal. (arXiv:2203.15441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15441">
<div class="article-summary-box-inner">
<span><p>Shadows are frequently encountered natural phenomena that significantly
hinder the performance of computer vision perception systems in practical
settings, e.g., autonomous driving. A solution to this would be to eliminate
shadow regions from the images before the processing of the perception system.
Yet, training such a solution requires pairs of aligned shadowed and
non-shadowed images which are difficult to obtain. We introduce a novel weakly
supervised shadow removal framework UnShadowNet trained using contrastive
learning. It comprises of a DeShadower network responsible for removal of the
extracted shadow under the guidance of an Illumination network which is trained
adversarially by the illumination critic and a Refinement network to further
remove artifacts. We show that UnShadowNet can also be easily extended to a
fully-supervised setup to exploit the ground-truth when available. UnShadowNet
outperforms existing state-of-the-art approaches on three publicly available
shadow datasets (ISTD, adjusted ISTD, SRD) in both the weakly and fully
supervised setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding. (arXiv:2203.15442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15442">
<div class="article-summary-box-inner">
<span><p>Visual grounding focuses on establishing fine-grained alignment between
vision and natural language, which has essential applications in multimodal
reasoning systems. Existing methods use pre-trained query-agnostic visual
backbones to extract visual feature maps independently without considering the
query information. We argue that the visual features extracted from the visual
backbones and the features really needed for multimodal reasoning are
inconsistent. One reason is that there are differences between pre-training
tasks and visual grounding. Moreover, since the backbones are query-agnostic,
it is difficult to completely avoid the inconsistency issue by training the
visual backbone end-to-end in the visual grounding framework. In this paper, we
propose a Query-modulated Refinement Network (QRNet) to address the
inconsistent issue by adjusting intermediate features in the visual backbone
with a novel Query-aware Dynamic Attention (QD-ATT) mechanism and query-aware
multiscale fusion. The QD-ATT can dynamically compute query-dependent visual
attention at the spatial and channel levels of the feature maps produced by the
visual backbone. We apply the QRNet to an end-to-end visual grounding
framework. Extensive experiments show that the proposed method outperforms
state-of-the-art methods on five widely used datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Naturalistic Database of Thermal Emotional Facial Expressions and Effects of Induced Emotions on Memory. (arXiv:2203.15443v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15443">
<div class="article-summary-box-inner">
<span><p>This work defines a procedure for collecting naturally induced emotional
facial expressions through the vision of movie excerpts with high emotional
contents and reports experimental data ascertaining the effects of emotions on
memory word recognition tasks. The induced emotional states include the four
basic emotions of sadness, disgust, happiness, and surprise, as well as the
neutral emotional state. The resulting database contains both thermal and
visible emotional facial expressions, portrayed by forty Italian subjects and
simultaneously acquired by appropriately synchronizing a thermal and a standard
visible camera. Each subject's recording session lasted 45 minutes, allowing
for each mode (thermal or visible) to collect a minimum of 2000 facial
expressions from which a minimum of 400 were selected as highly expressive of
each emotion category. The database is available to the scientific community
and can be obtained contacting one of the authors. For this pilot study, it was
found that emotions and/or emotion categories do not affect individual
performance on memory word recognition tasks and temperature changes in the
face or in some regions of it do not discriminate among emotional states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Virtual View Selection for 3D Hand Pose Estimation. (arXiv:2203.15458v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15458">
<div class="article-summary-box-inner">
<span><p>3D hand pose estimation from single depth is a fundamental problem in
computer vision, and has wide applications.However, the existing methods still
can not achieve satisfactory hand pose estimation results due to view variation
and occlusion of human hand. In this paper, we propose a new virtual view
selection and fusion module for 3D hand pose estimation from single depth.We
propose to automatically select multiple virtual viewpoints for pose estimation
and fuse the results of all and find this empirically delivers accurate and
robust pose estimation. In order to select most effective virtual views for
pose fusion, we evaluate the virtual views based on the confidence of virtual
views using a light-weight network via network distillation. Experiments on
three main benchmark datasets including NYU, ICVL and Hands2019 demonstrate
that our method outperforms the state-of-the-arts on NYU and ICVL, and achieves
very competitive performance on Hands2019-Task1, and our proposed virtual view
selection and fusion module is both effective for 3D hand pose estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstract Flow for Temporal Semantic Segmentation on the Permutohedral Lattice. (arXiv:2203.15469v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15469">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is a core ability required by autonomous agents, as
being able to distinguish which parts of the scene belong to which object class
is crucial for navigation and interaction with the environment. Approaches
which use only one time-step of data cannot distinguish between moving objects
nor can they benefit from temporal integration. In this work, we extend a
backbone LatticeNet to process temporal point cloud data. Additionally, we take
inspiration from optical flow methods and propose a new module called Abstract
Flow which allows the network to match parts of the scene with similar abstract
features and gather the information temporally. We obtain state-of-the-art
results on the SemanticKITTI dataset that contains LiDAR scans from real urban
environments. We share the PyTorch implementation of TemporalLatticeNet at
https://github.com/AIS-Bonn/temporal_latticenet .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAR-ShipNet: SAR-Ship Detection Neural Network via Bidirectional Coordinate Attention and Multi-resolution Feature Fusion. (arXiv:2203.15480v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15480">
<div class="article-summary-box-inner">
<span><p>This paper studies a practically meaningful ship detection problem from
synthetic aperture radar (SAR) images by the neural network. We broadly extract
different types of SAR image features and raise the intriguing question that
whether these extracted features are beneficial to (1) suppress data variations
(e.g., complex land-sea backgrounds, scattered noise) of real-world SAR images,
and (2) enhance the features of ships that are small objects and have different
aspect (length-width) ratios, therefore resulting in the improvement of ship
detection. To answer this question, we propose a SAR-ship detection neural
network (call SAR-ShipNet for short), by newly developing Bidirectional
Coordinate Attention (BCA) and Multi-resolution Feature Fusion (MRF) based on
CenterNet. Moreover, considering the varying length-width ratio of arbitrary
ships, we adopt elliptical Gaussian probability distribution in CenterNet to
improve the performance of base detector models. Experimental results on the
public SAR-Ship dataset show that our SAR-ShipNet achieves competitive
advantages in both speed and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Structured Gaussians to Approximate Deep Ensembles. (arXiv:2203.15485v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15485">
<div class="article-summary-box-inner">
<span><p>This paper proposes using a sparse-structured multivariate Gaussian to
provide a closed-form approximator for the output of probabilistic ensemble
models used for dense image prediction tasks. This is achieved through a
convolutional neural network that predicts the mean and covariance of the
distribution, where the inverse covariance is parameterised by a sparsely
structured Cholesky matrix. Similarly to distillation approaches, our single
network is trained to maximise the probability of samples from pre-trained
probabilistic models, in this work we use a fixed ensemble of networks. Once
trained, our compact representation can be used to efficiently draw spatially
correlated samples from the approximated output distribution. Importantly, this
approach captures the uncertainty and structured correlations in the
predictions explicitly in a formal distribution, rather than implicitly through
sampling alone. This allows direct introspection of the model, enabling
visualisation of the learned structure. Moreover, this formulation provides two
further benefits: estimation of a sample probability, and the introduction of
arbitrary spatial conditioning at test time. We demonstrate the merits of our
approach on monocular depth estimation and show that the advantages of our
approach are obtained with comparable quantitative performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Powerful Physical Adversarial Examples Against Practical Face Recognition Systems. (arXiv:2203.15498v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15498">
<div class="article-summary-box-inner">
<span><p>It is well-known that the most existing machine learning (ML)-based
safety-critical applications are vulnerable to carefully crafted input
instances called adversarial examples (AXs). An adversary can conveniently
attack these target systems from digital as well as physical worlds. This paper
aims to the generation of robust physical AXs against face recognition systems.
We present a novel smoothness loss function and a patch-noise combo attack for
realizing powerful physical AXs. The smoothness loss interjects the concept of
delayed constraints during the attack generation process, thereby causing
better handling of optimization complexity and smoother AXs for the physical
domain. The patch-noise combo attack combines patch noise and imperceptibly
small noises from different distributions to generate powerful
registration-based physical AXs. An extensive experimental analysis found that
our smoothness loss results in robust and more transferable digital and
physical AXs than the conventional techniques. Notably, our smoothness loss
results in a 1.17 and 1.97 times better mean attack success rate (ASR) in
physical white-box and black-box attacks, respectively. Our patch-noise combo
attack furthers the performance gains and results in 2.39 and 4.74 times higher
mean ASR than conventional technique in physical world white-box and black-box
attacks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of OODA Loop based on Adversarial for Complex Game Environments. (arXiv:2203.15502v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15502">
<div class="article-summary-box-inner">
<span><p>To address the problem of imperfect confrontation strategy caused by the lack
of information of game environment in the simulation of non-complete
information dynamic countermeasure modeling for intelligent game, the
hierarchical analysis game strategy of confrontation model based on OODA ring
(Observation, Orientation, Decision, Action) theory is proposed. At the same
time, taking into account the trend of unmanned future warfare, NetLogo
software simulation is used to construct a dynamic derivation of the
confrontation between two tanks. In the validation process, the OODA loop
theory is used to describe the operation process of the complex system between
red and blue sides, and the four-step cycle of observation, judgment, decision
and execution is carried out according to the number of armor of both sides,
and then the OODA loop system adjusts the judgment and decision time
coefficients for the next confrontation cycle according to the results of the
first cycle. Compared with traditional simulation methods that consider
objective factors such as loss rate and support rate, the OODA-loop-based
hierarchical game analysis can analyze the confrontation situation more
comprehensively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Treatment Learning Transformer for Noisy Image Classification. (arXiv:2203.15529v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15529">
<div class="article-summary-box-inner">
<span><p>Current top-notch deep learning (DL) based vision models are primarily based
on exploring and exploiting the inherent correlations between training data
samples and their associated labels. However, a known practical challenge is
their degraded performance against "noisy" data, induced by different
circumstances such as spurious correlations, irrelevant contexts, domain shift,
and adversarial attacks. In this work, we incorporate this binary information
of "existence of noise" as treatment into image classification tasks to improve
prediction accuracy by jointly estimating their treatment effects. Motivated
from causal variational inference, we propose a transformer-based architecture,
Treatment Learning Transformer (TLT), that uses a latent generative model to
estimate robust feature representations from current observational input for
noise image classification. Depending on the estimated noise level (modeled as
a binary treatment factor), TLT assigns the corresponding inference network
trained by the designed causal loss for prediction. We also create new noisy
image datasets incorporating a wide range of noise factors (e.g., object
masking, style transfer, and adversarial perturbation) for performance
benchmarking. The superior performance of TLT in noisy image classification is
further validated by several refutation evaluation metrics. As a by-product,
TLT also improves visual salience methods for perceiving noisy images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSOP: A Multi-Stage One Shot Object Pose Estimation Framework. (arXiv:2203.15533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15533">
<div class="article-summary-box-inner">
<span><p>We present a novel one-shot method for object detection and 6 DoF pose
estimation, that does not require training on target objects. At test time, it
takes as input a target image and a textured 3D query model. The core idea is
to represent a 3D model with a number of 2D templates rendered from different
viewpoints. This enables CNN-based direct dense feature extraction and
matching. The object is first localized in 2D, then its approximate viewpoint
is estimated, followed by dense 2D-3D correspondence prediction. The final pose
is computed with PnP. We evaluate the method on LineMOD, Occlusion, Homebrewed,
YCB-V and TLESS datasets and report very competitive performance in comparison
to the state-of-the-art methods trained on synthetic data, even though our
method is not trained on the object models used for testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15536">
<div class="article-summary-box-inner">
<span><p>Our goal is to recover the 3D shape and pose of dogs from a single image.
This is a challenging task because dogs exhibit a wide range of shapes and
appearances, and are highly articulated. Recent work has proposed to directly
regress the SMAL animal model, with additional limb scale parameters, from
images. Our method, called BARC (Breed-Augmented Regression using
Classification), goes beyond prior work in several important ways. First, we
modify the SMAL shape space to be more appropriate for representing dog shape.
But, even with a better shape model, the problem of regressing dog shape from
an image is still challenging because we lack paired images with 3D ground
truth. To compensate for the lack of paired data, we formulate novel losses
that exploit information about dog breeds. In particular, we exploit the fact
that dogs of the same breed have similar body shapes. We formulate a novel
breed similarity loss consisting of two parts: One term encourages the shape of
dogs from the same breed to be more similar than dogs of different breeds. The
second one, a breed classification loss, helps to produce recognizable
breed-specific shapes. Through ablation studies, we find that our breed losses
significantly improve shape accuracy over a baseline without them. We also
compare BARC qualitatively to WLDO with a perceptual study and find that our
approach produces dogs that are significantly more realistic. This work shows
that a-priori information about genetic similarity can help to compensate for
the lack of 3D training data. This concept may be applicable to other animal
species or groups of species. Our code is publicly available for research
purposes at https://barc.is.tue.mpg.de/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15547">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks need the construction of informative features,
which are determined by channel-wise and spatial-wise information at the
network's layers. In this research, we focus on bringing in a novel solution
that uses sophisticated optimization for enhancing both the spatial and channel
components inside each layer's receptive field. Capsule Networks were used to
understand the spatial association between features in the feature map.
Standalone capsule networks have shown good results on comparatively simple
datasets than on complex datasets as a result of the inordinate amount of
feature information. Thus, to tackle this issue, we have proposed ME-CapsNet by
introducing deeper convolutional layers to extract important features before
passing through modules of capsule layers strategically to improve the
performance of the network significantly. The deeper convolutional layer
includes blocks of Squeeze-Excitation networks which uses a soft-pooling
approach for progressively reducing the spatial size thereby dynamically
recalibrating the channels by reconstructing their interdependencies without
much loss of important feature information. Extensive experimentation was done
using commonly used datasets demonstrating the efficiency of the proposed
ME-CapsNet, which clearly outperforms various research works by achieving
higher accuracy with minimal model complexity in complex datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Segmentation with Adaptive Spatial Priors from Joint Registration. (arXiv:2203.15548v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15548">
<div class="article-summary-box-inner">
<span><p>Image segmentation is a crucial but challenging task that has many
applications. In medical imaging for instance, intensity inhomogeneity and
noise are common. In thigh muscle images, different muscles are closed packed
together and there are often no clear boundaries between them. Intensity based
segmentation models cannot separate one muscle from another. To solve such
problems, in this work we present a segmentation model with adaptive spatial
priors from joint registration. This model combines segmentation and
registration in a unified framework to leverage their positive mutual
influence. The segmentation is based on a modified Gaussian mixture model
(GMM), which integrates intensity inhomogeneity and spacial smoothness. The
registration plays the role of providing a shape prior. We adopt a modified sum
of squared difference (SSD) fidelity term and Tikhonov regularity term for
registration, and also utilize Gaussian pyramid and parametric method for
robustness. The connection between segmentation and registration is guaranteed
by the cross entropy metric that aims to make the segmentation map (from
segmentation) and deformed atlas (from registration) as similar as possible.
This joint framework is implemented within a constraint optimization framework,
which leads to an efficient algorithm. We evaluate our proposed model on
synthetic and thigh muscle MR images. Numerical results show the improvement as
compared to segmentation and registration performed separately and other joint
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC. (arXiv:2203.15565v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15565">
<div class="article-summary-box-inner">
<span><p>Learning discriminative deep feature embeddings by using million-scale
in-the-wild datasets and margin-based softmax loss is the current
state-of-the-art approach for face recognition. However, the memory and
computing cost of the Fully Connected (FC) layer linearly scales up to the
number of identities in the training set. Besides, the large-scale training
data inevitably suffers from inter-class conflict and long-tailed distribution.
In this paper, we propose a sparsely updating variant of the FC layer, named
Partial FC (PFC). In each iteration, positive class centers and a random subset
of negative class centers are selected to compute the margin-based softmax
loss. All class centers are still maintained throughout the whole training
process, but only a subset is selected and updated in each iteration.
Therefore, the computing requirement, the probability of inter-class conflict,
and the frequency of passive update on tail class centers, are dramatically
reduced. Extensive experiments across different training data and backbones
(e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the
proposed PFC. The source code is available at
\https://github.com/deepinsight/insightface/tree/master/recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Core Risk Minimization using Salient ImageNet. (arXiv:2203.15566v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15566">
<div class="article-summary-box-inner">
<span><p>Deep neural networks can be unreliable in the real world especially when they
heavily use spurious features for their predictions. Recently, Singla &amp; Feizi
(2022) introduced the Salient Imagenet dataset by annotating and localizing
core and spurious features of ~52k samples from 232 classes of Imagenet. While
this dataset is useful for evaluating the reliance of pretrained models on
spurious features, its small size limits its usefulness for training models. In
this work, we first introduce the Salient Imagenet-1M dataset with more than 1
million soft masks localizing core and spurious features for all 1000 Imagenet
classes. Using this dataset, we first evaluate the reliance of several Imagenet
pretrained models (42 total) on spurious features and observe that: (i)
transformers are more sensitive to spurious features compared to Convnets, (ii)
zero-shot CLIP transformers are highly susceptible to spurious features. Next,
we introduce a new learning paradigm called Core Risk Minimization (CoRM) whose
objective ensures that the model predicts a class using its core features. We
evaluate different computational approaches for solving CoRM and achieve
significantly higher (+12%) core accuracy (accuracy when non-core regions
corrupted using noise) with no drop in clean accuracy compared to models
trained via Empirical Risk Minimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning a Structured Latent Space for Unsupervised Point Cloud Completion. (arXiv:2203.15580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15580">
<div class="article-summary-box-inner">
<span><p>Unsupervised point cloud completion aims at estimating the corresponding
complete point cloud of a partial point cloud in an unpaired manner. It is a
crucial but challenging problem since there is no paired partial-complete
supervision that can be exploited directly. In this work, we propose a novel
framework, which learns a unified and structured latent space that encoding
both partial and complete point clouds. Specifically, we map a series of
related partial point clouds into multiple complete shape and occlusion code
pairs and fuse the codes to obtain their representations in the unified latent
space. To enforce the learning of such a structured latent space, the proposed
method adopts a series of constraints including structured ranking
regularization, latent code swapping constraint, and distribution supervision
on the related partial point clouds. By establishing such a unified and
structured latent space, better partial-complete geometry consistency and shape
completion accuracy can be achieved. Extensive experiments show that our
proposed method consistently outperforms state-of-the-art unsupervised methods
on both synthetic ShapeNet and real-world KITTI, ScanNet, and Matterport3D
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGB-D Neural Radiance Fields: Local Sampling for Faster Training. (arXiv:2203.15587v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15587">
<div class="article-summary-box-inner">
<span><p>Learning a 3D representation of a scene has been a challenging problem for
decades in computer vision. Recent advances in implicit neural representation
from images using neural radiance fields(NeRF) have shown promising results.
Some of the limitations of previous NeRF based methods include longer training
time, and inaccurate underlying geometry. The proposed method takes advantage
of RGB-D data to reduce training time by leveraging depth sensing to improve
local sampling. This paper proposes a depth-guided local sampling strategy and
a smaller neural network architecture to achieve faster training time without
compromising quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15588">
<div class="article-summary-box-inner">
<span><p>The rapid development of diagnostic technologies in healthcare is leading to
higher requirements for physicians to handle and integrate the heterogeneous,
yet complementary data that are produced during routine practice. For instance,
the personalized diagnosis and treatment planning for a single cancer patient
relies on the various images (e.g., radiological, pathological, and camera
images) and non-image data (e.g., clinical data and genomic data). However,
such decision-making procedures can be subjective, qualitative, and have large
inter-subject variabilities. With the recent advances in multi-modal deep
learning technologies, an increasingly large number of efforts have been
devoted to a key question: how do we extract and aggregate multi-modal
information to ultimately provide more objective, quantitative computer-aided
clinical decision making? This paper reviews the recent studies on dealing with
such a question. Briefly, this review will include the (1) overview of current
multi-modal learning workflows, (2) summarization of multi-modal fusion
methods, (3) discussion of the performance, (4) applications in disease
diagnosis and prognosis, and (5) challenges and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis. (arXiv:2008.05865v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05865">
<div class="article-summary-box-inner">
<span><p>Synthesizing high-quality realistic images from text descriptions is a
challenging task. Existing text-to-image Generative Adversarial Networks
generally employ a stacked architecture as the backbone yet still remain three
flaws. First, the stacked architecture introduces the entanglements between
generators of different image scales. Second, existing studies prefer to apply
and fix extra networks in adversarial learning for text-image semantic
consistency, which limits the supervision capability of these networks. Third,
the cross-modal attention-based text-image fusion that widely adopted by
previous works is limited on several special image scales because of the
computational cost. To these ends, we propose a simpler but more effective Deep
Fusion Generative Adversarial Networks (DF-GAN). To be specific, we propose:
(i) a novel one-stage text-to-image backbone that directly synthesizes
high-resolution images without entanglements between different generators, (ii)
a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty
and One-Way Output, which enhances the text-image semantic consistency without
introducing extra networks, (iii) a novel deep text-image fusion block, which
deepens the fusion process to make a full fusion between text and visual
features. Compared with current state-of-the-art methods, our proposed DF-GAN
is simpler but more efficient to synthesize realistic and text-matching images
and achieves better performance on widely used datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Animation with Perturbed Masks. (arXiv:2011.06922v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06922">
<div class="article-summary-box-inner">
<span><p>We present a novel approach for image-animation of a source image by a
driving video, both depicting the same type of object. We do not assume the
existence of pose models and our method is able to animate arbitrary objects
without the knowledge of the object's structure. Furthermore, both, the driving
video and the source image are only seen during test-time. Our method is based
on a shared mask generator, which separates the foreground object from its
background, and captures the object's general pose and shape. To control the
source of the identity of the output frame, we employ perturbations to
interrupt the unwanted identity information on the driver's mask. A
mask-refinement module then replaces the identity of the driver with the
identity of the source. Conditioned on the source image, the transformed mask
is then decoded by a multi-scale generator that renders a realistic image, in
which the content of the source frame is animated by the pose in the driving
video. Due to the lack of fully supervised data, we train on the task of
reconstructing frames from the same video the source image is taken from. Our
method is shown to greatly outperform the state-of-the-art methods on multiple
benchmarks. Our code and samples are available at
https://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Magnification-Flexible Upsampling over 3D Point Clouds. (arXiv:2011.12745v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12745">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of generating dense point clouds from given
sparse point clouds to model the underlying geometric structures of
objects/scenes. To tackle this challenging issue, we propose a novel end-to-end
learning-based framework. Specifically, by taking advantage of the linear
approximation theorem, we first formulate the problem explicitly, which boils
down to determining the interpolation weights and high-order approximation
errors. Then, we design a lightweight neural network to adaptively learn
unified and sorted interpolation weights as well as the high-order refinements,
by analyzing the local geometry of the input point cloud. The proposed method
can be interpreted by the explicit formulation, and thus is more
memory-efficient than existing ones. In sharp contrast to the existing methods
that work only for a pre-defined and fixed upsampling factor, the proposed
framework only requires a single neural network with one-time training to
handle various upsampling factors within a typical range, which is highly
desired in real-world applications. In addition, we propose a simple yet
effective training strategy to drive such a flexible ability. In addition, our
method can handle non-uniformly distributed and noisy data well. Extensive
experiments on both synthetic and real-world data demonstrate the superiority
of the proposed method over state-of-the-art methods both quantitatively and
qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Accurate Active Camera Localization. (arXiv:2012.04263v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04263">
<div class="article-summary-box-inner">
<span><p>In this work, we solve the problem of active camera localization, which
controls the camera movements actively to achieve an accurate camera pose. The
past solutions are mostly based on Markov Localization, which reduces the
position-wise camera uncertainty for localization. These approaches localize
the camera in the discrete pose space and are agnostic to the
localization-driven scene property, which restrict the camera pose accuracy in
the coarse scale. We propose to overcome these limitations via a novel active
camera localization algorithm, composed of a passive and an active localization
module. The former one optimizes the camera pose in the continuous pose space
by establishing the point-wise camera-world correspondences. The latter one
explicitly models the scene and camera uncertainty components to plan the right
path for accurate camera pose estimation. We validate our algorithm on the
challenging localization scenarios from both synthetic and scanned real-world
indoor scenes. Experimental results demonstrate that our algorithm outperforms
both the state-of-the-art Markov Localization based approach and other compared
approaches on the fine-scale camera pose accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05567">
<div class="article-summary-box-inner">
<span><p>Model explanations such as saliency maps can improve user trust in AI by
highlighting important features for a prediction. However, these become
distorted and misleading when explaining predictions of images that are subject
to systematic error (bias) by perturbations and corruptions. Furthermore, the
distortions persist despite model fine-tuning on images biased by different
factors (blur, color temperature, day/night). We present Debiased-CAM to
recover explanation faithfulness across various bias types and levels by
training a multi-input, multi-task model with auxiliary tasks for explanation
and bias level predictions. In simulation studies, the approach not only
enhanced prediction accuracy, but also generated highly faithful explanations
about these predictions as if the images were unbiased. In user studies,
debiased explanations improved user task performance, perceived truthfulness
and perceived helpfulness. Debiased training can provide a versatile platform
for robust performance and explanation faithfulness for a wide range of
applications with data biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Adaptive Negative Class Envision for Few-Shot Open-Set Recognition. (arXiv:2012.13073v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13073">
<div class="article-summary-box-inner">
<span><p>We study the problem of few-shot open-set recognition (FSOR), which learns a
recognition system capable of both fast adaptation to new classes with limited
labeled examples and rejection of unknown negative samples. Traditional
large-scale open-set methods have been shown ineffective for FSOR problem due
to data limitation. Current FSOR methods typically calibrate few-shot
closed-set classifiers to be sensitive to negative samples so that they can be
rejected via thresholding. However, threshold tuning is a challenging process
as different FSOR tasks may require different rejection powers. In this paper,
we instead propose task-adaptive negative class envision for FSOR to integrate
threshold tuning into the learning process. Specifically, we augment the
few-shot closed-set classifier with additional negative prototypes generated
from few-shot examples. By incorporating few-shot class correlations in the
negative generation process, we are able to learn dynamic rejection boundaries
for FSOR tasks. Besides, we extend our method to generalized few-shot open-set
recognition (GFSOR), which requires classification on both many-shot and
few-shot classes as well as rejection of negative samples. Extensive
experiments on public benchmarks validate our methods on both problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10407">
<div class="article-summary-box-inner">
<span><p>The ability to quickly learn from a small quantity oftraining data widens the
range of machine learning applications. In this paper, we propose a
data-efficient image captioning model, VisualGPT, which leverages the
linguistic knowledge from a large pretrained language model(LM). A crucial
challenge is to balance between the use of visual information in the image and
prior linguistic knowledge acquired from pretraining. We designed a novel
self-resurrecting encoder-decoder attention mechanism to quickly adapt the
pretrained LM as the language decoder ona small amount of in-domain training
data. The proposed self-resurrecting activation unit produces sparse
activations but has reduced susceptibility to zero gradients. We train the
proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual
Captions training data. Under these conditions, we outperform the best baseline
model by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual
Captions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,
a medical report generation dataset. To the best of our knowledge, this is the
first work that improves data efficiency of image captioning by utilizing LM
pretrained on unimodal data. Our code is available at:
https://github.com/Vision-CAIR/VisualGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Transformers. (arXiv:2103.01209v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01209">
<div class="article-summary-box-inner">
<span><p>We introduce the GANformer, a novel and efficient type of transformer, and
explore it for the task of visual generative modeling. The network employs a
bipartite structure that enables long-range interactions across the image,
while maintaining computation of linear efficiency, that can readily scale to
high-resolution synthesis. It iteratively propagates information from a set of
latent variables to the evolving visual features and vice versa, to support the
refinement of each in light of the other and encourage the emergence of
compositional representations of objects and scenes. In contrast to the classic
transformer architecture, it utilizes multiplicative integration that allows
flexible region-based modulation, and can thus be seen as a generalization of
the successful StyleGAN network. We demonstrate the model's strength and
robustness through a careful evaluation over a range of datasets, from
simulated multi-object environments to rich real-world indoor and outdoor
scenes, showing it achieves state-of-the-art results in terms of image quality
and diversity, while enjoying fast learning and better data-efficiency. Further
qualitative and quantitative experiments offer us an insight into the model's
inner workings, revealing improved interpretability and stronger
disentanglement, and illustrating the benefits and efficacy of our approach. An
implementation of the model is available at
https://github.com/dorarad/gansformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02850">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new distortion quantification method for point
clouds, the multiscale potential energy discrepancy (MPED). Currently, there is
a lack of effective distortion quantification for a variety of point cloud
perception tasks. Specifically, for dense point clouds, a distortion
quantification method is used to predict human subjective scores and optimize
the selection of human perception tasks parameters, such as compression and
enhancement. For sparse point clouds, a distortion quantification methods is
work as a loss function to guide the training of deep neural networks for
unsupervised learning tasks (e.g., point cloud reconstruction, completion and
upsampling). Therefore, an effective distortion quantification should be
differentiable, distortion discriminable and have a low computational
complexity. However, current distortion quantification cannot satisfy all three
conditions. To fill this gap, we propose a new point cloud feature description
method, the point potential energy (PPE), inspired by the classical physics. We
regard the point clouds are systems that have potential energy and the
distortion can change the total potential energy. By evaluating at various
neighborhood sizes, the proposed MPED achieves global-local tradeoffs,
capturing distortion in a multiscale fashion. We further theoretically show
that classical Chamfer distance is a special case of our MPED. Extensive
experiments show the proposed MPED superior to current methods on both human
and machine perception tasks. Our code is avaliable at
https://github.com/Qi-Yangsjtu/MPED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Whitney extension problem for near isometries and beyond. (arXiv:2103.09748v5 [math.CA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09748">
<div class="article-summary-box-inner">
<span><p>In this memoir, we develop a general framework which allows for a
simultaneous study of labeled and unlabeled near alignment data problems in
$\mathbb R^D$ and the Whitney near isometry extension problem for discrete and
non-discrete subsets of $\mathbb R^D$ with certain geometries. Connections of
this work to clustering, dimension reduction, manifold learning, vision as well
as minimal energy partitions, discrepancy and min-max optimization are
discussed. Numerous open problems in harmonic analysis, computer vision,
manifold learning and signal processing connected to our work are given. A
significant portion of the work in this memoir is based on joint research with
Charles Fefferman in the papers [48], [49], [50], [51].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MogFace: Towards a Deeper Appreciation on Face Detection. (arXiv:2103.11139v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11139">
<div class="article-summary-box-inner">
<span><p>Benefiting from the pioneering design of generic object detectors,
significant achievements have been made in the field of face detection.
Typically, the architectures of the backbone, feature pyramid layer, and
detection head module within the face detector all assimilate the excellent
experience from general object detectors. However, several effective methods,
including label assignment and scale-level data augmentation strategy, fail to
maintain consistent superiority when applying on the face detector directly.
Concretely, the former strategy involves a vast body of hyper-parameters and
the latter one suffers from the challenge of scale distribution bias between
different detection tasks, which both limit their generalization abilities.
Furthermore, in order to provide accurate face bounding boxes for facial
down-stream tasks, the face detector imperatively requires the elimination of
false alarms. As a result, practical solutions on label assignment, scale-level
data augmentation, and reducing false alarms are necessary for advancing face
detectors. In this paper, we focus on resolving three aforementioned challenges
that exiting methods are difficult to finish off and present a novel face
detector, termed MogFace. In our Mogface, three key components, Adaptive Online
Incremental Anchor Mining Strategy, Selective Scale Enhancement Strategy and
Hierarchical Context-Aware Module, are separately proposed to boost the
performance of face detectors. Finally, to the best of our knowledge, our
MogFace is the best face detector on the Wider Face leader-board, achieving all
champions across different testing scenarios. The code is available at
\url{https://github.com/damo-cv/MogFace}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selective Output Smoothing Regularization: Regularize Neural Networks by Softening Output Distributions. (arXiv:2103.15383v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15383">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Selective Output Smoothing Regularization, a novel
regularization method for training the Convolutional Neural Networks (CNNs).
Inspired by the diverse effects on training from different samples, Selective
Output Smoothing Regularization improves the performance by encouraging the
model to produce equal logits on incorrect classes when dealing with samples
that the model classifies correctly and over-confidently. This plug-and-play
regularization method can be conveniently incorporated into almost any
CNN-based project without extra hassle. Extensive experiments have shown that
Selective Output Smoothing Regularization consistently achieves significant
improvement in image classification benchmarks, such as CIFAR-100, Tiny
ImageNet, ImageNet, and CUB-200-2011. Particularly, our method obtains 77.30%
accuracy on ImageNet with ResNet-50, which gains 1.1% than baseline (76.2%). We
also empirically demonstrate the ability of our method to make further
improvements when combining with other widely used regularization techniques.
On Pascal detection, using the SOSR-trained ImageNet classifier as the
pretrained model leads to better detection performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opening up Open-World Tracking. (arXiv:2104.11221v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11221">
<div class="article-summary-box-inner">
<span><p>Tracking and detecting any object, including ones never-seen-before during
model training, is a crucial but elusive capability of autonomous systems. An
autonomous agent that is blind to never-seen-before objects poses a safety
hazard when operating in the real world - and yet this is how almost all
current systems work. One of the main obstacles towards advancing tracking any
object is that this task is notoriously difficult to evaluate. A benchmark that
would allow us to perform an apples-to-apples comparison of existing efforts is
a crucial first step towards advancing this important research field. This
paper addresses this evaluation deficit and lays out the landscape and
evaluation methodology for detecting and tracking both known and unknown
objects in the open-world setting. We propose a new benchmark, TAO-OW: Tracking
Any Object in an Open World, analyze existing efforts in multi-object tracking,
and construct a baseline for this task while highlighting future challenges. We
hope to open a new front in multi-object tracking research that will hopefully
bring us a step closer to intelligent systems that can operate safely in the
real world. https://openworldtracking.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition. (arXiv:2104.11934v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11934">
<div class="article-summary-box-inner">
<span><p>The visual relationship recognition (VRR) task aims at understanding the
pairwise visual relationships between interacting objects in an image. These
relationships typically have a long-tail distribution due to their
compositional nature. This problem gets more severe when the vocabulary becomes
large, rendering this task very challenging. This paper shows that modeling an
effective message-passing flow through an attention mechanism can be critical
to tackling the compositionality and long-tail challenges in VRR. The method,
called RelTransformer, represents each image as a fully-connected scene graph
and restructures the whole scene into the relation-triplet and global-scene
contexts. It directly passes the message from each element in the
relation-triplet and global-scene contexts to the target relation via
self-attention. We also design a learnable memory to augment the long-tail
relation representation learning. Through extensive experiments, we find that
our model generalizes well on many VRR benchmarks. Our model outperforms the
best-performing models on two large-scale long-tail VRR benchmarks, VG8K-LT
(+2.0% overall acc) and GQA-LT (+26.0% overall acc), both having a highly
skewed distribution towards the tail. It also achieves strong results on the
VG200 relation detection task. Our code is available at
https://github.com/Vision-CAIR/RelTransformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13450">
<div class="article-summary-box-inner">
<span><p>Digital watermarking is widely used for copyright protection. Traditional 3D
watermarking approaches or commercial software are typically designed to embed
messages into 3D meshes, and later retrieve the messages directly from
distorted/undistorted watermarked 3D meshes. However, in many cases, users only
have access to rendered 2D images instead of 3D meshes. Unfortunately,
retrieving messages from 2D renderings of 3D meshes is still challenging and
underexplored. We introduce a novel end-to-end learning framework to solve this
problem through: 1) an encoder to covertly embed messages in both mesh geometry
and textures; 2) a differentiable renderer to render watermarked 3D objects
from different camera angles and under varied lighting conditions; 3) a decoder
to recover the messages from 2D rendered images. From our experiments, we show
that our model can learn to embed information visually imperceptible to humans,
and to retrieve the embedded information from 2D renderings that undergo 3D
distortions. In addition, we demonstrate that our method can also work with
other renderers, such as ray tracers and real-time renderers with and without
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Visual Representation Learning by Online Constrained K-Means. (arXiv:2105.11527v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11527">
<div class="article-summary-box-inner">
<span><p>Cluster discrimination is an effective pretext task for unsupervised
representation learning, which often consists of two phases: clustering and
discrimination. Clustering is to assign each instance a pseudo label that will
be used to learn representations in discrimination. The main challenge resides
in clustering since prevalent clustering methods (e.g., k-means) have to run in
a batch mode. Besides, there can be a trivial solution consisting of a
dominating cluster. To address these challenges, we first investigate the
objective of clustering-based representation learning. Based on this, we
propose a novel clustering-based pretext task with online \textbf{Co}nstrained
\textbf{K}-m\textbf{e}ans (\textbf{CoKe}). Compared with the balanced
clustering that each cluster has exactly the same size, we only constrain the
minimal size of each cluster to flexibly capture the inherent data structure.
More importantly, our online assignment method has a theoretical guarantee to
approach the global optimum. By decoupling clustering and discrimination, CoKe
can achieve competitive performance when optimizing with only a single view
from each instance. Extensive experiments on ImageNet and other benchmark data
sets verify both the efficacy and efficiency of our proposal. Code is available
at \url{https://github.com/idstcv/CoKe}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering. (arXiv:2105.13353v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13353">
<div class="article-summary-box-inner">
<span><p>We present a novel approach for unsupervised activity segmentation which uses
video frame clustering as a pretext task and simultaneously performs
representation learning and online clustering. This is in contrast with prior
works where representation learning and clustering are often performed
sequentially. We leverage temporal information in videos by employing temporal
optimal transport. In particular, we incorporate a temporal regularization term
which preserves the temporal order of the activity into the standard optimal
transport module for computing pseudo-label cluster assignments. The temporal
optimal transport module enables our approach to learn effective
representations for unsupervised activity segmentation. Furthermore, previous
methods require storing learned features for the entire dataset before
clustering them in an offline manner, whereas our approach processes one
mini-batch at a time in an online manner. Extensive evaluations on three public
datasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset,
i.e., Desktop Assembly, show that our approach performs on par with or better
than previous methods, despite having significantly less memory constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DETReg: Unsupervised Pretraining with Region Priors for Object Detection. (arXiv:2106.04550v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04550">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised pretraining methods for object detection largely focus
on pretraining the backbone of the object detector, neglecting key parts of
detection architecture. Instead, we introduce DETReg, a new self-supervised
method that pretrains the entire object detection network, including the object
localization and embedding components. During pretraining, DETReg predicts
object localizations to match the localizations from an unsupervised region
proposal generator and simultaneously aligns the corresponding feature
embeddings with embeddings from a self-supervised image encoder. We implement
DETReg using the DETR family of detectors and show that it improves over
competitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Ship
benchmarks. In low-data regimes, including semi-supervised and few-shot
learning settings, DETReg establishes many state-of-the-art results, e.g., on
COCO we see a +6.0 AP improvement for 10-shot detection and +3.5 AP improvement
when training with only 1\% of the labels. For code and pretrained models,
visit the project page at https://amirbar.net/detreg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01189">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the first Challenge on Multi-modal Aerial View
Object Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at
CVPR. This challenge is composed of two different tracks using EO andSAR
imagery. Both EO and SAR sensors possess different advantages and drawbacks.
The purpose of this competition is to analyze how to use both sets of sensory
information in complementary ways. We discuss the top methods submitted for
this competition and evaluate their results on our blind test set. Our
challenge results show significant improvement of more than 15% accuracy from
our current baselines for each track of the competition
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-wise Loss for Biomedical Image Segmentation. (arXiv:2108.01405v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01405">
<div class="article-summary-box-inner">
<span><p>We propose Region-wise (RW) loss for biomedical image segmentation.
Region-wise loss is versatile, can simultaneously account for class imbalance
and pixel importance, and it can be easily implemented as the pixel-wise
multiplication between the softmax output and a RW map. We show that, under the
proposed RW loss framework, certain loss functions, such as Active Contour and
Boundary loss, can be reformulated similarly with appropriate RW maps, thus
revealing their underlying similarities and a new perspective to understand
these loss functions. We investigate the observed optimization instability
caused by certain RW maps, such as Boundary loss distance maps, and we
introduce a mathematically-grounded principle to avoid such instability. This
principle provides excellent adaptability to any dataset and practically
ensures convergence without extra regularization terms or optimization tricks.
Following this principle, we propose a simple version of boundary distance maps
called rectified Region-wise (RRW) maps that, as we demonstrate in our
experiments, achieve state-of-the-art performance with similar or better Dice
coefficients and Hausdorff distances than Dice, Focal, weighted Cross entropy,
and Boundary losses in three distinct segmentation tasks. We quantify the
optimization instability provided by Boundary loss distance maps, and we
empirically show that our RRW maps are stable to optimize. The code to run all
our experiments is publicly available at:
https://github.com/jmlipman/RegionWiseLoss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-Temporal Sparsity. (arXiv:2108.02297v4 [cs.AR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02297">
<div class="article-summary-box-inner">
<span><p>Long Short-Term Memory (LSTM) recurrent networks are frequently used for
tasks involving time-sequential data such as speech recognition. Unlike
previous LSTM accelerators that either exploit spatial weight sparsity or
temporal activation sparsity, this paper proposes a new accelerator called
"Spartus" that exploits spatio-temporal sparsity to achieve ultralow latency
inference. Spatial sparsity is induced using a new Column-Balanced Targeted
Dropout (CBTD) structured pruning method, which produces structured sparse
weight matrices for balanced workloads. The pruned networks running on Spartus
hardware achieve weight sparsity of up to 96% and 94% with negligible accuracy
loss on the TIMIT and the Librispeech datasets. To induce temporal sparsity in
LSTM, we extend the previous DeltaGRU method to the DeltaLSTM method. Combining
spatio-temporal sparsity with CBTD and DeltaLSTM saves on weight memory access
and associated arithmetic operations. The Spartus architecture is scalable and
supports real-time online speech recognition when implemented on small and
large FPGAs. Spartus per-sample latency for a single DeltaLSTM layer of 1024
neurons averages 1 us. Exploiting spatio-temporal sparsity leads to 46X speedup
of Spartus over its theoretical hardware performance to achieve 9.4 TOp/s
effective batch-1 throughput and 1.1 TOp/s/W power efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Face Inference Models using Hierarchical Network Dissection. (arXiv:2108.10360v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10360">
<div class="article-summary-box-inner">
<span><p>This paper presents Hierarchical Network Dissection, a general pipeline to
interpret the internal representation of face-centric inference models. Using a
probabilistic formulation, our pipeline pairs units of the model with concepts
in our "Face Dictionary", a collection of facial concepts with corresponding
sample images. Our pipeline is inspired by Network Dissection, a popular
interpretability model for object-centric and scene-centric models. However,
our formulation allows to deal with two important challenges of face-centric
models that Network Dissection cannot address: (1) spacial overlap of concepts:
there are different facial concepts that simultaneously occur in the same
region of the image, like "nose" (facial part) and "pointy nose" (facial
attribute); and (2) global concepts: there are units with affinity to concepts
that do not refer to specific locations of the face (e.g. apparent age). We use
Hierarchical Network Dissection to dissect different face-centric inference
models trained on widely-used facial datasets. The results show models trained
for different tasks learned different internal representations. Furthermore,
the interpretability results can reveal some biases in the training data and
some interesting characteristics of the face-centric inference tasks. Finally,
we conduct controlled experiments on biased data to showcase the potential of
Hierarchical Network Dissection for bias discovery. The results illustrate how
Hierarchical Network Dissection can be used to discover and quantify bias in
the training data that is also encoded in the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation via Semantic Knowledge Transfer and Self-Refinement. (arXiv:2109.09477v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09477">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised instance segmentation (WSIS) has been considered as a more
challenging task than weakly-supervised semantic segmentation (WSSS). Compared
to WSSS, WSIS requires instance-wise localization, which is difficult to
extract from image-level labels. To tackle the problem, most WSIS approaches
use off-the-shelf proposal techniques that require pre-training with instance
or object level labels, deviating the fundamental definition of the
fully-image-level supervised setting. In this paper, we propose a novel
approach including two innovative components. First, we propose a semantic
knowledge transfer to obtain pseudo instance labels by transferring the
knowledge of WSSS to WSIS while eliminating the need for the off-the-shelf
proposals. Second, we propose a self-refinement method to refine the pseudo
instance labels in a self-supervised scheme and to use the refined labels for
training in an online manner. Here, we discover an erroneous phenomenon,
semantic drift, that occurred by the missing instances in pseudo instance
labels categorized as background class. This semantic drift occurs confusion
between background and instance in training and consequently degrades the
segmentation performance. We term this problem as semantic drift problem and
show that our proposed self-refinement method eliminates the semantic drift
problem. The extensive experiments on PASCAL VOC 2012 and MS COCO demonstrate
the effectiveness of our approach, and we achieve a considerable performance
without off-the-shelf proposal techniques. The code is available at
https://github.com/clovaai/BESTIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early Lane Change Prediction for Automated Driving Systems Using Multi-Task Attention-based Convolutional Neural Networks. (arXiv:2109.10742v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10742">
<div class="article-summary-box-inner">
<span><p>Lane change (LC) is one of the safety-critical manoeuvres in highway driving
according to various road accident records. Thus, reliably predicting such
manoeuvre in advance is critical for the safe and comfortable operation of
automated driving systems. The majority of previous studies rely on detecting a
manoeuvre that has been already started, rather than predicting the manoeuvre
in advance. Furthermore, most of the previous works do not estimate the key
timings of the manoeuvre (e.g., crossing time), which can actually yield more
useful information for the decision making in the ego vehicle. To address these
shortcomings, this paper proposes a novel multi-task model to simultaneously
estimate the likelihood of LC manoeuvres and the time-to-lane-change (TTLC). In
both tasks, an attention-based convolutional neural network (CNN) is used as a
shared feature extractor from a bird's eye view representation of the driving
environment. The spatial attention used in the CNN model improves the feature
extraction process by focusing on the most relevant areas of the surrounding
environment. In addition, two novel curriculum learning schemes are employed to
train the proposed approach. The extensive evaluation and comparative analysis
of the proposed method in existing benchmark datasets show that the proposed
method outperforms state-of-the-art LC prediction models, particularly
considering long-term prediction performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IntentVizor: Towards Generic Query Guided Interactive Video Summarization. (arXiv:2109.14834v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14834">
<div class="article-summary-box-inner">
<span><p>The target of automatic video summarization is to create a short skim of the
original long video while preserving the major content/events. There is a
growing interest in the integration of user queries into video summarization or
query-driven video summarization. This video summarization method predicts a
concise synopsis of the original video based on the user query, which is
commonly represented by the input text. However, two inherent problems exist in
this query-driven way. First, the text query might not be enough to describe
the exact and diverse needs of the user. Second, the user cannot edit once the
summaries are produced, while we assume the needs of the user should be subtle
and need to be adjusted interactively. To solve these two problems, we propose
IntentVizor, an interactive video summarization framework guided by generic
multi-modality queries. The input query that describes the user's needs are not
limited to text but also the video snippets. We further represent these
multi-modality finer-grained queries as user `intent', which is interpretable,
interactable, editable, and can better quantify the user's needs. In this
paper, we use a set of the proposed intents to represent the user query and
design a new interactive visual analytic interface. Users can interactively
control and adjust these mixed-initiative intents to obtain a more satisfying
summary through the interface. Also, to improve the summarization quality via
video understanding, a novel Granularity-Scalable Ego-Graph Convolutional
Networks (GSE-GCN) is proposed. We conduct our experiments on two benchmark
datasets. Comparisons with the state-of-the-art methods verify the
effectiveness of the proposed framework. Code and dataset are available at
https://github.com/jnzs1836/intent-vizor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rescoring Sequence-to-Sequence Models for Text Line Recognition with CTC-Prefixes. (arXiv:2110.05909v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05909">
<div class="article-summary-box-inner">
<span><p>In contrast to Connectionist Temporal Classification (CTC) approaches,
Sequence-To-Sequence (S2S) models for Handwritten Text Recognition (HTR) suffer
from errors such as skipped or repeated words which often occur at the end of a
sequence. In this paper, to combine the best of both approaches, we propose to
use the CTC-Prefix-Score during S2S decoding. Hereby, during beam search, paths
that are invalid according to the CTC confidence matrix are penalised. Our
network architecture is composed of a Convolutional Neural Network (CNN) as
visual backbone, bidirectional Long-Short-Term-Memory-Cells (LSTMs) as encoder,
and a decoder which is a Transformer with inserted mutual attention layers. The
CTC confidences are computed on the encoder while the Transformer is only used
for character-wise S2S decoding. We evaluate this setup on three HTR data sets:
IAM, Rimes, and StAZH. On IAM, we achieve a competitive Character Error Rate
(CER) of 2.95% when pretraining our model on synthetic data and including a
character-based language model for contemporary English. Compared to other
state-of-the-art approaches, our model requires about 10-20 times less
parameters. Access our shared implementations via this link to GitHub:
https://github.com/Planet-AI-GmbH/tfaip-hybrid-ctc-s2s.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Dialogue Response Generation. (arXiv:2110.08515v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08515">
<div class="article-summary-box-inner">
<span><p>Responsing with image has been recognized as an important capability for an
intelligent conversational agent. Yet existing works only focus on exploring
the multimodal dialogue models which depend on retrieval-based methods, but
neglecting generation methods. To fill in the gaps, we first present a
multimodal dialogue generation model, which takes the dialogue history as
input, then generates a textual sequence or an image as response. Learning such
a model often requires multimodal dialogues containing both texts and images
which are difficult to obtain. Motivated by the challenge in practice, we
consider multimodal dialogue generation under a natural assumption that only
limited training examples are available. In such a low-resource setting, we
devise a novel conversational agent, Divter, in order to isolate parameters
that depend on multimodal dialogues from the entire generation model. By this
means, the major part of the model can be learned from a large number of
text-only dialogues and text-image pairs respectively, then the whole
parameters can be well fitted using the limited training examples. Extensive
experiments demonstrate our method achieves state-of-the-art results in both
automatic and human evaluation, and can generate informative text and
high-resolution image responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Representation Learning for Binary Networks by Joint Classifier Learning. (arXiv:2110.08851v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08851">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning is a promising unsupervised learning framework that
has achieved success with large floating point networks. But such networks are
not readily deployable to edge devices. To accelerate deployment of models with
the benefit of unsupervised representation learning to such resource limited
devices for various downstream tasks, we propose a self-supervised learning
method for binary networks that uses a moving target network. In particular, we
propose to jointly train a randomly initialized classifier, attached to a
pretrained floating point feature extractor, with a binary network.
Additionally, we propose a feature similarity loss, a dynamic loss balancing
and modified multi-stage training to further improve the accuracy, and call our
method BURN. Our empirical validations over five downstream tasks using seven
datasets show that BURN outperforms self-supervised baselines for binary
networks and sometimes outperforms supervised pretraining. Code is availabe at
https://github.com/naver-ai/burn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks. (arXiv:2110.12509v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12509">
<div class="article-summary-box-inner">
<span><p>Estimating the lung depth on x-ray images could provide both an accurate
opportunistic lung volume estimation during clinical routine and improve image
contrast in modern structural chest imaging techniques like x-ray dark-field
imaging. We present a method based on a convolutional neural network that
allows a per-pixel lung thickness estimation and subsequent total lung capacity
estimation. The network was trained and validated using 5250 simulated
radiographs generated from 525 real CT scans. The network was evaluated on a
test set of 131 synthetic radiographs and a retrospective evaluation was
performed on another test set of 45 standard clinical radiographs. The standard
clinical radiographs were obtained from 45 patients, who got a CT examination
between July 1, 2021 and September 1, 2021 and a chest x-ray 6 month before or
after the CT. For 45 standard clinical radiographs, the mean-absolute error
between the estimated lung volume and groundtruth volume was 0.75 liter with a
positive correlation (r = 0.78). When accounting for the patient diameter, the
error decreases to 0.69 liter with a positive correlation (r = 0.83).
Additionally, we predicted the lung thicknesses on the synthetic test set,
where the mean-absolute error between the total volumes was 0.19 liter with a
positive correlation (r = 0.99). The results show, that creation of lung
thickness maps and estimation of approximate total lung volume is possible from
standard clinical radiographs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Industrial Scene Text Detection with Refined Feature-attentive Network. (arXiv:2110.12663v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12663">
<div class="article-summary-box-inner">
<span><p>Detecting the marking characters of industrial metal parts remains
challenging due to low visual contrast, uneven illumination, corroded character
structures, and cluttered background of metal part images. Affected by these
factors, bounding boxes generated by most existing methods locate low-contrast
text areas inaccurately. In this paper, we propose a refined feature-attentive
network (RFN) to solve the inaccurate localization problem. Specifically, we
design a parallel feature integration mechanism to construct an adaptive
feature representation from multi-resolution features, which enhances the
perception of multi-scale texts at each scale-specific level to generate a
high-quality attention map. Then, an attentive refinement network is developed
by the attention map to rectify the location deviation of candidate boxes. In
addition, a re-scoring mechanism is designed to select text boxes with the best
rectified location. Moreover, we construct two industrial scene text datasets,
including a total of 102156 images and 1948809 text instances with various
character structures and metal parts. Extensive experiments on our dataset and
four public datasets demonstrate that our proposed method achieves the
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08918">
<div class="article-summary-box-inner">
<span><p>Recent works with an implicit neural function shed light on representing
images in arbitrary resolution. However, a standalone multi-layer perceptron
shows limited performance in learning high-frequency components. In this paper,
we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for
natural images, enabling an implicit function to capture fine details while
reconstructing images in a continuous manner. When jointly trained with a deep
super-resolution (SR) architecture, LTE is capable of characterizing image
textures in 2D Fourier space. We show that an LTE-based neural function
achieves favorable performance against existing deep SR methods within an
arbitrary-scale factor. Furthermore, we demonstrate that our implementation
takes the shortest running time compared to previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Variational Network: A Deep Learning Inverse Problem Solver applied to the task of Accelerated MRI Reconstruction. (arXiv:2111.09639v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09639">
<div class="article-summary-box-inner">
<span><p>Magnetic Resonance Imaging can produce detailed images of the anatomy and
physiology of the human body that can assist doctors in diagnosing and treating
pathologies such as tumours. However, MRI suffers from very long acquisition
times that make it susceptible to patient motion artifacts and limit its
potential to deliver dynamic treatments. Conventional approaches such as
Parallel Imaging and Compressed Sensing allow for an increase in MRI
acquisition speed by reconstructing MR images from sub-sampled MRI data
acquired using multiple receiver coils. Recent advancements in Deep Learning
combined with Parallel Imaging and Compressed Sensing techniques have the
potential to produce high-fidelity reconstructions from highly accelerated MRI
data. In this work we present a novel Deep Learning-based Inverse Problem
solver applied to the task of Accelerated MRI Reconstruction, called the
Recurrent Variational Network (RecurrentVarNet), by exploiting the properties
of Convolutional Recurrent Neural Networks and unrolled algorithms for solving
Inverse Problems. The RecurrentVarNet consists of multiple recurrent blocks,
each responsible for one iteration of the unrolled variational optimization
scheme for solving the inverse problem of multi-coil Accelerated MRI
Reconstruction. Contrary to traditional approaches, the optimization steps are
performed in the observation domain ($k$-space) instead of the image domain.
Each block of the RecurrentVarNet refines the observed $k$-space and comprises
a data consistency term and a recurrent unit which takes as input a learned
hidden state and the prediction of the previous block. Our proposed method
achieves new state of the art qualitative and quantitative reconstruction
results on 5-fold and 10-fold accelerated data from a public multi-coil brain
dataset, outperforming previous conventional and deep learning-based
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Captioner: Inducing Content-Style Separation in Vision-and-Language Model Training. (arXiv:2111.12727v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12727">
<div class="article-summary-box-inner">
<span><p>While captioning models have obtained compelling results in describing
natural images, there is a growing effort to increase their capability of
dealing with real-world concepts. In this paper, we address the task of
generating fluent descriptions by training on a non-uniform combination of data
sources, containing both human- and automatically-collected captions. To this
end, we propose a model which induces a separation between content and
descriptive style through the incorporation of stylistic parameters and
keywords extracted from large-scale multi-modal models as pivotal data. In
terms of visual features, our model avoids the need of object detectors and
employs grid-like features together with a single objective of prompt language
modeling. Experimentally, we consistently outperform existing methods in terms
of caption quality and capability of describing out-of-domain concepts.
Finally, our model obtains a new state of the art on both COCO and nocaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. (arXiv:2111.13152v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13152">
<div class="article-summary-box-inner">
<span><p>A classical problem in computer vision is to infer a 3D scene representation
from few images that can be used to render novel views at interactive rates.
Previous work focuses on reconstructing pre-defined 3D representations, e.g.
textured meshes, or implicit representations, e.g. radiance fields, and often
requires input images with precise camera poses and long processing times for
each novel scene.
</p>
<p>In this work, we propose the Scene Representation Transformer (SRT), a method
which processes posed or unposed RGB images of a new area, infers a "set-latent
scene representation", and synthesises novel views, all in a single
feed-forward pass. To calculate the scene representation, we propose a
generalization of the Vision Transformer to sets of images, enabling global
information integration, and hence 3D reasoning. An efficient decoder
transformer parameterizes the light field by attending into the scene
representation to render novel views. Learning is supervised end-to-end by
minimizing a novel-view reconstruction error.
</p>
<p>We show that this method outperforms recent baselines in terms of PSNR and
speed on synthetic datasets, including a new dataset created for the paper.
Further, we demonstrate that SRT scales to support interactive visualization
and semantic segmentation of real-world outdoor environments using Street View
imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Principled Disentanglement for Domain Generalization. (arXiv:2111.13839v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13839">
<div class="article-summary-box-inner">
<span><p>A fundamental challenge for machine learning models is generalizing to
out-of-distribution (OOD) data, in part due to spurious correlations. To tackle
this challenge, we first formalize the OOD generalization problem as
constrained optimization, called Disentanglement-constrained Domain
Generalization (DDG). We relax this non-trivial constrained optimization
problem to a tractable form with finite-dimensional parameterization and
empirical approximation. Then a theoretical analysis of the extent to which the
above transformations deviates from the original problem is provided. Based on
the transformation, we propose a primal-dual algorithm for joint representation
disentanglement and domain generalization. In contrast to traditional
approaches based on domain adversarial training and domain labels, DDG jointly
learns semantic and variation encoders for disentanglement, enabling flexible
manipulation and augmentation on training data. DDG aims to learn intrinsic
representations of semantic concepts that are invariant to nuisance factors and
generalizable across domains. Comprehensive experiments on popular benchmarks
show that DDG can achieve competitive OOD performance and uncover interpretable
salient structures within data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Image Transformations for Transfer-based Adversarial Attack. (arXiv:2111.13844v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13844">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks provide a good way to study the robustness of deep
learning models. One category of methods in transfer-based black-box attack
utilizes several image transformation operations to improve the transferability
of adversarial examples, which is effective, but fails to take the specific
characteristic of the input image into consideration. In this work, we propose
a novel architecture, called Adaptive Image Transformation Learner (AITL),
which incorporates different image transformation operations into a unified
framework to further improve the transferability of adversarial examples.
Unlike the fixed combinational transformations used in existing works, our
elaborately designed transformation learner adaptively selects the most
effective combination of image transformations specific to the input image.
Extensive experiments on ImageNet demonstrate that our method significantly
improves the attack success rates on both normally trained models and defense
models under various settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance-wise Occlusion and Depth Orders in Natural Scenes. (arXiv:2111.14562v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14562">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a new dataset, named InstaOrder, that can be used
to understand the geometrical relationships of instances in an image. The
dataset consists of 2.9M annotations of geometric orderings for class-labeled
instances in 101K natural scenes. The scenes were annotated by 3,659
crowd-workers regarding (1) occlusion order that identifies occluder/occludee
and (2) depth order that describes ordinal relations that consider relative
distance from the camera. The dataset provides joint annotation of two kinds of
orderings for the same instances, and we discover that the occlusion order and
depth order are complementary. We also introduce a geometric order prediction
network called InstaOrderNet, which is superior to state-of-the-art approaches.
Moreover, we propose a dense depth prediction network called InstaDepthNet that
uses auxiliary geometric order loss to boost the accuracy of the
state-of-the-art depth prediction approach, MiDaS [56].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceAtlasAR: Atlas of Facial Acupuncture Points in Augmented Reality. (arXiv:2111.14755v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14755">
<div class="article-summary-box-inner">
<span><p>Acupuncture is a technique in which practitioners stimulate specific points
on the body. Those points, called acupuncture points (or acupoints),
anatomically define areas on the skin relative to specific landmarks on the
body. However, mapping the acupoints to individuals could be challenging for
inexperienced acupuncturists. In this project, we proposed a system to localize
and visualize facial acupoints for individuals in an augmented reality (AR)
context. This system combines a face alignment model and a hair segmentation
model to provide dense reference points for acupoints localization in real-time
(60FPS). The localization process takes the proportional bone (B-cun or
skeletal) measurement method, which is commonly operated by specialists;
however, in the real practice, operators sometimes find it inaccurate due to
skill-related error. With this system, users, even without any skills, can
locate the facial acupoints as a part of the self-training or self-treatment
process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Decomposition for Stochastic Normal-Abnormal Transport. (arXiv:2111.14777v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14777">
<div class="article-summary-box-inner">
<span><p>Advection-diffusion equations describe a large family of natural transport
processes, e.g., fluid flow, heat transfer, and wind transport. They are also
used for optical flow and perfusion imaging computations. We develop a machine
learning model, D^2-SONATA, built upon a stochastic advection-diffusion
equation, which predicts the velocity and diffusion fields that drive 2D/3D
image time-series of transport. In particular, our proposed model incorporates
a model of transport atypicality, which isolates abnormal differences between
expected normal transport behavior and the observed transport. In a medical
context such a normal-abnormal decomposition can be used, for example, to
quantify pathologies. Specifically, our model identifies the advection and
diffusion contributions from the transport time-series and simultaneously
predicts an anomaly value field to provide a decomposition into normal and
abnormal advection and diffusion behavior. To achieve improved estimation
performance for the velocity and diffusion-tensor fields underlying the
advection-diffusion process and for the estimation of the anomaly fields, we
create a 2D/3D anomaly-encoded advection-diffusion simulator, which allows for
supervised learning. We further apply our model on a brain perfusion dataset
from ischemic stroke patients via transfer learning. Extensive comparisons
demonstrate that our model successfully distinguishes stroke lesions (abnormal)
from normal brain regions, while reconstructing the underlying velocity and
diffusion tensor fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14820">
<div class="article-summary-box-inner">
<span><p>Learning behavioral patterns from observational data has been a de-facto
approach to motion forecasting. Yet, the current paradigm suffers from two
shortcomings: brittle under distribution shifts and inefficient for knowledge
transfer. In this work, we propose to address these challenges from a causal
representation perspective. We first introduce a causal formalism of motion
forecasting, which casts the problem as a dynamic process with three groups of
latent variables, namely invariant variables, style confounders, and spurious
features. We then introduce a learning framework that treats each group
separately: (i) unlike the common practice \revision{mixing} datasets collected
from different locations, we exploit their subtle distinctions by means of an
invariance loss encouraging the model to suppress spurious correlations; (ii)
we devise a modular architecture that factorizes the representations of
invariant mechanisms and style confounders to approximate a sparse causal
graph; (iii) we introduce a style contrastive loss that not only enforces the
structure of style representations but also serves as a self-supervisory signal
for test-time refinement on the fly. Experiments on synthetic and real datasets
show that our proposed method improves the robustness and reusability of
learned motion representations, significantly outperforming prior
state-of-the-art motion forecasting models for out-of-distribution
generalization and low-shot transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation. (arXiv:2111.14887v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14887">
<div class="article-summary-box-inner">
<span><p>As acquiring pixel-wise annotations of real-world images for semantic
segmentation is a costly process, a model can instead be trained with more
accessible synthetic data and adapted to real images without requiring their
annotations. This process is studied in unsupervised domain adaptation (UDA).
Even though a large number of methods propose new adaptation strategies, they
are mostly based on outdated network architectures. As the influence of recent
network architectures has not been systematically studied, we first benchmark
different network architectures for UDA and newly reveal the potential of
Transformers for UDA semantic segmentation. Based on the findings, we propose a
novel UDA method, DAFormer. The network architecture of DAFormer consists of a
Transformer encoder and a multi-level context-aware feature fusion decoder. It
is enabled by three simple but crucial training strategies to stabilize the
training and to avoid overfitting to the source domain: While (1) Rare Class
Sampling on the source domain improves the quality of the pseudo-labels by
mitigating the confirmation bias of self-training toward common classes, (2) a
Thing-Class ImageNet Feature Distance and (3) a learning rate warmup promote
feature transfer from ImageNet pretraining. DAFormer represents a major advance
in UDA. It improves the state of the art by 10.8 mIoU for GTA-to-Cityscapes and
5.4 mIoU for Synthia-to-Cityscapes and enables learning even difficult classes
such as train, bus, and truck well. The implementation is available at
https://github.com/lhoyer/DAFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing. (arXiv:2111.15666v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15666">
<div class="article-summary-box-inner">
<span><p>The inversion of real images into StyleGAN's latent space is a well-studied
problem. Nevertheless, applying existing approaches to real-world scenarios
remains an open challenge, due to an inherent trade-off between reconstruction
and editability: latent space regions which can accurately represent real
images typically suffer from degraded semantic control. Recent work proposes to
mitigate this trade-off by fine-tuning the generator to add the target image to
well-behaved, editable regions of the latent space. While promising, this
fine-tuning scheme is impractical for prevalent use as it requires a lengthy
training phase for each new image. In this work, we introduce this approach
into the realm of encoder-based inversion. We propose HyperStyle, a
hypernetwork that learns to modulate StyleGAN's weights to faithfully express a
given image in editable regions of the latent space. A naive modulation
approach would require training a hypernetwork with over three billion
parameters. Through careful network design, we reduce this to be in line with
existing encoders. HyperStyle yields reconstructions comparable to those of
optimization techniques with the near real-time inference capabilities of
encoders. Lastly, we demonstrate HyperStyle's effectiveness on several
applications beyond the inversion task, including the editing of out-of-domain
images which were never seen during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data. (arXiv:2112.00054v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00054">
<div class="article-summary-box-inner">
<span><p>Pre-training models on Imagenet or other massive datasets of real images has
led to major advances in computer vision, albeit accompanied with shortcomings
related to curation cost, privacy, usage rights, and ethical issues. In this
paper, for the first time, we study the transferability of pre-trained models
based on synthetic data generated by graphics simulators to downstream tasks
from very different domains. In using such synthetic data for pre-training, we
find that downstream performance on different tasks are favored by different
configurations of simulation parameters (e.g. lighting, object pose,
backgrounds, etc.), and that there is no one-size-fits-all solution. It is thus
better to tailor synthetic pre-training data to a specific downstream task, for
best performance. We introduce Task2Sim, a unified model mapping downstream
task representations to optimal simulation parameters to generate synthetic
pre-training data for them. Task2Sim learns this mapping by training to find
the set of best parameters on a set of "seen" tasks. Once trained, it can then
be used to predict best simulation parameters for novel "unseen" tasks in one
shot, without requiring additional training. Given a budget in number of images
per class, our extensive experiments with 20 diverse downstream tasks show
Task2Sim's task-adaptive pre-training data results in significantly better
downstream performance than non-adaptively choosing simulation parameters on
both seen and unseen tasks. It is even competitive with pre-training on real
images from Imagenet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoScene: Monocular 3D Semantic Scene Completion. (arXiv:2112.00726v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00726">
<div class="article-summary-box-inner">
<span><p>MonoScene proposes a 3D Semantic Scene Completion (SSC) framework, where the
dense geometry and semantics of a scene are inferred from a single monocular
RGB image. Different from the SSC literature, relying on 2.5 or 3D input, we
solve the complex problem of 2D to 3D scene reconstruction while jointly
inferring its semantics. Our framework relies on successive 2D and 3D UNets
bridged by a novel 2D-3D features projection inspiring from optics and
introduces a 3D context relation prior to enforce spatio-semantic consistency.
Along with architectural contributions, we introduce novel global scene and
local frustums losses. Experiments show we outperform the literature on all
metrics and datasets while hallucinating plausible scenery even beyond the
camera field of view. Our code and trained models are available at
https://github.com/cv-rits/MonoScene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Learning in Semantic Segmentation from Image Labels. (arXiv:2112.01882v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01882">
<div class="article-summary-box-inner">
<span><p>Although existing semantic segmentation approaches achieve impressive
results, they still struggle to update their models incrementally as new
categories are uncovered. Furthermore, pixel-by-pixel annotations are expensive
and time-consuming. This paper proposes a novel framework for Weakly
Incremental Learning for Semantic Segmentation, that aims at learning to
segment new classes from cheap and largely available image-level labels. As
opposed to existing approaches, that need to generate pseudo-labels offline, we
use an auxiliary classifier, trained with image-level labels and regularized by
the segmentation model, to obtain pseudo-supervision online and update the
model incrementally. We cope with the inherent noise in the process by using
soft-labels generated by the auxiliary classifier. We demonstrate the
effectiveness of our approach on the Pascal VOC and COCO datasets,
outperforming offline weakly-supervised methods and obtaining results
comparable with incremental learning methods with full supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novel Class Discovery in Semantic Segmentation. (arXiv:2112.01900v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01900">
<div class="article-summary-box-inner">
<span><p>We introduce a new setting of Novel Class Discovery in Semantic Segmentation
(NCDSS), which aims at segmenting unlabeled images containing new classes given
prior knowledge from a labeled set of disjoint classes. In contrast to existing
approaches that look at novel class discovery in image classification, we focus
on the more challenging semantic segmentation. In NCDSS, we need to distinguish
the objects and background, and to handle the existence of multiple classes
within an image, which increases the difficulty in using the unlabeled data. To
tackle this new setting, we leverage the labeled base data and a saliency model
to coarsely cluster novel classes for model training in our basic framework.
Additionally, we propose the Entropy-based Uncertainty Modeling and
Self-training (EUMS) framework to overcome noisy pseudo-labels, further
improving the model performance on the novel classes. Our EUMS utilizes an
entropy ranking technique and a dynamic reassignment to distill clean labels,
thereby making full use of the noisy data via self-supervised learning. We
build the NCDSS benchmark on the PASCAL-5$^i$ dataset and COCO-20$^i$ dataset.
Extensive experiments demonstrate the feasibility of the basic framework
(achieving an average mIoU of 49.81% on PASCAL-5$^i$) and the effectiveness of
EUMS framework (outperforming the basic framework by 9.28% mIoU on
PASCAL-5$^i$).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemanticStyleGAN: Learning Compositional Generative Priors for Controllable Image Synthesis and Editing. (arXiv:2112.02236v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02236">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that StyleGANs provide promising prior models for
downstream tasks on image synthesis and editing. However, since the latent
codes of StyleGANs are designed to control global styles, it is hard to achieve
a fine-grained control over synthesized images. We present SemanticStyleGAN,
where a generator is trained to model local semantic parts separately and
synthesizes images in a compositional way. The structure and texture of
different local parts are controlled by corresponding latent codes.
Experimental results demonstrate that our model provides a strong
disentanglement between different spatial areas. When combined with editing
methods designed for StyleGANs, it can achieve a more fine-grained control to
edit synthesized or real images. The model can also be extended to other
domains via transfer learning. Thus, as a generic prior model with built-in
disentanglement, it could facilitate the development of GAN-based applications
and enable more potential downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations. (arXiv:2112.02290v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02290">
<div class="article-summary-box-inner">
<span><p>Learning visual concepts from raw images without strong supervision is a
challenging task. In this work, we show the advantages of prototype
representations for understanding and revising the latent space of neural
concept learners. For this purpose, we introduce interactive Concept Swapping
Networks (iCSNs), a novel framework for learning concept-grounded
representations via weak supervision and implicit prototype representations.
iCSNs learn to bind conceptual information to specific prototype slots by
swapping the latent representations of paired images. This semantically
grounded and discrete latent space facilitates human understanding and
human-machine interaction. We support this claim by conducting experiments on
our novel data set "Elementary Concept Reasoning" (ECR), focusing on visual
concepts shared by geometric objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Practical Monocular Indoor Depth Estimation. (arXiv:2112.02306v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02306">
<div class="article-summary-box-inner">
<span><p>The majority of prior monocular depth estimation methods without groundtruth
depth guidance focus on driving scenarios. We show that such methods generalize
poorly to unseen complex indoor scenes, where objects are cluttered and
arbitrarily arranged in the near field. To obtain more robustness, we propose a
structure distillation approach to learn knacks from an off-the-shelf relative
depth estimator that produces structured but metric-agnostic depth. By
combining structure distillation with a branch that learns metrics from
left-right consistency, we attain structured and metric depth for generic
indoor scenes and make inferences in real-time. To facilitate learning and
evaluation, we collect SimSIN, a dataset from simulation with thousands of
environments, and UniSIN, a dataset that contains about 500 real scan sequences
of generic indoor environments. We experiment in both sim-to-real and
real-to-real settings, and show improvements, as well as in downstream
applications using our depth maps. This work provides a full study, covering
methods, data, and applications aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting LiDAR Registration and Reconstruction: A Range Image Perspective. (arXiv:2112.02779v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02779">
<div class="article-summary-box-inner">
<span><p>Spinning LiDAR data are prevalent for 3D vision tasks. Since LiDAR data is
presented in the form of point clouds, expensive 3D operations are usually
required. This paper revisits spinning LiDAR scan formation and presents a
cylindrical range image representation with a ray-wise projection/unprojection
model. It is built upon raw scans and supports lossless conversion from 2D to
3D, allowing fast 2D operations, including 2D index-based neighbor search and
downsampling. We then propose, to the best of our knowledge, the first
multi-scale registration and dense signed distance function (SDF)
reconstruction system for LiDAR range images. We further collect a dataset of
indoor and outdoor LiDAR scenes in the posed range image format. A
comprehensive evaluation of registration and reconstruction is conducted on the
proposed dataset and the KITTI dataset. Experiments demonstrate that our
approach outperforms surface reconstruction baselines and achieves similar
performance to state-of-the-art LiDAR registration methods, including a modern
learning-based registration approach. Thanks to the simplicity, our
registration runs at 100Hz and SDF reconstruction in real time. The dataset and
a modularized C++/Python toolbox will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Facial Representation Learning in a Visual-Linguistic Manner. (arXiv:2112.03109v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03109">
<div class="article-summary-box-inner">
<span><p>How to learn a universal facial representation that boosts all face analysis
tasks? This paper takes one step toward this goal. In this paper, we study the
transfer performance of pre-trained models on face analysis tasks and introduce
a framework, called FaRL, for general Facial Representation Learning in a
visual-linguistic manner. On one hand, the framework involves a contrastive
loss to learn high-level semantic meaning from image-text pairs. On the other
hand, we propose exploring low-level information simultaneously to further
enhance the face representation, by adding a masked image modeling. We perform
pre-training on LAION-FACE, a dataset containing large amount of face
image-text pairs, and evaluate the representation capability on multiple
downstream tasks. We show that FaRL achieves better transfer performance
compared with previous pre-trained models. We also verify its superiority in
the low-data regime. More importantly, our model surpasses the state-of-the-art
methods on face analysis tasks including face parsing and face alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection. (arXiv:2112.03902v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03902">
<div class="article-summary-box-inner">
<span><p>Action detection is an essential and challenging task, especially for densely
labelled datasets of untrimmed videos. The temporal relation is complex in
those datasets, including challenges like composite action, and co-occurring
action. For detecting actions in those complex videos, efficiently capturing
both short-term and long-term temporal information in the video is critical. To
this end, we propose a novel ConvTransformer network for action detection. This
network comprises three main components: (1) Temporal Encoder module
extensively explores global and local temporal relations at multiple temporal
resolutions. (2) Temporal Scale Mixer module effectively fuses the multi-scale
features to have a unified feature representation. (3) Classification module is
used to learn the instance center-relative position and predict the frame-level
classification scores. The extensive experiments on multiple datasets,
including Charades, TSU and MultiTHUMOS, confirm the effectiveness of our
proposed method. Our network outperforms the state-of-the-art methods on all
three datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vehicle trajectory prediction works, but not everywhere. (arXiv:2112.03909v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03909">
<div class="article-summary-box-inner">
<span><p>Vehicle trajectory prediction is nowadays a fundamental pillar of
self-driving cars. Both the industry and research communities have acknowledged
the need for such a pillar by providing public benchmarks. While
state-of-the-art methods are impressive, i.e., they have no off-road
prediction, their generalization to cities outside of the benchmark remains
unexplored. In this work, we show that those methods do not generalize to new
scenes. We present a method that automatically generates realistic scenes
causing state-of-the-art models to go off-road. We frame the problem through
the lens of adversarial scene generation. The method is a simple yet effective
generative model based on atomic scene generation functions along with physical
constraints. Our experiments show that more than 60% of existing scenes from
the current benchmarks can be modified in a way to make prediction methods fail
(i.e., predicting off-road). We further show that the generated scenes (i) are
realistic since they do exist in the real world, and (ii) can be used to make
existing models more robust, yielding 30-40 reductions in the off-road rate.
The code is available online: https://s-attack.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures. (arXiv:2112.05135v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05135">
<div class="article-summary-box-inner">
<span><p>In real-world applications of machine learning, reliable and safe systems
must consider measures of performance beyond standard test set accuracy. These
other goals include out-of-distribution (OOD) robustness, prediction
consistency, resilience to adversaries, calibrated uncertainty estimates, and
the ability to detect anomalous inputs. However, improving performance towards
these goals is often a balancing act that today's methods cannot achieve
without sacrificing performance on other safety axes. For instance, adversarial
training improves adversarial robustness but sharply degrades other classifier
performance metrics. Similarly, strong data augmentation and regularization
techniques often improve OOD robustness but harm anomaly detection, raising the
question of whether a Pareto improvement on all existing safety measures is
possible. To meet this challenge, we design a new data augmentation strategy
utilizing the natural structural complexity of pictures such as fractals, which
outperforms numerous baselines, is near Pareto-optimal, and roundly improves
safety measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label, Verify, Correct: A Simple Few Shot Object Detection Method. (arXiv:2112.05749v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05749">
<div class="article-summary-box-inner">
<span><p>The objective of this paper is few-shot object detection (FSOD) -- the task
of expanding an object detector for a new category given only a few instances
for training. We introduce a simple pseudo-labelling method to source
high-quality pseudo-annotations from the training set, for each new category,
vastly increasing the number of training instances and reducing class
imbalance; our method finds previously unlabelled instances. Na\"ively training
with model predictions yields sub-optimal performance; we present two novel
methods to improve the precision of the pseudo-labelling process: first, we
introduce a verification technique to remove candidate detections with
incorrect class labels; second, we train a specialised model to correct poor
quality bounding boxes. After these two novel steps, we obtain a large set of
high-quality pseudo-annotations that allow our final detector to be trained
end-to-end. Additionally, we demonstrate our method maintains base class
performance, and the utility of simple augmentations in FSOD. While
benchmarking on PASCAL VOC and MS-COCO, our method achieves state-of-the-art or
second-best performance compared to existing approaches across all number of
shots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Keypoint Detection with Uncertainty Learning for Unseen Species. (arXiv:2112.06183v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06183">
<div class="article-summary-box-inner">
<span><p>Current non-rigid object keypoint detectors perform well on a chosen kind of
species and body parts, and require a large amount of labelled keypoints for
training. Moreover, their heatmaps, tailored to specific body parts, cannot
recognize novel keypoints (keypoints not labelled for training) on unseen
species. We raise an interesting yet challenging question: how to detect both
base (annotated for training) and novel keypoints for unseen species given a
few annotated samples? Thus, we propose a versatile Few-shot Keypoint Detection
(FSKD) pipeline, which can detect a varying number of keypoints of different
kinds. Our FSKD provides the uncertainty estimation of predicted keypoints.
Specifically, FSKD involves main and auxiliary keypoint representation
learning, similarity learning, and keypoint localization with uncertainty
modeling to tackle the localization noise. Moreover, we model the uncertainty
across groups of keypoints by multivariate Gaussian distribution to exploit
implicit correlations between neighboring keypoints. We show the effectiveness
of our FSKD on (i) novel keypoint detection for unseen species, and (ii)
few-shot Fine-Grained Visual Recognition (FGVR) and (iii) Semantic Alignment
(SA) downstream tasks. For FGVR, detected keypoints improve the classification
accuracy. For SA, we showcase a novel thin-plate-spline warping that uses
estimated keypoint uncertainty under imperfect keypoint corespondences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Unsupervised Domain Adaptive Person Re-identification with Coordinated Anti-forgetting and Adaptation. (arXiv:2112.06632v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06632">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptive person re-identification (ReID) has been
extensively investigated to mitigate the adverse effects of domain gaps. Those
works assume the target domain data can be accessible all at once. However, for
the real-world streaming data, this hinders the timely adaptation to changing
data statistics and sufficient exploitation of increasing samples. In this
paper, to address more practical scenarios, we propose a new task, Lifelong
Unsupervised Domain Adaptive (LUDA) person ReID. This is challenging because it
requires the model to continuously adapt to unlabeled data in the target
environments while alleviating catastrophic forgetting for such a fine-grained
person retrieval task. We design an effective scheme for this task, dubbed
CLUDA-ReID, where the anti-forgetting is harmoniously coordinated with the
adaptation. Specifically, a meta-based Coordinated Data Replay strategy is
proposed to replay old data and update the network with a coordinated
optimization direction for both adaptation and memorization. Moreover, we
propose Relational Consistency Learning for old knowledge
distillation/inheritance in line with the objective of retrieval-based tasks.
We set up two evaluation settings to simulate the practical application
scenarios. Extensive experiments demonstrate the effectiveness of our
CLUDA-ReID for both scenarios with stationary target streams and scenarios with
dynamic target streams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Depth Estimation by Fusing Single-View Depth Probability with Multi-View Geometry. (arXiv:2112.08177v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08177">
<div class="article-summary-box-inner">
<span><p>Multi-view depth estimation methods typically require the computation of a
multi-view cost-volume, which leads to huge memory consumption and slow
inference. Furthermore, multi-view matching can fail for texture-less surfaces,
reflective surfaces and moving objects. For such failure modes, single-view
depth estimation methods are often more reliable. To this end, we propose
MaGNet, a novel framework for fusing single-view depth probability with
multi-view geometry, to improve the accuracy, robustness and efficiency of
multi-view depth estimation. For each frame, MaGNet estimates a single-view
depth probability distribution, parameterized as a pixel-wise Gaussian. The
distribution estimated for the reference frame is then used to sample per-pixel
depth candidates. Such probabilistic sampling enables the network to achieve
higher accuracy while evaluating fewer depth candidates. We also propose depth
consistency weighting for the multi-view matching score, to ensure that the
multi-view depth is consistent with the single-view predictions. The proposed
method achieves state-of-the-art performance on ScanNet, 7-Scenes and KITTI.
Qualitative evaluation demonstrates that our method is more robust against
challenging artifacts such as texture-less/reflective surfaces and moving
objects. Our code and model weights are available at
https://github.com/baegwangbin/MaGNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Putting People in their Place: Monocular Regression of 3D People in Depth. (arXiv:2112.08274v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08274">
<div class="article-summary-box-inner">
<span><p>Given an image with multiple people, our goal is to directly regress the pose
and shape of all the people as well as their relative depth. Inferring the
depth of a person in an image, however, is fundamentally ambiguous without
knowing their height. This is particularly problematic when the scene contains
people of very different sizes, e.g. from infants to adults. To solve this, we
need several things. First, we develop a novel method to infer the poses and
depth of multiple people in a single image. While previous work that estimates
multiple people does so by reasoning in the image plane, our method, called
BEV, adds an additional imaginary Bird's-Eye-View representation to explicitly
reason about depth. BEV reasons simultaneously about body centers in the image
and in depth and, by combing these, estimates 3D body position. Unlike prior
work, BEV is a single-shot method that is end-to-end differentiable. Second,
height varies with age, making it impossible to resolve depth without also
estimating the age of people in the image. To do so, we exploit a 3D body model
space that lets BEV infer shapes from infants to adults. Third, to train BEV,
we need a new dataset. Specifically, we create a "Relative Human" (RH) dataset
that includes age labels and relative depth relationships between the people in
the images. Extensive experiments on RH and AGORA demonstrate the effectiveness
of the model and training scheme. BEV outperforms existing methods on depth
reasoning, child shape estimation, and robustness to occlusion. The code and
dataset are released for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICON: Implicit Clothed humans Obtained from Normals. (arXiv:2112.09127v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09127">
<div class="article-summary-box-inner">
<span><p>Current methods for learning realistic and animatable 3D clothed avatars need
either posed 3D scans or 2D images with carefully controlled user poses. In
contrast, our goal is to learn an avatar from only 2D images of people in
unconstrained poses. Given a set of images, our method estimates a detailed 3D
surface from each image and then combines these into an animatable avatar.
Implicit functions are well suited to the first task, as they can capture
details like hair and clothes. Current methods, however, are not robust to
varied human poses and often produce 3D surfaces with broken or disembodied
limbs, missing details, or non-human shapes. The problem is that these methods
use global feature encoders that are sensitive to global pose. To address this,
we propose ICON ("Implicit Clothed humans Obtained from Normals"), which,
instead, uses local features. ICON has two main modules, both of which exploit
the SMPL(-X) body model. First, ICON infers detailed clothed-human normals
(front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware
implicit surface regressor produces an iso-surface of a human occupancy field.
Importantly, at inference time, a feedback loop alternates between refining the
SMPL(-X) mesh using the inferred clothed normals and then refining the normals.
Given multiple reconstructed frames of a subject in varied poses, we use
SCANimate to produce an animatable avatar from them. Evaluation on the AGORA
and CAPE datasets shows that ICON outperforms the state of the art in
reconstruction, even with heavily limited training data. Additionally, it is
much more robust to out-of-distribution samples, e.g., in-the-wild poses/images
and out-of-frame cropping. ICON takes a step towards robust 3D clothed human
reconstruction from in-the-wild images. This enables creating avatars directly
from video with personalized and natural pose-dependent cloth deformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light Field Neural Rendering. (arXiv:2112.09687v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09687">
<div class="article-summary-box-inner">
<span><p>Classical light field rendering for novel view synthesis can accurately
reproduce view-dependent effects such as reflection, refraction, and
translucency, but requires a dense view sampling of the scene. Methods based on
geometric reconstruction need only sparse views, but cannot accurately model
non-Lambertian effects. We introduce a model that combines the strengths and
mitigates the limitations of these two directions. By operating on a
four-dimensional representation of the light field, our model learns to
represent view-dependent effects accurately. By enforcing geometric constraints
during training and inference, the scene geometry is implicitly learned from a
sparse set of views. Concretely, we introduce a two-stage transformer-based
model that first aggregates features along epipolar lines, then aggregates
features along reference views to produce the color of a target ray. Our model
outperforms the state-of-the-art on multiple forward-facing and 360{\deg}
datasets, with larger margins on scenes with severe view-dependent variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs. (arXiv:2112.10703v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10703">
<div class="article-summary-box-inner">
<span><p>We use neural radiance fields (NeRFs) to build interactive 3D environments
from large-scale visual captures spanning buildings or even multiple city
blocks collected primarily from drones. In contrast to single object scenes (on
which NeRFs are traditionally evaluated), our scale poses multiple challenges
including (1) the need to model thousands of images with varying lighting
conditions, each of which capture only a small subset of the scene, (2)
prohibitively large model capacities that make it infeasible to train on a
single GPU, and (3) significant challenges for fast rendering that would enable
interactive fly-throughs.
</p>
<p>To address these challenges, we begin by analyzing visibility statistics for
large-scale scenes, motivating a sparse network structure where parameters are
specialized to different regions of the scene. We introduce a simple geometric
clustering algorithm for data parallelism that partitions training images (or
rather pixels) into different NeRF submodules that can be trained in parallel.
</p>
<p>We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as
well as against our own drone footage, improving training speed by 3x and PSNR
by 12%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and
introduce a novel method that exploits temporal coherence. Our technique
achieves a 40x speedup over conventional NeRF rendering while remaining within
0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NinjaDesc: Content-Concealing Visual Descriptors via Adversarial Learning. (arXiv:2112.12785v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12785">
<div class="article-summary-box-inner">
<span><p>In the light of recent analyses on privacy-concerning scene revelation from
visual descriptors, we develop descriptors that conceal the input image
content. In particular, we propose an adversarial learning framework for
training visual descriptors that prevent image reconstruction, while
maintaining the matching accuracy. We let a feature encoding network and image
reconstruction network compete with each other, such that the feature encoder
tries to impede the image reconstruction with its generated descriptors, while
the reconstructor tries to recover the input image from the descriptors. The
experimental results demonstrate that the visual descriptors obtained with our
method significantly deteriorate the image reconstruction quality with minimal
impact on correspondence matching and camera localization performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EM-driven unsupervised learning for efficient motion segmentation. (arXiv:2201.02074v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02074">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a CNN-based fully unsupervised method for motion
segmentation from optical flow. We assume that the input optical flow can be
represented as a piecewise set of parametric motion models, typically, affine
or quadratic motion models. The core idea of our work is to leverage the
Expectation-Maximization (EM) framework in order to design in a well-founded
manner a loss function and a training procedure of our motion segmentation
neural network that does not require either ground-truth or manual annotation.
However, in contrast to the classical iterative EM, once the network is
trained, we can provide a segmentation for any unseen optical flow field in a
single inference step and without estimating any motion models. Different loss
functions have been investigated including robust ones. We also propose a novel
efficient data augmentation technique on the optical flow field, applicable to
any network taking optical flow as input. In addition, our method is able by
design to segment multiple motions. Our motion segmentation network was tested
on four benchmarks, DAVIS2016, SegTrackV2, FBMS59, and MoCA, and performed very
well, while being fast at test time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eye Know You Too: A DenseNet Architecture for End-to-end Eye Movement Biometrics. (arXiv:2201.02110v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02110">
<div class="article-summary-box-inner">
<span><p>Eye movement biometrics (EMB) is a relatively recent behavioral biometric
modality that may have the potential to become the primary authentication
method in virtual- and augmented-reality devices due to their emerging use of
eye-tracking sensors to enable foveated rendering techniques. However, existing
EMB models have yet to demonstrate levels of performance that would be
acceptable for real-world use. Deep learning approaches to EMB have largely
employed plain convolutional neural networks (CNNs), but there have been many
milestone improvements to convolutional architectures over the years including
residual networks (ResNets) and densely connected convolutional networks
(DenseNets). The present study employs a DenseNet architecture for end-to-end
EMB and compares the proposed model against the most relevant prior works. The
proposed technique not only outperforms the previous state of the art, but is
also the first to approach a level of authentication performance that would be
acceptable for real-world use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stereo Magnification with Multi-Layer Images. (arXiv:2201.05023v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05023">
<div class="article-summary-box-inner">
<span><p>Representing scenes with multiple semi-transparent colored layers has been a
popular and successful choice for real-time novel view synthesis. Existing
approaches infer colors and transparency values over regularly-spaced layers of
planar or spherical shape. In this work, we introduce a new view synthesis
approach based on multiple semi-transparent layers with scene-adapted geometry.
Our approach infers such representations from stereo pairs in two stages. The
first stage infers the geometry of a small number of data-adaptive layers from
a given pair of views. The second stage infers the color and the transparency
values for these layers producing the final representation for novel view
synthesis. Importantly, both stages are connected through a differentiable
renderer and are trained in an end-to-end manner. In the experiments, we
demonstrate the advantage of the proposed approach over the use of
regularly-spaced layers with no adaptation to scene geometry. Despite being
orders of magnitude faster during rendering, our approach also outperforms a
recently proposed IBRNet system based on implicit geometry representation. See
results at https://samsunglabs.github.io/StereoLayers .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency Constrained Arbitrary Image Style Transfer using SIFT and DCNN. (arXiv:2201.05346v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05346">
<div class="article-summary-box-inner">
<span><p>This paper develops a new image synthesis approach to transfer an example
image (style image) to other images (content images) by using Deep
Convolutional Neural Networks (DCNN) model. When common neural style transfer
methods are used, the textures and colors in the style image are usually
transferred imperfectly to the content image, or some visible errors are
generated. This paper proposes a novel saliency constrained method to reduce or
avoid such effects. It first evaluates some existing saliency detection methods
to select the most suitable one for use in our method. The selected saliency
detection method is used to detect the object in the style image, corresponding
to the object of the content image with the same saliency. In addition, aim to
solve the problem that the size or resolution is different in the style image
and content, the scale-invariant feature transform is used to generate a series
of style images and content images which can be used to generate more feature
maps for patches matching. It then proposes a new loss function combining the
saliency loss, style loss and content loss, adding gradient of saliency
constraint into style transfer in iterations. Finally the source images and
saliency detection results are utilized as multichannel input to an improved
deep CNN framework for style transfer. The experiments show that the saliency
maps of source images can help find the correct matching and avoid artifacts.
Experimental results on different kind of images demonstrate that our method
outperforms nine representative methods from recent publications and has good
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data. (arXiv:2201.05905v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05905">
<div class="article-summary-box-inner">
<span><p>Capsule network is a recent new deep network architecture that has been
applied successfully for medical image segmentation tasks. This work extends
capsule networks for volumetric medical image segmentation with self-supervised
learning. To improve on the problem of weight initialization compared to
previous capsule networks, we leverage self-supervised learning for capsule
networks pre-training, where our pretext-task is optimized by
self-reconstruction. Our capsule network, SS-3DCapsNet, has a UNet-based
architecture with a 3D Capsule encoder and 3D CNNs decoder. Our experiments on
multiple datasets including iSeg-2017, Hippocampus, and Cardiac demonstrate
that our 3D capsule network with self-supervised pre-training considerably
outperforms previous capsule networks and 3D-UNets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?. (arXiv:2201.06346v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06346">
<div class="article-summary-box-inner">
<span><p>Even though image generation with Generative Adversarial Networks has been
showing remarkable ability to generate high-quality images, GANs do not always
guarantee photorealistic images will be generated. Sometimes they generate
images that have defective or unnatural objects, which are referred to as
'artifacts'. Research to determine why the artifacts emerge and how they can be
detected and removed has not been sufficiently carried out. To analyze this, we
first hypothesize that rarely activated neurons and frequently activated
neurons have different purposes and responsibilities for the progress of
generating images. By analyzing the statistics and the roles for those neurons,
we empirically show that rarely activated neurons are related to failed results
of making diverse objects and lead to artifacts. In addition, we suggest a
correction method, called 'sequential ablation', to repair the defective part
of the generated images without complex computational cost and manual efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ShapeFormer: Transformer-based Shape Completion via Sparse Representation. (arXiv:2201.10326v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10326">
<div class="article-summary-box-inner">
<span><p>We present ShapeFormer, a transformer-based network that produces a
distribution of object completions, conditioned on incomplete, and possibly
noisy, point clouds. The resultant distribution can then be sampled to generate
likely completions, each exhibiting plausible shape details while being
faithful to the input. To facilitate the use of transformers for 3D, we
introduce a compact 3D representation, vector quantized deep implicit function,
that utilizes spatial sparsity to represent a close approximation of a 3D shape
by a short sequence of discrete variables. Experiments demonstrate that
ShapeFormer outperforms prior art for shape completion from ambiguous partial
inputs in terms of both completion quality and diversity. We also show that our
approach effectively handles a variety of shape types, incomplete patterns, and
real-world scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactron: Embodied Adaptive Object Detection. (arXiv:2202.00660v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00660">
<div class="article-summary-box-inner">
<span><p>Over the years various methods have been proposed for the problem of object
detection. Recently, we have witnessed great strides in this domain owing to
the emergence of powerful deep neural networks. However, there are typically
two main assumptions common among these approaches. First, the model is trained
on a fixed training set and is evaluated on a pre-recorded test set. Second,
the model is kept frozen after the training phase, so no further updates are
performed after the training is finished. These two assumptions limit the
applicability of these methods to real-world settings. In this paper, we
propose Interactron, a method for adaptive object detection in an interactive
setting, where the goal is to perform object detection in images observed by an
embodied agent navigating in different environments. Our idea is to continue
training during inference and adapt the model at test time without any explicit
supervision via interacting with the environment. Our adaptive object detection
model provides a 11.8 point improvement in AP (and 19.1 points in AP50) over
DETR, a recent, high-performance object detector. Moreover, we show that our
object detection model adapts to environments with completely different
appearance characteristics, and its performance is on par with a model trained
with full supervision for those environments. The code is available at:
https://github.com/allenai/interactron .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space. (arXiv:2202.03800v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03800">
<div class="article-summary-box-inner">
<span><p>Face clustering has attracted rising research interest recently to take
advantage of massive amounts of face images on the web. State-of-the-art
performance has been achieved by Graph Convolutional Networks (GCN) due to
their powerful representation capacity. However, existing GCN-based methods
build face graphs mainly according to kNN relations in the feature space, which
may lead to a lot of noise edges connecting two faces of different classes. The
face features will be polluted when messages pass along these noise edges, thus
degrading the performance of GCNs. In this paper, a novel algorithm named
Ada-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In
Ada-NETS, each face is transformed to a new structure space, obtaining robust
features by considering face features of the neighbour images. Then, an
adaptive neighbour discovery strategy is proposed to determine a proper number
of edges connecting to each face image. It significantly reduces the noise
edges while maintaining the good ones to build a graph with clean yet rich
edges for GCNs to cluster faces. Experiments on multiple public clustering
datasets show that Ada-NETS significantly outperforms current state-of-the-art
methods, proving its superiority and generalization. Code is available at
https://github.com/damo-cv/Ada-NETS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining the Silhouette and Skeleton Data for Gait Recognition. (arXiv:2202.10645v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10645">
<div class="article-summary-box-inner">
<span><p>Gait recognition, a promising long-distance biometric technology, has aroused
intense interest in computer vision. Existing works on gait recognition can be
divided into appearance-based methods and model-based methods, which extract
features from silhouettes and skeleton data, respectively. However, since
appearance-based methods are greatly affected by clothing changing and carrying
condition, and model-based methods are limited by the accuracy of pose
estimation approaches, gait recognition remains challenging in practical
applications. In order to integrate the advantages of such two approaches, a
two-branch neural network (NN) is proposed in this paper. Our method contains
two branches, namely a CNN-based branch taking silhouettes as input and a
GCN-based branch taking skeletons as input. In addition, two new modules are
proposed in the GCN-based branch for better gait representation. First, we
present a simple yet effective fully connected graph convolution operator to
integrate the multi-scale graph convolutions and alleviate the dependence on
natural human joint connections. Second, we deploy a multi-dimension attention
module named STC-Att to learn spatial, temporal and channel-wise attention
simultaneously. We evaluated the proposed two-branch neural network on the
CASIA-B dataset. The experimental results show that our method achieves
state-of-the-art performance in various conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Free Object Segments for Long-Tailed Instance Segmentation. (arXiv:2202.11124v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11124">
<div class="article-summary-box-inner">
<span><p>One fundamental challenge in building an instance segmentation model for a
large number of classes in complex scenes is the lack of training examples,
especially for rare objects. In this paper, we explore the possibility to
increase the training examples without laborious data collection and
annotation. We find that an abundance of instance segments can potentially be
obtained freely from object-centric images, according to two insights: (i) an
object-centric image usually contains one salient object in a simple
background; (ii) objects from the same class often share similar appearances or
similar contrasts to the background. Motivated by these insights, we propose a
simple and scalable framework FreeSeg for extracting and leveraging these
"free" object foreground segments to facilitate model training in long-tailed
instance segmentation. Concretely, we investigate the similarity among
object-centric images of the same class to propose candidate segments of
foreground instances, followed by a novel ranking of segment quality. The
resulting high-quality object segments can then be used to augment the existing
long-tailed datasets, e.g., by copying and pasting the segments onto the
original training images. Extensive experiments show that FreeSeg yields
substantial improvements on top of strong baselines and achieves
state-of-the-art accuracy for segmenting rare object categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeachAugment: Data Augmentation Optimization Using Teacher Knowledge. (arXiv:2202.12513v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12513">
<div class="article-summary-box-inner">
<span><p>Optimization of image transformation functions for the purpose of data
augmentation has been intensively studied. In particular, adversarial data
augmentation strategies, which search augmentation maximizing task loss, show
significant improvement in the model generalization for many tasks. However,
the existing methods require careful parameter tuning to avoid excessively
strong deformations that take away image features critical for acquiring
generalization. In this paper, we propose a data augmentation optimization
method based on the adversarial strategy called TeachAugment, which can produce
informative transformed images to the model without requiring careful tuning by
leveraging a teacher model. Specifically, the augmentation is searched so that
augmented images are adversarial for the target model and recognizable for the
teacher model. We also propose data augmentation using neural networks, which
simplifies the search space design and allows for updating of the data
augmentation using the gradient method. We show that TeachAugment outperforms
existing methods in experiments of image classification, semantic segmentation,
and unsupervised representation learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvNeXt-backbone HoVerNet for nuclei segmentation and classification. (arXiv:2202.13560v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13560">
<div class="article-summary-box-inner">
<span><p>This manuscript gives a brief description of the algorithm used to
participate in CoNIC Challenge 2022. After the baseline was made available, we
follow the method in it and replace the ResNet baseline with ConvNeXt one.
Moreover, we propose to first convert RGB space to Haematoxylin-Eosin-DAB(HED)
space, then use Haematoxylin composition of origin image to smooth semantic one
hot label. Afterwards, nuclei distribution of train and valid set are explored
to select the best fold split for training model for final test phase
submission. Results on validation set shows that even with channel of each
stage smaller in number, HoVerNet with ConvNeXt-tiny backbone still improves
the mPQ+ by 0.04 and multi r2 by 0.0144
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion. (arXiv:2203.00838v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00838">
<div class="article-summary-box-inner">
<span><p>A well-known challenge in applying deep-learning methods to omnidirectional
images is spherical distortion. In dense regression tasks such as depth
estimation, where structural details are required, using a vanilla CNN layer on
the distorted 360 image results in undesired information loss. In this paper,
we propose a 360 monocular depth estimation pipeline, OmniFusion, to tackle the
spherical distortion issue. Our pipeline transforms a 360 image into
less-distorted perspective patches (i.e. tangent images) to obtain patch-wise
predictions via CNN, and then merge the patch-wise results for final output. To
handle the discrepancy between patch-wise predictions which is a major issue
affecting the merging quality, we propose a new framework with the following
key components. First, we propose a geometry-aware feature fusion mechanism
that combines 3D geometric features with 2D image features to compensate for
the patch-wise discrepancy. Second, we employ the self-attention-based
transformer architecture to conduct a global aggregation of patch-wise
information, which further improves the consistency. Last, we introduce an
iterative depth refinement mechanism, to further refine the estimated depth
based on the more accurate geometric features. Experiments show that our method
greatly mitigates the distortion issue, and achieves state-of-the-art
performances on several 360 monocular depth estimation benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01305">
<div class="article-summary-box-inner">
<span><p>We present in this paper a novel denoising training method to speedup DETR
(DEtection TRansformer) training and offer a deepened understanding of the slow
convergence issue of DETR-like methods. We show that the slow convergence
results from the instability of bipartite graph matching which causes
inconsistent optimization goals in early training stages. To address this
issue, except for the Hungarian loss, our method additionally feeds
ground-truth bounding boxes with noises into Transformer decoder and trains the
model to reconstruct the original boxes, which effectively reduces the
bipartite graph matching difficulty and leads to a faster convergence. Our
method is universal and can be easily plugged into any DETR-like methods by
adding dozens of lines of code to achieve a remarkable improvement. As a
result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the
same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and
$50$ epochs of training respectively) among DETR-like methods with ResNet-$50$
backbone. Compared with the baseline under the same setting, DN-DETR achieves
comparable performance with $50\%$ training epochs. Code is available at
\url{https://github.com/FengLi-ust/DN-DETR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01522">
<div class="article-summary-box-inner">
<span><p>Despite the success of deep neural networks, there are still many challenges
in deep representation learning due to the data scarcity issues such as data
imbalance, unseen distribution, and domain shift. To address the
above-mentioned issues, a variety of methods have been devised to explore the
sample relationships in a vanilla way (i.e., from the perspectives of either
the input or the loss function), failing to explore the internal structure of
deep neural networks for learning with sample relationships. Inspired by this,
we propose to enable deep neural networks themselves with the ability to learn
the sample relationships from each mini-batch. Specifically, we introduce a
batch transformer module or BatchFormer, which is then applied into the batch
dimension of each mini-batch to implicitly explore sample relationships during
training. By doing this, the proposed method enables the collaboration of
different samples, e.g., the head-class samples can also contribute to the
learning of the tail classes for long-tailed recognition. Furthermore, to
mitigate the gap between training and testing, we share the classifier between
with or without the BatchFormer during training, which can thus be removed
during testing. We perform extensive experiments on over ten datasets and the
proposed method achieves significant improvements on different data scarcity
applications without any bells and whistles, including the tasks of long-tailed
recognition, compositional zero-shot learning, domain generalization, and
contrastive learning. Code will be made publicly available at
https://github.com/zhihou7/BatchFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction. (arXiv:2203.01577v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01577">
<div class="article-summary-box-inner">
<span><p>We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 3M RGB-D egocentric video frames over 5000 sequences collected by 9
participants interacting with 1000 different object instances from 20
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-ERD: Responsive and Coherent Online Motion Style Transfer. (arXiv:2203.02574v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02574">
<div class="article-summary-box-inner">
<span><p>Motion style transfer is a common method for enriching character animation.
Motion style transfer algorithms are often designed for offline settings where
motions are processed in segments. However, for online animation applications,
such as realtime avatar animation from motion capture, motions need to be
processed as a stream with minimal latency. In this work, we realize a
flexible, high-quality motion style transfer method for this setting. We
propose a novel style transfer model, Style-ERD, to stylize motions in an
online manner with an Encoder-Recurrent-Decoder structure, along with a novel
discriminator that combines feature attention and temporal attention. Our
method stylizes motions into multiple target styles with a unified model.
Although our method targets online settings, it outperforms previous offline
methods in motion realism and style expressiveness and provides significant
gains in runtime efficiency
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03605">
<div class="article-summary-box-inner">
<span><p>We present DINO (\textbf{D}ETR with \textbf{I}mproved de\textbf{N}oising
anch\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in
this paper. DINO improves over previous DETR-like models in performance and
efficiency by using a contrastive way for denoising training, a mixed query
selection method for anchor initialization, and a look forward twice scheme for
box prediction. DINO achieves $48.3$AP in $12$ epochs and $51.0$AP in $36$
epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a
significant improvement of $\textbf{+4.9}$\textbf{AP} and
$\textbf{+2.4}$\textbf{AP}, respectively, compared to DN-DETR, the previous
best DETR-like model. DINO scales well in both model size and data size.
Without bells and whistles, after pre-training on the Objects365 dataset with a
SwinL backbone, DINO obtains the best results on both COCO \texttt{val2017}
($\textbf{63.2}$\textbf{AP}) and \texttt{test-dev}
(\textbf{$\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO
significantly reduces its model size and pre-training data size while achieving
better results. Our code will be available at
\url{https://github.com/IDEACVR/DINO}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Aware Object Placement for Visual Environment Reconstruction. (arXiv:2203.03609v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03609">
<div class="article-summary-box-inner">
<span><p>Humans are in constant contact with the world as they move through it and
interact with it. This contact is a vital source of information for
understanding 3D humans, 3D scenes, and the interactions between them. In fact,
we demonstrate that these human-scene interactions (HSIs) can be leveraged to
improve the 3D reconstruction of a scene from a monocular RGB video. Our key
idea is that, as a person moves through a scene and interacts with it, we
accumulate HSIs across multiple input images, and optimize the 3D scene to
reconstruct a consistent, physically plausible and functional 3D scene layout.
Our optimization-based approach exploits three types of HSI constraints: (1)
humans that move in a scene are occluded or occlude objects, thus, defining the
depth ordering of the objects, (2) humans move through free space and do not
interpenetrate objects, (3) when humans and objects are in contact, the contact
surfaces occupy the same place in space. Using these constraints in an
optimization formulation across all observations, we significantly improve the
3D scene layout reconstruction. Furthermore, we show that our scene
reconstruction can be used to refine the initial 3D human pose and shape (HPS)
estimation. We evaluate the 3D scene layout reconstruction and HPS estimation
qualitatively and quantitatively using the PROX and PiGraphs datasets. The code
and data are available for research purposes at https://mover.is.tue.mpg.de/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Rectangling for Image Stitching: A Learning Baseline. (arXiv:2203.03831v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03831">
<div class="article-summary-box-inner">
<span><p>Stitched images provide a wide field-of-view (FoV) but suffer from unpleasant
irregular boundaries. To deal with this problem, existing image rectangling
methods devote to searching an initial mesh and optimizing a target mesh to
form the mesh deformation in two stages. Then rectangular images can be
generated by warping stitched images. However, these solutions only work for
images with rich linear structures, leading to noticeable distortions for
portraits and landscapes with non-linear objects. In this paper, we address
these issues by proposing the first deep learning solution to image
rectangling. Concretely, we predefine a rigid target mesh and only estimate an
initial mesh to form the mesh deformation, contributing to a compact one-stage
solution. The initial mesh is predicted using a fully convolutional network
with a residual progressive regression strategy. To obtain results with high
content fidelity, a comprehensive objective function is proposed to
simultaneously encourage the boundary rectangular, mesh shape-preserving, and
content perceptually natural. Besides, we build the first image stitching
rectangling dataset with a large diversity in irregular boundaries and scenes.
Experiments demonstrate our superiority over traditional methods both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChiTransformer:Towards Reliable Stereo from Cues. (arXiv:2203.04554v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04554">
<div class="article-summary-box-inner">
<span><p>Current stereo matching techniques are challenged by restricted searching
space, occluded regions and sheer size. While single image depth estimation is
spared from these challenges and can achieve satisfactory results with the
extracted monocular cues, the lack of stereoscopic relationship renders the
monocular prediction less reliable on its own, especially in highly dynamic or
cluttered environments. To address these issues in both scenarios, we present
an optic-chiasm-inspired self-supervised binocular depth estimation method,
wherein a vision transformer (ViT) with gated positional cross-attention (GPCA)
layers is designed to enable feature-sensitive pattern retrieval between views
while retaining the extensive context information aggregated through
self-attentions. Monocular cues from a single view are thereafter conditionally
rectified by a blending layer with the retrieved pattern pairs. This crossover
design is biologically analogous to the optic-chasma structure in the human
visual system and hence the name, ChiTransformer. Our experiments show that
this architecture yields substantial improvements over state-of-the-art
self-supervised stereo approaches by 11%, and can be used on both rectilinear
and non-rectilinear (e.g., fisheye) images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05074">
<div class="article-summary-box-inner">
<span><p>Paradoxically, a Variational Autoencoder (VAE) could be pushed in two
opposite directions, utilizing powerful decoder model for generating realistic
images but collapsing the learned representation, or increasing regularization
coefficient for disentangling representation but ultimately generating blurry
examples. Existing methods narrow the issues to the rate-distortion trade-off
between compression and reconstruction. We argue that a good reconstruction
model does learn high capacity latents that encode more details, however, its
use is hindered by two major issues: the prior is random noise which is
completely detached from the posterior and allow no controllability in the
generation; mean-field variational inference doesn't enforce hierarchy
structure which makes the task of recombining those units into plausible novel
output infeasible. As a result, we develop a system that learns a hierarchy of
disentangled representation together with a mechanism for recombining the
learned representation for generalization. This is achieved by introducing a
minimal amount of inductive bias to learn controllable prior for the VAE. The
idea is supported by here developed transitive information theory, that is, the
mutual information between two target variables could alternately be maximized
through the mutual information to the third variable, thus bypassing the
rate-distortion bottleneck in VAE design. In particular, we show that our
model, named SemafoVAE (inspired by the similar concept in computer science),
could generate high-quality examples in a controllable manner, perform smooth
traversals of the disentangled factors and intervention at a different level of
representation hierarchy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
derived from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps of the semantic features
between the textual question and visual answer, existing methods adopting
visual span predictor perform poorly in the TAGV task. To bridge these gaps, we
propose a visual-prompt text span localizing (VPTSL) method, which introduces
the timestamped subtitles as a passage to perform the text span localization
for the input text question, and prompts the visual highlight features into the
pre-trained language model (PLM) for enhancing the joint semantic
representations. Specifically, the context query attention is utilized to
perform cross-modal interaction between the extracted textual and visual
features. Then, the highlight features are obtained through the video-text
highlighting for the visual prompt. To alleviate semantic differences between
textual and visual features, we design the text span predictor by encoding the
question, the subtitles, and the prompted visual highlight features with the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the visual answer. Extensive experiments on the medical instructional
dataset, namely MedVidQA, show that the proposed VPTSL outperforms the
state-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,
which demonstrates the effectiveness of the proposed visual prompt and the text
span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning What Not to Segment: A New Perspective on Few-Shot Segmentation. (arXiv:2203.07615v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07615">
<div class="article-summary-box-inner">
<span><p>Recently few-shot segmentation (FSS) has been extensively developed. Most
previous works strive to achieve generalization through the meta-learning
framework derived from classification tasks; however, the trained models are
biased towards the seen classes instead of being ideally class-agnostic, thus
hindering the recognition of new concepts. This paper proposes a fresh and
straightforward insight to alleviate the problem. Specifically, we apply an
additional branch (base learner) to the conventional FSS model (meta learner)
to explicitly identify the targets of base classes, i.e., the regions that do
not need to be segmented. Then, the coarse results output by these two learners
in parallel are adaptively integrated to yield precise segmentation prediction.
Considering the sensitivity of meta learner, we further introduce an adjustment
factor to estimate the scene differences between the input image pairs for
facilitating the model ensemble forecasting. The substantial performance gains
on PASCAL-5i and COCO-20i verify the effectiveness, and surprisingly, our
versatile scheme sets a new state-of-the-art even with two plain learners.
Moreover, in light of the unique nature of the proposed approach, we also
extend it to a more realistic but challenging setting, i.e., generalized FSS,
where the pixels of both base and novel classes are required to be determined.
The source code is available at github.com/chunbolang/BAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Feature Decoupling with Depthwise Quantization. (arXiv:2203.08080v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08080">
<div class="article-summary-box-inner">
<span><p>Quantization has been applied to multiple domains in Deep Neural Networks
(DNNs). We propose Depthwise Quantization (DQ) where $\textit{quantization}$ is
applied to a decomposed sub-tensor along the $\textit{feature axis}$ of weak
statistical dependence. The feature decomposition leads to an exponential
increase in $\textit{representation capacity}$ with a linear increase in memory
and parameter cost. In addition, DQ can be directly applied to existing
encoder-decoder frameworks without modification of the DNN architecture. We use
DQ in the context of Hierarchical Auto-Encoder and train end-to-end on an image
feature representation. We provide an analysis on cross-correlation between
spatial and channel features and we propose a decomposition of the image
feature representation along the channel axis. The improved performance of the
depthwise operator is due to the increased representation capacity from
implicit feature decoupling. We evaluate DQ on the likelihood estimation task,
where it outperforms the previous state-of-the-art on CIFAR-10, ImageNet-32 and
ImageNet-64. We progressively train with increasing image size a single
hierarchical model that uses 69% less parameters and has a faster convergence
than the previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SocialVAE: Human Trajectory Prediction using Timewise Latents. (arXiv:2203.08207v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08207">
<div class="article-summary-box-inner">
<span><p>Predicting pedestrian movement is critical for human behavior analysis and
also for safe and efficient human-agent interactions. However, despite
significant advancements, it is still challenging for existing approaches to
capture the uncertainty and multimodality of human navigation decision making.
In this paper, we propose SocialVAE, a novel approach for human trajectory
prediction. The core of SocialVAE is a timewise variational autoencoder
architecture that exploits stochastic recurrent neural networks to perform
prediction, combined with a social attention mechanism and backward posterior
approximation to allow for better extraction of pedestrian navigation
strategies. We show that SocialVAE improves current state-of-the-art
performance on several pedestrian trajectory prediction benchmarks, including
the ETH/UCY benchmark, the Stanford Drone Dataset and SportVU NBA movement
dataset. Code is available at: https://github.com/xupei0610/SocialVAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Flow: Cross-layer Graph Flow Distillation for Dual Efficient Medical Image Segmentation. (arXiv:2203.08667v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08667">
<div class="article-summary-box-inner">
<span><p>With the development of deep convolutional neural networks, medical image
segmentation has achieved a series of breakthroughs in recent years. However,
the higher-performance convolutional neural networks always mean numerous
parameters and high computation costs, which will hinder the applications in
clinical scenarios. Meanwhile, the scarceness of large-scale annotated medical
image datasets further impedes the application of high-performance networks. To
tackle these problems, we propose Graph Flow, a comprehensive knowledge
distillation framework, for both network-efficiency and annotation-efficiency
medical image segmentation. Specifically, our core Graph Flow Distillation
transfer the essence of cross-layer variations from a well-trained cumbersome
teacher network to a non-trained compact student network. In addition, an
unsupervised Paraphraser Module is designed to purify the knowledge of the
teacher network, which is also beneficial for the stabilization of training
procedure. Furthermore, we build a unified distillation framework by
integrating the adversarial distillation and the vanilla logits distillation,
which can further refine the final predictions of the compact network.
Extensive experiments conducted on Gastric Cancer Segmentation Dataset and
Synapse Multi-organ Segmentation Dataset demonstrate the prominent ability of
our method which achieves state-of-the-art performance on these
different-modality and multi-category medical image datasets. Moreover, we
demonstrate the effectiveness of our Graph Flow through a new semi-supervised
paradigm for dual efficient medical image segmentation. Our code will be
available at Graph Flow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MotionAug: Augmentation with Physical Correction for Human Motion Prediction. (arXiv:2203.09116v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09116">
<div class="article-summary-box-inner">
<span><p>This paper presents a motion data augmentation scheme incorporating motion
synthesis encouraging diversity and motion correction imposing physical
plausibility. This motion synthesis consists of our modified Variational
AutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed
sampling-near-samples method generates various valid motions even with
insufficient training motion data. Our IK-based motion synthesis method allows
us to generate a variety of motions semi-automatically. Since these two schemes
generate unrealistic artifacts in the synthesized motions, our motion
correction rectifies them. This motion correction scheme consists of imitation
learning with physics simulation and subsequent motion debiasing. For this
imitation learning, we propose the PD-residual force that significantly
accelerates the training process. Furthermore, our motion debiasing
successfully offsets the motion bias induced by imitation learning to maximize
the effect of augmentation. As a result, our method outperforms previous
noise-based motion augmentation methods by a large margin on both Recurrent
Neural Network-based and Graph Convolutional Network-based human motion
prediction models. The code is available at
https://github.com/meaten/MotionAug.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localizing Visual Sounds the Easy Way. (arXiv:2203.09324v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09324">
<div class="article-summary-box-inner">
<span><p>Unsupervised audio-visual source localization aims at localizing visible
sound sources in a video without relying on ground-truth localization for
training. Previous works often seek high audio-visual similarities for likely
positive (sounding) regions and low similarities for likely negative regions.
However, accurately distinguishing between sounding and non-sounding regions is
challenging without manual annotations. In this work, we propose a simple yet
effective approach for Easy Visual Sound Localization, namely EZ-VSL, without
relying on the construction of positive and/or negative regions during
training. Instead, we align audio and visual spaces by seeking audio-visual
representations that are aligned in, at least, one location of the associated
image, while not matching other images, at any location. We also introduce a
novel object guided localization scheme at inference time for improved
precision. Our simple and effective framework achieves state-of-the-art
performance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In
particular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to
83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is
available at https://github.com/stoneMo/EZ-VSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding. (arXiv:2203.10886v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10886">
<div class="article-summary-box-inner">
<span><p>Recently, learned image compression techniques have achieved remarkable
performance, even surpassing the best manually designed lossy image coders.
They are promising to be large-scale adopted. For the sake of practicality, a
thorough investigation of the architecture design of learned image compression,
regarding both compression performance and running speed, is essential. In this
paper, we first propose uneven channel-conditional adaptive coding, motivated
by the observation of energy compaction in learned image compression. Combining
the proposed uneven grouping model with existing context models, we obtain a
spatial-channel contextual adaptive model to improve the coding performance
without damage to running speed. Then we study the structure of the main
transform and propose an efficient model, ELIC, to achieve state-of-the-art
speed and compression ability. With superior performance, the proposed model
also supports extremely fast preview decoding and progressive decoding, which
makes the coming application of learning-based image compression more
promising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixFormer: End-to-End Tracking with Iterative Mixed Attention. (arXiv:2203.11082v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11082">
<div class="article-summary-box-inner">
<span><p>Tracking often uses a multi-stage pipeline of feature extraction, target
information integration, and bounding box estimation. To simplify this pipeline
and unify the process of feature extraction and target information integration,
we present a compact tracking framework, termed as MixFormer, built upon
transformers. Our core design is to utilize the flexibility of attention
operations, and propose a Mixed Attention Module (MAM) for simultaneous feature
extraction and target information integration. This synchronous modeling scheme
allows to extract target-specific discriminative features and perform extensive
communication between target and search area. Based on MAM, we build our
MixFormer tracking framework simply by stacking multiple MAMs with progressive
patch embedding and placing a localization head on top. In addition, to handle
multiple target templates during online tracking, we devise an asymmetric
attention scheme in MAM to reduce computational cost, and propose an effective
score prediction module to select high-quality templates. Our MixFormer sets a
new state-of-the-art performance on five tracking benchmarks, including LaSOT,
TrackingNet, VOT2020, GOT-10k, and UAV123. In particular, our MixFormer-L
achieves NP score of 79.9% on LaSOT, 88.9% on TrackingNet and EAO of 0.555 on
VOT2020. We also perform in-depth ablation studies to demonstrate the
effectiveness of simultaneous feature extraction and information integration.
Code and trained models are publicly available at
https://github.com/MCG-NJU/MixFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed Differential Privacy in Computer Vision. (arXiv:2203.11481v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11481">
<div class="article-summary-box-inner">
<span><p>We introduce AdaMix, an adaptive differentially private algorithm for
training deep neural network classifiers using both private and public image
data. While pre-training language models on large public datasets has enabled
strong differential privacy (DP) guarantees with minor loss of accuracy, a
similar practice yields punishing trade-offs in vision tasks. A few-shot or
even zero-shot learning baseline that ignores private data can outperform
fine-tuning on a large private dataset. AdaMix incorporates few-shot training,
or cross-modal zero-shot learning, on public data prior to private fine-tuning,
to improve the trade-off. AdaMix reduces the error increase from the
non-private upper bound from the 167-311\% of the baseline, on average across 6
datasets, to 68-92\% depending on the desired privacy level selected by the
user. AdaMix tackles the trade-off arising in visual classification, whereby
the most privacy sensitive data, corresponding to isolated points in
representation space, are also critical for high classification accuracy. In
addition, AdaMix comes with strong theoretical privacy guarantees and
convergence analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo. (arXiv:2203.12082v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12082">
<div class="article-summary-box-inner">
<span><p>We present a novel framework named PlaneMVS for 3D plane reconstruction from
multiple input views with known camera poses. Most previous learning-based
plane reconstruction methods reconstruct 3D planes from single images, which
highly rely on single-view regression and suffer from depth scale ambiguity. In
contrast, we reconstruct 3D planes with a multi-view-stereo (MVS) pipeline that
takes advantage of multi-view geometry. We decouple plane reconstruction into a
semantic plane detection branch and a plane MVS branch. The semantic plane
detection branch is based on a single-view plane detection framework but with
differences. The plane MVS branch adopts a set of slanted plane hypotheses to
replace conventional depth hypotheses to perform plane sweeping strategy and
finally learns pixel-level plane parameters and its planar depth map. We
present how the two branches are learned in a balanced way, and propose a
soft-pooling loss to associate the outputs of the two branches and make them
benefit from each other. Extensive experiments on various indoor datasets show
that PlaneMVS significantly outperforms state-of-the-art (SOTA) single-view
plane reconstruction methods on both plane detection and 3D geometry metrics.
Our method even outperforms a set of SOTA learning-based MVS methods thanks to
the learned plane priors. To the best of our knowledge, this is the first work
on 3D plane reconstruction within an end-to-end MVS framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Object Detection for Streaming Perception. (arXiv:2203.12338v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12338">
<div class="article-summary-box-inner">
<span><p>Autonomous driving requires the model to perceive the environment and (re)act
within a low latency for safety. While past works ignore the inevitable changes
in the environment after processing, streaming perception is proposed to
jointly evaluate the latency and accuracy into a single metric for video online
perception. In this paper, instead of searching trade-offs between accuracy and
speed like previous works, we point out that endowing real-time models with the
ability to predict the future is the key to dealing with this problem. We build
a simple and effective framework for streaming perception. It equips a novel
DualFlow Perception module (DFP), which includes dynamic and static flows to
capture the moving trend and basic detection feature for streaming prediction.
Further, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to
generate adaptive weights for objects with different moving speeds. Our simple
method achieves competitive performance on Argoverse-HD dataset and improves
the AP by 4.9% compared to the strong baseline, validating its effectiveness.
Our code will be made available at https://github.com/yancie-yjr/StreamYOLO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CroMo: Cross-Modal Learning for Monocular Depth Estimation. (arXiv:2203.12485v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12485">
<div class="article-summary-box-inner">
<span><p>Learning-based depth estimation has witnessed recent progress in multiple
directions; from self-supervision using monocular video to supervised methods
offering highest accuracy. Complementary to supervision, further boosts to
performance and robustness are gained by combining information from multiple
signals. In this paper we systematically investigate key trade-offs associated
with sensor and modality design choices as well as related model training
strategies. Our study leads us to a new method, capable of connecting
modality-specific advantages from polarisation, Time-of-Flight and
structured-light inputs. We propose a novel pipeline capable of estimating
depth from monocular polarisation for which we evaluate various training
signals. The inversion of differentiable analytic models thereby connects scene
geometry with polarisation and ToF signals and enables self-supervised and
cross-modal learning. In the absence of existing multimodal datasets, we
examine our approach with a custom-made multi-modal camera rig and collect
CroMo; the first dataset to consist of synchronized stereo polarisation,
indirect ToF and structured-light depth, captured at video rates. Extensive
experiments on challenging video scenes confirm both qualitative and
quantitative pipeline advantages where we are able to outperform competitive
monocular depth estimation method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12667">
<div class="article-summary-box-inner">
<span><p>A long-term goal of AI research is to build intelligent agents that can
communicate with humans in natural language, perceive the environment, and
perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental
and interdisciplinary research topic towards this goal, and receives increasing
attention from natural language processing, computer vision, robotics, and
machine learning communities. In this paper, we review contemporary studies in
the emerging field of VLN, covering tasks, evaluation metrics, methods, etc.
Through structured analysis of current progress and challenges, we highlight
the limitations of current VLN and opportunities for future work. This paper
serves as a thorough reference for the VLN research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to generate line drawings that convey geometry and semantics. (arXiv:2203.12691v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12691">
<div class="article-summary-box-inner">
<span><p>This paper presents an unpaired method for creating line drawings from
photographs. Current methods often rely on high quality paired datasets to
generate line drawings. However, these datasets often have limitations due to
the subjects of the drawings belonging to a specific domain, or in the amount
of data collected. Although recent work in unsupervised image-to-image
translation has shown much progress, the latest methods still struggle to
generate compelling line drawings. We observe that line drawings are encodings
of scene information and seek to convey 3D shape and semantic meaning. We build
these observations into a set of objectives and train an image translation to
map photographs into line drawings. We introduce a geometry loss which predicts
depth information from the image features of a line drawing, and a semantic
loss which matches the CLIP features of a line drawing with its corresponding
photograph. Our approach outperforms state-of-the-art unpaired image
translation and line drawing generation methods on creating line drawings from
arbitrary photographs. For code and demo visit our webpage
carolineec.github.io/informative_drawings
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation. (arXiv:2203.12707v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12707">
<div class="article-summary-box-inner">
<span><p>Unpaired image-to-image translation (I2I) is an ill-posed problem, as an
infinite number of translation functions can map the source domain distribution
to the target distribution. Therefore, much effort has been put into designing
suitable constraints, e.g., cycle consistency (CycleGAN), geometry consistency
(GCGAN), and contrastive learning-based constraints (CUTGAN), that help better
pose the problem. However, these well-known constraints have limitations: (1)
they are either too restrictive or too weak for specific I2I tasks; (2) these
methods result in content distortion when there is a significant spatial
variation between the source and target domains. This paper proposes a
universal regularization technique called maximum spatial perturbation
consistency (MSPC), which enforces a spatial perturbation function (T ) and the
translation operator (G) to be commutative (i.e., TG = GT ). In addition, we
introduce two adversarial training components for learning the spatial
perturbation function. The first one lets T compete with G to achieve maximum
perturbation. The second one lets G and T compete with discriminators to align
the spatial variations caused by the change of object size, object distortion,
background interruptions, etc. Our method outperforms the state-of-the-art
methods on most I2I benchmarks. We also introduce a new benchmark, namely the
front face to profile face dataset, to emphasize the underlying challenges of
I2I for real-world applications. We finally perform ablation experiments to
study the sensitivity of our method to the severity of spatial perturbation and
its effectiveness for distribution alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12818">
<div class="article-summary-box-inner">
<span><p>In this paper we describe our approach to the arousal and valence track of
the 3rd Workshop and Competition on Affective Behavior Analysis in-the-wild
(ABAW). We extracted facial features using OpenFace and used them to train a
multiple output random forest regressor. Our approach performed comparable to
the baseline approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Emotion Descriptors Estimation at the ABAW3 Challenge. (arXiv:2203.12845v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12845">
<div class="article-summary-box-inner">
<span><p>To describe complex emotional states, psychologists have proposed multiple
emotion descriptors: sparse descriptors like facial action units; continuous
descriptors like valence and arousal; and discrete class descriptors like
happiness and anger. According to Ekman and Friesen, 1969, facial action units
are sign vehicles that convey the emotion message, while discrete or continuous
emotion descriptors are the messages perceived and expressed by human.
</p>
<p>In this paper, we designed an architecture for multiple emotion descriptors
estimation in participating the ABAW3 Challenge. Based on the theory of Ekman
and Friesen, 1969, we designed distinct architectures to measure the sign
vehicles (i.e., facial action units) and the message (i.e., discrete emotions,
valence and arousal) given their different properties. The quantitative
experiments on the ABAW3 challenge dataset has shown the superior performance
of our approach over two baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intrinsic Bias Identification on Medical Image Datasets. (arXiv:2203.12872v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12872">
<div class="article-summary-box-inner">
<span><p>Machine learning based medical image analysis highly depends on datasets.
Biases in the dataset can be learned by the model and degrade the
generalizability of the applications. There are studies on debiased models.
However, scientists and practitioners are difficult to identify implicit biases
in the datasets, which causes lack of reliable unbias test datasets to valid
models. To tackle this issue, we first define the data intrinsic bias
attribute, and then propose a novel bias identification framework for medical
image datasets. The framework contains two major components, KlotskiNet and
Bias Discriminant Direction Analysis(bdda), where KlostkiNet is to build the
mapping which makes backgrounds to distinguish positive and negative samples
and bdda provides a theoretical solution on determining bias attributes.
Experimental results on three datasets show the effectiveness of the bias
attributes discovered by the framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image. (arXiv:2203.13009v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13009">
<div class="article-summary-box-inner">
<span><p>Recently, significant progress has been made on image denoising with strong
supervision from large-scale datasets. However, obtaining well-aligned
noisy-clean training image pairs for each specific scenario is complicated and
costly in practice. Consequently, applying a conventional supervised denoising
network on in-the-wild noisy inputs is not straightforward. Although several
studies have challenged this problem without strong supervision, they rely on
less practical assumptions and cannot be applied to practical situations
directly. To address the aforementioned challenges, we propose a novel and
powerful self-supervised denoising method called CVF-SID based on a Cyclic
multi-Variate Function (CVF) module and a self-supervised image disentangling
(SID) framework. The CVF module can output multiple decomposed variables of the
input and take a combination of the outputs back as an input in a cyclic
manner. Our CVF-SID can disentangle a clean image and noise maps from the input
by leveraging various self-supervised loss terms. Unlike several methods that
only consider the signal-independent noise models, we also deal with
signal-dependent noise components for real-world applications. Furthermore, we
do not rely on any prior assumptions about the underlying noise distribution,
making CVF-SID more generalizable toward realistic noise. Extensive experiments
on real-world datasets show that CVF-SID achieves state-of-the-art
self-supervised image denoising performance and is comparable to other existing
approaches. The code is publicly available from
https://github.com/Reyhanehne/CVF-SID_PyTorch .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation. (arXiv:2203.13254v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13254">
<div class="article-summary-box-inner">
<span><p>Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is
a long-standing problem in computer vision. Driven by end-to-end deep learning,
recent studies suggest interpreting PnP as a differentiable layer, so that
2D-3D point correspondences can be partly learned by backpropagating the
gradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D
points from scratch fails to converge with existing approaches, since the
deterministic pose is inherently non-differentiable. In this paper, we propose
the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation,
which outputs a distribution of pose on the SE(3) manifold, essentially
bringing categorical Softmax to the continuous domain. The 2D-3D coordinates
and corresponding weights are treated as intermediate variables learned by
minimizing the KL divergence between the predicted and target pose
distribution. The underlying principle unifies the existing approaches and
resembles the attention mechanism. EPro-PnP significantly outperforms
competitive baselines, closing the gap between PnP-based method and the
task-specific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D
object detection benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Blind Denoising via Swin-Conv-UNet and Data Synthesis. (arXiv:2203.13278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13278">
<div class="article-summary-box-inner">
<span><p>While recent years have witnessed a dramatic upsurge of exploiting deep
neural networks toward solving image denoising, existing methods mostly rely on
simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG
compression noise and camera sensor noise, and a general-purpose blind
denoising method for real images remains unsolved. In this paper, we attempt to
solve this problem from the perspective of network architecture design and
training data synthesis. Specifically, for the network architecture design, we
propose a swin-conv block to incorporate the local modeling ability of residual
convolutional layer and non-local modeling ability of swin transformer block,
and then plug it as the main building block into the widely-used image-to-image
translation UNet architecture. For the training data synthesis, we design a
practical noise degradation model which takes into consideration different
kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and
processed camera sensor noises) and resizing, and also involves a random
shuffle strategy and a double degradation strategy. Extensive experiments on
AGWN removal and real image denoising demonstrate that the new network
architecture design achieves state-of-the-art performance and the new
degradation model can help to significantly improve the practicability. We
believe our work can provide useful insights into current denoising research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition. (arXiv:2203.13285v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13285">
<div class="article-summary-box-inner">
<span><p>In this paper, we present our submission to 3rd Affective Behavior Analysis
in-the-wild (ABAW) challenge. Learningcomplex interactions among multimodal
sequences is critical to recognise dimensional affect from in-the-wild
audiovisual data. Recurrence and attention are the two widely used sequence
modelling mechanisms in the literature. To clearly understand the performance
differences between recurrent and attention models in audiovisual affect
recognition, we present a comprehensive evaluation of fusion models based on
LSTM-RNNs, self-attention and cross-modal attention, trained for valence and
arousal estimation. Particularly, we study the impact of some key design
choices: the modelling complexity of CNN backbones that provide features to the
the temporal models, with and without end-to-end learning. We trained the
audiovisual affect recognition models on in-the-wild ABAW corpus by
systematically tuning the hyper-parameters involved in the network architecture
design and training optimisation. Our extensive evaluation of the audiovisual
fusion models shows that LSTM-RNNs can outperform the attention models when
coupled with low-complex CNN backbones and trained in an end-to-end fashion,
implying that attention models may not necessarily be the optimal choice for
continuous-time multimodal emotion recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceVerse: a Fine-grained and Detail-controllable 3D Face Morphable Model from a Hybrid Dataset. (arXiv:2203.14057v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14057">
<div class="article-summary-box-inner">
<span><p>We present FaceVerse, a fine-grained 3D Neural Face Model, which is built
from hybrid East Asian face datasets containing 60K fused RGB-D images and 2K
high-fidelity 3D head scan models. A novel coarse-to-fine structure is proposed
to take better advantage of our hybrid dataset. In the coarse module, we
generate a base parametric model from large-scale RGB-D images, which is able
to predict accurate rough 3D face models in different genders, ages, etc. Then
in the fine module, a conditional StyleGAN architecture trained with
high-fidelity scan models is introduced to enrich elaborate facial geometric
and texture details. Note that different from previous methods, our base and
detailed modules are both changeable, which enables an innovative application
of adjusting both the basic attributes and the facial details of 3D face
models. Furthermore, we propose a single-image fitting framework based on
differentiable rendering. Rich experiments show that our method outperforms the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Selective Transformer for Semantic Image Segmentation. (arXiv:2203.14124v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14124">
<div class="article-summary-box-inner">
<span><p>Recently, it has attracted more and more attentions to fuse multi-scale
features for semantic image segmentation. Various works were proposed to employ
progressive local or global fusion, but the feature fusions are not rich enough
for modeling multi-scale context features. In this work, we focus on fusing
multi-scale features from Transformer-based backbones for semantic
segmentation, and propose a Feature Selective Transformer (FeSeFormer), which
aggregates features from all scales (or levels) for each query feature.
Specifically, we first propose a Scale-level Feature Selection (SFS) module,
which can choose an informative subset from the whole multi-scale feature set
for each scale, where those features that are important for the current scale
(or level) are selected and the redundant are discarded. Furthermore, we
propose a Full-scale Feature Fusion (FFF) module, which can adaptively fuse
features of all scales for queries. Based on the proposed SFS and FFF modules,
we develop a Feature Selective Transformer (FeSeFormer), and evaluate our
FeSeFormer on four challenging semantic segmentation benchmarks, including
PASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Adaptive Activity Recognition Across Video Domains. (arXiv:2203.14240v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14240">
<div class="article-summary-box-inner">
<span><p>This paper strives for activity recognition under domain shift, for example
caused by change of scenery or camera viewpoint. The leading approaches reduce
the shift in activity appearance by adversarial training and self-supervised
learning. Different from these vision-focused works we leverage activity sounds
for domain adaptation as they have less variance across domains and can
reliably indicate which activities are not happening. We propose an
audio-adaptive encoder and associated learning methods that discriminatively
adjust the visual feature representation as well as addressing shifts in the
semantic distribution. To further eliminate domain-specific features and
include domain-invariant activity sounds for recognition, an audio-infused
recognizer is proposed, which effectively models the cross-modal interaction
across domains. We also introduce the new task of actor shift, with a
corresponding audio-visual dataset, to challenge our method with situations
where the activity appearance changes dramatically. Experiments on this
dataset, EPIC-Kitchens and CharadesEgo show the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperMVS: Non-Uniform Cost Volume For High-Resolution Multi-View Stereo. (arXiv:2203.14331v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14331">
<div class="article-summary-box-inner">
<span><p>Different from most state-of-the-art~(SOTA) algorithms that use static and
uniform sampling methods with a lot of hypothesis planes to get fine depth
sampling. In this paper, we propose a free-moving hypothesis plane method for
dynamic and non-uniform sampling in a wide depth range to build the cost
volume, which not only greatly reduces the number of planes but also finers
sampling, for both of reducing computational cost and improving accuracy, named
Non-Uniform Cost Volume. We present the SuperMVS network to implement
Multi-View Stereo with Non-Uniform Cost Volume. SuperMVS is a coarse-to-fine
framework with four cascade stages. It can output higher resolution and
accurate depth map. Our SuperMVS achieves the SOTA results with low memory, low
runtime, and fewer planes on the DTU datasets and Tanks \&amp; Temples dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locality-Aware Inter-and Intra-Video Reconstruction for Self-Supervised Correspondence Learning. (arXiv:2203.14333v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14333">
<div class="article-summary-box-inner">
<span><p>Our target is to learn visual correspondence from unlabeled videos. We
develop LIIR, a locality-aware inter-and intra-video reconstruction framework
that fills in three missing pieces, i.e., instance discrimination, location
awareness, and spatial compactness, of self-supervised correspondence learning
puzzle. First, instead of most existing efforts focusing on intra-video
self-supervision only, we exploit cross video affinities as extra negative
samples within a unified, inter-and intra-video reconstruction scheme. This
enables instance discriminative representation learning by contrasting desired
intra-video pixel association against negative inter-video correspondence.
Second, we merge position information into correspondence matching, and design
a position shifting strategy to remove the side-effect of position encoding
during inter-video affinity computation, making our LIIR location-sensitive.
Third, to make full use of the spatial continuity nature of video data, we
impose a compactness-based constraint on correspondence matching, yielding more
sparse and reliable solutions. The learned representation surpasses
self-supervised state-of-the-arts on label propagation tasks including objects,
semantic parts, and keypoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Hierarchical Semantic Segmentation. (arXiv:2203.14335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14335">
<div class="article-summary-box-inner">
<span><p>Humans are able to recognize structured relations in observation, allowing us
to decompose complex scenes into simpler parts and abstract the visual world in
multiple levels. However, such hierarchical reasoning ability of human
perception remains largely unexplored in current literature of semantic
segmentation. Existing work is often aware of flatten labels and predicts
target classes exclusively for each pixel. In this paper, we instead address
hierarchical semantic segmentation (HSS), which aims at structured, pixel-wise
description of visual observation in terms of a class hierarchy. We devise
HSSN, a general HSS framework that tackles two critical issues in this task: i)
how to efficiently adapt existing hierarchy-agnostic segmentation networks to
the HSS setting, and ii) how to leverage the hierarchy information to
regularize HSS network learning. To address i), HSSN directly casts HSS as a
pixel-wise multi-label classification task, only bringing minimal architecture
change to current segmentation models. To solve ii), HSSN first explores
inherent properties of the hierarchy as a training objective, which enforces
segmentation predictions to obey the hierarchy structure. Further, with
hierarchy-induced margin constraints, HSSN reshapes the pixel embedding space,
so as to generate well-structured pixel representations and improve
segmentation eventually. We conduct experiments on four semantic segmentation
datasets (i.e., Mapillary Vistas 2.0, Cityscapes, LIP, and PASCAL-Person-Part),
with different class hierarchies, segmentation network architectures and
backbones, showing the generalization and superiority of HSSN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFSNet: A Multi Focus Segmentation Network for Skin Lesion Segmentation. (arXiv:2203.14341v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14341">
<div class="article-summary-box-inner">
<span><p>Segmentation is essential for medical image analysis to identify and localize
diseases, monitor morphological changes, and extract discriminative features
for further diagnosis. Skin cancer is one of the most common types of cancer
globally, and its early diagnosis is pivotal for the complete elimination of
malignant tumors from the body. This research develops an Artificial
Intelligence (AI) framework for supervised skin lesion segmentation employing
the deep learning approach. The proposed framework, called MFSNet (Multi-Focus
Segmentation Network), uses differently scaled feature maps for computing the
final segmentation mask using raw input RGB images of skin lesions. In doing
so, initially, the images are preprocessed to remove unwanted artifacts and
noises. The MFSNet employs the Res2Net backbone, a recently proposed
convolutional neural network (CNN), for obtaining deep features used in a
Parallel Partial Decoder (PPD) module to get a global map of the segmentation
mask. In different stages of the network, convolution features and multi-scale
maps are used in two boundary attention (BA) modules and two reverse attention
(RA) modules to generate the final segmentation output. MFSNet, when evaluated
on three publicly available datasets: $PH^2$, ISIC 2017, and HAM10000,
outperforms state-of-the-art methods, justifying the reliability of the
framework. The relevant codes for the proposed approach are accessible at
https://github.com/Rohit-Kundu/MFSNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thin-Plate Spline Motion Model for Image Animation. (arXiv:2203.14367v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14367">
<div class="article-summary-box-inner">
<span><p>Image animation brings life to the static object in the source image
according to the driving video. Recent works attempt to perform motion transfer
on arbitrary objects through unsupervised methods without using a priori
knowledge. However, it remains a significant challenge for current unsupervised
methods when there is a large pose gap between the objects in the source and
driving images. In this paper, a new end-to-end unsupervised motion transfer
framework is proposed to overcome such issue. Firstly, we propose thin-plate
spline motion estimation to produce a more flexible optical flow, which warps
the feature maps of the source image to the feature domain of the driving
image. Secondly, in order to restore the missing regions more realistically, we
leverage multi-resolution occlusion masks to achieve more effective feature
fusion. Finally, additional auxiliary loss functions are designed to ensure
that there is a clear division of labor in the network modules, encouraging the
network to generate high-quality images. Our method can animate a variety of
objects, including talking faces, human bodies, and pixel animations.
Experiments demonstrate that our method performs better on most benchmarks than
the state of the art with visible improvements in pose-related metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARCS: Accurate Rotation and Correspondence Search. (arXiv:2203.14493v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14493">
<div class="article-summary-box-inner">
<span><p>This paper is about the old Wahba problem in its more general form, which we
call "simultaneous rotation and correspondence search". In this generalization
we need to find a rotation that best aligns two partially overlapping $3$D
point sets, of sizes $m$ and $n$ respectively with $m\geq n$. We first propose
a solver, $\texttt{ARCS}$, that i) assumes noiseless point sets in general
position, ii) requires only $2$ inliers, iii) uses $O(m\log m)$ time and $O(m)$
space, and iv) can successfully solve the problem even with, e.g., $m,n\approx
10^6$ in about $0.1$ seconds. We next robustify $\texttt{ARCS}$ to noise, for
which we approximately solve consensus maximization problems using ideas from
robust subspace learning and interval stabbing. Thirdly, we refine the
approximately found consensus set by a Riemannian subgradient descent approach
over the space of unit quaternions, which we show converges globally to an
$\varepsilon$-stationary point in $O(\varepsilon^{-4})$ iterations, or locally
to the ground-truth at a linear rate in the absence of noise. We combine these
algorithms into $\texttt{ARCS+}$, to simultaneously search for rotations and
correspondences. Experiments show that $\texttt{ARCS+}$ achieves
state-of-the-art performance on large-scale datasets with more than $10^6$
points with a $10^4$ time-speedup over alternative methods.
\url{https://github.com/liangzu/ARCS}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affordance Transfer Learning for Human-Object Interaction Detection. (arXiv:2104.02867v2 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02867">
<div class="article-summary-box-inner">
<span><p>Reasoning the human-object interactions (HOI) is essential for deeper scene
understanding, while object affordances (or functionalities) are of great
importance for human to discover unseen HOIs with novel objects. Inspired by
this, we introduce an affordance transfer learning approach to jointly detect
HOIs with novel objects and recognize affordances. Specifically, HOI
representations can be decoupled into a combination of affordance and object
representations, making it possible to compose novel interactions by combining
affordance representations and novel object representations from additional
images, i.e. transferring the affordance to novel objects. With the proposed
affordance transfer learning, the model is also capable of inferring the
affordances of novel objects from known affordance representations. The
proposed method can thus be used to 1) improve the performance of HOI
detection, especially for the HOIs with unseen objects; and 2) infer the
affordances of novel objects. Experimental results on two datasets, HICO-DET
and HOI-COCO (from V-COCO), demonstrate significant improvements over recent
state-of-the-art methods for HOI detection and object affordance detection.
Code is available at https://github.com/zhihou7/HOI-CL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Human-Object Interaction Concepts via Self-Compositional Learning. (arXiv:2203.14272v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14272">
<div class="article-summary-box-inner">
<span><p>A comprehensive understanding of human-object interaction (HOI) requires
detecting not only a small portion of predefined HOI concepts (or categories)
but also other reasonable HOI concepts, while current approaches usually fail
to explore a huge portion of unknown HOI concepts (i.e., unknown but reasonable
combinations of verbs and objects). In this paper, 1) we introduce a novel and
challenging task for a comprehensive HOI understanding, which is termed as HOI
Concept Discovery; and 2) we devise a self-compositional learning framework (or
SCL) for HOI concept discovery. Specifically, we maintain an online updated
concept confidence matrix during training: 1) we assign pseudo-labels for all
composite HOI instances according to the concept confidence matrix for
self-training; and 2) we update the concept confidence matrix using the
predictions of all composite HOI instances. Therefore, the proposed method
enables the learning on both known and unknown HOI concepts. We perform
extensive experiments on several popular HOI datasets to demonstrate the
effectiveness of the proposed method for HOI concept discovery, object
affordance recognition and HOI detection. For example, the proposed
self-compositional learning framework significantly improves the performance of
1) HOI concept discovery by over 10% on HICO-DET and over 3% on V-COCO,
respectively; 2) object affordance recognition by over 9% mAP on MS-COCO and
HICO-DET; and 3) rare-first and non-rare-first unknown HOI detection relatively
over 30% and 20%, respectively. Code and models will be made publicly available
at https://github.com/zhihou7/HOI-CL.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-30 23:08:12.065766506 UTC">2022-03-30 23:08:12 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>