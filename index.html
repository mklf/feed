<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-22T01:30:00Z">09-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCoDisCo at the DISRPT2021 Shared Task: A System for Discourse Segmentation, Classification, and Connective Detection. (arXiv:2109.09777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09777">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to the DISRPT2021 Shared Task on
Discourse Unit Segmentation, Connective Detection, and Relation Classification.
Our system, called DisCoDisCo, is a Transformer-based neural classifier which
enhances contextualized word embeddings (CWEs) with hand-crafted features,
relying on tokenwise sequence tagging for discourse segmentation and connective
detection, and a feature-rich, encoder-less sentence pair classifier for
relation classification. Our results for the first two tasks outperform SOTA
scores from the previous 2019 shared task, and results on relation
classification suggest strong performance on the new 2021 benchmark. Ablation
tests show that including features beyond CWEs are helpful for both tasks, and
a partial evaluation of multiple pre-trained Transformer-based language models
indicates that models pre-trained on the Next Sentence Prediction (NSP) task
are optimal for relation classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology. (arXiv:2109.09780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09780">
<div class="article-summary-box-inner">
<span><p>An important question concerning contextualized word embedding (CWE) models
like BERT is how well they can represent different word senses, especially
those in the long tail of uncommon senses. Rather than build a WSD system as in
previous work, we investigate contextualized embedding neighborhoods directly,
formulating a query-by-example nearest neighbor retrieval task and examining
ranking performance for words and senses in different frequency bands. In an
evaluation on two English sense-annotated corpora, we find that several popular
CWE models all outperform a random baseline even for proportionally rare
senses, without explicit sense supervision. However, performance varies
considerably even among models with similar architectures and pretraining
regimes, with especially large differences for rare word senses, revealing that
CWE models are not all created equal when it comes to approximating word senses
in their native representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization. (arXiv:2109.09784v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09784">
<div class="article-summary-box-inner">
<span><p>State-of-the-art abstractive summarization systems often generate
\emph{hallucinations}; i.e., content that is not directly inferable from the
source text. Despite being assumed incorrect, many of the hallucinated contents
are consistent with world knowledge (factual hallucinations). Including these
factual hallucinations into a summary can be beneficial in providing additional
background information. In this work, we propose a novel detection approach
that separates factual from non-factual hallucinations of entities. Our method
is based on an entity's prior and posterior probabilities according to
pre-trained and finetuned masked language models, respectively. Empirical
results suggest that our method vastly outperforms three strong baselines in
both accuracy and F1 scores and has a strong correlation with human judgments
on factuality classification tasks. Furthermore, our approach can provide
insight into whether a particular hallucination is caused by the summarizer's
pre-training or fine-tuning step.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency Induction Through the Lens of Visual Perception. (arXiv:2109.09790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09790">
<div class="article-summary-box-inner">
<span><p>Most previous work on grammar induction focuses on learning phrasal or
dependency structure purely from text. However, because the signal provided by
text alone is limited, recently introduced visually grounded syntax models make
use of multimodal information leading to improved performance in constituency
grammar induction. However, as compared to dependency grammars, constituency
grammars do not provide a straightforward way to incorporate visual information
without enforcing language-specific heuristics. In this paper, we propose an
unsupervised grammar induction model that leverages word concreteness and a
structural vision-based heuristic to jointly learn constituency-structure and
dependency-structure grammars. Our experiments find that concreteness is a
strong indicator for learning dependency grammars, improving the direct
attachment score (DAS) by over 50\% as compared to state-of-the-art models
trained on pure text. Next, we propose an extension of our model that leverages
both word concreteness and visual semantic role labels in constituency and
dependency parsing. Our experiments show that the proposed extension
outperforms the current state-of-the-art visually grounded models in
constituency parsing even with a smaller grammar size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming Fake News: Robust Generalisable News Classification Using Transformers. (arXiv:2109.09796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09796">
<div class="article-summary-box-inner">
<span><p>As online news has become increasingly popular and fake news increasingly
prevalent, the ability to audit the veracity of online news content has become
more important than ever. Such a task represents a binary classification
challenge, for which transformers have achieved state-of-the-art results. Using
the publicly available ISOT and Combined Corpus datasets, this study explores
transformers' abilities to identify fake news, with particular attention given
to investigating generalisation to unseen datasets with varying styles, topics
and class distributions. Moreover, we explore the idea that opinion-based news
articles cannot be classified as real or fake due to their subjective nature
and often sensationalised language, and propose a novel two-step classification
pipeline to remove such articles from both model training and the final
deployed inference system. Experiments over the ISOT and Combined Corpus
datasets show that transformers achieve an increase in F1 scores of up to 4.9%
for out of distribution generalisation compared to baseline approaches, with a
further increase of 10.1% following the implementation of our two-step
classification pipeline. To the best of our knowledge, this study is the first
to investigate generalisation of transformers in this context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Span Representation for Domain-adapted Coreference Resolution. (arXiv:2109.09811v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09811">
<div class="article-summary-box-inner">
<span><p>Recent work has shown fine-tuning neural coreference models can produce
strong performance when adapting to different domains. However, at the same
time, this can require a large amount of annotated target examples. In this
work, we focus on supervised domain adaptation for clinical notes, proposing
the use of concept knowledge to more efficiently adapt coreference models to a
new domain. We develop methods to improve the span representations via (1) a
retrofitting loss to incentivize span representations to satisfy a
knowledge-based distance function and (2) a scaffolding loss to guide the
recovery of knowledge from the span representation. By integrating these
losses, our model is able to improve our baseline precision and F-1 score. In
particular, we show that incorporating knowledge with end-to-end coreference
models results in better performance on the most challenging, domain-specific
spans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation Methods for Anaphoric Zero Pronouns. (arXiv:2109.09825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09825">
<div class="article-summary-box-inner">
<span><p>In pro-drop language like Arabic, Chinese, Italian, Japanese, Spanish, and
many others, unrealized (null) arguments in certain syntactic positions can
refer to a previously introduced entity, and are thus called anaphoric zero
pronouns. The existing resources for studying anaphoric zero pronoun
interpretation are however still limited. In this paper, we use five data
augmentation methods to generate and detect anaphoric zero pronouns
automatically. We use the augmented data as additional training materials for
two anaphoric zero pronoun systems for Arabic. Our experimental results show
that data augmentation improves the performance of the two systems, surpassing
the state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StreamSide: A Fully-Customizable Open-Source Toolkit for Efficient Annotation of Meaning Representations. (arXiv:2109.09853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09853">
<div class="article-summary-box-inner">
<span><p>This demonstration paper presents StreamSide, an open-source toolkit for
annotating multiple kinds of meaning representations. StreamSide supports
frame-based annotation schemes e.g., Abstract Meaning Representation (AMR) and
frameless annotation schemes e.g., Widely Interpretable Semantic Representation
(WISeR). Moreover, it supports both sentence-level and document-level
annotation by allowing annotators to create multi-rooted graphs for input text.
It can open and automatically convert between several types of input formats
including plain text, Penman notation, and its own JSON format enabling richer
annotation. It features reference frames for AMR predicate argument structures,
and also concept-to-text alignment. StreamSide is released under the Apache 2.0
license, and is completely open-source so that it can be customized to annotate
enriched meaning representations in different languages (e.g., Uniform Meaning
Representations). All StreamSide resources are publicly distributed through our
open source project at: https://github.com/emorynlp/StreamSide.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intensionalizing Abstract Meaning Representations: Non-Veridicality and Scope. (arXiv:2109.09858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09858">
<div class="article-summary-box-inner">
<span><p>Abstract Meaning Representation (AMR) is a graphical meaning representation
language designed to represent propositional information about argument
structure. However, at present it is unable to satisfyingly represent
non-veridical intensional contexts, often licensing inappropriate inferences.
In this paper, we show how to resolve the problem of non-veridicality without
appealing to layered graphs through a mapping from AMRs into Simply-Typed
Lambda Calculus (STLC). At least for some cases, this requires the introduction
of a new role :content which functions as an intensional operator. The
translation proposed is inspired by the formal linguistics literature on the
event semantics of attitude reports. Next, we address the interaction of
quantifier scope and intensional operators in so-called de re/de dicto
ambiguities. We adopt a scope node from the literature and provide an explicit
multidimensional semantics utilizing Cooper storage which allows us to derive
the de re and de dicto scope readings as well as intermediate scope readings
which prove difficult for accounts without a scope node.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Identification with a Reciprocal Rank Classifier. (arXiv:2109.09862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09862">
<div class="article-summary-box-inner">
<span><p>Language identification is a critical component of language processing
pipelines (Jauhiainen et al.,2019) and is not a solved problem in real-world
settings. We present a lightweight and effective language identifier that is
robust to changes of domain and to the absence of copious training data.
</p>
<p>The key idea for classification is that the reciprocal of the rank in a
frequency table makes an effective additive feature score, hence the term
Reciprocal Rank Classifier (RRC). The key finding for language classification
is that ranked lists of words and frequencies of characters form a sufficient
and robust representation of the regularities of key languages and their
orthographies.
</p>
<p>We test this on two 22-language data sets and demonstrate zero-effort domain
adaptation from a Wikipedia training set to a Twitter test set. When trained on
Wikipedia but applied to Twitter the macro-averaged F1-score of a
conventionally trained SVM classifier drops from 90.9% to 77.7%. By contrast,
the macro F1-score of RRC drops only from 93.1% to 90.6%. These classifiers are
compared with those from fastText and langid. The RRC performs better than
these established systems in most experiments, especially on short Wikipedia
texts and Twitter.
</p>
<p>The RRC classifier can be improved for particular domains and conversational
situations by adding words to the ranked lists. Using new terms learned from
such conversations, we demonstrate a further 7.9% increase in accuracy of
sample message classification, and 1.7% increase for conversation
classification. Surprisingly, this made results on Twitter data slightly worse.
</p>
<p>The RRC classifier is available as an open source Python package
(https://github.com/LivePersonInc/lplangid).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Short Text Clustering. (arXiv:2109.09894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09894">
<div class="article-summary-box-inner">
<span><p>Effective representation learning is critical for short text clustering due
to the sparse, high-dimensional and noise attributes of short text corpus.
Existing pre-trained models (e.g., Word2vec and BERT) have greatly improved the
expressiveness for short text representations with more condensed,
low-dimensional and continuous features compared to the traditional
Bag-of-Words (BoW) model. However, these models are trained for general
purposes and thus are suboptimal for the short text clustering task. In this
paper, we propose two methods to exploit the unsupervised autoencoder (AE)
framework to further tune the short text representations based on these
pre-trained text models for optimal clustering performance. In our first method
Structural Text Network Graph Autoencoder (STN-GAE), we exploit the structural
text information among the corpus by constructing a text network, and then
adopt graph convolutional network as encoder to fuse the structural features
with the pre-trained text features for text representation learning. In our
second method Soft Cluster Assignment Autoencoder (SCA-AE), we adopt an extra
soft cluster assignment constraint on the latent space of autoencoder to
encourage the learned text representations to be more clustering-friendly. We
tested two methods on seven popular short text datasets, and the experimental
results show that when only using the pre-trained model for short text
clustering, BERT performs better than BoW and Word2vec. However, as long as we
further tune the pre-trained representations, the proposed method like SCA-AE
can greatly increase the clustering performance, and the accuracy improvement
compared to use BERT alone could reach as much as 14\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalization in Text-based Games via Hierarchical Reinforcement Learning. (arXiv:2109.09968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09968">
<div class="article-summary-box-inner">
<span><p>Deep reinforcement learning provides a promising approach for text-based
games in studying natural language communication between humans and artificial
agents. However, the generalization still remains a big challenge as the agents
depend critically on the complexity and variety of training tasks. In this
paper, we address this problem by introducing a hierarchical framework built
upon the knowledge graph-based RL agent. In the high level, a meta-policy is
executed to decompose the whole game into a set of subtasks specified by
textual goals, and select one of them based on the KG. Then a sub-policy in the
low level is executed to conduct goal-conditioned reinforcement learning. We
carry out experiments on games with various difficulty levels and show that the
proposed method enjoys favorable generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Kernel-Smoothed Machine Translation with Retrieved Examples. (arXiv:2109.09991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09991">
<div class="article-summary-box-inner">
<span><p>How to effectively adapt neural machine translation (NMT) models according to
emerging cases without retraining? Despite the great success of neural machine
translation, updating the deployed models online remains a challenge. Existing
non-parametric approaches that retrieve similar examples from a database to
guide the translation process are promising but are prone to overfit the
retrieved examples. However, non-parametric methods are prone to overfit the
retrieved examples. In this work, we propose to learn Kernel-Smoothed
Translation with Example Retrieval (KSTER), an effective approach to adapt
neural machine translation models online. Experiments on domain adaptation and
multi-domain machine translation datasets show that even without expensive
retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over
the best existing online adaptation methods. The code and trained models are
released at https://github.com/jiangqn/KSTER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negation-Instance Based Evaluation of End-to-End Negation Resolution. (arXiv:2109.10013v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10013">
<div class="article-summary-box-inner">
<span><p>In this paper, we revisit the task of negation resolution, which includes the
subtasks of cue detection (e.g. "not", "never") and scope resolution. In the
context of previous shared tasks, a variety of evaluation metrics have been
proposed. Subsequent works usually use different subsets of these, including
variations and custom implementations, rendering meaningful comparisons between
systems difficult. Examining the problem both from a linguistic perspective and
from a downstream viewpoint, we here argue for a negation-instance based
approach to evaluating negation resolution. Our proposed metrics correspond to
expectations over per-instance scores and hence are intuitively interpretable.
To render research comparable and to foster future work, we provide results for
a set of current state-of-the-art systems for negation resolution on three
English corpora, and make our implementation of the evaluation scripts publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Comments are Equal: Insights into Comment Moderation from a Topic-Aware Model. (arXiv:2109.10033v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10033">
<div class="article-summary-box-inner">
<span><p>Moderation of reader comments is a significant problem for online news
platforms. Here, we experiment with models for automatic moderation, using a
dataset of comments from a popular Croatian newspaper. Our analysis shows that
while comments that violate the moderation rules mostly share common linguistic
and thematic features, their content varies across the different sections of
the newspaper. We therefore make our models topic-aware, incorporating semantic
features from a topic model into the classification decision. Our results show
that topic information improves the performance of the model, increases its
confidence in correct outputs, and helps us understand the model's outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Something Old, Something New: Grammar-based CCG Parsing with Transformer Models. (arXiv:2109.10044v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10044">
<div class="article-summary-box-inner">
<span><p>This report describes the parsing problem for Combinatory Categorial Grammar
(CCG), showing how a combination of Transformer-based neural models and a
symbolic CCG grammar can lead to substantial gains over existing approaches.
The report also documents a 20-year research program, showing how NLP methods
have evolved over this time. The staggering accuracy improvements provided by
neural models for CCG parsing can be seen as a reflection of the improvements
seen in NLP more generally. The report provides a minimal introduction to CCG
and CCG parsing, with many pointers to the relevant literature. It then
describes the CCG supertagging problem, and some recent work from Tian et al.
(2020) which applies Transformer-based models to supertagging with great
effect. I use this existing model to develop a CCG multitagger, which can serve
as a front-end to an existing CCG parser. Simply using this new multitagger
provides substantial gains in parsing accuracy. I then show how a
Transformer-based model from the parsing literature can be combined with the
grammar-based CCG parser, setting a new state-of-the-art for the CCGbank
parsing task of almost 93% F-score for labelled dependencies, with complete
sentence accuracies of over 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?. (arXiv:2109.10052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10052">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate what types of stereotypical information are
captured by pretrained language models. We present the first dataset comprising
stereotypical attributes of a range of social groups and propose a method to
elicit stereotypes encoded by pretrained language models in an unsupervised
fashion. Moreover, we link the emergent stereotypes to their manifestation as
basic emotions as a means to study their emotional effects in a more
generalized manner. To demonstrate how our methods can be used to analyze
emotion and stereotype shifts due to linguistic experience, we use fine-tuning
on news sources as a case study. Our experiments expose how attitudes towards
different social groups vary across models and how quickly emotions and
stereotypes can shift at the fine-tuning stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations. (arXiv:2109.10080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10080">
<div class="article-summary-box-inner">
<span><p>Adverse Drug Event (ADE) extraction mod-els can rapidly examine large
collections of so-cial media texts, detecting mentions of drug-related adverse
reactions and trigger medicalinvestigations. However, despite the recent
ad-vances in NLP, it is currently unknown if suchmodels are robust in face
ofnegation, which ispervasive across language varieties.In this paper we
evaluate three state-of-the-artsystems, showing their fragility against
nega-tion, and then we introduce two possible strate-gies to increase the
robustness of these mod-els: a pipeline approach, relying on a
specificcomponent for negation detection; an augmen-tation of an ADE extraction
dataset to artifi-cially create negated samples and further trainthe models.We
show that both strategies bring significantincreases in performance, lowering
the num-ber of spurious entities predicted by the mod-els. Our dataset and code
will be publicly re-leased to encourage research on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval. (arXiv:2109.10086v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10086">
<div class="article-summary-box-inner">
<span><p>In neural Information Retrieval (IR), ongoing research is directed towards
improving the first retriever in ranking pipelines. Learning dense embeddings
to conduct retrieval using efficient approximate nearest neighbors methods has
proven to work well. Meanwhile, there has been a growing interest in learning
\emph{sparse} representations for documents and queries, that could inherit
from the desirable properties of bag-of-words models such as the exact matching
of terms and the efficiency of inverted indexes. Introduced recently, the
SPLADE model provides highly sparse representations and competitive results
with respect to state-of-the-art dense and sparse approaches. In this paper, we
build on SPLADE and propose several significant improvements in terms of
effectiveness and/or efficiency. More specifically, we modify the pooling
mechanism, benchmark a model solely based on document expansion, and introduce
models trained with distillation. We also report results on the BEIR benchmark.
Overall, SPLADE is considerably improved with more than $9$\% gains on NDCG@10
on TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InvBERT: Text Reconstruction from Contextualized Embeddings used for Derived Text Formats of Literary Works. (arXiv:2109.10104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10104">
<div class="article-summary-box-inner">
<span><p>Digital Humanities and Computational Literary Studies apply text mining
methods to investigate literature. Such automated approaches enable
quantitative studies on large corpora which would not be feasible by manual
inspection alone. However, due to copyright restrictions, the availability of
relevant digitized literary works is limited. Derived Text Formats (DTFs) have
been proposed as a solution. Here, textual materials are transformed in such a
way that copyright-critical features are removed, but that the use of certain
analytical methods remains possible. Contextualized word embeddings produced by
transformer-encoders (like BERT) are promising candidates for DTFs because they
allow for state-of-the-art performance on various analytical tasks and, at
first sight, do not disclose the original text. However, in this paper we
demonstrate that under certain conditions the reconstruction of the original
copyrighted text becomes feasible and its publication in the form of
contextualized word representations is not safe. Our attempts to invert BERT
suggest, that publishing parts of the encoder together with the contextualized
embeddings is critical, since it allows to generate data to train a decoder
with a reconstruction accuracy sufficient to violate copyright laws.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Difficulty of Segmenting Words with Attention. (arXiv:2109.10107v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10107">
<div class="article-summary-box-inner">
<span><p>Word segmentation, the problem of finding word boundaries in speech, is of
interest for a range of tasks. Previous papers have suggested that for
sequence-to-sequence models trained on tasks such as speech translation or
speech recognition, attention can be used to locate and segment the words. We
show, however, that even on monolingual data this approach is brittle. In our
experiments with different input types, data sizes, and segmentation
algorithms, only models trained to predict phones from words succeed in the
task. Models trained to predict words from either phones or speech (i.e., the
opposite direction needed to generalize to new data), yield much worse results,
suggesting that attention-based segmentation is only useful in limited
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review on Summarizing Financial News Using Deep Learning. (arXiv:2109.10118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10118">
<div class="article-summary-box-inner">
<span><p>Investors make investment decisions depending on several factors such as
fundamental analysis, technical analysis, and quantitative analysis. Another
factor on which investors can make investment decisions is through sentiment
analysis of news headlines, the sole purpose of this study. Natural Language
Processing techniques are typically used to deal with such a large amount of
data and get valuable information out of it. NLP algorithms convert raw text
into numerical representations that machines can easily understand and
interpret. This conversion can be done using various embedding techniques. In
this research, embedding techniques used are BoW, TF-IDF, Word2Vec, BERT,
GloVe, and FastText, and then fed to deep learning models such as RNN and LSTM.
This work aims to evaluate these model's performance to choose the robust model
in identifying the significant factors influencing the prediction. During this
research, it was expected that Deep Leaming would be applied to get the desired
results or achieve better accuracy than the state-of-the-art. The models are
compared to check their outputs to know which one has performed better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvFiT: Conversational Fine-Tuning of Pretrained Language Models. (arXiv:2109.10126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10126">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (LMs) pretrained on large text collections
are proven to store a wealth of semantic knowledge. However, 1) they are not
effective as sentence encoders when used off-the-shelf, and 2) thus typically
lag behind conversationally pretrained (e.g., via response selection) encoders
on conversational tasks such as intent detection (ID). In this work, we propose
ConvFiT, a simple and efficient two-stage procedure which turns any pretrained
LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and
task-specialised sentence encoder (after Stage 2). We demonstrate that 1)
full-blown conversational pretraining is not required, and that LMs can be
quickly transformed into effective conversational encoders with much smaller
amounts of unannotated data; 2) pretrained LMs can be fine-tuned into
task-specialised sentence encoders, optimised for the fine-grained semantics of
a particular task. Consequently, such specialised sentence encoders allow for
treating ID as a simple semantic similarity task based on interpretable nearest
neighbours retrieval. We validate the robustness and versatility of the ConvFiT
framework with such similarity-based inference on the standard ID evaluation
sets: ConvFiT-ed LMs achieve state-of-the-art ID performance across the board,
with particular gains in the most challenging, few-shot setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement. (arXiv:2109.10133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10133">
<div class="article-summary-box-inner">
<span><p>Many recent works have demonstrated that unsupervised sentence
representations of neural networks encode syntactic information by observing
that neural language models are able to predict the agreement between a verb
and its subject. We take a critical look at this line of research by showing
that it is possible to achieve high accuracy on this agreement task with simple
surface heuristics, indicating a possible flaw in our assessment of neural
networks' syntactic ability. Our fine-grained analyses of results on the
long-range French object-verb agreement show that contrary to LSTMs,
Transformers are able to capture a non-trivial amount of grammatical structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation with Noisy Labels for Natural Language Understanding. (arXiv:2109.10147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10147">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is extensively used to compress and deploy large
pre-trained language models on edge devices for real-world applications.
However, one neglected area of research is the impact of noisy (corrupted)
labels on KD. We present, to the best of our knowledge, the first study on KD
with noisy labels in Natural Language Understanding (NLU). We document the
scope of the problem and present two methods to mitigate the impact of label
noise. Experiments on the GLUE benchmark show that our methods are effective
even under high noise levels. Nevertheless, our results indicate that more
research is necessary to cope with label noise under the KD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation. (arXiv:2109.10164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10164">
<div class="article-summary-box-inner">
<span><p>Intermediate layer knowledge distillation (KD) can improve the standard KD
technique (which only targets the output of teacher and student models)
especially over large pre-trained language models. However, intermediate layer
distillation suffers from excessive computational burdens and engineering
efforts required for setting up a proper layer mapping. To address these
problems, we propose a RAndom Intermediate Layer Knowledge Distillation
(RAIL-KD) approach in which, intermediate layers from the teacher model are
selected randomly to be distilled into the intermediate layers of the student
model. This randomized selection enforce that: all teacher layers are taken
into account in the training process, while reducing the computational cost of
intermediate layer distillation. Also, we show that it act as a regularizer for
improving the generalizability of the student model. We perform extensive
experiments on GLUE tasks as well as on out-of-domain test sets. We show that
our proposed RAIL-KD approach outperforms other state-of-the-art intermediate
layer KD methods considerably in both performance and training-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Familiar Does That Sound? Cross-Lingual Representational Similarity Analysis of Acoustic Word Embeddings. (arXiv:2109.10179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10179">
<div class="article-summary-box-inner">
<span><p>How do neural networks "perceive" speech sounds from unknown languages? Does
the typological similarity between the model's training language (L1) and an
unknown language (L2) have an impact on the model representations of L2 speech
signals? To answer these questions, we present a novel experimental design
based on representational similarity analysis (RSA) to analyze acoustic word
embeddings (AWEs) -- vector representations of variable-duration spoken-word
segments. First, we train monolingual AWE models on seven Indo-European
languages with various degrees of typological similarity. We then employ RSA to
quantify the cross-lingual similarity by simulating native and non-native
spoken-word processing using AWEs. Our experiments show that typological
similarity indeed affects the representational similarity of the models in our
study. We further discuss the implications of our work on modeling speech
processing and language similarity with neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TranslateLocally: Blazing-fast translation running on the local CPU. (arXiv:2109.10194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10194">
<div class="article-summary-box-inner">
<span><p>Every day, millions of people sacrifice their privacy and browsing habits in
exchange for online machine translation. Companies and governments with
confidentiality requirements often ban online translation or pay a premium to
disable logging. To bring control back to the end user and demonstrate speed,
we developed translateLocally. Running locally on a desktop or laptop CPU,
translateLocally delivers cloud-like translation speed and quality even on 10
year old hardware. The open-source software is based on Marian and runs on
Linux, Windows, and macOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Source, Two Targets: Challenges and Rewards of Dual Decoding. (arXiv:2109.10197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10197">
<div class="article-summary-box-inner">
<span><p>Machine translation is generally understood as generating one target text
from an input source document. In this paper, we consider a stronger
requirement: to jointly generate two texts so that each output side effectively
depends on the other. As we discuss, such a device serves several practical
purposes, from multi-target machine translation to the generation of controlled
variations of the target text. We present an analysis of possible
implementations of dual decoding, and experiment with four applications.
Viewing the problem from multiple angles allows us to better highlight the
challenges of dual decoding and to also thoroughly analyze the benefits of
generating matched, rather than independent, translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blindness to Modality Helps Entailment Graph Mining. (arXiv:2109.10227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10227">
<div class="article-summary-box-inner">
<span><p>Understanding linguistic modality is widely seen as important for downstream
tasks such as Question Answering and Knowledge Graph Population. Entailment
Graph learning might also be expected to benefit from attention to modality. We
build Entailment Graphs using a news corpus filtered with a modality parser,
and show that stripping modal modifiers from predicates in fact increases
performance. This suggests that for some tasks, the pragmatics of modal
modification of predicates allows them to contribute as evidence of entailment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets. (arXiv:2109.10234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10234">
<div class="article-summary-box-inner">
<span><p>We introduce BERTweetFR, the first large-scale pre-trained language model for
French tweets. Our model is initialized using the general-domain French
language model CamemBERT which follows the base architecture of RoBERTa.
Experiments show that BERTweetFR outperforms all previous general-domain French
language models on two downstream Twitter NLP tasks of offensiveness
identification and named entity recognition. The dataset used in the
offensiveness detection task is first created and annotated by our team,
filling in the gap of such analytic datasets in French. We make our model
publicly available in the transformers library with the aim of promoting future
research in analytic tasks for French tweets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Vision-and-Language Pretraining Improve Lexical Grounding?. (arXiv:2109.10246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10246">
<div class="article-summary-box-inner">
<span><p>Linguistic representations derived from text alone have been criticized for
their lack of grounding, i.e., connecting words to their meanings in the
physical world. Vision-and-Language (VL) models, trained jointly on text and
image or video data, have been offered as a response to such criticisms.
However, while VL pretraining has shown success on multimodal tasks such as
visual question answering, it is not yet known how the internal linguistic
representations themselves compare to their text-only counterparts. This paper
compares the semantic representations learned via VL vs. text-only pretraining
for two recent VL models using a suite of analyses (clustering, probing, and
performance on a commonsense question answering task) in a language-only
setting. We find that the multimodal models fail to significantly outperform
the text-only variants, suggesting that future work is required if multimodal
pretraining is to be pursued as a means of improving NLP in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audiomer: A Convolutional Transformer for Keyword Spotting. (arXiv:2109.10252v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10252">
<div class="article-summary-box-inner">
<span><p>Transformers have seen an unprecedented rise in Natural Language Processing
and Computer Vision tasks. However, in audio tasks, they are either infeasible
to train due to extremely large sequence length of audio waveforms or reach
competitive performance after feature extraction through Fourier-based methods,
incurring a loss-floor. In this work, we introduce an architecture, Audiomer,
where we combine 1D Residual Networks with Performer Attention to achieve
state-of-the-art performance in Keyword Spotting with raw audio waveforms,
out-performing all previous methods while also being computationally cheaper,
much more parameter and data-efficient. Audiomer allows for deployment in
compute-constrained devices and training on smaller datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10255">
<div class="article-summary-box-inner">
<span><p>The recognition of hate speech and offensive language (HOF) is commonly
formulated as a classification task to decide if a text contains HOF. We
investigate whether HOF detection can profit by taking into account the
relationships between HOF and similar concepts: (a) HOF is related to sentiment
analysis because hate speech is typically a negative statement and expresses a
negative opinion; (b) it is related to emotion analysis, as expressed hate
points to the author experiencing (or pretending to experience) anger while the
addressees experience (or are intended to experience) fear. (c) Finally, one
constituting element of HOF is the mention of a targeted person or group. On
this basis, we hypothesize that HOF detection shows improvements when being
modeled jointly with these concepts, in a multi-task learning setup. We base
our experiments on existing data sets for each of these concepts (sentiment,
emotion, target of HOF) and evaluate our models as a participant (as team
IMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection
experiments in which we consider multiple available resources and submissions
to the shared task, we find that the combination of the CrowdFlower emotion
corpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target
detection data leads to an F1 =.79 in a multi-head multi-task learning model
based on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test
data, this result is more substantial with an increase by 2pp in F1 and a
considerable increase in recall. Across both data sets (2019, 2021), the recall
is particularly increased for the class of HOF (6pp for the 2019 data and 3pp
for the 2021 data), showing that MTL with emotion, sentiment, and target
identification is an appropriate approach for early warning systems that might
be deployed in social media platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Trade-offs of Domain Adaptation for Neural Language Models. (arXiv:2109.10274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10274">
<div class="article-summary-box-inner">
<span><p>In this paper, we connect language model adaptation with concepts of machine
learning theory. We consider a training setup with a large out-of-domain set
and a small in-domain set. As a first contribution, we derive how the benefit
of training a model on either set depends on the size of the sets and the
distance between their underlying distribution. As a second contribution, we
present how the most popular data selection techniques -- importance sampling,
intelligent data selection and influence functions -- can be presented in a
common framework which highlights their similarity and also their subtle
differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10282">
<div class="article-summary-box-inner">
<span><p>Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From English to Signal Temporal Logic. (arXiv:2109.10294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10294">
<div class="article-summary-box-inner">
<span><p>Formal methods provide very powerful tools and techniques for the design and
analysis of complex systems. Their practical application remains however
limited, due to the widely accepted belief that formal methods require
extensive expertise and a steep learning curve. Writing correct formal
specifications in form of logical formulas is still considered to be a
difficult and error prone task.
</p>
<p>In this paper we propose DeepSTL, a tool and technique for the translation of
informal requirements, given as free English sentences, into Signal Temporal
Logic (STL), a formal specification language for cyber-physical systems, used
both by academia and advanced research labs in industry. A major challenge to
devise such a translator is the lack of publicly available informal
requirements and formal specifications. We propose a two-step workflow to
address this challenge. We first design a grammar-based generation technique of
synthetic data, where each output is a random STL formula and its associated
set of possible English translations. In the second step, we use a
state-of-the-art transformer-based neural translation technique, to train an
accurate attentional translator of English to STL. The experimental results
show high translation quality for patterns of English requirements that have
been well trained, making this workflow promising to be extended for processing
more complex translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents. (arXiv:2109.10341v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10341">
<div class="article-summary-box-inner">
<span><p>Document-level neural machine translation (DocNMT) delivers coherent
translations by incorporating cross-sentence context. However, for most
language pairs there's a shortage of parallel documents, although parallel
sentences are readily available. In this paper, we study whether and how
contextual modeling in DocNMT is transferable from sentences to documents in a
zero-shot fashion (i.e. no parallel documents for student languages) through
multilingual modeling. Using simple concatenation-based DocNMT, we explore the
effect of 3 factors on multilingual transfer: the number of document-supervised
teacher languages, the data schedule for parallel documents at training, and
the data condition of parallel documents (genuine vs. backtranslated). Our
experiments on Europarl-7 and IWSLT-10 datasets show the feasibility of
multilingual transfer for DocNMT, particularly on document-specific metrics. We
observe that more teacher languages and adequate data schedule both contribute
to better transfer quality. Surprisingly, the transfer is less sensitive to the
data condition and multilingual DocNMT achieves comparable performance with
both back-translated and genuine document pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-Guided Pre-Training for Open-Domain Question Answering. (arXiv:2109.10346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10346">
<div class="article-summary-box-inner">
<span><p>Answering complex open-domain questions requires understanding the latent
relations between involving entities. However, we found that the existing QA
datasets are extremely imbalanced in some types of relations, which hurts the
generalization performance over questions with long-tail relations. To remedy
this problem, in this paper, we propose a Relation-Guided Pre-Training
(RGPT-QA) framework. We first generate a relational QA dataset covering a wide
range of relations from both the Wikidata triplets and Wikipedia hyperlinks. We
then pre-train a QA model to infer the latent relations from the question, and
then conduct extractive QA to get the target answer entity. We demonstrate that
by pretraining with propoed RGPT-QA techique, the popular open-domain QA model,
Dense Passage Retriever (DPR), achieves 2.2%, 2.4%, and 6.3% absolute
improvement in Exact Match accuracy on Natural Questions, TriviaQA, and
WebQuestions. Particularly, we show that RGPT-QA improves significantly on
questions with long-tail relations
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Informed Sampling for Diversity in Concept-to-Text NLG. (arXiv:2004.14364v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14364">
<div class="article-summary-box-inner">
<span><p>Deep-learning models for language generation tasks tend to produce repetitive
output. Various methods have been proposed to encourage lexical diversity
during decoding, but this often comes at a cost to the perceived fluency and
adequacy of the output. In this work, we propose to ameliorate this cost by
using an Imitation Learning approach to explore the level of diversity that a
language generation model can reliably produce. Specifically, we augment the
decoding process with a meta-classifier trained to distinguish which words at
any given timestep will lead to high-quality output. We focus our experiments
on concept-to-text generation where models are sensitive to the inclusion of
irrelevant words due to the strict relation between input and output. Our
analysis shows that previous methods for diversity underperform in this
setting, while human evaluation suggests that our proposed method achieves a
high level of diversity with minimal effect to the output's fluency and
adequacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models. (arXiv:2009.13267v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13267">
<div class="article-summary-box-inner">
<span><p>The discrepancy between maximum likelihood estimation (MLE) and task measures
such as BLEU score has been studied before for autoregressive neural machine
translation (NMT) and resulted in alternative training algorithms (Ranzato et
al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However,
MLE training remains the de facto approach for autoregressive NMT because of
its computational efficiency and stability. Despite this mismatch between the
training objective and task measure, we notice that the samples drawn from an
MLE-based trained NMT support the desired distribution -- there are samples
with much higher BLEU score comparing to the beam decoding output. To benefit
from this observation, we train an energy-based model to mimic the behavior of
the task measure (i.e., the energy-based model assigns lower energy to samples
with higher BLEU score), which is resulted in a re-ranking algorithm based on
the samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal
energy models (over target sentence) and joint energy models (over both source
and target sentences). Our EBR with the joint energy model consistently
improves the performance of the Transformer-based NMT: +4 BLEU points on
IWSLT'14 German-English, +3.0 BELU points on Sinhala-English, +1.2 BLEU on
WMT'16 English-German tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not all parameters are born equal: Attention is mostly what you need. (arXiv:2010.11859v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11859">
<div class="article-summary-box-inner">
<span><p>Transformers are widely used in state-of-the-art machine translation, but the
key to their success is still unknown. To gain insight into this, we consider
three groups of parameters: embeddings, attention, and feed forward neural
network (FFN) layers. We examine the relative importance of each by performing
an ablation study where we initialise them at random and freeze them, so that
their weights do not change over the course of the training. Through this, we
show that the attention and FFN are equally important and fulfil the same
functionality in a model. We show that the decision about whether a component
is frozen or allowed to train is at least as important for the final model
performance as its number of parameters. At the same time, the number of
parameters alone is not indicative of a component's importance. Finally, while
the embedding layer is the least essential for machine translation tasks, it is
the most important component for language modelling tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Highs and Lows of Simple Lexical Domain Adaptation Approaches for Neural Machine Translation. (arXiv:2101.00421v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00421">
<div class="article-summary-box-inner">
<span><p>Machine translation systems are vulnerable to domain mismatch, especially in
a low-resource scenario. Out-of-domain translations are often of poor quality
and prone to hallucinations, due to exposure bias and the decoder acting as a
language model. We adopt two approaches to alleviate this problem: lexical
shortlisting restricted by IBM statistical alignments, and hypothesis
re-ranking based on similarity. The methods are computationally cheap, widely
known, but not extensively experimented on domain adaptation. We demonstrate
success on low-resource out-of-domain test sets, however, the methods are
ineffective when there is sufficient data or too great domain mismatch. This is
due to both the IBM model losing its advantage over the implicitly learned
neural alignment, and issues with subword segmentation of out-of-domain words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Retrieval Conversational Machine Reading. (arXiv:2102.08633v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08633">
<div class="article-summary-box-inner">
<span><p>In conversational machine reading, systems need to interpret natural language
rules, answer high-level questions such as "May I qualify for VA health care
benefits?", and ask follow-up clarification questions whose answer is necessary
to answer the original question. However, existing works assume the rule text
is provided for each user question, which neglects the essential retrieval step
in real scenarios. In this work, we propose and investigate an open-retrieval
setting of conversational machine reading. In the open-retrieval setting, the
relevant rule texts are unknown so that a system needs to retrieve
question-relevant evidence from a collection of rule texts, and answer users'
high-level questions according to multiple retrieved rule texts in a
conversational manner. We propose MUDERN, a Multi-passage Discourse-aware
Entailment Reasoning Network which extracts conditions in the rule texts
through discourse segmentation, conducts multi-passage entailment reasoning to
answer user questions directly, or asks clarification follow-up questions to
inquiry more information. On our created OR-ShARC dataset, MUDERN achieves the
state-of-the-art performance, outperforming existing single-passage
conversational machine reading models as well as a new multi-passage
conversational machine reading baseline by a large margin. In addition, we
conduct in-depth analyses to provide new insights into this new setting and our
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-autoregressive Mandarin-English Code-switching Speech Recognition. (arXiv:2104.02258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02258">
<div class="article-summary-box-inner">
<span><p>Mandarin-English code-switching (CS) is frequently used among East and
Southeast Asian people. However, the intra-sentence language switching of the
two very different languages makes recognizing CS speech challenging.
Meanwhile, the recent successful non-autoregressive (NAR) ASR models remove the
need for left-to-right beam decoding in autoregressive (AR) models and achieved
outstanding performance and fast inference speed, but it has not been applied
to Mandarin-English CS speech recognition. This paper takes advantage of the
Mask-CTC NAR ASR framework to tackle the CS speech recognition issue. We
further propose to change the Mandarin output target of the encoder to Pinyin
for faster encoder training and introduce the Pinyin-to-Mandarin decoder to
learn contextualized information. Moreover, we use word embedding label
smoothing to regularize the decoder with contextualized information and
projection matrix regularization to bridge that gap between the encoder and
decoder. We evaluate these methods on the SEAME corpus and achieved exciting
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders. (arXiv:2104.03630v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03630">
<div class="article-summary-box-inner">
<span><p>Powerful sentence encoders trained for multiple languages are on the rise.
These systems are capable of embedding a wide range of linguistic properties
into vector representations. While explicit probing tasks can be used to verify
the presence of specific linguistic properties, it is unclear whether the
vector representations can be manipulated to indirectly steer such properties.
For efficient learning, we investigate the use of a geometric mapping in
embedding space to transform linguistic properties, without any tuning of the
pre-trained sentence encoder or decoder. We validate our approach on three
linguistic properties using a pre-trained multilingual autoencoder and analyze
the results in both monolingual and cross-lingual settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Condenser: a Pre-training Architecture for Dense Retrieval. (arXiv:2104.08253v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08253">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformer language models (LM) have become go-to text
representation encoders. Prior research fine-tunes deep LMs to encode text
sequences such as sentences and passages into single dense vector
representations for efficient text comparison and retrieval. However, dense
encoders require a lot of data and sophisticated techniques to effectively
train and suffer in low data situations. This paper finds a key reason is that
standard LMs' internal attention structure is not ready-to-use for dense
encoders, which needs to aggregate text information into the dense
representation. We propose to pre-train towards dense encoder with a novel
Transformer architecture, Condenser, where LM prediction CONditions on DENSE
Representation. Our experiments show Condenser improves over standard LM by
large margins on various text retrieval and similarity tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12227">
<div class="article-summary-box-inner">
<span><p>Classic information extraction techniques consist in building questions and
answers about the facts. Indeed, it is still a challenge to subjective
information extraction systems to identify opinions and feelings in context. In
sentiment-based NLP tasks, there are few resources to information extraction,
above all offensive or hateful opinions in context. To fill this important gap,
this short paper provides a new cross-lingual and contextual offensive lexicon,
which consists of explicit and implicit offensive and swearing expressions of
opinion, which were annotated in two different classes: context dependent and
context-independent offensive. In addition, we provide markers to identify hate
speech. Annotation approach was evaluated at the expression-level and achieves
high human inter-annotator agreement. The provided offensive lexicon is
available in Portuguese and English languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Lexicon-Based Approach for Hate Speech and Offensive Language Detection. (arXiv:2104.12265v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12265">
<div class="article-summary-box-inner">
<span><p>This paper provides a new approach for offensive language and hate speech
detection on social media. Our approach incorporates an offensive lexicon
composed of implicit and explicit offensive and swearing expressions annotated
with binary classes: context-dependent and context-independent offensive. Due
to the severity of the hate speech and offensive comments in Brazil, and the
lack of research in Portuguese, Brazilian Portuguese is the language used to
validate the proposed method. Nevertheless, our proposal may be applied to any
other language or domain. Based on the obtained results, the proposed approach
showed high-performance overcoming the current baselines for European and
Brazilian Portuguese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiaKG: an Annotated Diabetes Dataset for Medical Knowledge Graph Construction. (arXiv:2105.15033v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15033">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph has been proven effective in modeling structured information
and conceptual knowledge, especially in the medical domain. However, the lack
of high-quality annotated corpora remains a crucial problem for advancing the
research and applications on this task. In order to accelerate the research for
domain-specific knowledge graphs in the medical domain, we introduce DiaKG, a
high-quality Chinese dataset for Diabetes knowledge graph, which contains
22,050 entities and 6,890 relations in total. We implement recent typical
methods for Named Entity Recognition and Relation Extraction as a benchmark to
evaluate the proposed dataset thoroughly. Empirical results show that the DiaKG
is challenging for most existing methods and further analysis is conducted to
discuss future research direction for improvements. We hope the release of this
dataset can assist the construction of diabetes knowledge graphs and facilitate
AI-based applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilateral Personalized Dialogue Generation with Contrastive Learning. (arXiv:2106.07857v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07857">
<div class="article-summary-box-inner">
<span><p>Generating personalized responses is one of the major challenges in natural
human-robot interaction. Current researches in this field mainly focus on
generating responses consistent with the robot's pre-assigned persona, while
ignoring the user's persona. Such responses may be inappropriate or even
offensive, which may lead to the bad user experience. Therefore, we propose a
Bilateral Personalized Dialogue Generation (BPDG) method for dyadic
conversation, which integrates user and robot personas into dialogue generation
via designing a dynamic persona-aware fusion method. To bridge the gap between
the learning objective function and evaluation metrics, the Conditional Mutual
Information Maximum (CMIM) criterion is adopted with contrastive learning to
select the proper response from the generated candidates. Moreover, a bilateral
persona accuracy metric is designed to measure the degree of bilateral
personalization. Experimental results demonstrate that, compared with several
state-of-the-art methods, the final results of the proposed method are more
personalized and consistent with bilateral personas in terms of both automatic
and manual evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. (arXiv:2106.09700v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09700">
<div class="article-summary-box-inner">
<span><p>Biomedical knowledge graphs (KGs) hold rich information on entities such as
diseases, drugs, and genes. Predicting missing links in these graphs can boost
many important applications, such as drug design and repurposing. Recent work
has shown that general-domain language models (LMs) can serve as "soft" KGs,
and that they can be fine-tuned for the task of KG completion. In this work, we
study scientific LMs for KG completion, exploring whether we can tap into their
latent knowledge to enhance biomedical link prediction. We evaluate several
domain-specific LMs, fine-tuning them on datasets centered on drugs and
diseases that we represent as KGs and enrich with textual entity descriptions.
We integrate the LM-based models with KG embedding models, using a router
method that learns to assign each input example to either type of model and
provides a substantial boost in performance. Finally, we demonstrate the
advantage of LM models in the inductive setting with novel scientific entities.
Our datasets and code are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You should evaluate your language model on marginal likelihood over tokenisations. (arXiv:2109.02550v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02550">
<div class="article-summary-box-inner">
<span><p>Neural language models typically tokenise input text into sub-word units to
achieve an open vocabulary. The standard approach is to use a single canonical
tokenisation at both train and test time. We suggest that this approach is
unsatisfactory and may bottleneck our evaluation of language model performance.
Using only the one-best tokenisation ignores tokeniser uncertainty over
alternative tokenisations, which may hurt model out-of-domain performance.
</p>
<p>In this paper, we argue that instead, language models should be evaluated on
their marginal likelihood over tokenisations. We compare different estimators
for the marginal likelihood based on sampling, and show that it is feasible to
estimate the marginal likelihood with a manageable number of samples. We then
evaluate pretrained English and German language models on both the
one-best-tokenisation and marginal perplexities, and show that the marginal
perplexity can be significantly better than the one best, especially on
out-of-domain data. We link this difference in perplexity to the tokeniser
uncertainty as measured by tokeniser entropy. We discuss some implications of
our results for language model training and evaluation, particularly with
regard to tokenisation robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes. (arXiv:2109.08828v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08828">
<div class="article-summary-box-inner">
<span><p>Empathy is a complex cognitive ability based on the reasoning of others'
affective states. In order to better understand others and express stronger
empathy in dialogues, we argue that two issues must be tackled at the same
time: (i) identifying which word is the cause for the other's emotion from his
or her utterance and (ii) reflecting those specific words in the response
generation. However, previous approaches for recognizing emotion cause words in
text require sub-utterance level annotations, which can be demanding. Taking
inspiration from social cognition, we leverage a generative estimator to infer
emotion cause words from utterances with no word-level label. Also, we
introduce a novel method based on pragmatics to make dialogue models focus on
targeted words in the input during generation. Our method is applicable to any
dialogue models with no additional training on the fly. We show our approach
improves multiple best-performing dialogue agents on generating more focused
empathetic responses in terms of both automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study. (arXiv:2109.09105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09105">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) have been ubiquitously leveraged in various tasks
including spoken language understanding (SLU). Spoken language requires careful
understanding of speaker interactions, dialog states and speech induced
multimodal behaviors to generate a meaningful representation of the
conversation. In this work, we propose to dissect SLU into three representative
properties:conversational (disfluency, pause, overtalk), channel (speaker-type,
turn-tasks) and ASR (insertion, deletion,substitution). We probe BERT based
language models (BERT, RoBERTa) trained on spoken transcripts to investigate
its ability to understand multifarious properties in absence of any speech
cues. Empirical results indicate that LM is surprisingly good at capturing
conversational properties such as pause prediction and overtalk detection from
lexical tokens. On the downsides, the LM scores low on turn-tasks and ASR
errors predictions. Additionally, pre-training the LM on spoken transcripts
restrain its linguistic understanding. Finally, we establish the efficacy and
transferability of the mentioned properties on two benchmark datasets:
Switchboard Dialog Act and Disfluency datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries. (arXiv:2109.09195v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09195">
<div class="article-summary-box-inner">
<span><p>Current pre-trained models applied to summarization are prone to factual
inconsistencies which either misrepresent the source text or introduce
extraneous information. Thus, comparing the factual consistency of summaries is
necessary as we develop improved models. However, the optimal human evaluation
setup for factual consistency has not been standardized. To address this issue,
we crowdsourced evaluations for factual consistency using the rating-based
Likert scale and ranking-based Best-Worst Scaling protocols, on 100 articles
from each of the CNN-Daily Mail and XSum datasets over four state-of-the-art
models, to determine the most reliable evaluation framework. We find that
ranking-based protocols offer a more reliable measure of summary quality across
datasets, while the reliability of Likert ratings depends on the target dataset
and the evaluation design. Our crowdsourcing templates and summary evaluations
will be publicly available to facilitate future research on factual consistency
in summarization.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaMedSeg: Volumetric Meta-learning for Few-Shot Organ Segmentation. (arXiv:2109.09734v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09734">
<div class="article-summary-box-inner">
<span><p>The lack of sufficient annotated image data is a common issue in medical
image segmentation. For some organs and densities, the annotation may be
scarce, leading to poor model training convergence, while other organs have
plenty of annotated data. In this work, we present MetaMedSeg, a gradient-based
meta-learning algorithm that redefines the meta-learning task for the
volumetric medical data with the goal to capture the variety between the
slices. We also explore different weighting schemes for gradients aggregation,
arguing that different tasks might have different complexity, and hence,
contribute differently to the initialization. We propose an importance-aware
weighting scheme to train our model. In the experiments, we present an
evaluation of the medical decathlon dataset by extracting 2D slices from CT and
MRI volumes of different organs and performing semantic segmentation. The
results show that our proposed volumetric task definition leads to up to 30%
improvement in terms of IoU compared to related baselines. The proposed update
rule is also shown to improve the performance for complex scenarios where the
data distribution of the target organ is very different from the source organs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-Free Domain Adaptive Fundus Image Segmentation with Denoised Pseudo-Labeling. (arXiv:2109.09735v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09735">
<div class="article-summary-box-inner">
<span><p>Domain adaptation typically requires to access source domain data to utilize
their distribution information for domain alignment with the target data.
However, in many real-world scenarios, the source data may not be accessible
during the model adaptation in the target domain due to privacy issue. This
paper studies the practical yet challenging source-free unsupervised domain
adaptation problem, in which only an existing source model and the unlabeled
target data are available for model adaptation. We present a novel denoised
pseudo-labeling method for this problem, which effectively makes use of the
source model and unlabeled target data to promote model self-adaptation from
pseudo labels. Importantly, considering that the pseudo labels generated from
source model are inevitably noisy due to domain shift, we further introduce two
complementary pixel-level and class-level denoising schemes with uncertainty
estimation and prototype estimation to reduce noisy pseudo labels and select
reliable ones to enhance the pseudo-labeling efficacy. Experimental results on
cross-domain fundus image segmentation show that without using any source
images or altering source training, our approach achieves comparable or even
higher performance than state-of-the-art source-dependent unsupervised domain
adaptation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation with Semantic Consistency across Heterogeneous Modalities for MRI Prostate Lesion Segmentation. (arXiv:2109.09736v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09736">
<div class="article-summary-box-inner">
<span><p>Any novel medical imaging modality that differs from previous protocols e.g.
in the number of imaging channels, introduces a new domain that is
heterogeneous from previous ones. This common medical imaging scenario is
rarely considered in the domain adaptation literature, which handles shifts
across domains of the same dimensionality. In our work we rely on stochastic
generative modeling to translate across two heterogeneous domains at pixel
space and introduce two new loss functions that promote semantic consistency.
Firstly, we introduce a semantic cycle-consistency loss in the source domain to
ensure that the translation preserves the semantics. Secondly, we introduce a
pseudo-labelling loss, where we translate target data to source, label them by
a source-domain network, and use the generated pseudo-labels to supervise the
target-domain network. Our results show that this allows us to extract
systematically better representations for the target domain. In particular, we
address the challenge of enhancing performance on VERDICT-MRI, an advanced
diffusion-weighted imaging technique, by exploiting labeled mp-MRI data. When
compared to several unsupervised domain adaptation approaches, our approach
yields substantial improvements, that consistently carry over to the
semi-supervised and supervised learning settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Optimal Control Framework for Joint-channel Parallel MRI Reconstruction without Coil Sensitivities. (arXiv:2109.09738v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09738">
<div class="article-summary-box-inner">
<span><p>Goal: This work aims at developing a novel calibration-free fast parallel MRI
(pMRI) reconstruction method incorporate with discrete-time optimal control
framework. The reconstruction model is designed to learn a regularization that
combines channels and extracts features by leveraging the information sharing
among channels of multi-coil images. We propose to recover both magnitude and
phase information by taking advantage of structured multiplayer convolutional
networks in image and Fourier spaces. Methods: We develop a novel variational
model with a learnable objective function that integrates an adaptive
multi-coil image combination operator and effective image regularization in the
image and Fourier spaces. We cast the reconstruction network as a structured
discrete-time optimal control system, resulting in an optimal control
formulation of parameter training where the parameters of the objective
function play the role of control variables. We demonstrate that the Lagrangian
method for solving the control problem is equivalent to back-propagation,
ensuring the local convergence of the training algorithm. Results: We conduct a
large number of numerical experiments of the proposed method with comparisons
to several state-of-the-art pMRI reconstruction networks on real pMRI datasets.
The numerical results demonstrate the promising performance of the proposed
method evidently. Conclusion: The proposed method provides a general deep
network design and training framework for efficient joint-channel pMRI
reconstruction. Significance: By learning multi-coil image combination operator
and performing regularizations in both image domain and k-space domain, the
proposed method achieves a highly efficient image reconstruction network for
pMRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multifield Cosmology with Artificial Intelligence. (arXiv:2109.09747v1 [astro-ph.CO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09747">
<div class="article-summary-box-inner">
<span><p>Astrophysical processes such as feedback from supernovae and active galactic
nuclei modify the properties and spatial distribution of dark matter, gas, and
galaxies in a poorly understood way. This uncertainty is one of the main
theoretical obstacles to extract information from cosmological surveys. We use
2,000 state-of-the-art hydrodynamic simulations from the CAMELS project
spanning a wide variety of cosmological and astrophysical models and generate
hundreds of thousands of 2-dimensional maps for 13 different fields: from dark
matter to gas and stellar properties. We use these maps to train convolutional
neural networks to extract the maximum amount of cosmological information while
marginalizing over astrophysical effects at the field level. Although our maps
only cover a small area of $(25~h^{-1}{\rm Mpc})^2$, and the different fields
are contaminated by astrophysical effects in very different ways, our networks
can infer the values of $\Omega_{\rm m}$ and $\sigma_8$ with a few percent
level precision for most of the fields. We find that the marginalization
performed by the network retains a wealth of cosmological information compared
to a model trained on maps from gravity-only N-body simulations that are not
contaminated by astrophysical effects. Finally, we train our networks on
multifields -- 2D maps that contain several fields as different colors or
channels -- and find that not only they can infer the value of all parameters
with higher accuracy than networks trained on individual fields, but they can
constrain the value of $\Omega_{\rm m}$ with higher accuracy than the maps from
the N-body simulations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrated Construction of Multimodal Atlases with Structural Connectomes in the Space of Riemannian Metrics. (arXiv:2109.09808v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09808">
<div class="article-summary-box-inner">
<span><p>The structural network of the brain, or structural connectome, can be
represented by fiber bundles generated by a variety of tractography methods.
While such methods give qualitative insights into brain structure, there is
controversy over whether they can provide quantitative information, especially
at the population level. In order to enable population-level statistical
analysis of the structural connectome, we propose representing a connectome as
a Riemannian metric, which is a point on an infinite-dimensional manifold. We
equip this manifold with the Ebin metric, a natural metric structure for this
space, to get a Riemannian manifold along with its associated geometric
properties. We then use this Riemannian framework to apply object-oriented
statistical analysis to define an atlas as the Fr\'echet mean of a population
of Riemannian metrics. This formulation ties into the existing framework for
diffeomorphic construction of image atlases, allowing us to construct a
multimodal atlas by simultaneously integrating complementary white matter
structure details from DWMRI and cortical details from T1-weighted MRI. We
illustrate our framework with 2D data examples of connectome registration and
atlas formation. Finally, we build an example 3D multimodal atlas using T1
images and connectomes derived from diffusion tensors estimated from a subset
of subjects from the Human Connectome Project.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09818">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks have demonstrated dermatologist-level
performance in the classification of melanoma and other skin lesions, but
prediction irregularities due to biases seen within the training data are an
issue that should be addressed before widespread deployment is possible. In
this work, we robustly remove bias and spurious variation from an automated
melanoma classification pipeline using two leading bias unlearning techniques.
We show that the biases introduced by surgical markings and rulers presented in
previous studies can be reasonably mitigated using these bias removal methods.
We also demonstrate the generalisation benefits of unlearning spurious
variation relating to the imaging instrument used to capture lesion images.
Contributions of this work include the application of different debiasing
techniques for artefact bias removal and the concept of instrument bias
unlearning for domain generalisation in melanoma detection. Our experimental
results provide evidence that the effects of each of the aforementioned biases
are notably reduced, with different debiasing techniques excelling at different
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09824">
<div class="article-summary-box-inner">
<span><p>This paper investigates the effectiveness of systematically probing Google
Trendsagainst textual translations of visual aspects as exogenous knowledge to
predict the sales of brand-new fashion items, where past sales data is not
available, but only an image and few metadata are available. In particular, we
propose GTM-Transformer, standing for Google Trends Multimodal Transformer,
whose encoder works on the representation of the exogenous time series, while
the decoder forecasts the sales using the Google Trends encoding, and the
available visual and metadata information. Our model works in a
non-autoregressive manner, avoiding the compounding effect of the first-step
errors. As a second contribution, we present the VISUELLE dataset, which is the
first publicly available dataset for the task of new fashion product sales
forecasting, containing the sales of 5577 new products sold between 2016-2019,
derived from genuine historical data ofNunalie, an Italian fast-fashion
company. Our dataset is equipped with images of products, metadata, related
sales, and associated Google Trends. We use VISUELLE to compare our approach
against state-of-the-art alternatives and numerous baselines, showing that
GTM-Transformer is the most accurate in terms of both percentage and absolute
error. It is worth noting that the addition of exogenous knowledge boosts the
forecasting accuracy by 1.5% WAPE wise, showing the importance of exploiting
Google Trends. The code and dataset are both available at
https://github.com/HumaticsLAB/GTM-Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Viewpoint Invariant Dense Matching for Visual Geolocalization. (arXiv:2109.09827v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09827">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a novel method for image matching based on dense
local features and tailored for visual geolocalization. Dense local features
matching is robust against changes in illumination and occlusions, but not
against viewpoint shifts which are a fundamental aspect of geolocalization. Our
method, called GeoWarp, directly embeds invariance to viewpoint shifts in the
process of extracting dense features. This is achieved via a trainable module
which learns from the data an invariance that is meaningful for the task of
recognizing places. We also devise a new self-supervised loss and two new
weakly supervised losses to train this module using only unlabeled data and
weak labels. GeoWarp is implemented efficiently as a re-ranking method that can
be easily embedded into pre-existing visual geolocalization pipelines.
Experimental validation on standard geolocalization benchmarks demonstrates
that GeoWarp boosts the accuracy of state-of-the-art retrieval architectures.
The code and trained models are available at
https://github.com/gmberton/geo_warp
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced-MixUp for Highly Imbalanced Medical Image Classification. (arXiv:2109.09850v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09850">
<div class="article-summary-box-inner">
<span><p>Highly imbalanced datasets are ubiquitous in medical image classification
problems. In such problems, it is often the case that rare classes associated
to less prevalent diseases are severely under-represented in labeled databases,
typically resulting in poor performance of machine learning algorithms due to
overfitting in the learning process. In this paper, we propose a novel
mechanism for sampling training data based on the popular MixUp regularization
technique, which we refer to as Balanced-MixUp. In short, Balanced-MixUp
simultaneously performs regular (i.e., instance-based) and balanced (i.e.,
class-based) sampling of the training data. The resulting two sets of samples
are then mixed-up to create a more balanced training distribution from which a
neural network can effectively learn without incurring in heavily under-fitting
the minority classes. We experiment with a highly imbalanced dataset of retinal
images (55K samples, 5 classes) and a long-tail dataset of gastro-intestinal
video frames (10K images, 23 classes), using two CNNs of varying representation
capabilities. Experimental results demonstrate that applying Balanced-MixUp
outperforms other conventional sampling schemes and loss functions specifically
designed to deal with imbalanced data. Code is released at
https://github.com/agaldran/balanced_mixup .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Detection in Thermal Spectrum for Advanced Driver-Assistance Systems (ADAS). (arXiv:2109.09854v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09854">
<div class="article-summary-box-inner">
<span><p>Object detection in thermal infrared spectrum provides more reliable data
source in low-lighting conditions and different weather conditions, as it is
useful both in-cabin and outside for pedestrian, animal, and vehicular
detection as well as for detecting street-signs &amp; lighting poles. This paper is
about exploring and adapting state-of-the-art object detection and classifier
framework on thermal vision with seven distinct classes for advanced
driver-assistance systems (ADAS). The trained network variants on public
datasets are validated on test data with three different test approaches which
include test-time with no augmentation, test-time augmentation, and test-time
with model ensembling. Additionally, the efficacy of trained networks is tested
on locally gathered novel test-data captured with an uncooled LWIR prototype
thermal camera in challenging weather and environmental scenarios. The
performance analysis of trained models is investigated by computing precision,
recall, and mean average precision scores (mAP). Furthermore, the trained model
architecture is optimized using TensorRT inference accelerator and deployed on
resource-constrained edge hardware Nvidia Jetson Nano to explicitly reduce the
inference time on GPU as well as edge devices for further real-time onboard
installations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Depth Estimation with Geospatial Context. (arXiv:2109.09879v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09879">
<div class="article-summary-box-inner">
<span><p>Modern cameras are equipped with a wide array of sensors that enable
recording the geospatial context of an image. Taking advantage of this, we
explore depth estimation under the assumption that the camera is geocalibrated,
a problem we refer to as geo-enabled depth estimation. Our key insight is that
if capture location is known, the corresponding overhead viewpoint offers a
valuable resource for understanding the scale of the scene. We propose an
end-to-end architecture for depth estimation that uses geospatial context to
infer a synthetic ground-level depth map from a co-located overhead image, then
fuses it inside of an encoder/decoder style segmentation network. To support
evaluation of our methods, we extend a recently released dataset with overhead
imagery and corresponding height maps. Results demonstrate that integrating
geospatial context significantly reduces error compared to baselines, both at
close ranges and when evaluating at much larger distances than existing
benchmarks consider.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation. (arXiv:2109.09881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09881">
<div class="article-summary-box-inner">
<span><p>Surface normal estimation from a single image is an important task in 3D
scene understanding. In this paper, we address two limitations shared by the
existing methods: the inability to estimate the aleatoric uncertainty and lack
of detail in the prediction. The proposed network estimates the per-pixel
surface normal probability distribution. We introduce a new parameterization
for the distribution, such that its negative log-likelihood is the angular loss
with learned attenuation. The expected value of the angular error is then used
as a measure of the aleatoric uncertainty. We also present a novel decoder
framework where pixel-wise multi-layer perceptrons are trained on a subset of
pixels sampled based on the estimated uncertainty. The proposed
uncertainty-guided sampling prevents the bias in training towards large planar
surfaces and improves the quality of prediction, especially near object
boundaries and on small structures. Experimental results show that the proposed
method outperforms the state-of-the-art in ScanNet and NYUv2, and that the
estimated uncertainty correlates well with the prediction error. Code is
available at https://github.com/baegwangbin/surface_normal_uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Importance of Distractors for Few-Shot Classification. (arXiv:2109.09883v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09883">
<div class="article-summary-box-inner">
<span><p>Few-shot classification aims at classifying categories of a novel task by
learning from just a few (typically, 1 to 5) labelled examples. An effective
approach to few-shot classification involves a prior model trained on a
large-sample base domain, which is then finetuned over the novel few-shot task
to yield generalizable representations. However, task-specific finetuning is
prone to overfitting due to the lack of enough training examples. To alleviate
this issue, we propose a new finetuning approach based on contrastive learning
that reuses unlabelled examples from the base domain in the form of
distractors. Unlike the nature of unlabelled data used in prior works,
distractors belong to classes that do not overlap with the novel categories. We
demonstrate for the first time that inclusion of such distractors can
significantly boost few-shot generalization. Our technical novelty includes a
stochastic pairing of examples sharing the same category in the few-shot task
and a weighting term that controls the relative influence of task-specific
negatives and distractors. An important aspect of our finetuning objective is
that it is agnostic to distractor labels and hence applicable to various base
domain settings. Compared to state-of-the-art approaches, our method shows
accuracy gains of up to $12\%$ in cross-domain and up to $5\%$ in unsupervised
prior-learning settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirDOS: Dynamic SLAM benefits from Articulated Objects. (arXiv:2109.09903v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09903">
<div class="article-summary-box-inner">
<span><p>Dynamic Object-aware SLAM (DOS) exploits object-level information to enable
robust motion estimation in dynamic environments. It has attracted increasing
attention with the recent success of learning-based models. Existing methods
mainly focus on identifying and excluding dynamic objects from the
optimization. In this paper, we show that feature-based visual SLAM systems can
also benefit from the presence of dynamic articulated objects by taking
advantage of two observations: (1) The 3D structure of an articulated object
remains consistent over time; (2) The points on the same object follow the same
motion. In particular, we present AirDOS, a dynamic object-aware system that
introduces rigidity and motion constraints to model articulated objects. By
jointly optimizing the camera pose, object motion, and the object 3D structure,
we can rectify the camera pose estimation, preventing tracking loss, and
generate 4D spatio-temporal maps for both dynamic objects and static scenes.
Experiments show that our algorithm improves the robustness of visual SLAM
algorithms in challenging crowded urban environments. To the best of our
knowledge, AirDOS is the first dynamic object-aware SLAM system demonstrating
that camera pose estimation can be improved by incorporating dynamic
articulated objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-based Human Motion Estimation and Synthesis from Videos. (arXiv:2109.09913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09913">
<div class="article-summary-box-inner">
<span><p>Human motion synthesis is an important problem with applications in graphics,
gaming and simulation environments for robotics. Existing methods require
accurate motion capture data for training, which is costly to obtain. Instead,
we propose a framework for training generative models of physically plausible
human motion directly from monocular RGB videos, which are much more widely
available. At the core of our method is a novel optimization formulation that
corrects imperfect image-based pose estimations by enforcing physics
constraints and reasons about contacts in a differentiable way. This
optimization yields corrected 3D poses and motions, as well as their
corresponding contact forces. Results show that our physically-corrected
motions significantly outperform prior work on pose estimation. We can then use
these to train a generative model to synthesize future motion. We demonstrate
both qualitatively and quantitatively significantly improved motion estimation,
synthesis quality and physical plausibility achieved by our method on the large
scale Human3.6m dataset \cite{h36m_pami} as compared to prior kinematic and
physics-based methods. By enabling learning of motion synthesis from video, our
method paves the way for large-scale, realistic and diverse motion synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey: Transformer based Video-Language Pre-training. (arXiv:2109.09920v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09920">
<div class="article-summary-box-inner">
<span><p>Inspired by the success of transformer-based pre-training methods on natural
language tasks and further computer vision tasks, researchers have begun to
apply transformer to video processing. This survey aims to give a comprehensive
overview on transformer-based pre-training methods for Video-Language learning.
We first briefly introduce the transformer tructure as the background
knowledge, including attention mechanism, position encoding etc. We then
describe the typical paradigm of pre-training &amp; fine-tuning on Video-Language
processing in terms of proxy tasks, downstream tasks and commonly used video
datasets. Next, we categorize transformer models into Single-Stream and
Multi-Stream structures, highlight their innovations and compare their
performances. Finally, we analyze and discuss the current challenges and
possible future research directions for Video-Language pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoPhoto: Aesthetic Photo Capture using Reinforcement Learning. (arXiv:2109.09923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09923">
<div class="article-summary-box-inner">
<span><p>The process of capturing a well-composed photo is difficult and it takes
years of experience to master. We propose a novel pipeline for an autonomous
agent to automatically capture an aesthetic photograph by navigating within a
local region in a scene. Instead of classical optimization over heuristics such
as the rule-of-thirds, we adopt a data-driven aesthetics estimator to assess
photo quality. A reinforcement learning framework is used to optimize the model
with respect to the learned aesthetics metric. We train our model in simulation
with indoor scenes, and we demonstrate that our system can capture aesthetic
photos in both simulation and real world environments on a ground robot. To our
knowledge, this is the first system that can automatically explore an
environment to capture an aesthetic photo with respect to a learned aesthetic
estimator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Estimation of Reflection Symmetry in Noisy and Partial 3D Point Clouds. (arXiv:2109.09927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09927">
<div class="article-summary-box-inner">
<span><p>Detecting the reflection symmetry plane of an object represented by a 3D
point cloud is a fundamental problem in 3D computer vision and geometry
processing due to its various applications such as compression, object
detection, robotic grasping, 3D surface reconstruction, etc. There exist
several efficient approaches for solving this problem for clean 3D point
clouds. However, this problem becomes difficult to solve in the presence of
outliers and missing parts due to occlusions while scanning the objects through
3D scanners. The existing methods try to overcome these challenges mostly by
voting-based techniques but fail in challenging settings. In this work, we
propose a statistical estimator for the plane of reflection symmetry that is
robust to outliers and missing parts. We pose the problem of finding the
optimal estimator as an optimization problem on a 2-sphere that quickly
converges to the global solution. We further propose a 3D point descriptor that
is invariant to 3D reflection symmetry using the spectral properties of the
geodesic distance matrix constructed from the neighbors of a point. This helps
us in decoupling the chicken-and-egg problem of finding optimal symmetry plane
and correspondences between the reflective symmetric points. We show that the
proposed approach achieves the state-of-the-art performance on the benchmarks
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MESSFN : a Multi-level and Enhanced Spectral-Spatial Fusion Network for Pan-sharpening. (arXiv:2109.09937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09937">
<div class="article-summary-box-inner">
<span><p>Dominant pan-sharpening frameworks simply concatenate the MS stream and the
PAN stream once at a specific level. This way of fusion neglects the
multi-level spectral-spatial correlation between the two streams, which is
vital to improving the fusion performance. In consideration of this, we propose
a Multi-level and Enhanced Spectral-Spatial Fusion Network (MESSFN) with the
following innovations: First, to fully exploit and strengthen the above
correlation, a Hierarchical Multi-level Fusion Architecture (HMFA) is carefully
designed. A novel Spectral-Spatial (SS) stream is established to hierarchically
derive and fuse the multi-level prior spectral and spatial expertise from the
MS stream and the PAN stream. This helps the SS stream master a joint
spectral-spatial representation in the hierarchical network for better modeling
the fusion relationship. Second, to provide superior expertise, consequently,
based on the intrinsic characteristics of the MS image and the PAN image, two
feature extraction blocks are specially developed. In the MS stream, a Residual
Spectral Attention Block (RSAB) is proposed to mine the potential spectral
correlations between different spectra of the MS image through adjacent
cross-spectrum interaction. While in the PAN stream, a Residual Multi-scale
Spatial Attention Block (RMSAB) is proposed to capture multi-scale information
and reconstruct precise high-frequency details from the PAN image through an
improved spatial attention-based inception structure. The spectral and spatial
feature representations are enhanced. Extensive experiments on two datasets
demonstrate that the proposed network is competitive with or better than
state-of-the-art methods. Our code can be found in github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IgNet. A Super-precise Convolutional Neural Network. (arXiv:2109.09939v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09939">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNN) are known to be an effective means to
detect and analyze images. Their power is essentially based on the ability to
extract out images common features. There exist, however, images involving
unique, irregular features or details. Such is a collection of unusual children
drawings reflecting the kids imagination and individuality. These drawings were
analyzed by means of a CNN constructed by means of Keras-TensorFlow. The same
problem - on a significantly higher level - was solved with newly developed
family of networks called IgNet that is described in this paper. It proved able
to learn by 100 % all the categorical characteristics of the drawings. In the
case of a regression task (learning the young artists ages) IgNet performed
with an error of no more than 0.4 %. The principles are discussed of IgNet
design that made it possible to reach such substantial results with rather
simple network topology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Domain Few-Shot Learning and Dataset for Agricultural Applications. (arXiv:2109.09952v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09952">
<div class="article-summary-box-inner">
<span><p>Automatic classification of pests and plants (both healthy and diseased) is
of paramount importance in agriculture to improve yield. Conventional deep
learning models based on convolutional neural networks require thousands of
labeled examples per category. In this work we propose a method to learn from a
few samples to automatically classify different pests, plants, and their
diseases, using Few-Shot Learning (FSL). We learn a feature extractor to
generate embeddings and then update the embeddings using Transformers. Using
Mahalanobis distance, a class-covariance-based metric, we then calculate the
similarity of the transformed embeddings with the embedding of the image to be
classified. Using our proposed architecture, we conduct extensive experiments
on multiple datasets showing the effectiveness of our proposed model. We
conduct 42 experiments in total to comprehensively analyze the model and it
achieves up to 14% and 24% performance gains on few-shot image classification
benchmarks on two datasets.
</p>
<p>We also compile a new FSL dataset containing images of healthy and diseased
plants taken in real-world settings. Using our proposed architecture which has
been shown to outperform several existing FSL architectures in agriculture, we
provide strong baselines on our newly proposed dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enforcing Mutual Consistency of Hard Regions for Semi-supervised Medical Image Segmentation. (arXiv:2109.09960v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09960">
<div class="article-summary-box-inner">
<span><p>In this paper, we proposed a novel mutual consistency network (MC-Net+) to
effectively exploit the unlabeled hard regions for semi-supervised medical
image segmentation. The MC-Net+ model is motivated by the observation that deep
models trained with limited annotations are prone to output highly uncertain
and easily mis-classified predictions in the ambiguous regions (e.g. adhesive
edges or thin branches) for the image segmentation task. Leveraging these
region-level challenging samples can make the semi-supervised segmentation
model training more effective. Therefore, our proposed MC-Net+ model consists
of two new designs. First, the model contains one shared encoder and multiple
sightly different decoders (i.e. using different up-sampling strategies). The
statistical discrepancy of multiple decoders' outputs is computed to denote the
model's uncertainty, which indicates the unlabeled hard regions. Second, a new
mutual consistency constraint is enforced between one decoder's probability
output and other decoders' soft pseudo labels. In this way, we minimize the
model's uncertainty during training and force the model to generate invariant
and low-entropy results in such challenging areas of unlabeled data, in order
to learn a generalized feature representation. We compared the segmentation
results of the MC-Net+ with five state-of-the-art semi-supervised approaches on
three public medical datasets. Extension experiments with two common
semi-supervised settings demonstrate the superior performance of our model over
other existing methods, which sets a new state of the art for semi-supervised
medical image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Source Video Domain Adaptation with Temporal Attentive Moment Alignment. (arXiv:2109.09964v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09964">
<div class="article-summary-box-inner">
<span><p>Multi-Source Domain Adaptation (MSDA) is a more practical domain adaptation
scenario in real-world scenarios. It relaxes the assumption in conventional
Unsupervised Domain Adaptation (UDA) that source data are sampled from a single
domain and match a uniform data distribution. MSDA is more difficult due to the
existence of different domain shifts between distinct domain pairs. When
considering videos, the negative transfer would be provoked by spatial-temporal
features and can be formulated into a more challenging Multi-Source Video
Domain Adaptation (MSVDA) problem. In this paper, we address the MSVDA problem
by proposing a novel Temporal Attentive Moment Alignment Network (TAMAN) which
aims for effective feature transfer by dynamically aligning both spatial and
temporal feature moments. TAMAN further constructs robust global temporal
features by attending to dominant domain-invariant local temporal features with
high local classification confidence and low disparity between global and local
feature discrepancies. To facilitate future research on the MSVDA problem, we
introduce comprehensive benchmarks, covering extensive MSVDA scenarios.
Empirical results demonstrate a superior performance of the proposed TAMAN
across multiple MSVDA benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated segmentation and extraction of posterior eye segment using OCT scans. (arXiv:2109.10000v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10000">
<div class="article-summary-box-inner">
<span><p>This paper proposes an automated method for the segmentation and extraction
of the posterior segment of the human eye, including the vitreous, retina,
choroid, and sclera compartments, using multi-vendor optical coherence
tomography (OCT) scans. The proposed method works in two phases. First extracts
the retinal pigment epithelium (RPE) layer by applying the adaptive
thresholding technique to identify the retina-choroid junction. Then, it
exploits the structure tensor guided approach to extract the inner limiting
membrane (ILM) and the choroidal stroma (CS) layers, locating the
vitreous-retina and choroid-sclera junctions in the candidate OCT scan.
Furthermore, these three junction boundaries are utilized to conduct posterior
eye compartmentalization effectively for both healthy and disease eye OCT
scans. The proposed framework is evaluated over 1000 OCT scans, where it
obtained the mean intersection over union (IoU) and mean Dice similarity
coefficient (DSC) scores of 0.874 and 0.930, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Abstract Reasoning for Raven's Problem Matrices. (arXiv:2109.10011v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10011">
<div class="article-summary-box-inner">
<span><p>Raven's Progressive Matrices (RPM) is highly correlated with human
intelligence, and it has been widely used to measure the abstract reasoning
ability of humans. In this paper, to study the abstract reasoning capability of
deep neural networks, we propose the first unsupervised learning method for
solving RPM problems. Since the ground truth labels are not allowed, we design
a pseudo target based on the prior constraints of the RPM formulation to
approximate the ground truth label, which effectively converts the unsupervised
learning strategy into a supervised one. However, the correct answer is wrongly
labelled by the pseudo target, and thus the noisy contrast will lead to
inaccurate model training. To alleviate this issue, we propose to improve the
model performance with negative answers. Moreover, we develop a
decentralization method to adapt the feature representation to different RPM
problems. Extensive experiments on three datasets demonstrate that our method
even outperforms some of the supervised approaches. Our code is available at
https://github.com/visiontao/ncd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Action-Space Prediction for Automated Driving. (arXiv:2109.10024v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10024">
<div class="article-summary-box-inner">
<span><p>Making informed driving decisions requires reliable prediction of other
vehicles' trajectories. In this paper, we present a novel learned multi-modal
trajectory prediction architecture for automated driving. It achieves
kinematically feasible predictions by casting the learning problem into the
space of accelerations and steering angles -- by performing action-space
prediction, we can leverage valuable model knowledge. Additionally, the
dimensionality of the action manifold is lower than that of the state manifold,
whose intrinsically correlated states are more difficult to capture in a
learned manner. For the purpose of action-space prediction, we present the
simple Feed-Forward Action-Space Prediction (FFW-ASP) architecture. Then, we
build on this notion and introduce the novel Self-Supervised Action-Space
Prediction (SSP-ASP) architecture that outputs future environment context
features in addition to trajectories. A key element in the self-supervised
architecture is that, based on an observed action history and past context
features, future context features are predicted prior to future trajectories.
The proposed methods are evaluated on real-world datasets containing urban
intersections and roundabouts, and show accurate predictions, outperforming
state-of-the-art for kinematically feasible predictions in several prediction
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VPN: Video Provenance Network for Robust Content Attribution. (arXiv:2109.10038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10038">
<div class="article-summary-box-inner">
<span><p>We present VPN - a content attribution method for recovering provenance
information from videos shared online. Platforms, and users, often transform
video into different quality, codecs, sizes, shapes, etc. or slightly edit its
content such as adding text or emoji, as they are redistributed online. We
learn a robust search embedding for matching such video, invariant to these
transformations, using full-length or truncated video queries. Once matched
against a trusted database of video clips, associated information on the
provenance of the clip is presented to the user. We use an inverted index to
match temporal chunks of video using late-fusion to combine both visual and
audio features. In both cases, features are extracted via a deep neural network
trained using contrastive learning on a dataset of original and augmented video
clips. We demonstrate high accuracy recall over a corpus of 100,000 videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Person Pose Estimation: A Survey. (arXiv:2109.10056v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10056">
<div class="article-summary-box-inner">
<span><p>Human pose estimation in unconstrained images and videos is a fundamental
computer vision task. To illustrate the evolutionary path in technique, in this
survey we summarize representative human pose methods in a structured taxonomy,
with a particular focus on deep learning models and single-person image
setting. Specifically, we examine and survey all the components of a typical
human pose estimation pipeline, including data augmentation, model architecture
and backbone, supervision representation, post-processing, standard datasets,
evaluation metrics. To envisage the future directions, we finally discuss the
key unsolved problems and potential trends for human pose estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LOTR: Face Landmark Localization Using Localization Transformer. (arXiv:2109.10057v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10057">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel Transformer-based facial landmark localization
network named Localization Transformer (LOTR). The proposed framework is a
direct coordinate regression approach leveraging a Transformer network to
better utilize the spatial information in the feature map. An LOTR model
consists of three main modules: 1) a visual backbone that converts an input
image into a feature map, 2) a Transformer module that improves the feature
representation from the visual backbone, and 3) a landmark prediction head that
directly predicts the landmark coordinates from the Transformer's
representation. Given cropped-and-aligned face images, the proposed LOTR can be
trained end-to-end without requiring any post-processing steps. This paper also
introduces the smooth-Wing loss function, which addresses the gradient
discontinuity of the Wing loss, leading to better convergence than standard
loss functions such as L1, L2, and Wing loss. Experimental results on the JD
landmark dataset provided by the First Grand Challenge of 106-Point Facial
Landmark Localization indicate the superiority of LOTR over the existing
methods on the leaderboard and two recent heatmap-based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Transformers. (arXiv:2109.10060v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10060">
<div class="article-summary-box-inner">
<span><p>Dynamic networks have shown their promising capability in reducing
theoretical computation complexity by adapting their architectures to the input
during inference. However, their practical runtime usually lags behind the
theoretical acceleration due to inefficient sparsity. Here, we explore a
hardware-efficient dynamic inference regime, named dynamic weight slicing,
which adaptively slice a part of network parameters for inputs with diverse
difficulty levels, while keeping parameters stored statically and contiguously
in hardware to prevent the extra burden of sparse computation. Based on this
scheme, we present dynamic slimmable network (DS-Net) and dynamic slice-able
network (DS-Net++) by input-dependently adjusting filter numbers of CNNs and
multiple dimensions in both CNNs and transformers, respectively. To ensure
sub-network generality and routing fairness, we propose a disentangled
two-stage optimization scheme with training techniques such as in-place
bootstrapping (IB), multi-view consistency (MvCo) and sandwich gate
sparsification (SGS) to train supernet and gate separately. Extensive
experiments on 4 datasets and 3 different network architectures demonstrate our
method consistently outperforms state-of-the-art static and dynamic model
compression methods by a large margin (up to 6.6%). Typically, DS-Net++
achieves 2-4x computation reduction and 1.62x real-world acceleration over
MobileNet, ResNet-50 and Vision Transformer, with minimal accuracy drops
(0.1-0.3%) on ImageNet. Code release: https://github.com/changlin31/DS-Net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-aware direct monocular odometry. (arXiv:2109.10077v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10077">
<div class="article-summary-box-inner">
<span><p>We present a framework for direct monocular odometry based on depth
prediction from a deep neural network. In contrast with existing methods where
depth information is only partially exploited, we formulate a novel depth
prediction residual which allows us to incorporate multi-view depth
information. In addition, we propose to use a truncated robust cost function
which prevents considering inconsistent depth estimations. The photometric and
depth-prediction measurements are integrated in a tightly-coupled optimization
leading to a scale-aware monocular system which does not accumulate scale
drift. We demonstrate the validity of our proposal evaluating it on the KITTI
odometry dataset and comparing it with state-of-the-art monocular and stereo
SLAM systems. Experiments show that our proposal largely outperforms classic
monocular SLAM, being 5 to 9 times more precise, with an accuracy which is
closer to that of stereo systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Interpretable Concept Groups in CNNs. (arXiv:2109.10078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10078">
<div class="article-summary-box-inner">
<span><p>We propose a novel training methodology -- Concept Group Learning (CGL) --
that encourages training of interpretable CNN filters by partitioning filters
in each layer into concept groups, each of which is trained to learn a single
visual concept. We achieve this through a novel regularization strategy that
forces filters in the same group to be active in similar image regions for a
given layer. We additionally use a regularizer to encourage a sparse weighting
of the concept groups in each layer so that a few concept groups can have
greater importance than others. We quantitatively evaluate CGL's model
interpretability using standard interpretability evaluation techniques and find
that our method increases interpretability scores in most cases. Qualitatively
we compare the image regions that are most active under filters learned using
CGL versus filters learned without CGL and find that CGL activation regions
more strongly concentrate around semantically relevant features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PDFNet: Pointwise Dense Flow Network for Urban-Scene Segmentation. (arXiv:2109.10083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10083">
<div class="article-summary-box-inner">
<span><p>In recent years, using a deep convolutional neural network (CNN) as a feature
encoder (or backbone) is the most commonly observed architectural pattern in
several computer vision methods, and semantic segmentation is no exception. The
two major drawbacks of this architectural pattern are: (i) the networks often
fail to capture small classes such as wall, fence, pole, traffic light, traffic
sign, and bicycle, which are crucial for autonomous vehicles to make accurate
decisions. (ii) due to the arbitrarily increasing depth, the networks require
massive labeled data and additional regularization techniques to converge and
to prevent the risk of over-fitting, respectively. While regularization
techniques come at minimal cost, the collection of labeled data is an expensive
and laborious process. In this work, we address these two drawbacks by
proposing a novel lightweight architecture named point-wise dense flow network
(PDFNet). In PDFNet, we employ dense, residual, and multiple shortcut
connections to allow a smooth gradient flow to all parts of the network. The
extensive experiments on Cityscapes and CamVid benchmarks demonstrate that our
method significantly outperforms baselines in capturing small classes and in
few-data regimes. Moreover, our method achieves considerable performance in
classifying out-of-the training distribution samples, evaluated on Cityscapes
to KITTI dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Confidence Calibration for Epistemic Uncertainty Modelling. (arXiv:2109.10092v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10092">
<div class="article-summary-box-inner">
<span><p>Modern neural networks have found to be miscalibrated in terms of confidence
calibration, i.e., their predicted confidence scores do not reflect the
observed accuracy or precision. Recent work has introduced methods for post-hoc
confidence calibration for classification as well as for object detection to
address this issue. Especially in safety critical applications, it is crucial
to obtain a reliable self-assessment of a model. But what if the calibration
method itself is uncertain, e.g., due to an insufficient knowledge base?
</p>
<p>We introduce Bayesian confidence calibration - a framework to obtain
calibrated confidence estimates in conjunction with an uncertainty of the
calibration method. Commonly, Bayesian neural networks (BNN) are used to
indicate a network's uncertainty about a certain prediction. BNNs are
interpreted as neural networks that use distributions instead of weights for
inference. We transfer this idea of using distributions to confidence
calibration. For this purpose, we use stochastic variational inference to build
a calibration mapping that outputs a probability distribution rather than a
single calibrated estimate. Using this approach, we achieve state-of-the-art
calibration performance for object detection calibration. Finally, we show that
this additional type of uncertainty can be used as a sufficient criterion for
covariate shift detection. All code is open source and available at
https://github.com/EFS-OpenSource/calibration-framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation. (arXiv:2109.10115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10115">
<div class="article-summary-box-inner">
<span><p>We present a large-scale stereo RGB image object pose estimation dataset
named the $\textbf{StereOBJ-1M}$ dataset. The dataset is designed to address
challenging cases such as object transparency, translucency, and specular
reflection, in addition to the common challenges of occlusion, symmetry, and
variations in illumination and environments. In order to collect data of
sufficient scale for modern deep learning models, we propose a novel method for
efficiently annotating pose data in a multi-view fashion that allows data
capturing in complex and flexible environments. Fully annotated with 6D object
poses, our dataset contains over 396K frames and over 1.5M annotations of 18
objects recorded in 183 scenes constructed in 11 different environments. The 18
objects include 8 symmetric objects, 7 transparent objects, and 8 reflective
objects. We benchmark two state-of-the-art pose estimation frameworks on
StereOBJ-1M as baselines for future work. We also propose a novel object-level
pose optimization method for computing 6D pose from keypoint predictions in
multiple images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Semantic Stereo Matching / Semantic Depth Estimation. (arXiv:2109.10123v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10123">
<div class="article-summary-box-inner">
<span><p>Stereo matching is one of the widely used techniques for inferring depth from
stereo images owing to its robustness and speed. It has become one of the major
topics of research since it finds its applications in autonomous driving,
robotic navigation, 3D reconstruction, and many other fields. Finding pixel
correspondences in non-textured, occluded and reflective areas is the major
challenge in stereo matching. Recent developments have shown that semantic cues
from image segmentation can be used to improve the results of stereo matching.
Many deep neural network architectures have been proposed to leverage the
advantages of semantic segmentation in stereo matching. This paper aims to give
a comparison among the state of art networks both in terms of accuracy and in
terms of speed which are of higher importance in real-time applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KDFNet: Learning Keypoint Distance Field for 6D Object Pose Estimation. (arXiv:2109.10127v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10127">
<div class="article-summary-box-inner">
<span><p>We present KDFNet, a novel method for 6D object pose estimation from RGB
images. To handle occlusion, many recent works have proposed to localize 2D
keypoints through pixel-wise voting and solve a Perspective-n-Point (PnP)
problem for pose estimation, which achieves leading performance. However, such
voting process is direction-based and cannot handle long and thin objects where
the direction intersections cannot be robustly found. To address this problem,
we propose a novel continuous representation called Keypoint Distance Field
(KDF) for projected 2D keypoint locations. Formulated as a 2D array, each
element of the KDF stores the 2D Euclidean distance between the corresponding
image pixel and a specified projected 2D keypoint. We use a fully convolutional
neural network to regress the KDF for each keypoint. Using this KDF encoding of
projected object keypoint locations, we propose to use a distance-based voting
scheme to localize the keypoints by calculating circle intersections in a
RANSAC fashion. We validate the design choices of our framework by extensive
ablation experiments. Our proposed method achieves state-of-the-art performance
on Occlusion LINEMOD dataset with an average ADD(-S) accuracy of 50.3% and TOD
dataset mug subset with an average ADD accuracy of 75.72%. Extensive
experiments and visualizations demonstrate that the proposed method is able to
robustly estimate the 6D pose in challenging scenarios including occlusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Representation Learning for Reliable Robotic Monitoring of Fruit Anomalies. (arXiv:2109.10135v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10135">
<div class="article-summary-box-inner">
<span><p>Data augmentation can be a simple yet powerful tool for autonomous robots to
fully utilise available data for self-supervised identification of atypical
scenes or objects. State-of-the-art augmentation methods arbitrarily embed
structural peculiarity in focal objects on typical images so that classifying
these artefacts can provide guidance for learning representations for the
detection of anomalous visual inputs. In this paper, however, we argue that
learning such structure-sensitive representations can be a suboptimal approach
to some classes of anomaly (e.g., unhealthy fruits) which are better recognised
by a different type of visual element such as "colour". We thus propose Channel
Randomisation as a novel data augmentation method for restricting neural
network models to learn encoding of "colour irregularity" whilst predicting
channel-randomised images to ultimately build reliable fruit-monitoring robots
identifying atypical fruit qualities. Our experiments show that (1) the
colour-based alternative can better learn representations for consistently
accurate identification of fruit anomalies in various fruit species, and (2)
validation accuracy can be monitored for early stopping of training due to
positive correlation between the colour-learning task and fruit anomaly
detection. Moreover, the proposed approach is evaluated on a new anomaly
dataset Riseholme-2021, consisting of 3:5K strawberry images collected from a
mobile robot, which we share with the community to encourage active
agri-robotics research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Point Cloud Completion with Geometric-Aware Adversarial Augmentation. (arXiv:2109.10161v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10161">
<div class="article-summary-box-inner">
<span><p>With the popularity of 3D sensors in self-driving and other robotics
applications, extensive research has focused on designing novel neural network
architectures for accurate 3D point cloud completion. However, unlike in point
cloud classification and reconstruction, the role of adversarial samples in3D
point cloud completion has seldom been explored. In this work, we show that
training with adversarial samples can improve the performance of neural
networks on 3D point cloud completion tasks. We propose a novel approach to
generate adversarial samples that benefit both the performance of clean and
adversarial samples. In contrast to the PGD-k attack, our method generates
adversarial samples that keep the geometric features in clean samples and
contain few outliers. In particular, we use principal directions to constrain
the adversarial perturbations for each input point. The gradient components in
the mean direction of principal directions are taken as adversarial
perturbations. In addition, we also investigate the effect of using the minimum
curvature direction. Besides, we adopt attack strength accumulation and
auxiliary Batch Normalization layers method to speed up the training process
and alleviate the distribution mismatch between clean and adversarial samples.
Experimental results show that training with the adversarial samples crafted by
our method effectively enhances the performance of PCN on the ShapeNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Oriented Object Detection in Aerial Images Based on Area Ratio of Parallelogram. (arXiv:2109.10187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10187">
<div class="article-summary-box-inner">
<span><p>Rotated object detection is a challenging task in aerial images as the object
in aerial images are displayed in arbitrary directions and usually densely
packed. Although considerable progress has been made, there are still
challenges that existing regression-based rotation detectors suffer the problem
of discontinuous boundaries, which is directly caused by angular periodicity or
corner ordering. In this paper, we propose a simple effective framework to
address the above challenges. Instead of directly regressing the five
parameters (coordinates of the central point, width, height, and rotation
angle) or the four vertices, we use the area ratio of parallelogram (ARP) to
accurately describe a multi-oriented object. Specifically, we regress
coordinates of center point, height and width of minimum circumscribed
rectangle of oriented object and three area ratios {\lambda}_1, {\lambda}_2 and
{\lambda}_3. This may facilitate the offset learning and avoid the issue of
angular periodicity or label points sequence for oriented objects. To further
remedy the confusion issue nearly horizontal objects, we employ the area ratio
between the object and its horizontal bounding box (minimum circumscribed
rectangle) to guide the selection of horizontal or oriented detection for each
object. We also propose a rotation efficient IoU loss (R-EIoU) to connect the
horizontal bounding box with the three area ratios and improve the accurate for
the rotating bounding box. Experimental results on three remote sensing
datasets including HRSC2016, DOTA and UCAS-AOD and scene text including
ICDAR2015 show that our method achieves superior detection performance compared
with many state-of-the-art approaches. The code and model will be coming with
paper published.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Vision-and-Language Pretraining Improve Lexical Grounding?. (arXiv:2109.10246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10246">
<div class="article-summary-box-inner">
<span><p>Linguistic representations derived from text alone have been criticized for
their lack of grounding, i.e., connecting words to their meanings in the
physical world. Vision-and-Language (VL) models, trained jointly on text and
image or video data, have been offered as a response to such criticisms.
However, while VL pretraining has shown success on multimodal tasks such as
visual question answering, it is not yet known how the internal linguistic
representations themselves compare to their text-only counterparts. This paper
compares the semantic representations learned via VL vs. text-only pretraining
for two recent VL models using a suite of analyses (clustering, probing, and
performance on a commonsense question answering task) in a language-only
setting. We find that the multimodal models fail to significantly outperform
the text-only variants, suggesting that future work is required if multimodal
pretraining is to be pursued as a means of improving NLP in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs. (arXiv:2109.10257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10257">
<div class="article-summary-box-inner">
<span><p>Several applications such as autonomous driving, augmented reality and
virtual reality requires a precise prediction of the 3D human pose. Recently, a
new problem was introduced in the field to predict the 3D human poses from an
observed 2D poses. We propose Skeleton-Graph, a deep spatio-temporal graph CNN
model that predicts the future 3D skeleton poses in a single pass from the 2D
ones. Unlike prior works, Skeleton-Graph focuses on modeling the interaction
between the skeleton joints by exploiting their spatial configuration. This is
being achieved by formulating the problem as a graph structure while learning a
suitable graph adjacency kernel. By the design, Skeleton-Graph predicts the
future 3D poses without divergence on the long-term unlike prior works. We also
introduce a new metric that measures the divergence of predictions on the
long-term. Our results show an FDE improvement of at least 27% and an ADE of 4%
on both the GTA-IM and PROX datasets respectively in comparison with prior
works. Also, we are 88% and 93% less divergence on the long-term motion
prediction in comparison with prior works on both GTA-IM and PROX datasets.
https://github.com/abduallahmohamed/Skeleton-Graph.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of single and multitask learning for predicting cognitive decline based on MRI data. (arXiv:2109.10266v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10266">
<div class="article-summary-box-inner">
<span><p>The Alzheimer's Disease Assessment Scale-Cognitive subscale (ADAS-Cog) is a
neuropsychological tool that has been designed to assess the severity of
cognitive symptoms of dementia. Personalized prediction of the changes in
ADAS-Cog scores could help in timing therapeutic interventions in dementia and
at-risk populations. In the present work, we compared single and multitask
learning approaches to predict the changes in ADAS-Cog scores based on
T1-weighted anatomical magnetic resonance imaging (MRI). In contrast to most
machine learning-based prediction methods ADAS-Cog changes, we stratified the
subjects based on their baseline diagnoses and evaluated the prediction
performances in each group. Our experiments indicated a positive relationship
between the predicted and observed ADAS-Cog score changes in each diagnostic
group, suggesting that T1-weighted MRI has a predictive value for evaluating
cognitive decline in the entire AD continuum. We further studied whether
correction of the differences in the magnetic field strength of MRI would
improve the ADAS-Cog score prediction. The partial least square-based domain
adaptation slightly improved the prediction performance, but the improvement
was marginal. In summary, this study demonstrated that ADAS-Cog change could
be, to some extent, predicted based on anatomical MRI. Based on this study, the
recommended method for learning the predictive models is a single-task
regularized linear regression due to its simplicity and good performance. It
appears important to combine the training data across all subject groups for
the most effective predictive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemCal: Semantic LiDAR-Camera Calibration using Neural MutualInformation Estimator. (arXiv:2109.10270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10270">
<div class="article-summary-box-inner">
<span><p>This paper proposes SemCal: an automatic, targetless, extrinsic calibration
algorithm for a LiDAR and camera system using semantic information. We leverage
a neural information estimator to estimate the mutual information (MI) of
semantic information extracted from each sensor measurement, facilitating
semantic-level data association. By using a matrix exponential formulation of
the $se(3)$ transformation and a kernel-based sampling method to sample from
camera measurement based on LiDAR projected points, we can formulate the
LiDAR-Camera calibration problem as a novel differentiable objective function
that supports gradient-based optimization methods. We also introduce a
semantic-based initial calibration method using 2D MI-based image registration
and Perspective-n-Point (PnP) solver. To evaluate performance, we demonstrate
the robustness of our method and quantitatively analyze the accuracy using a
synthetic dataset. We also evaluate our algorithm qualitatively on an urban
dataset (KITTI360) and an off-road dataset (RELLIS-3D) benchmark datasets using
both hand-annotated ground truth labels as well as labels predicted by the
state-of-the-art deep learning models, showing improvement over recent
comparable calibration approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10282">
<div class="article-summary-box-inner">
<span><p>Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning PAC-Bayes Priors for Probabilistic Neural Networks. (arXiv:2109.10304v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10304">
<div class="article-summary-box-inner">
<span><p>Recent works have investigated deep learning models trained by optimising
PAC-Bayes bounds, with priors that are learnt on subsets of the data. This
combination has been shown to lead not only to accurate classifiers, but also
to remarkably tight risk certificates, bearing promise towards self-certified
learning (i.e. use all the data to learn a predictor and certify its quality).
In this work, we empirically investigate the role of the prior. We experiment
on 6 datasets with different strategies and amounts of data to learn
data-dependent PAC-Bayes priors, and we compare them in terms of their effect
on test performance of the learnt predictors and tightness of their risk
certificate. We ask what is the optimal amount of data which should be
allocated for building the prior and show that the optimum may be dataset
dependent. We demonstrate that using a small percentage of the prior-building
data for validation of the prior leads to promising results. We include a
comparison of underparameterised and overparameterised models, along with an
empirical study of different training objectives and regularisation strategies
to learn the prior distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CondNet: Conditional Classifier for Scene Segmentation. (arXiv:2109.10322v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10322">
<div class="article-summary-box-inner">
<span><p>The fully convolutional network (FCN) has achieved tremendous success in
dense visual recognition tasks, such as scene segmentation. The last layer of
FCN is typically a global classifier (1x1 convolution) to recognize each pixel
to a semantic label. We empirically show that this global classifier, ignoring
the intra-class distinction, may lead to sub-optimal results.
</p>
<p>In this work, we present a conditional classifier to replace the traditional
global classifier, where the kernels of the classifier are generated
dynamically conditioned on the input. The main advantages of the new classifier
consist of: (i) it attends on the intra-class distinction, leading to stronger
dense recognition capability; (ii) the conditional classifier is simple and
flexible to be integrated into almost arbitrary FCN architectures to improve
the prediction. Extensive experiments demonstrate that the proposed classifier
performs favourably against the traditional classifier on the FCN architecture.
The framework equipped with the conditional classifier (called CondNet)
achieves new state-of-the-art performances on two datasets. The code and models
are available at https://git.io/CondNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-driven controllers and the need for perception systems in underwater manipulation. (arXiv:2109.10327v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10327">
<div class="article-summary-box-inner">
<span><p>The underwater environment poses a complex problem for developing autonomous
capabilities for Underwater Vehicle Manipulator Systems (UVMSs). The modeling
of UVMSs is a complicated and costly process due to the highly nonlinear
dynamics and the presence of unknown hydrodynamical effects. This is aggravated
in tasks where the manipulation of objects is necessary, as this may not only
introduce external disturbances that can lead to a fast degradation of the
control system performance, but also requires the coordinating with a vision
system for the correct grasping and operation of the object. In this article,
we introduce a control strategy for UVMSs working with unknown payloads. The
proposed control strategy is based on a data-driven optimal controller. We
present a number of experimental results showing the benefits of the proposed
strategy. Furthermore, we include a discussion regarding the visual perception
requirements for the UVMS in order to achieve full autonomy in underwater
manipulation tasks of unknown payloads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Homography augumented momentum constrastive learning for SAR image retrieval. (arXiv:2109.10329v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10329">
<div class="article-summary-box-inner">
<span><p>Deep learning-based image retrieval has been emphasized in computer vision.
Representation embedding extracted by deep neural networks (DNNs) not only aims
at containing semantic information of the image, but also can manage
large-scale image retrieval tasks. In this work, we propose a deep
learning-based image retrieval approach using homography transformation
augmented contrastive learning to perform large-scale synthetic aperture radar
(SAR) image search tasks. Moreover, we propose a training method for the DNNs
induced by contrastive learning that does not require any labeling procedure.
This may enable tractability of large-scale datasets with relative ease.
Finally, we verify the performance of the proposed method by conducting
experiments on the polarimetric SAR image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism. (arXiv:2003.03955v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.03955">
<div class="article-summary-box-inner">
<span><p>Food retrieval is an important task to perform analysis of food-related
information, where we are interested in retrieving relevant information about
the queried food item such as ingredients, cooking instructions, etc. In this
paper, we investigate cross-modal retrieval between food images and cooking
recipes. The goal is to learn an embedding of images and recipes in a common
feature space, such that the corresponding image-recipe embeddings lie close to
one another. Two major challenges in addressing this problem are 1) large
intra-variance and small inter-variance across cross-modal food data; and 2)
difficulties in obtaining discriminative recipe representations. To address
these two problems, we propose Semantic-Consistent and Attention-based Networks
(SCAN), which regularize the embeddings of the two modalities through aligning
output semantic probabilities. Besides, we exploit a self-attention mechanism
to improve the embedding of recipes. We evaluate the performance of the
proposed method on the large-scale Recipe1M dataset, and show that we can
outperform several state-of-the-art cross-modal retrieval strategies for food
images and cooking recipes by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Cameras to Feel: Estimating Tactile Physical Properties of Surfaces From Images. (arXiv:2004.14487v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14487">
<div class="article-summary-box-inner">
<span><p>The connection between visual input and tactile sensing is critical for
object manipulation tasks such as grasping and pushing. In this work, we
introduce the challenging task of estimating a set of tactile physical
properties from visual information. We aim to build a model that learns the
complex mapping between visual information and tactile physical properties. We
construct a first of its kind image-tactile dataset with over 400 multiview
image sequences and the corresponding tactile properties. A total of fifteen
tactile physical properties across categories including friction, compliance,
adhesion, texture, and thermal conductance are measured and then estimated by
our models. We develop a cross-modal framework comprised of an adversarial
objective and a novel visuo-tactile joint classification loss. Additionally, we
develop a neural architecture search framework capable of selecting optimal
combinations of viewing angles for estimating a given physical property.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Distributionally Robust Learning for Calibrated Uncertainties under Domain Shift. (arXiv:2010.05784v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05784">
<div class="article-summary-box-inner">
<span><p>We propose a deep distributionally robust learning framework for calibrated
uncertainties under domain shifts. We consider cases where the source
(training) distribution differs significantly from the target (test)
distribution. In addition to the standard class predictor, our framework
contains a binary domain classifier which estimates the density ratio between
the source and target domains. We incorporate both with neural networks and
train them end-to-end. The framework is demonstrated to generate calibrated
uncertainties that benefit many downstream tasks, including unsupervised domain
adaptation (UDA) and semi-supervised learning (SSL) where methods such as
self-training and FixMatch use uncertainties to select confident pseudo-labels.
Our experiments show that the introduction of DRL to these methods leads to
significant improvements in cross-domain performance. We also demonstrate that
the produced density ratio estimates show agreement with the human selection
frequencies, suggesting a match with the human perceived uncertainties. The
source code of this work will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Encoding Clinical Priori in 3D Convolutional Neural Networks for Prostate Cancer Detection in bpMRI. (arXiv:2011.00263v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00263">
<div class="article-summary-box-inner">
<span><p>We hypothesize that anatomical priors can be viable mediums to infuse
domain-specific clinical knowledge into state-of-the-art convolutional neural
networks (CNN) based on the U-Net architecture. We introduce a probabilistic
population prior which captures the spatial prevalence and zonal distinction of
clinically significant prostate cancer (csPCa), in order to improve its
computer-aided detection (CAD) in bi-parametric MR imaging (bpMRI). To evaluate
performance, we train 3D adaptations of the U-Net, U-SEResNet, UNet++ and
Attention U-Net using 800 institutional training-validation scans, paired with
radiologically-estimated annotations and our computed prior. For 200
independent testing bpMRI scans with histologically-confirmed delineations of
csPCa, our proposed method of encoding clinical priori demonstrates a strong
ability to improve patient-based diagnosis (upto 8.70% increase in AUROC) and
lesion-level detection (average increase of 1.08 pAUC between 0.1-10 false
positives per patient) across all four architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Descriptor Visual Localization and Mapping. (arXiv:2012.01377v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01377">
<div class="article-summary-box-inner">
<span><p>Visual localization and mapping is the key technology underlying the majority
of mixed reality and robotics systems. Most state-of-the-art approaches rely on
local features to establish correspondences between images. In this paper, we
present three novel scenarios for localization and mapping which require the
continuous update of feature representations and the ability to match across
different feature types. While localization and mapping is a fundamental
computer vision problem, the traditional setup supposes the same local features
are used throughout the evolution of a map. Thus, whenever the underlying
features are changed, the whole process is repeated from scratch. However, this
is typically impossible in practice, because raw images are often not stored
and re-building the maps could lead to loss of the attached digital content. To
overcome the limitations of current approaches, we present the first principled
solution to cross-descriptor localization and mapping. Our data-driven approach
is agnostic to the feature descriptor type, has low computational requirements,
and scales linearly with the number of description algorithms. Extensive
experiments demonstrate the effectiveness of our approach on state-of-the-art
benchmarks for a variety of handcrafted and learned features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data InStance Prior (DISP) in Generative Adversarial Networks. (arXiv:2012.04256v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04256">
<div class="article-summary-box-inner">
<span><p>Recent advances in generative adversarial networks (GANs) have shown
remarkable progress in generating high-quality images. However, this gain in
performance depends on the availability of a large amount of training data. In
limited data regimes, training typically diverges, and therefore the generated
samples are of low quality and lack diversity. Previous works have addressed
training in low data setting by leveraging transfer learning and data
augmentation techniques. We propose a novel transfer learning method for GANs
in the limited data domain by leveraging informative data prior derived from
self-supervised/supervised pre-trained networks trained on a diverse source
domain. We perform experiments on several standard vision datasets using
various GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the
proposed method effectively transfers knowledge to domains with few target
images, outperforming existing state-of-the-art techniques in terms of image
quality and diversity. We also show the utility of data instance prior in
large-scale unconditional image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Culture to Clothing: Discovering the World Events Behind A Century of Fashion Images. (arXiv:2102.01690v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01690">
<div class="article-summary-box-inner">
<span><p>Fashion is intertwined with external cultural factors, but identifying these
links remains a manual process limited to only the most salient phenomena. We
propose a data-driven approach to identify specific cultural factors affecting
the clothes people wear. Using large-scale datasets of news articles and
vintage photos spanning a century, we present a multi-modal statistical model
to detect influence relationships between happenings in the world and people's
choice of clothing. Furthermore, on two image datasets we apply our model to
improve the concrete vision tasks of visual style forecasting and photo
timestamping. Our work is a first step towards a computational, scalable, and
easily refreshable approach to link culture to clothing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weak Adaptation Learning -- Addressing Cross-domain Data Insufficiency with Weak Annotator. (arXiv:2102.07358v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07358">
<div class="article-summary-box-inner">
<span><p>Data quantity and quality are crucial factors for data-driven learning
methods. In some target problem domains, there are not many data samples
available, which could significantly hinder the learning process. While data
from similar domains may be leveraged to help through domain adaptation,
obtaining high-quality labeled data for those source domains themselves could
be difficult or costly. To address such challenges on data insufficiency for
classification problem in a target domain, we propose a weak adaptation
learning (WAL) approach that leverages unlabeled data from a similar source
domain, a low-cost weak annotator that produces labels based on task-specific
heuristics, labeling rules, or other methods (albeit with inaccuracy), and a
small amount of labeled data in the target domain. Our approach first conducts
a theoretical analysis on the error bound of the trained classifier with
respect to the data quantity and the performance of the weak annotator, and
then introduces a multi-stage weak adaptation learning method to learn an
accurate classifier by lowering the error bound. Our experiments demonstrate
the effectiveness of our approach in learning an accurate classifier with
limited labeled data in the target domain and unlabeled data in the source
domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association. (arXiv:2103.02440v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02440">
<div class="article-summary-box-inner">
<span><p>Many image-based perception tasks can be formulated as detecting, associating
and tracking semantic keypoints, e.g., human body pose estimation and tracking.
In this work, we present a general framework that jointly detects and forms
spatio-temporal keypoint associations in a single stage, making this the first
real-time pose detection and tracking algorithm. We present a generic neural
network architecture that uses Composite Fields to detect and construct a
spatio-temporal pose which is a single, connected graph whose nodes are the
semantic keypoints (e.g., a person's body joints) in multiple frames. For the
temporal associations, we introduce the Temporal Composite Association Field
(TCAF) which requires an extended network architecture and training method
beyond previous Composite Fields. Our experiments show competitive accuracy
while being an order of magnitude faster on multiple publicly available
datasets such as COCO, CrowdPose and the PoseTrack 2017 and 2018 datasets. We
also show that our method generalizes to any class of semantic keypoints such
as car and animal parts to provide a holistic perception framework that is well
suited for urban mobility such as self-driving cars and delivery robots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quadruple Augmented Pyramid Network for Multi-class COVID-19 Segmentation via CT. (arXiv:2103.05546v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05546">
<div class="article-summary-box-inner">
<span><p>COVID-19, a new strain of coronavirus disease, has been one of the most
serious and infectious disease in the world. Chest CT is essential in
prognostication, diagnosing this disease, and assessing the complication. In
this paper, a multi-class COVID-19 CT segmentation is proposed aiming at
helping radiologists estimate the extent of effected lung volume. We utilized
four augmented pyramid networks on an encoder-decoder segmentation framework.
Quadruple Augmented Pyramid Network (QAP-Net) not only enable CNN capture
features from variation size of CT images, but also act as spatial
interconnections and down-sampling to transfer sufficient feature information
for semantic segmentation. Experimental results achieve competitive performance
in segmentation with the Dice of 0.8163, which outperforms other
state-of-the-art methods, demonstrating the proposed framework can segments of
consolidation as well as glass, ground area via COVID-19 chest CT efficiently
and accurately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Higher Order Recurrent Space-Time Transformer for Video Action Prediction. (arXiv:2104.08665v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08665">
<div class="article-summary-box-inner">
<span><p>Endowing visual agents with predictive capability is a key step towards video
intelligence at scale. The predominant modeling paradigm for this is sequence
learning, mostly implemented through LSTMs. Feed-forward Transformer
architectures have replaced recurrent model designs in ML applications of
language processing and also partly in computer vision. In this paper we
investigate on the competitiveness of Transformer-style architectures for video
predictive tasks. To do so we propose HORST, a novel higher order recurrent
layer design whose core element is a spatial-temporal decomposition of
self-attention for video. HORST achieves state of the art competitive
performance on Something-Something early action recognition and EPIC-Kitchens
action anticipation, showing evidence of predictive capability that we
attribute to our recurrent higher order design of self-attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Example Detection for DNN Models: A Review and Experimental Comparison. (arXiv:2105.00203v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00203">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) has shown great success in many human-related tasks, which
has led to its adoption in many computer vision based applications, such as
security surveillance systems, autonomous vehicles and healthcare. Such
safety-critical applications have to draw their path to success deployment once
they have the capability to overcome safety-critical challenges. Among these
challenges are the defense against or/and the detection of the adversarial
examples (AEs). Adversaries can carefully craft small, often imperceptible,
noise called perturbations to be added to the clean image to generate the AE.
The aim of AE is to fool the DL model which makes it a potential risk for DL
applications. Many test-time evasion attacks and countermeasures,i.e., defense
or detection methods, are proposed in the literature. Moreover, few reviews and
surveys were published and theoretically showed the taxonomy of the threats and
the countermeasure methods with little focus in AE detection methods. In this
paper, we focus on image classification tasks and attempt to provide a survey
for detection methods of test-time evasion attacks on neural network
classifiers. A detailed discussion for such methods is provided with
experimental results for eight state-of-the-art detectors under different
scenarios on four datasets. We also provide potential challenges and future
perspectives for this research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Discovery and Attribution of Open-world GAN Generated Images. (arXiv:2105.04580v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04580">
<div class="article-summary-box-inner">
<span><p>With the recent progress in Generative Adversarial Networks (GANs), it is
imperative for media and visual forensics to develop detectors which can
identify and attribute images to the model generating them. Existing works have
shown to attribute images to their corresponding GAN sources with high
accuracy. However, these works are limited to a closed set scenario, failing to
generalize to GANs unseen during train time and are therefore, not scalable
with a steady influx of new GANs. We present an iterative algorithm for
discovering images generated from previously unseen GANs by exploiting the fact
that all GANs leave distinct fingerprints on their generated images. Our
algorithm consists of multiple components including network training,
out-of-distribution detection, clustering, merge and refine steps. Through
extensive experiments, we show that our algorithm discovers unseen GANs with
high accuracy and also generalizes to GANs trained on unseen real datasets. We
additionally apply our algorithm to attribution and discovery of GANs in an
online fashion as well as to the more standard task of real/fake detection. Our
experiments demonstrate the effectiveness of our approach to discover new GANs
and can be used in an open-world setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CI-dataset and DetDSCI methodology for detecting too small and too large critical infrastructures in satellite images: Airports and electrical substations as case study. (arXiv:2105.11844v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11844">
<div class="article-summary-box-inner">
<span><p>The detection of critical infrastructures in large territories represented by
aerial and satellite images is of high importance in several fields such as in
security, anomaly detection, land use planning and land use change detection.
However, the detection of such infrastructures is complex as they have highly
variable shapes and sizes, i.e., some infrastructures, such as electrical
substations, are too small while others, such as airports, are too large.
Besides, airports can have a surface area either small or too large with
completely different shapes, which makes its correct detection challenging. As
far as we know, these limitations have not been tackled yet in previous works.
This paper presents (1) a smart Critical Infrastructure dataset, named
CI-dataset, organised into two scales, small and large scales critical
infrastructures and (2) a two-level resolution-independent critical
infrastructure detection (DetDSCI) methodology that first determines the
spatial resolution of the input image using a classification model, then
analyses the image using the appropriate detector for that spatial resolution.
The present study targets two representative classes, airports and electrical
substations. Our experiments show that DetDSCI methodology achieves up to
37,53% F1 improvement with respect to Faster R-CNN, one of the most influential
detection models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis. (arXiv:2106.03776v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03776">
<div class="article-summary-box-inner">
<span><p>Background modeling and subtraction is a promising research area with a
variety of applications for video surveillance. Recent years have witnessed a
proliferation of effective learning-based deep neural networks in this area.
However, the techniques have only provided limited descriptions of scenes'
properties while requiring heavy computations, as their single-valued mapping
functions are learned to approximate the temporal conditional averages of
observed target backgrounds and foregrounds. On the other hand, statistical
learning in imagery domains has been a prevalent approach with high adaptation
to dynamic context transformation, notably using Gaussian Mixture Models (GMM)
with its generalization capabilities. By leveraging both, we propose a novel
method called CDN-MEDAL-net for background modeling and subtraction with two
convolutional neural networks. The first architecture, CDN-GM, is grounded on
an unsupervised GMM statistical learning strategy to describe observed scenes'
salient features. The second one, MEDAL-net, implements a light-weighted
pipeline of online video background subtraction. Our two-stage architecture is
small, but it is very effective with rapid convergence to representations of
intricate motion patterns. Our experiments show that the proposed approach is
not only capable of effectively extracting regions of moving objects in unseen
cases, but it is also very efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Edge-Aware Interactive Colorization against Color-Bleeding Effects. (arXiv:2107.01619v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01619">
<div class="article-summary-box-inner">
<span><p>Deep neural networks for automatic image colorization often suffer from the
color-bleeding artifact, a problematic color spreading near the boundaries
between adjacent objects. Such color-bleeding artifacts debase the reality of
generated outputs, limiting the applicability of colorization models in
practice. Although previous approaches have attempted to address this problem
in an automatic manner, they tend to work only in limited cases where a high
contrast of gray-scale values are given in an input image. Alternatively,
leveraging user interactions would be a promising approach for solving this
color-breeding artifacts. In this paper, we propose a novel edge-enhancing
network for the regions of interest via simple user scribbles indicating where
to enhance. In addition, our method requires a minimal amount of effort from
users for their satisfactory enhancement. Experimental results demonstrate that
our interactive edge-enhancing approach effectively improves the color-bleeding
artifacts compared to the existing baselines across various datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic and Geometric Depth: Detecting Objects in Perspective. (arXiv:2107.14160v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14160">
<div class="article-summary-box-inner">
<span><p>3D object detection is an important capability needed in various practical
applications such as driver assistance systems. Monocular 3D detection, as a
representative general setting among image-based approaches, provides a more
economical solution than conventional settings relying on LiDARs. It has drawn
increasing attention recently but still yields unsatisfactory results. This
paper first presents a systematic study on this problem. We observe that the
current monocular 3D detection can be simplified as an instance depth
estimation problem: The inaccurate instance depth blocks all the other 3D
attribute predictions from improving the overall detection performance.
However, recent methods directly estimate the depth based on isolated instances
or pixels while ignoring the geometric relations across different objects.
These geometric relations can be valuable constraints as the key information
about depth is not directly manifest in the monocular image. Therefore, we
construct geometric relation graphs across predicted objects and use the graph
to facilitate depth estimation. As the preliminary depth estimation of each
instance is usually inaccurate in this ill-posed setting, we incorporate a
probabilistic representation to capture the uncertainty. It provides an
important indicator to identify confident predictions and further guide the
depth propagation. Despite the simplicity of the basic idea, our method obtains
significant improvements on KITTI and nuScenes benchmarks, achieving 1st place
out of all monocular vision-only methods while still maintaining real-time
efficiency. Code and models will be released at
https://github.com/open-mmlab/mmdetection3d.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing object recognition in humans and deep convolutional neural networks -- An eye tracking study. (arXiv:2108.00107v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00107">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks (DCNNs) and the ventral visual pathway
share vast architectural and functional similarities in visual challenges such
as object recognition. Recent insights have demonstrated that both hierarchical
cascades can be compared in terms of both exerted behavior and underlying
activation. However, these approaches ignore key differences in spatial
priorities of information processing. In this proof-of-concept study, we
demonstrate a comparison of human observers (N = 45) and three feedforward
DCNNs through eye tracking and saliency maps. The results reveal fundamentally
different resolutions in both visualization methods that need to be considered
for an insightful comparison. Moreover, we provide evidence that a DCNN with
biologically plausible receptive field sizes called vNet reveals higher
agreement with human viewing behavior as contrasted with a standard ResNet
architecture. We find that image-specific factors such as category, animacy,
arousal, and valence have a direct link to the agreement of spatial object
recognition priorities in humans and DCNNs, while other measures such as
difficulty and general image properties do not. With this approach, we try to
open up new perspectives at the intersection of biological and computer vision
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARAPReg: An As-Rigid-As Possible Regularization Loss for Learning Deformable Shape Generators. (arXiv:2108.09432v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09432">
<div class="article-summary-box-inner">
<span><p>This paper introduces an unsupervised loss for training parametric
deformation shape generators. The key idea is to enforce the preservation of
local rigidity among the generated shapes. Our approach builds on an
approximation of the as-rigid-as possible (or ARAP) deformation energy. We show
how to develop the unsupervised loss via a spectral decomposition of the
Hessian of the ARAP energy. Our loss nicely decouples pose and shape variations
through a robust norm. The loss admits simple closed-form expressions. It is
easy to train and can be plugged into any standard generation models, e.g.,
variational auto-encoder (VAE) and auto-decoder (AD). Experimental results show
that our approach outperforms existing shape generation approaches considerably
on public benchmark datasets of various shape categories such as human, animal
and bone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor Pooling Driven Instance Segmentation Framework for Baggage Threat Recognition. (arXiv:2108.09603v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09603">
<div class="article-summary-box-inner">
<span><p>Automated systems designed for screening contraband items from the X-ray
imagery are still facing difficulties with high clutter, concealment, and
extreme occlusion. In this paper, we addressed this challenge using a novel
multi-scale contour instance segmentation framework that effectively identifies
the cluttered contraband data within the baggage X-ray scans. Unlike standard
models that employ region-based or keypoint-based techniques to generate
multiple boxes around objects, we propose to derive proposals according to the
hierarchy of the regions defined by the contours. The proposed framework is
rigorously validated on three public datasets, dubbed GDXray, SIXray, and
OPIXray, where it outperforms the state-of-the-art methods by achieving the
mean average precision score of 0.9779, 0.9614, and 0.8396, respectively.
Furthermore, to the best of our knowledge, this is the first contour instance
segmentation framework that leverages multi-scale information to recognize
cluttered and concealed contraband data from the colored and grayscale security
X-ray imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Prompt for Vision-Language Models. (arXiv:2109.01134v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01134">
<div class="article-summary-box-inner">
<span><p>Vision-language pre-training has recently emerged as a promising alternative
for representation learning. It shifts from the tradition of using images and
discrete labels for learning a fixed set of weights, seen as visual concepts,
to aligning images and raw text for two separate encoders. Such a paradigm
benefits from a broader source of supervision and allows zero-shot transfer to
downstream tasks since visual concepts can be diametrically generated from
natural language, known as prompt. In this paper, we identify that a major
challenge of deploying such models in practice is prompt engineering. This is
because designing a proper prompt, especially for context words surrounding a
class name, requires domain expertise and typically takes a significant amount
of time for words tuning since a slight change in wording could have a huge
impact on performance. Moreover, different downstream tasks require specific
designs, further hampering the efficiency of deployment. To overcome this
challenge, we propose a novel approach named context optimization (CoOp). The
main idea is to model context in prompts using continuous representations and
perform end-to-end learning from data while keeping the pre-trained parameters
fixed. In this way, the design of task-relevant prompts can be fully automated.
Experiments on 11 datasets show that CoOp effectively turns pre-trained
vision-language models into data-efficient visual learners, requiring as few as
one or two shots to beat hand-crafted prompts with a decent margin and able to
gain significant improvements when using more shots (e.g., at 16 shots the
average gain is around 17% with the highest reaching over 50%). CoOp also
exhibits strong robustness to distribution shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation. (arXiv:2109.01827v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01827">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose GOHOME, a method leveraging graph representations
of the High Definition Map and sparse projections to generate a heatmap output
representing the future position probability distribution for a given agent in
a traffic scene. This heatmap output yields an unconstrained 2D grid
representation of agent future possible locations, allowing inherent
multimodality and a measure of the uncertainty of the prediction. Our
graph-oriented model avoids the high computation burden of representing the
surrounding context as squared images and processing it with classical CNNs,
but focuses instead only on the most probable lanes where the agent could end
up in the immediate future. GOHOME reaches 2$nd$ on Argoverse Motion
Forecasting Benchmark on the MissRate$_6$ metric while achieving significant
speed-up and memory burden diminution compared to Argoverse 1$^{st}$ place
method HOME. We also highlight that heatmap output enables multimodal
ensembling and improve 1$^{st}$ place MissRate$_6$ by more than 15$\%$ with our
best ensemble on Argoverse. Finally, we evaluate and reach state-of-the-art
performance on the other trajectory prediction datasets nuScenes and
Interaction, demonstrating the generalizability of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03201">
<div class="article-summary-box-inner">
<span><p>Transformers, the default model of choices in natural language processing,
have drawn scant attention from the medical imaging community. Given the
ability to exploit long-term dependencies, transformers are promising to help
atypical convolutional neural networks (convnets) to overcome its inherent
shortcomings of spatial inductive bias. However, most of recently proposed
transformer-based segmentation approaches simply treated transformers as
assisted modules to help encode global context into convolutional
representations without investigating how to optimally combine self-attention
(i.e., the core of transformers) with convolution. To address this issue, in
this paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful
segmentation model with an interleaved architecture based on empirical
combination of self-attention and convolution. In practice, nnFormer learns
volumetric representations from 3D local volumes. Compared to the naive
voxel-level self-attention implementation, such volume-based operations help to
reduce the computational complexity by approximate 98% and 99.5% on Synapse and
ACDC datasets, respectively. In comparison to prior-art network configurations,
nnFormer achieves tremendous improvements over previous transformer-based
methods on two commonly used datasets Synapse and ACDC. For instance, nnFormer
outperforms Swin-UNet by over 7 percents on Synapse. Even when compared to
nnUNet, currently the best performing fully-convolutional medical segmentation
network, nnFormer still provides slightly better performance on Synapse and
ACDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Tensor Network Representation for High-Order Tensor Completion. (arXiv:2109.04022v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04022">
<div class="article-summary-box-inner">
<span><p>This work studies the problem of high-dimensional data (referred to as
tensors) completion from partially observed samplings. We consider that a
tensor is a superposition of multiple low-rank components. In particular, each
component can be represented as multilinear connections over several latent
factors and naturally mapped to a specific tensor network (TN) topology. In
this paper, we propose a fundamental tensor decomposition (TD) framework:
Multi-Tensor Network Representation (MTNR), which can be regarded as a linear
combination of a range of TD models, e.g., CANDECOMP/PARAFAC (CP)
decomposition, Tensor Train (TT), and Tensor Ring (TR). Specifically, MTNR
represents a high-order tensor as the addition of multiple TN models, and the
topology of each TN is automatically generated instead of manually
pre-designed. For the optimization phase, an adaptive topology learning (ATL)
algorithm is presented to obtain latent factors of each TN based on a rank
incremental strategy and a projection error measurement strategy. In addition,
we theoretically establish the fundamental multilinear operations for the
tensors with TN representation, and reveal the structural transformation of
MTNR to a single TN. Finally, MTNR is applied to a typical task, tensor
completion, and two effective algorithms are proposed for the exact recovery of
incomplete data based on the Alternating Least Squares (ALS) scheme and
Alternating Direction Method of Multiplier (ADMM) framework. Extensive
numerical experiments on synthetic data and real-world datasets demonstrate the
effectiveness of MTNR compared with the start-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2109.06274v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06274">
<div class="article-summary-box-inner">
<span><p>Automatic methods to segment the vestibular schwannoma (VS) tumors and the
cochlea from magnetic resonance imaging (MRI) are critical to VS treatment
planning. Although supervised methods have achieved satisfactory performance in
VS segmentation, they require full annotations by experts, which is laborious
and time-consuming. In this work, we aim to tackle the VS and cochlea
segmentation problem in an unsupervised domain adaptation setting. Our proposed
method leverages both the image-level domain alignment to minimize the domain
divergence and semi-supervised training to further boost the performance.
Furthermore, we propose to fuse the labels predicted from multiple models via
noisy label correction. Our results on the challenge validation leaderboard
showed that our unsupervised method has achieved promising VS and cochlea
segmentation performance with mean dice score of 0.8261 $\pm$ 0.0416; The mean
dice value for the tumor is 0.8302 $\pm$ 0.0772. This is comparable to the
weakly-supervised based method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SketchHairSalon: Deep Sketch-based Hair Image Synthesis. (arXiv:2109.07874v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07874">
<div class="article-summary-box-inner">
<span><p>Recent deep generative models allow real-time generation of hair images from
sketch inputs. Existing solutions often require a user-provided binary mask to
specify a target hair shape. This not only costs users extra labor but also
fails to capture complicated hair boundaries. Those solutions usually encode
hair structures via orientation maps, which, however, are not very effective to
encode complex structures. We observe that colored hair sketches already
implicitly define target hair shapes as well as hair appearance and are more
flexible to depict hair structures than orientation maps. Based on these
observations, we present SketchHairSalon, a two-stage framework for generating
realistic hair images directly from freehand sketches depicting desired hair
structure and appearance. At the first stage, we train a network to predict a
hair matte from an input hair sketch, with an optional set of non-hair strokes.
At the second stage, another network is trained to synthesize the structure and
appearance of hair images from the input sketch and the generated matte. To
make the networks in the two stages aware of long-term dependency of strokes,
we apply self-attention modules to them. To train these networks, we present a
new dataset containing thousands of annotated hair sketch-image pairs and
corresponding hair mattes. Two efficient methods for sketch completion are
proposed to automatically complete repetitive braided parts and hair strokes,
respectively, thus reducing the workload of users. Based on the trained
networks and the two sketch completion strategies, we build an intuitive
interface to allow even novice users to design visually pleasing hair images
exhibiting various hair structures and appearance via freehand sketches. The
qualitative and quantitative evaluations show the advantages of the proposed
system over the existing or alternative solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V-SlowFast Network for Efficient Visual Sound Separation. (arXiv:2109.08867v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08867">
<div class="article-summary-box-inner">
<span><p>The objective of this paper is to perform visual sound separation: i) we
study visual sound separation on spectrograms of different temporal
resolutions; ii) we propose a new light yet efficient three-stream framework
V-SlowFast that operates on Visual frame, Slow spectrogram, and Fast
spectrogram. The Slow spectrogram captures the coarse temporal resolution while
the Fast spectrogram contains the fine-grained temporal resolution; iii) we
introduce two contrastive objectives to encourage the network to learn
discriminative visual features for separating sounds; iv) we propose an
audio-visual global attention module for audio and visual feature fusion; v)
the introduced V-SlowFast model outperforms previous state-of-the-art in
single-frame based visual sound separation on small- and large-scale datasets:
MUSIC-21, AVE, and VGG-Sound. We also propose a small V-SlowFast architecture
variant, which achieves 74.2% reduction in the number of model parameters and
81.4% reduction in GMACs compared to the previous multi-stage models. Project
page: https://ly-zhu.github.io/V-SlowFast
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DECORAS: detection and characterization of radio-astronomical sources using deep learning. (arXiv:2109.09077v2 [astro-ph.IM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09077">
<div class="article-summary-box-inner">
<span><p>We present DECORAS, a deep learning based approach to detect both point and
extended sources from Very Long Baseline Interferometry (VLBI) observations.
Our approach is based on an encoder-decoder neural network architecture that
uses a low number of convolutional layers to provide a scalable solution for
source detection. In addition, DECORAS performs source characterization in
terms of the position, effective radius and peak brightness of the detected
sources. We have trained and tested the network with images that are based on
realistic Very Long Baseline Array (VLBA) observations at 20 cm. Also, these
images have not gone through any prior de-convolution step and are directly
related to the visibility data via a Fourier transform. We find that the source
catalog generated by DECORAS has a better overall completeness and purity, when
compared to a traditional source detection algorithm. DECORAS is complete at
the 7.5$\sigma$ level, and has an almost factor of two improvement in
reliability at 5.5$\sigma$. We find that DECORAS can recover the position of
the detected sources to within 0.61 $\pm$ 0.69 mas, and the effective radius
and peak surface brightness are recovered to within 20 per cent for 98 and 94
per cent of the sources, respectively. Overall, we find that DECORAS provides a
reliable source detection and characterization solution for future wide-field
VLBI surveys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning. (arXiv:2109.09129v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09129">
<div class="article-summary-box-inner">
<span><p>Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that
affect patients' social abilities. In recent years, deep learning methods have
been employed to detect ASD through functional MRI (fMRI). However, existing
approaches solely concentrated on the abnormal brain functional connections but
ignored the importance of regional activities. Due to this biased prior
knowledge, previous diagnosis models suffered from inter-site heterogeneity and
inter-individual phenotypical differences. To address this issue, we propose a
novel feature extraction method for fMRI that can learn a personalized
lowe-resolution representation of the entire brain networking regarding both
the functional connections and regional activities. First, we abstract the
brain imaging as a graph structure, where nodes represent brain areas and edges
denote functional connections, and downsample it to a sparse network by
hierarchical graph pooling. Subsequently, by assigning each subject with the
extracted features and building edges through inter-individual non-imaging
characteristics, we build a population graph. The non-identically distributed
node features are further recalibrated to node embeddings learned by graph
convolutional networks. By these means, our framework can extract features
directly and efficiently from the entire fMRI and be aware of implicit
inter-individual differences. We have evaluated our framework on the ABIDE-I
dataset with 10-fold cross-validation. The present model has achieved a mean
classification accuracy of 85.95\% and a mean AUC of 0.92, which is better than
the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Cycle-consistent Generative Adversarial Networks for Pan-sharpening. (arXiv:2109.09395v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09395">
<div class="article-summary-box-inner">
<span><p>Deep learning based pan-sharpening has received significant research interest
in recent years. Most of existing methods fall into the supervised learning
framework in which they down-sample the multi-spectral (MS) and panchromatic
(PAN) images and regard the original MS images as ground truths to form
training samples. Although impressive performance could be achieved, they have
difficulties generalizing to the original full-scale images due to the scale
gap, which makes them lack of practicability. In this paper, we propose an
unsupervised generative adversarial framework that learns from the full-scale
images without the ground truths to alleviate this problem. We extract the
modality-specific features from the PAN and MS images with a two-stream
generator, perform fusion in the feature domain, and then reconstruct the
pan-sharpened images. Furthermore, we introduce a novel hybrid loss based on
the cycle-consistency and adversarial scheme to improve the performance.
Comparison experiments with the state-of-the-art methods are conducted on
GaoFen-2 and WorldView-3 satellites. Results demonstrate that the proposed
method can greatly improve the pan-sharpening performance on the full-scale
images, which clearly show its practical value. Codes and datasets will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Annotation Uncertainty with Gaussian Heatmaps in Landmark Localization. (arXiv:2109.09533v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09533">
<div class="article-summary-box-inner">
<span><p>In landmark localization, due to ambiguities in defining their exact
position, landmark annotations may suffer from large observer variabilities,
which result in uncertain annotations. To model the annotation ambiguities of
the training dataset, we propose to learn anisotropic Gaussian parameters
modeling the shape of the target heatmap during optimization. Furthermore, our
method models the prediction uncertainty of individual samples by fitting
anisotropic Gaussian functions to the predicted heatmaps during inference.
Besides state-of-the-art results, our experiments on datasets of hand
radiographs and lateral cephalograms also show that Gaussian functions are
correlated with both localization accuracy and observer variability. As a final
experiment, we show the importance of integrating the uncertainty into decision
making by measuring the influence of the predicted location uncertainty on the
classification of anatomical abnormalities in lateral cephalograms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Trash Detection for Modern Societies using CCTV to Identifying Trash by utilizing Deep Convolutional Neural Network. (arXiv:2109.09611v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09611">
<div class="article-summary-box-inner">
<span><p>To protect the environment from trash pollution, especially in societies, and
to take strict action against the red-handed people who throws the trash. As
modern societies are developing and these societies need a modern solution to
make the environment clean. Artificial intelligence (AI) evolution, especially
in Deep Learning, gives an excellent opportunity to develop real-time trash
detection using CCTV cameras. The inclusion of this project is real-time trash
detection using a deep model of Convolutional Neural Network (CNN). It is used
to obtain eight classes mask, tissue papers, shoppers, boxes, automobile parts,
pampers, bottles, and juices boxes. After detecting the trash, the camera
records the video of that person for ten seconds who throw trash in society.
The challenging part of this paper is preparing a complex custom dataset that
took too much time. The dataset consists of more than 2100 images. The CNN
model was created, labeled, and trained. The detection time accuracy and
average mean precision (mAP) benchmark both models' performance. In
experimental phase the mAP performance and accuracy of the improved CNN model
was superior in all aspects. The model is used on a CCTV camera to detect trash
in real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR. (arXiv:2109.09628v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09628">
<div class="article-summary-box-inner">
<span><p>Self-supervised monocular depth prediction provides a cost-effective solution
to obtain the 3D location of each pixel. However, the existing approaches
usually lead to unsatisfactory accuracy, which is critical for autonomous
robots. In this paper, we propose a novel two-stage network to advance the
self-supervised monocular dense depth learning by leveraging low-cost sparse
(e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly
in a manner of time-consuming iterative post-processing, our model fuses
monocular image features and sparse LiDAR features to predict initial depth
maps. Then, an efficient feed-forward refine network is further designed to
correct the errors in these initial depth maps in pseudo-3D space with
real-time performance. Extensive experiments show that our proposed model
significantly outperforms all the state-of-the-art self-supervised methods, as
well as the sparse-LiDAR-based methods on both self-supervised monocular depth
prediction and completion tasks. With the accurate dense depth prediction, our
model outperforms the state-of-the-art sparse-LiDAR-based method
(Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object
detection on the KITTI Leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging. (arXiv:2109.09658v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09658">
<div class="article-summary-box-inner">
<span><p>The recent advancements in artificial intelligence (AI) combined with the
extensive amount of data generated by today's clinical systems, has led to the
development of imaging AI solutions across the whole value chain of medical
imaging, including image reconstruction, medical image segmentation,
image-based diagnosis and treatment planning. Notwithstanding the successes and
future potential of AI in medical imaging, many stakeholders are concerned of
the potential risks and ethical implications of imaging AI solutions, which are
perceived as complex, opaque, and difficult to comprehend, utilise, and trust
in critical clinical applications. Despite these concerns and risks, there are
currently no concrete guidelines and best practices for guiding future AI
developments in medical imaging towards increased trust, safety and adoption.
To bridge this gap, this paper introduces a careful selection of guiding
principles drawn from the accumulated experiences, consensus, and best
practices from five large European projects on AI in Health Imaging. These
guiding principles are named FUTURE-AI and its building blocks consist of (i)
Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness
and (vi) Explainability. In a step-by-step approach, these guidelines are
further translated into a framework of concrete recommendations for specifying,
developing, evaluating, and deploying technically, clinically and ethically
trustworthy AI solutions into clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trust Your Robots! Predictive Uncertainty Estimation of Neural Networks with Sparse Gaussian Processes. (arXiv:2109.09690v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09690">
<div class="article-summary-box-inner">
<span><p>This paper presents a probabilistic framework to obtain both reliable and
fast uncertainty estimates for predictions with Deep Neural Networks (DNNs).
Our main contribution is a practical and principled combination of DNNs with
sparse Gaussian Processes (GPs). We prove theoretically that DNNs can be seen
as a special case of sparse GPs, namely mixtures of GP experts (MoE-GP), and we
devise a learning algorithm that brings the derived theory into practice. In
experiments from two different robotic tasks -- inverse dynamics of a
manipulator and object detection on a micro-aerial vehicle (MAV) -- we show the
effectiveness of our approach in terms of predictive uncertainty, improved
scalability, and run-time efficiency on a Jetson TX2. We thus argue that our
approach can pave the way towards reliable and fast robot learning systems with
uncertainty awareness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Precise Pruning Points Detection using Semantic-Instance-Aware Plant Models for Grapevine Winter Pruning Automation. (arXiv:2109.07247v1 [cs.RO] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07247">
<div class="article-summary-box-inner">
<span><p>Grapevine winter pruning is a complex task, that requires skilled workers to
execute it correctly. The complexity makes it time consuming. It is an
operation that requires about 80-120 hours per hectare annually, making an
automated robotic system that helps in speeding up the process a crucial tool
in large-size vineyards. We will describe (a) a novel expert annotated dataset
for grapevine segmentation, (b) a state of the art neural network
implementation and (c) generation of pruning points following agronomic rules,
leveraging the simplified structure of the plant. With this approach, we are
able to generate a set of pruning points on the canes, paving the way towards a
correct automation of grapevine winter pruning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Correlation Aggregation: on the Path to Better Graph Neural Networks. (arXiv:2109.09300v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09300">
<div class="article-summary-box-inner">
<span><p>Prior to the introduction of Graph Neural Networks (GNNs), modeling and
analyzing irregular data, particularly graphs, was thought to be the Achilles'
heel of deep learning. The core concept of GNNs is to find a representation by
recursively aggregating the representations of a central node and those of its
neighbors. The core concept of GNNs is to find a representation by recursively
aggregating the representations of a central node and those of its neighbor,
and its success has been demonstrated by many GNNs' designs. However, most of
them only focus on using the first-order information between a node and its
neighbors. In this paper, we introduce a central node permutation variant
function through a frustratingly simple and innocent-looking modification to
the core operation of a GNN, namely the Feature cOrrelation aGgregation (FOG)
module which learns the second-order information from feature correlation
between a node and its neighbors in the pipeline. By adding FOG into existing
variants of GNNs, we empirically verify this second-order information
complements the features generated by original GNNs across a broad set of
benchmarks. A tangible boost in performance of the model is observed where the
model surpasses previous state-of-the-art results by a significant margin while
employing fewer parameters. (e.g., 33.116% improvement on a real-world
molecular dataset using graph convolutional networks).
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-22 23:02:36.579250153 UTC">2021-09-22 23:02:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>