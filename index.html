<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-08-01T01:30:00Z">08-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Eye Gaze Estimation Model Analysis. (arXiv:2207.14373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14373">
<div class="article-summary-box-inner">
<span><p>We explore techniques for eye gaze estimation using machine learning. Eye
gaze estimation is a common problem for various behavior analysis and
human-computer interfaces. The purpose of this work is to discuss various model
types for eye gaze estimation and present the results from predicting gaze
direction using eye landmarks in unconstrained settings. In unconstrained
real-world settings, feature-based and model-based methods are outperformed by
recent appearance-based methods due to factors like illumination changes and
other visual artifacts. We discuss a learning-based method for eye region
landmark localization trained exclusively on synthetic data. We discuss how to
use detected landmarks as input to iterative model-fitting and lightweight
learning-based gaze estimation methods and how to use the model for
person-independent and personalized gaze estimations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14382">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been transformative. They are pre-trained
foundational models that can be adapted with fine tuning to many different
natural language tasks, each of which previously would have required a separate
network model. This is one step closer to the extraordinary versatility of
human language. GPT-3 and more recently LaMDA can carry on dialogs with humans
on many topics after minimal priming with a few examples. However, there has
been a wide range of reactions on whether these LLMs understand what they are
saying or exhibit signs of intelligence. This high variance is exhibited in
three interviews with LLMs reaching wildly different conclusions. A new
possibility was uncovered that could explain this divergence. What appears to
be intelligence in LLMs may in fact be a mirror that reflects the intelligence
of the interviewer, a remarkable twist that could be considered a Reverse
Turing Test. If so, then by studying interviews we may be learning more about
the intelligence and beliefs of the interviewer than the intelligence of the
LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Model Finetuning for Text Classification via Data Filtering. (arXiv:2207.14386v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14386">
<div class="article-summary-box-inner">
<span><p>As model finetuning is central to the modern NLP, we set to maximize its
efficiency. Motivated by training examples are often redundant, we design an
algorithm that filters the examples in a streaming fashion. Our key techniques
are two: (1) automatically determine a training loss threshold for skipping the
backward propagation; and (2) maintain a meta predictor for further skipping
the forward propagation. Incarnated as a three-stage process, on a diverse set
of benchmarks our algorithm reduces the required training examples by up to
5$\times$ while only seeing minor degradation on average. Our method is
effective even for as few as one training epoch, where each training example is
encountered once. It is simple to implement and is compatible with the existing
model finetuning optimizations such as layer freezing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAD: Language Models as Data for Zero-Shot Dialog. (arXiv:2207.14393v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14393">
<div class="article-summary-box-inner">
<span><p>To facilitate zero-shot generalization in taskoriented dialog, this paper
proposes Language Models as Data (LAD). LAD is a paradigm for creating diverse
and accurate synthetic data which conveys the necessary structural constraints
and can be used to train a downstream neural dialog model. LAD leverages GPT-3
to induce linguistic diversity. LAD achieves significant performance gains in
zero-shot settings on intent prediction (+15%), slot filling (+31.4 F-1) and
next action prediction (+11 F1). Furthermore, an interactive human evaluation
shows that training with LAD is competitive with training on human dialogs. LAD
is open-sourced, with the code and data available at
https://github.com/Shikib/lad.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Evaluation of Dialog Track at DSTC9. (arXiv:2207.14403v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14403">
<div class="article-summary-box-inner">
<span><p>The ultimate goal of dialog research is to develop systems that can be
effectively used in interactive settings by real users. To this end, we
introduced the Interactive Evaluation of Dialog Track at the 9th Dialog System
Technology Challenge. This track consisted of two sub-tasks. The first sub-task
involved building knowledge-grounded response generation models. The second
sub-task aimed to extend dialog models beyond static datasets by assessing them
in an interactive setting with real users. Our track challenges participants to
develop strong response generation models and explore strategies that extend
them to back-and-forth interactions with real users. The progression from
static corpora to interactive evaluation introduces unique challenges and
facilitates a more thorough assessment of open-domain dialog systems. This
paper provides an overview of the track, including the methodology and results.
Furthermore, it provides insights into how to best evaluate open-domain dialog
models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Specific Wav2vec 2.0 Fine-tuning For The SE&R 2022 Challenge. (arXiv:2207.14418v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14418">
<div class="article-summary-box-inner">
<span><p>This paper presents our efforts to build a robust ASR model for the shared
task Automatic Speech Recognition for spontaneous and prepared speech &amp; Speech
Emotion Recognition in Portuguese (SE&amp;R 2022). The goal of the challenge is to
advance the ASR research for the Portuguese language, considering prepared and
spontaneous speech in different dialects. Our method consist on fine-tuning an
ASR model in a domain-specific approach, applying gain normalization and
selective noise insertion. The proposed method improved over the strong
baseline provided on the test set in 3 of the 4 tracks available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Comment Inconsistency Detection with BERT and Longformer. (arXiv:2207.14444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14444">
<div class="article-summary-box-inner">
<span><p>Comments, or natural language descriptions of source code, are standard
practice among software developers. By communicating important aspects of the
code such as functionality and usage, comments help with software project
maintenance. However, when the code is modified without an accompanying
correction to the comment, an inconsistency between the comment and code can
arise, which opens up the possibility for developer confusion and bugs. In this
paper, we propose two models based on BERT (Devlin et al., 2019) and Longformer
(Beltagy et al., 2020) to detect such inconsistencies in a natural language
inference (NLI) context. Through an evaluation on a previously established
corpus of comment-method pairs both during and after code changes, we
demonstrate that our models outperform multiple baselines and yield comparable
results to the state-of-the-art models that exclude linguistic and lexical
features. We further discuss ideas for future research in using pretrained
language models for both inconsistency detection and automatic comment
updating.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GTrans: Grouping and Fusing Transformer Layers for Neural Machine Translation. (arXiv:2207.14467v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14467">
<div class="article-summary-box-inner">
<span><p>Transformer structure, stacked by a sequence of encoder and decoder network
layers, achieves significant development in neural machine translation.
However, vanilla Transformer mainly exploits the top-layer representation,
assuming the lower layers provide trivial or redundant information and thus
ignoring the bottom-layer feature that is potentially valuable. In this work,
we propose the Group-Transformer model (GTrans) that flexibly divides
multi-layer representations of both encoder and decoder into different groups
and then fuses these group features to generate target words. To corroborate
the effectiveness of the proposed method, extensive experiments and analytic
experiments are conducted on three bilingual translation benchmarks and two
multilingual translation tasks, including the IWLST-14, IWLST-17, LDC, WMT-14
and OPUS-100 benchmark. Experimental and analytical results demonstrate that
our model outperforms its Transformer counterparts by a consistent gain.
Furthermore, it can be successfully scaled up to 60 encoder layers and 36
decoder layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Azerbaijani Neural Machine Translation. (arXiv:2207.14473v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14473">
<div class="article-summary-box-inner">
<span><p>Little research has been done on Neural Machine Translation (NMT) for
Azerbaijani. In this paper, we benchmark the performance of Azerbaijani-English
NMT systems on a range of techniques and datasets. We evaluate which
segmentation techniques work best on Azerbaijani translation and benchmark the
performance of Azerbaijani NMT models across several domains of text. Our
results show that while Unigram segmentation improves NMT performance and
Azerbaijani translation models scale better with dataset quality than quantity,
cross-domain generalization remains a challenge
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curriculum Learning for Data-Efficient Vision-Language Alignment. (arXiv:2207.14525v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14525">
<div class="article-summary-box-inner">
<span><p>Aligning image and text encoders from scratch using contrastive learning
requires large amounts of paired image-text data. We alleviate this need by
aligning individually pre-trained language and vision representation models
using a much smaller amount of paired data, augmented with a curriculum
learning algorithm to learn fine-grained vision-language alignments. TOnICS
(Training with Ontology-Informed Contrastive Sampling) initially samples
minibatches whose image-text pairs contain a wide variety of objects to learn
object-level alignment, and progressively samples minibatches where all
image-text pairs contain the same object to learn finer-grained contextual
alignment. Aligning pre-trained BERT and VinVL models to each other using
TOnICS outperforms CLIP on downstream zero-shot image retrieval while using
less than 1% as much training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SERCNN: Stacked Embedding Recurrent Convolutional Neural Network in Detecting Depression on Twitter. (arXiv:2207.14535v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14535">
<div class="article-summary-box-inner">
<span><p>Conventional approaches to identify depression are not scalable, and the
public has limited awareness of mental health, especially in developing
countries. As evident by recent studies, social media has the potential to
complement mental health screening on a greater scale. The vast amount of
first-person narrative posts in chronological order can provide insights into
one's thoughts, feelings, behavior, or mood for some time, enabling a better
understanding of depression symptoms reflected in the online space. In this
paper, we propose SERCNN, which improves the user representation by (1)
stacking two pretrained embeddings from different domains and (2) reintroducing
the embedding context to the MLP classifier. Our SERCNN shows great performance
over state-of-the-art and other baselines, achieving 93.7% accuracy in a 5-fold
cross-validation setting. Since not all users share the same level of online
activity, we introduced the concept of a fixed observation window that
quantifies the observation period in a predefined number of posts. With as
minimal as 10 posts per user, SERCNN performed exceptionally well with an 87%
accuracy, which is on par with the BERT model, while having 98% less in the
number of parameters. Our findings open up a promising direction for detecting
depression on social media with a smaller number of posts for inference,
towards creating solutions for a cost-effective and timely intervention. We
hope that our work can bring this research area closer to real-world adoption
in existing clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Phone Recognition from Unpaired Audio and Phone Sequences Based on Generative Adversarial Network. (arXiv:2207.14568v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14568">
<div class="article-summary-box-inner">
<span><p>ASR has been shown to achieve great performance recently. However, most of
them rely on massive paired data, which is not feasible for low-resource
languages worldwide. This paper investigates how to learn directly from
unpaired phone sequences and speech utterances. We design a two-stage iterative
framework. GAN training is adopted in the first stage to find the mapping
relationship between unpaired speech and phone sequence. In the second stage,
another HMM model is introduced to train from the generator's output, which
boosts the performance and provides a better segmentation for the next
iteration. In the experiment, we first investigate different choices of model
designs. Then we compare the framework to different types of baselines: (i)
supervised methods (ii) acoustic unit discovery based methods (iii) methods
learning from unpaired data. Our framework performs consistently better than
all acoustic unit discovery methods and previous methods learning from unpaired
data based on the TIMIT dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pronunciation-aware unique character encoding for RNN Transducer-based Mandarin speech recognition. (arXiv:2207.14578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14578">
<div class="article-summary-box-inner">
<span><p>For Mandarin end-to-end (E2E) automatic speech recognition (ASR) tasks,
compared to character-based modeling units, pronunciation-based modeling units
could improve the sharing of modeling units in model training but meet
homophone problems. In this study, we propose to use a novel
pronunciation-aware unique character encoding for building E2E RNN-T-based
Mandarin ASR systems. The proposed encoding is a combination of
pronunciation-base syllable and character index (CI). By introducing the CI,
the RNN-T model can overcome the homophone problem while utilizing the
pronunciation information for extracting modeling units. With the proposed
encoding, the model outputs can be converted into the final recognition result
through a one-to-one mapping. We conducted experiments on Aishell and MagicData
datasets, and the experimental results showed the effectiveness of the proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KG-NSF: Knowledge Graph Completion with a Negative-Sample-Free Approach. (arXiv:2207.14617v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14617">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph (KG) completion is an important task that greatly benefits
knowledge discovery in many fields (e.g. biomedical research). In recent years,
learning KG embeddings to perform this task has received considerable
attention. Despite the success of KG embedding methods, they predominantly use
negative sampling, resulting in increased computational complexity as well as
biased predictions due to the closed world assumption. To overcome these
limitations, we propose \textbf{KG-NSF}, a negative sampling-free framework for
learning KG embeddings based on the cross-correlation matrices of embedding
vectors. It is shown that the proposed method achieves comparable link
prediction performance to negative sampling-based methods while converging much
faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking. (arXiv:2207.14627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14627">
<div class="article-summary-box-inner">
<span><p>While communicating with a user, a task-oriented dialogue system has to track
the user's needs at each turn according to the conversation history. This
process called dialogue state tracking (DST) is crucial because it directly
informs the downstream dialogue policy. DST has received a lot of interest in
recent years with the text-to-text paradigm emerging as the favored approach.
In this review paper, we first present the task and its associated datasets.
Then, considering a large number of recent publications, we identify highlights
and advances of research in 2021-2022. Although neural approaches have enabled
significant progress, we argue that some critical aspects of dialogue systems
such as generalizability are still underexplored. To motivate future studies,
we propose several research avenues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Spam Reviews on Vietnamese E-commerce Websites. (arXiv:2207.14636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14636">
<div class="article-summary-box-inner">
<span><p>The reviews of customers play an essential role in online shopping. People
often refer to reviews or comments of previous customers to decide whether to
buy a new product. Catching up with this behavior, some people create untruths
and illegitimate reviews to hoax customers about the fake quality of products.
These reviews are called spam reviews, which confuse consumers on online
shopping platforms and negatively affect online shopping behaviors. We propose
the dataset called ViSpamReviews, which has a strict annotation procedure for
detecting spam reviews on e-commerce platforms. Our dataset consists of two
tasks: the binary classification task for detecting whether a review is a spam
or not and the multi-class classification task for identifying the type of
spam. The PhoBERT obtained the highest results on both tasks, 88.93% and
72.17%, respectively, by macro average F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Relation of User and News Representations in Content-Based Neural News Recommendation. (arXiv:2207.14704v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14704">
<div class="article-summary-box-inner">
<span><p>A number of models for neural content-based news recommendation have been
proposed. However, there is limited understanding of the relative importances
of the three main components of such systems (news encoder, user encoder, and
scoring function) and the trade-offs involved. In this paper, we assess the
hypothesis that the most widely used means of matching user and candidate news
representations is not expressive enough. We allow our system to model more
complex relations between the two by assessing more expressive scoring
functions. Across a wide range of baseline and established systems this results
in consistent improvements of around 6 points in AUC. Our results also indicate
a trade-off between the complexity of news encoder and scoring function: A
fairly simple baseline model scores well above 68% AUC on the MIND dataset and
comes within 2 points of the published state-of-the-art, while requiring a
fraction of the computational costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple-hypothesis RNN-T Loss for Unsupervised Fine-tuning and Self-training of Neural Transducer. (arXiv:2207.14736v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14736">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new approach to perform unsupervised fine-tuning and
self-training using unlabeled speech data for recurrent neural network
(RNN)-Transducer (RNN-T) end-to-end (E2E) automatic speech recognition (ASR)
systems. Conventional systems perform fine-tuning/self-training using ASR
hypothesis as the targets when using unlabeled audio data and are susceptible
to the ASR performance of the base model. Here in order to alleviate the
influence of ASR errors while using unlabeled data, we propose a
multiple-hypothesis RNN-T loss that incorporates multiple ASR 1-best hypotheses
into the loss function. For the fine-tuning task, ASR experiments on
Librispeech show that the multiple-hypothesis approach achieves a relative
reduction of 14.2% word error rate (WER) when compared to the single-hypothesis
approach, on the test_other set. For the self-training task, ASR models are
trained using supervised data from Wall Street Journal (WSJ), Aurora-4 along
with CHiME-4 real noisy data as unlabeled data. The multiple-hypothesis
approach yields a relative reduction of 3.3% WER on the CHiME-4's
single-channel real noisy evaluation set when compared with the
single-hypothesis approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rating the Crisis of Online Public Opinion Using a Multi-Level Index System. (arXiv:2207.14740v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14740">
<div class="article-summary-box-inner">
<span><p>Online public opinion usually spreads rapidly and widely, thus a small
incident probably evolves into a large social crisis in a very short time, and
results in a heavy loss in credit or economic aspects. We propose a method to
rate the crisis of online public opinion based on a multi-level index system to
evaluate the impact of events objectively. Firstly, the dissemination mechanism
of online public opinion is explained from the perspective of information
ecology. According to the mechanism, some evaluation indexes are selected
through correlation analysis and principal component analysis. Then, a
classification model of text emotion is created via the training by deep
learning to achieve the accurate quantification of the emotional indexes in the
index system. Finally, based on the multi-level evaluation index system and
grey correlation analysis, we propose a method to rate the crisis of online
public opinion. The experiment with the real-time incident show that this
method can objectively evaluate the emotional tendency of Internet users and
rate the crisis in different dissemination stages of online public opinion. It
is helpful to realizing the crisis warning of online public opinion and timely
blocking the further spread of the crisis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval. (arXiv:2207.14757v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14757">
<div class="article-summary-box-inner">
<span><p>Image-text matching is gaining a leading role among tasks involving the joint
understanding of vision and language. In literature, this task is often used as
a pre-training objective to forge architectures able to jointly deal with
images and texts. Nonetheless, it has a direct downstream application:
cross-modal retrieval, which consists in finding images related to a given
query text or vice-versa. Solving this task is of critical importance in
cross-modal search engines. Many recent methods proposed effective solutions to
the image-text matching problem, mostly using recent large vision-language (VL)
Transformer networks. However, these models are often computationally
expensive, especially at inference time. This prevents their adoption in
large-scale cross-modal retrieval scenarios, where results should be provided
to the user almost instantaneously. In this paper, we propose to fill in the
gap between effectiveness and efficiency by proposing an ALign And DIstill
Network (ALADIN). ALADIN first produces high-effective scores by aligning at
fine-grained level images and texts. Then, it learns a shared embedding space -
where an efficient kNN search can be performed - by distilling the relevance
scores obtained from the fine-grained alignments. We obtained remarkable
results on MS-COCO, showing that our method can compete with state-of-the-art
VL Transformers while being almost 90 times faster. The code for reproducing
our results is available at https://github.com/mesnico/ALADIN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilingual Terminology Extraction from Comparable E-Commerce Corpora. (arXiv:2104.07398v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07398">
<div class="article-summary-box-inner">
<span><p>Bilingual terminologies are important machine translation resources in the
field of e-commerce, which are usually either manually translated or
automatically extracted from parallel data. The human translation is costly and
e-commerce parallel corpora is very scarce. However, the comparable data in
different languages in the same commodity field is abundant. In this paper, we
propose a novel framework of extracting e-commercial bilingual terminologies
from comparable data. Benefiting from the cross-lingual pre-training in
e-commerce, our framework can make full use of the deep semantic relationship
between source-side terminology and target-side sentence to extract
corresponding target terminology. Experimental results on various language
pairs show that our approaches achieve significantly better performance than
various strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dive into Deep Learning. (arXiv:2106.11342v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11342">
<div class="article-summary-box-inner">
<span><p>This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech. (arXiv:2111.04040v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04040">
<div class="article-summary-box-inner">
<span><p>Personalizing a speech synthesis system is a highly desired application,
where the system can generate speech with the user's voice with rare enrolled
recordings. There are two main approaches to build such a system in recent
works: speaker adaptation and speaker encoding. On the one hand, speaker
adaptation methods fine-tune a trained multi-speaker text-to-speech (TTS) model
with few enrolled samples. However, they require at least thousands of
fine-tuning steps for high-quality adaptation, making it hard to apply on
devices. On the other hand, speaker encoding methods encode enrollment
utterances into a speaker embedding. The trained TTS model can synthesize the
user's speech conditioned on the corresponding speaker embedding. Nevertheless,
the speaker encoder suffers from the generalization gap between the seen and
unseen speakers.
</p>
<p>In this paper, we propose applying a meta-learning algorithm to the speaker
adaptation method. More specifically, we use Model Agnostic Meta-Learning
(MAML) as the training algorithm of a multi-speaker TTS model, which aims to
find a great meta-initialization to adapt the model to any few-shot speaker
adaptation tasks quickly. Therefore, we can also adapt the meta-trained TTS
model to unseen speakers efficiently. Our experiments compare the proposed
method (Meta-TTS) with two baselines: a speaker adaptation method baseline and
a speaker encoding method baseline. The evaluation results show that Meta-TTS
can synthesize high speaker-similarity speech from few enrollment samples with
fewer adaptation steps than the speaker adaptation baseline and outperforms the
speaker encoding baseline under the same training scheme. When the speaker
encoder of the baseline is pre-trained with extra 8371 speakers of data,
Meta-TTS can still outperform the baseline on LibriTTS dataset and achieve
comparable results on VCTK dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech. (arXiv:2111.10367v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10367">
<div class="article-summary-box-inner">
<span><p>Progress in speech processing has been facilitated by shared datasets and
benchmarks. Historically these have focused on automatic speech recognition
(ASR), speaker identification, or other lower-level tasks. Interest has been
growing in higher-level spoken language understanding tasks, including using
end-to-end models, but there are fewer annotated datasets for such tasks. At
the same time, recent work shows the possibility of pre-training generic
representations and then fine-tuning for several tasks using relatively little
labeled data. We propose to create a suite of benchmark tasks for Spoken
Language Understanding Evaluation (SLUE) consisting of limited-size labeled
training sets and corresponding evaluation sets. This resource would allow the
research community to track progress, evaluate pre-trained representations for
higher-level tasks, and study open questions such as the utility of pipeline
versus end-to-end approaches. We present the first phase of the SLUE benchmark
suite, consisting of named entity recognition, sentiment analysis, and ASR on
the corresponding datasets. We focus on naturally produced (not read or
synthesized) speech, and freely available datasets. We provide new
transcriptions and annotations on subsets of the VoxCeleb and VoxPopuli
datasets, evaluation metrics and results for baseline models, and an
open-source toolkit to reproduce the baselines and evaluate new models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning for Monolingual End-to-End Automatic Speech Recognition. (arXiv:2112.09427v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09427">
<div class="article-summary-box-inner">
<span><p>Adapting Automatic Speech Recognition (ASR) models to new domains results in
a deterioration of performance on the original domain(s), a phenomenon called
Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to
new accents, dialects, topics, etc. without suffering from CF, making them
unable to be continually enhanced without storing all past data. Fortunately,
Continual Learning (CL) methods, which aim to enable continual adaptation while
overcoming CF, can be used. In this paper, we implement an extensive number of
CL methods for End-to-End ASR and test and compare their ability to extend a
monolingual Hybrid CTC-Transformer model across four new tasks. We find that
the best performing CL method closes the gap between the fine-tuned model
(lower bound) and the model trained jointly on all tasks (upper bound) by more
than 40%, while requiring access to only 0.6% of the original data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v4 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13366">
<div class="article-summary-box-inner">
<span><p>For a long time, different recommendation tasks typically require designing
task-specific architectures and training objectives. As a result, it is hard to
transfer the learned knowledge and representations from one task to another,
thus restricting the generalization ability of existing recommendation
approaches, e.g., a sequential recommendation model can hardly be applied or
transferred to a review generation method. To deal with such issues,
considering that language can describe almost anything and language grounding
is a powerful medium to represent various problems or tasks, we present a
flexible and unified text-to-text paradigm called "Pretrain, Personalized
Prompt, and Predict Paradigm" (P5) for recommendation, which unifies various
recommendation tasks in a shared framework. In P5, all data such as user-item
interactions, user descriptions, item metadata, and user reviews are converted
to a common format -- natural language sequences. The rich information from
natural language assists P5 to capture deeper semantics for personalization and
recommendation. Specifically, P5 learns different tasks with the same language
modeling objective during pretraining. Thus, it serves as the foundation model
for various downstream recommendation tasks, allows easy integration with other
modalities, and enables instruction-based recommendation based on prompts. P5
advances recommender systems from shallow model to deep model to big model, and
will revolutionize the technical form of recommender systems towards universal
recommendation engine. With adaptive personalized prompt for different users,
P5 is able to make predictions in a zero-shot or few-shot manner and largely
reduces the necessity for extensive fine-tuning. On several recommendation
benchmarks, we conduct experiments to show the effectiveness of P5. We release
the source code at \url{https://github.com/jeykigung/P5}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transducer-based language embedding for spoken language identification. (arXiv:2204.03888v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03888">
<div class="article-summary-box-inner">
<span><p>The acoustic and linguistic features are important cues for the spoken
language identification (LID) task. Recent advanced LID systems mainly use
acoustic features that lack the usage of explicit linguistic feature encoding.
In this paper, we propose a novel transducer-based language embedding approach
for LID tasks by integrating an RNN transducer model into a language embedding
framework. Benefiting from the advantages of the RNN transducer's linguistic
representation capability, the proposed method can exploit both
phonetically-aware acoustic features and explicit linguistic features for LID
tasks. Experiments were carried out on the large-scale multilingual LibriSpeech
and VoxLingua107 datasets. Experimental results showed the proposed method
significantly improves the performance on LID tasks with 12% to 59% and 16% to
24% relative improvement on in-domain and cross-domain datasets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation. (arXiv:2204.03958v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03958">
<div class="article-summary-box-inner">
<span><p>This paper introduces a model for incomplete utterance restoration (IUR)
called JET (\textbf{J}oint learning token \textbf{E}xtraction and \textbf{T}ext
generation). Different from prior studies that only work on extraction or
abstraction datasets, we design a simple but effective model, working for both
scenarios of IUR. Our design simulates the nature of IUR, where omitted tokens
from the context contribute to restoration. From this, we construct a Picker
that identifies the omitted tokens. To support the picker, we design two label
creation methods (soft and hard labels), which can work in cases of no
annotation data for the omitted tokens. The restoration is done by using a
Generator with the help of the Picker on joint learning. Promising results on
four benchmark datasets in extraction and abstraction scenarios show that our
model is better than the pretrained T5 and non-generative language model
methods in both rich and limited training data settings.\footnote{The code is
available at \url{https://github.com/shumpei19/JET}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"FIJO": a French Insurance Soft Skill Detection Dataset. (arXiv:2204.05208v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05208">
<div class="article-summary-box-inner">
<span><p>Understanding the evolution of job requirements is becoming more important
for workers, companies and public organizations to follow the fast
transformation of the employment market. Fortunately, recent natural language
processing (NLP) approaches allow for the development of methods to
automatically extract information from job ads and recognize skills more
precisely. However, these efficient approaches need a large amount of annotated
data from the studied domain which is difficult to access, mainly due to
intellectual property. This article proposes a new public dataset, FIJO,
containing insurance job offers, including many soft skill annotations. To
understand the potential of this dataset, we detail some characteristics and
some limitations. Then, we present the results of skill detection algorithms
using a named entity recognition approach and show that transformers-based
models have good token-wise performances on this dataset. Lastly, we analyze
some errors made by our best model to emphasize the difficulties that may arise
when applying NLP approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Email Spam Detection Using Hierarchical Attention Hybrid Deep Learning Method. (arXiv:2204.07390v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07390">
<div class="article-summary-box-inner">
<span><p>Email is one of the most widely used ways to communicate, with millions of
people and businesses relying on it to communicate and share knowledge and
information on a daily basis. Nevertheless, the rise in email users has
occurred a dramatic increase in spam emails in recent years. Processing and
managing emails properly for individuals and companies are getting increasingly
difficult. This article proposes a novel technique for email spam detection
that is based on a combination of convolutional neural networks, gated
recurrent units, and attention mechanisms. During system training, the network
is selectively focused on necessary parts of the email text. The usage of
convolution layers to extract more meaningful, abstract, and generalizable
features by hierarchical representation is the major contribution of this
study. Additionally, this contribution incorporates cross-dataset evaluation,
which enables the generation of more independent performance results from the
model's training dataset. According to cross-dataset evaluation results, the
proposed technique advances the results of the present attention-based
techniques by utilizing temporal convolutions, which give us more flexible
receptive field sizes are utilized. The suggested technique's findings are
compared to those of state-of-the-art models and show that our approach
outperforms them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter Efficient Diff Pruning for Bias Mitigation. (arXiv:2205.15171v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15171">
<div class="article-summary-box-inner">
<span><p>In recent years language models have achieved state of the art performance on
a wide variety of natural language processing tasks. As these models are
continuously growing in size it becomes increasingly important to explore
methods to make them more storage efficient. At the same time their increase
cognitive abilities increase the danger that societal bias existing in datasets
are implicitly encoded in the model weights. We propose an architecture which
deals with these two challenges at the same time using two techniques:
DiffPruning and adversarial Training. The result is a modular architecture
which extends the original DiffPruning setup with and additional sparse
subnetwork applied as a mask to diminish the effects of a predefined protected
attribute at inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Pass Low Latency End-to-End Spoken Language Understanding. (arXiv:2207.06670v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06670">
<div class="article-summary-box-inner">
<span><p>End-to-end (E2E) models are becoming increasingly popular for spoken language
understanding (SLU) systems and are beginning to achieve competitive
performance to pipeline-based approaches. However, recent work has shown that
these models struggle to generalize to new phrasings for the same intent
indicating that models cannot understand the semantic content of the given
utterance. In this work, we incorporated language models pre-trained on
unlabeled text data inside E2E-SLU frameworks to build strong semantic
representations. Incorporating both semantic and acoustic information can
increase the inference time, leading to high latency when deployed for
applications like voice assistants. We developed a 2-pass SLU system that makes
low latency prediction using acoustic information from the few seconds of the
audio in the first pass and makes higher quality prediction in the second pass
by combining semantic and acoustic representations. We take inspiration from
prior work on 2-pass end-to-end speech recognition systems that attends on both
audio and first-pass hypothesis using a deliberation network. The proposed
2-pass SLU system outperforms the acoustic-based SLU model on the Fluent Speech
Commands Challenge Set and SLURP dataset and reduces latency, thus improving
user experience. Our code and models are publicly available as part of the
ESPnet-SLU toolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowing Where and What: Unified Word Block Pretraining for Document Understanding. (arXiv:2207.13979v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13979">
<div class="article-summary-box-inner">
<span><p>Due to the complex layouts of documents, it is challenging to extract
information for documents. Most previous studies develop multimodal pre-trained
models in a self-supervised way. In this paper, we focus on the embedding
learning of word blocks containing text and layout information, and propose
UTel, a language model with Unified TExt and Layout pre-training. Specifically,
we propose two pre-training tasks: Surrounding Word Prediction (SWP) for the
layout learning, and Contrastive learning of Word Embeddings (CWE) for
identifying different word blocks. Moreover, we replace the commonly used 1D
position embedding with a 1D clipped relative position embedding. In this way,
the joint training of Masked Layout-Language Modeling (MLLM) and two newly
proposed tasks enables the interaction between semantic and spatial features in
a unified way. Additionally, the proposed UTel can process arbitrary-length
sequences by removing the 1D position embedding, while maintaining competitive
performance. Extensive experimental results show UTel learns better joint
representations and achieves superior performance than previous methods on
various downstream tasks, though requiring no image modality. Code is available
at \url{https://github.com/taosong2019/UTel}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Type Prediction Leveraging Graph Walks and Entity Descriptions. (arXiv:2207.14094v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14094">
<div class="article-summary-box-inner">
<span><p>The entity type information in Knowledge Graphs (KGs) such as DBpedia,
Freebase, etc. is often incomplete due to automated generation or human
curation. Entity typing is the task of assigning or inferring the semantic type
of an entity in a KG. This paper presents \textit{GRAND}, a novel approach for
entity typing leveraging different graph walk strategies in RDF2vec together
with textual entity descriptions. RDF2vec first generates graph walks and then
uses a language model to obtain embeddings for each node in the graph. This
study shows that the walk generation strategy and the embedding model have a
significant effect on the performance of the entity typing task. The proposed
approach outperforms the baseline approaches on the benchmark datasets DBpedia
and FIGER for entity typing in KGs for both fine-grained and coarse-grained
classes. The results show that the combination of order-aware RDF2vec variants
together with the contextual embeddings of the textual entity descriptions
achieve the best results.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse Reinforcement Learning from Diverse Third-Person Videos via Graph Abstraction. (arXiv:2207.14299v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14299">
<div class="article-summary-box-inner">
<span><p>Research on Inverse Reinforcement Learning (IRL) from third-person videos has
shown encouraging results on removing the need for manual reward design for
robotic tasks. However, most prior works are still limited by training from a
relatively restricted domain of videos. In this paper, we argue that the true
potential of third-person IRL lies in increasing the diversity of videos for
better scaling. To learn a reward function from diverse videos, we propose to
perform graph abstraction on the videos followed by temporal matching in the
graph space to measure the task progress. Our insight is that a task can be
described by entity interactions that form a graph, and this graph abstraction
can help remove irrelevant information such as textures, resulting in more
robust reward functions. We evaluate our approach, GraphIRL, on
cross-embodiment learning in X-MAGICAL and learning from human demonstrations
for real-robot manipulation. We show significant improvements in robustness to
diverse video demonstrations over previous approaches, and even achieve better
results than manual reward design on a real robot pushing task. Videos are
available at https://sateeshkumar21.github.io/GraphIRL .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation. (arXiv:2207.14315v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14315">
<div class="article-summary-box-inner">
<span><p>Visual anomaly detection is commonly used in industrial quality inspection.
In this paper, we present a new dataset as well as a new self-supervised
learning method for ImageNet pre-training to improve anomaly detection and
segmentation in 1-class and 2-class 5/10/high-shot training setups. We release
the Visual Anomaly (VisA) Dataset consisting of 10,821 high-resolution color
images (9,621 normal and 1,200 anomalous samples) covering 12 objects in 3
domains, making it the largest industrial anomaly detection dataset to date.
Both image and pixel-level labels are provided. We also propose a new
self-supervised framework - SPot-the-difference (SPD) - which can regularize
contrastive self-supervised pre-training, such as SimSiam, MoCo and SimCLR, to
be more suitable for anomaly detection tasks. Our experiments on VisA and
MVTec-AD dataset show that SPD consistently improves these contrastive
pre-training baselines and even the supervised pre-training. For example, SPD
improves Area Under the Precision-Recall curve (AU-PR) for anomaly segmentation
by 5.9% and 6.8% over SimSiam and supervised pre-training respectively in the
2-class high-shot regime. We open-source the project at
<a href="http://github.com/amazon-research/spot-diff">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aztec curve: proposal for a new space-filling curve. (arXiv:2207.14345v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14345">
<div class="article-summary-box-inner">
<span><p>Different space-filling curves (SFCs) are briefly reviewed in this paper, and
a new one is proposed. A century has passed between the inception of this kind
of curves, since then they have been found useful in computer science,
particularly in data storage and indexing due to their clustering properties,
being Hilbert curve the most well-known member of the family of fractals. The
proposed Aztec curve, with similar characteristics to the Hilbert's curve, is
introduced in this paper, accompanied by a grammatical description for its
construction. It yields the possibility of creating bi-dimensional clusters,
not available for Hilbert nor Peano curves. Additional to this, a case of
application on the scope of Compressed Sensing is implemented, in which the use
of Hilbert curve is contrasted with Aztec curve, having a similar performance,
and positioning the Aztec curve as viable and a new alternative for future
exploitation on applications that make use of SFC's.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training a universal instance segmentation network for live cell images of various cell types and imaging modalities. (arXiv:2207.14347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14347">
<div class="article-summary-box-inner">
<span><p>We share our recent findings in an attempt to train a universal segmentation
network for various cell types and imaging modalities. Our method was built on
the generalized U-Net architecture, which allows the evaluation of each
component individually. We modified the traditional binary training targets to
include three classes for direct instance segmentation. Detailed experiments
were performed regarding training schemes, training settings, network
backbones, and individual modules on the segmentation performance. Our proposed
training scheme draws minibatches in turn from each dataset, and the gradients
are accumulated before an optimization step. We found that the key to training
a universal network is all-time supervision on all datasets, and it is
necessary to sample each dataset in an unbiased way. Our experiments also
suggest that there might exist common features to define cell boundaries across
cell types and imaging modalities, which could allow application of trained
models to totally unseen datasets. A few training tricks can further boost the
segmentation performance, including uneven class weights in the cross-entropy
loss function, well-designed learning rate scheduler, larger image crops for
contextual information, and additional loss terms for unbalanced classes. We
also found that segmentation performance can benefit from group normalization
layer and Atrous Spatial Pyramid Pooling module, thanks to their more reliable
statistics estimation and improved semantic understanding, respectively. We
participated in the 6th Cell Tracking Challenge (CTC) held at IEEE
International Symposium on Biomedical Imaging (ISBI) 2021 using one of the
developed variants. Our method was evaluated as the best runner up during the
initial submission for the primary track, and also secured the 3rd place in an
additional round of competition in preparation for the summary publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eye Gaze Estimation Model Analysis. (arXiv:2207.14373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14373">
<div class="article-summary-box-inner">
<span><p>We explore techniques for eye gaze estimation using machine learning. Eye
gaze estimation is a common problem for various behavior analysis and
human-computer interfaces. The purpose of this work is to discuss various model
types for eye gaze estimation and present the results from predicting gaze
direction using eye landmarks in unconstrained settings. In unconstrained
real-world settings, feature-based and model-based methods are outperformed by
recent appearance-based methods due to factors like illumination changes and
other visual artifacts. We discuss a learning-based method for eye region
landmark localization trained exclusively on synthetic data. We discuss how to
use detected landmarks as input to iterative model-fitting and lightweight
learning-based gaze estimation methods and how to use the model for
person-independent and personalized gaze estimations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pro-tuning: Unified Prompt Tuning for Vision Tasks. (arXiv:2207.14381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14381">
<div class="article-summary-box-inner">
<span><p>In computer vision, fine-tuning is the de-facto approach to leverage
pre-trained vision models to perform downstream tasks. However, deploying it in
practice is quite challenging, due to adopting parameter inefficient global
update and heavily relying on high-quality downstream data. Recently,
prompt-based learning, which adds a task-relevant prompt to adapt the
downstream tasks to pre-trained models, has drastically boosted the performance
of many natural language downstream tasks. In this work, we extend this notable
transfer ability benefited from prompt into vision models as an alternative to
fine-tuning. To this end, we propose parameter-efficient Prompt tuning
(Pro-tuning) to adapt frozen vision models to various downstream vision tasks.
The key to Pro-tuning is prompt-based tuning, i.e., learning task-specific
vision prompts for downstream input images with the pre-trained model frozen.
By only training a few additional parameters, it can work on diverse CNN-based
and Transformer-based architectures. Extensive experiments evidence that
Pro-tuning outperforms fine-tuning in a broad range of vision tasks and
scenarios, including image classification (generic objects, class imbalance,
image corruption, adversarial robustness, and out-of-distribution
generalization), and dense prediction tasks such as object detection and
semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Generative Approach to Oversampling in Ptychography. (arXiv:2207.14392v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14392">
<div class="article-summary-box-inner">
<span><p>Ptychography is a well-studied phase imaging method that makes non-invasive
imaging possible at a nanometer scale. It has developed into a mainstream
technique with various applications across a range of areas such as material
science or the defense industry. One major drawback of ptychography is the long
data acquisition time due to the high overlap requirement between adjacent
illumination areas to achieve a reasonable reconstruction. Traditional
approaches with reduced overlap between scanning areas result in
reconstructions with artifacts. In this paper, we propose complementing
sparsely acquired or undersampled data with data sampled from a deep generative
network to satisfy the oversampling requirement in ptychography. Because the
deep generative network is pre-trained and its output can be computed as we
collect data, the experimental data and the time to acquire the data can be
reduced. We validate the method by presenting the reconstruction quality
compared to the previously proposed and traditional approaches and comment on
the strengths and drawbacks of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low Cost Embedded Vision System For Location And Tracking Of A Color Object. (arXiv:2207.14396v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14396">
<div class="article-summary-box-inner">
<span><p>This paper describes the development of an embedded vision system for
detection, location, and tracking of a color object; it makes use of a single
32-bit microprocessor to acquire image data, process, and perform actions
according to the interpreted data. The system is intended for applications that
need to make use of artificial vision for detection, location and tracking of a
color object and its objective is to have achieve at reduced terms of size,
power consumption, and cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning for understanding multilabel imbalanced Chest X-ray datasets. (arXiv:2207.14408v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14408">
<div class="article-summary-box-inner">
<span><p>Over the last few years, convolutional neural networks (CNNs) have dominated
the field of computer vision thanks to their ability to extract features and
their outstanding performance in classification problems, for example in the
automatic analysis of X-rays. Unfortunately, these neural networks are
considered black-box algorithms, i.e. it is impossible to understand how the
algorithm has achieved the final result. To apply these algorithms in different
fields and test how the methodology works, we need to use eXplainable AI
techniques. Most of the work in the medical field focuses on binary or
multiclass classification problems. However, in many real-life situations, such
as chest X-rays, radiological signs of different diseases can appear at the
same time. This gives rise to what is known as "multilabel classification
problems". A disadvantage of these tasks is class imbalance, i.e. different
labels do not have the same number of samples. The main contribution of this
paper is a Deep Learning methodology for imbalanced, multilabel chest X-ray
datasets. It establishes a baseline for the currently underutilised PadChest
dataset and a new eXplainable AI technique based on heatmaps. This technique
also includes probabilities and inter-model matching. The results of our system
are promising, especially considering the number of labels used. Furthermore,
the heatmaps match the expected areas, i.e. they mark the areas that an expert
would use to make the decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image. (arXiv:2207.14425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14425">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate an open research task of generating 3D cartoon
face shapes from single 2D GAN generated human faces and without 3D
supervision, where we can also manipulate the facial expressions of the 3D
shapes. To this end, we discover the semantic meanings of StyleGAN latent
space, such that we are able to produce face images of various expressions,
poses, and lighting by controlling the latent codes. Specifically, we first
finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding
the same latent codes to face and cartoon generation models, we aim to realize
the translation from 2D human face images to cartoon styled avatars. We then
discover semantic directions of the GAN latent space, in an attempt to change
the facial expressions while preserving the original identity. As we do not
have any 3D annotations for cartoon faces, we manipulate the latent codes to
generate images with different poses and lighting, such that we can reconstruct
the 3D cartoon face shapes. We validate the efficacy of our method on three
cartoon datasets qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paired Cross-Modal Data Augmentation for Fine-Grained Image-to-Text Retrieval. (arXiv:2207.14428v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14428">
<div class="article-summary-box-inner">
<span><p>This paper investigates an open research problem of generating text-image
pairs to improve the training of fine-grained image-to-text cross-modal
retrieval task, and proposes a novel framework for paired data augmentation by
uncovering the hidden semantic information of StyleGAN2 model. Specifically, we
first train a StyleGAN2 model on the given dataset. We then project the real
images back to the latent space of StyleGAN2 to obtain the latent codes. To
make the generated images manipulatable, we further introduce a latent space
alignment module to learn the alignment between StyleGAN2 latent codes and the
corresponding textual caption features. When we do online paired data
augmentation, we first generate augmented text through random token
replacement, then pass the augmented text into the latent space alignment
module to output the latent codes, which are finally fed to StyleGAN2 to
generate the augmented images. We evaluate the efficacy of our augmented data
approach on two public cross-modal retrieval datasets, in which the promising
experimental results demonstrate the augmented text-image pair data can be
trained together with the original data to boost the image-to-text cross-modal
retrieval performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-Based Small Bowel Path Tracking with Cylindrical Constraints. (arXiv:2207.14436v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14436">
<div class="article-summary-box-inner">
<span><p>We present a new graph-based method for small bowel path tracking based on
cylindrical constraints. A distinctive characteristic of the small bowel
compared to other organs is the contact between parts of itself along its
course, which makes the path tracking difficult together with the indistinct
appearance of the wall. It causes the tracked path to easily cross over the
walls when relying on low-level features like the wall detection. To circumvent
this, a series of cylinders that are fitted along the course of the small bowel
are used to guide the tracking to more reliable directions. It is implemented
as soft constraints using a new cost function. The proposed method is evaluated
against ground-truth paths that are all connected from start to end of the
small bowel for 10 abdominal CT scans. The proposed method showed clear
improvements compared to the baseline method in tracking the path without
making an error. Improvements of 6.6% and 17.0%, in terms of the tracked
length, were observed for two different settings related to the small bowel
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset and Evaluation algorithm design for GOALS Challenge. (arXiv:2207.14447v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14447">
<div class="article-summary-box-inner">
<span><p>Glaucoma causes irreversible vision loss due to damage to the optic nerve,
and there is no cure for glaucoma.OCT imaging modality is an essential
technique for assessing glaucomatous damage since it aids in quantifying fundus
structures. To promote the research of AI technology in the field of
OCT-assisted diagnosis of glaucoma, we held a Glaucoma OCT Analysis and Layer
Segmentation (GOALS) Challenge in conjunction with the International Conference
on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022 to
provide data and corresponding annotations for researchers studying layer
segmentation from OCT images and the classification of glaucoma. This paper
describes the released 300 circumpapillary OCT images, the baselines of the two
sub-tasks, and the evaluation methodology. The GOALS Challenge is accessible at
https://aistudio.baidu.com/aistudio/competition/detail/230.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PC-GANs: Progressive Compensation Generative Adversarial Networks for Pan-sharpening. (arXiv:2207.14451v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14451">
<div class="article-summary-box-inner">
<span><p>The fusion of multispectral and panchromatic images is always dubbed
pansharpening. Most of the available deep learning-based pan-sharpening methods
sharpen the multispectral images through a one-step scheme, which strongly
depends on the reconstruction ability of the network. However, remote sensing
images always have large variations, as a result, these one-step methods are
vulnerable to the error accumulation and thus incapable of preserving spatial
details as well as the spectral information. In this paper, we propose a novel
two-step model for pan-sharpening that sharpens the MS image through the
progressive compensation of the spatial and spectral information. Firstly, a
deep multiscale guided generative adversarial network is used to preliminarily
enhance the spatial resolution of the MS image. Starting from the pre-sharpened
MS image in the coarse domain, our approach then progressively refines the
spatial and spectral residuals over a couple of generative adversarial networks
(GANs) that have reverse architectures. The whole model is composed of triple
GANs, and based on the specific architecture, a joint compensation loss
function is designed to enable the triple GANs to be trained simultaneously.
Moreover, the spatial-spectral residual compensation structure proposed in this
paper can be extended to other pan-sharpening methods to further enhance their
fusion results. Extensive experiments are performed on different datasets and
the results demonstrate the effectiveness and efficiency of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-based Occluded Person Re-identification: A Survey. (arXiv:2207.14452v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14452">
<div class="article-summary-box-inner">
<span><p>Occluded person re-identification (Re-ID) aims at addressing the occlusion
problem when retrieving the person of interest across multiple cameras. With
the promotion of deep learning technology and the increasing demand for
intelligent video surveillance, the frequent occlusion in real-world
applications has made occluded person Re-ID draw considerable interest from
researchers. A large number of occluded person Re-ID methods have been proposed
while there are few surveys that focus on occlusion. To fill this gap and help
boost future research, this paper provides a systematic survey of occluded
person Re-ID. Through an in-depth analysis of the occlusion in person Re-ID,
most existing methods are found to only consider part of the problems brought
by occlusion. Therefore, we review occlusion-related person Re-ID methods from
the perspective of issues and solutions. We summarize four issues caused by
occlusion in person Re-ID, i.e., position misalignment, scale misalignment,
noisy information, and missing information. The occlusion-related methods
addressing different issues are then categorized and introduced accordingly.
After that, we summarize and compare the performance of recent occluded person
Re-ID methods on four popular datasets: Partial-ReID, Partial-iLIDS,
Occluded-ReID, and Occluded-DukeMTMC. Finally, we provide insights on promising
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Density-Distance Fields. (arXiv:2207.14455v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14455">
<div class="article-summary-box-inner">
<span><p>The success of neural fields for 3D vision tasks is now indisputable.
Following this trend, several methods aiming for visual localization (e.g.,
SLAM) have been proposed to estimate distance or density fields using neural
fields. However, it is difficult to achieve high localization performance by
only density fields-based methods such as Neural Radiance Field (NeRF) since
they do not provide density gradient in most empty regions. On the other hand,
distance field-based methods such as Neural Implicit Surface (NeuS) have
limitations in objects' surface shapes. This paper proposes Neural
Density-Distance Field (NeDDF), a novel 3D representation that reciprocally
constrains the distance and density fields. We extend distance field
formulation to shapes with no explicit boundary surface, such as fur or smoke,
which enable explicit conversion from distance field to density field.
Consistent distance and density fields realized by explicit conversion enable
both robustness to initial values and high-quality registration. Furthermore,
the consistency between fields allows fast convergence from sparse point
clouds. Experiments show that NeDDF can achieve high localization performance
while providing comparable results to NeRF on novel view synthesis. The code is
available at https://github.com/ueda0319/neddf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Complexity Loeffler DCT Approximations for Image and Video Coding. (arXiv:2207.14463v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14463">
<div class="article-summary-box-inner">
<span><p>This paper introduced a matrix parametrization method based on the Loeffler
discrete cosine transform (DCT) algorithm. As a result, a new class of
eight-point DCT approximations was proposed, capable of unifying the
mathematical formalism of several eight-point DCT approximations archived in
the literature. Pareto-efficient DCT approximations are obtained through
multicriteria optimization, where computational complexity, proximity, and
coding performance are considered. Efficient approximations and their scaled
16- and 32-point versions are embedded into image and video encoders, including
a JPEG-like codec and H.264/AVC and H.265/HEVC standards. Results are compared
to the unmodified standard codecs. Efficient approximations are mapped and
implemented on a Xilinx VLX240T FPGA and evaluated for area, speed, and power
consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Retrieval Prompt Tuning. (arXiv:2207.14465v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14465">
<div class="article-summary-box-inner">
<span><p>Fine-grained object retrieval aims to learn discriminative representation to
retrieve visually similar objects. However, existing top-performing works
usually impose pairwise similarities on the semantic embedding spaces to
continually fine-tune the entire model in limited-data regimes, thus resulting
in easily converging to suboptimal solutions. In this paper, we develop
Fine-grained Retrieval Prompt Tuning (FRPT), which steers a frozen pre-trained
model to perform the fine-grained retrieval task from the perspectives of
sample prompt and feature adaptation. Specifically, FRPT only needs to learn
fewer parameters in the prompt and adaptation instead of fine-tuning the entire
model, thus solving the convergence to suboptimal solutions caused by
fine-tuning the entire model. Technically, as sample prompts, a structure
perturbation prompt (SPP) is introduced to zoom and even exaggerate some pixels
contributing to category prediction via a content-aware inhomogeneous sampling
operation. In this way, SPP can make the fine-grained retrieval task aided by
the perturbation prompts close to the solved task during the original
pre-training. Besides, a category-specific awareness head is proposed and
regarded as feature adaptation, which removes the species discrepancies in the
features extracted by the pre-trained model using instance normalization, and
thus makes the optimized features only include the discrepancies among
subcategories. Extensive experiments demonstrate that our FRPT with fewer
learnable parameters achieves the state-of-the-art performance on three
widely-used fine-grained datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Domain-agnostic Depth Completion. (arXiv:2207.14466v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14466">
<div class="article-summary-box-inner">
<span><p>Existing depth completion methods are often targeted at a specific sparse
depth type, and generalize poorly across task domains. We present a method to
complete sparse/semi-dense, noisy, and potentially low-resolution depth maps
obtained by various range sensors, including those in modern mobile phones, or
by multi-view reconstruction algorithms. Our method leverages a data driven
prior in the form of a single image depth prediction network trained on
large-scale datasets, the output of which is used as an input to our model. We
propose an effective training scheme where we simulate various sparsity
patterns in typical task domains. In addition, we design two new benchmarks to
evaluate the generalizability and the robustness of depth completion methods.
Our simple method shows superior cross-domain generalization ability against
state-of-the-art depth completion methods, introducing a practical solution to
high quality depth capture on a mobile device. Code is available at:
https://github.com/YvanYin/FillDepth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image Segmentation. (arXiv:2207.14472v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14472">
<div class="article-summary-box-inner">
<span><p>Automatic tumor or lesion segmentation is a crucial step in medical image
analysis for computer-aided diagnosis. Although the existing methods based on
Convolutional Neural Networks (CNNs) have achieved the state-of-the-art
performance, many challenges still remain in medical tumor segmentation. This
is because, although the human visual system can detect symmetries in 2D images
effectively, regular CNNs can only exploit translation invariance, overlooking
further inherent symmetries existing in medical images such as rotations and
reflections. To solve this problem, we propose a novel group equivariant
segmentation framework by encoding those inherent symmetries for learning more
precise representations. First, kernel-based equivariant operations are devised
on each orientation, which allows it to effectively address the gaps of
learning symmetries in existing approaches. Then, to keep segmentation networks
globally equivariant, we design distinctive group layers with layer-wise
symmetry constraints. Finally, based on our novel framework, extensive
experiments conducted on real-world clinical data demonstrate that a Group
Equivariant Res-UNet (named GER-UNet) outperforms its regular CNN-based
counterpart and the state-of-the-art segmentation methods in the tasks of
hepatic tumor segmentation, COVID-19 lung infection segmentation and retinal
vessel detection. More importantly, the newly built GER-UNet also shows
potential in reducing the sample complexity and the redundancy of filters,
upgrading current segmentation CNNs and delineating organs on other medical
imaging modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Centrality and Consistency: Two-Stage Clean Samples Identification for Learning with Instance-Dependent Noisy Labels. (arXiv:2207.14476v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14476">
<div class="article-summary-box-inner">
<span><p>Deep models trained with noisy labels are prone to over-fitting and struggle
in generalization. Most existing solutions are based on an ideal assumption
that the label noise is class-conditional, i.e., instances of the same class
share the same noise model, and are independent of features. While in practice,
the real-world noise patterns are usually more fine-grained as
instance-dependent ones, which poses a big challenge, especially in the
presence of inter-class imbalance. In this paper, we propose a two-stage clean
samples identification method to address the aforementioned challenge. First,
we employ a class-level feature clustering procedure for the early
identification of clean samples that are near the class-wise prediction
centers. Notably, we address the class imbalance problem by aggregating rare
classes according to their prediction entropy. Second, for the remaining clean
samples that are close to the ground truth class boundary (usually mixed with
the samples with instance-dependent noises), we propose a novel
consistency-based classification method that identifies them using the
consistency of two classifier heads: the higher the consistency, the larger the
probability that a sample is clean. Extensive experiments on several
challenging benchmarks demonstrate the superior performance of our method
against the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCSN: Global Context Aware Segmentation by Learning the Fourier Coefficients of Objects in Medical Images. (arXiv:2207.14477v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14477">
<div class="article-summary-box-inner">
<span><p>The encoder-decoder model is a commonly used Deep Neural Network (DNN) model
for medical image segmentation. Conventional encoder-decoder models make
pixel-wise predictions focusing heavily on local patterns around the pixel.
This makes it challenging to give segmentation that preserves the object's
shape and topology, which often requires an understanding of the global context
of the object. In this work, we propose a Fourier Coefficient Segmentation
Network~(FCSN) -- a novel DNN-based model that segments an object by learning
the complex Fourier coefficients of the object's masks. The Fourier
coefficients are calculated by integrating over the whole contour. Therefore,
for our model to make a precise estimation of the coefficients, the model is
motivated to incorporate the global context of the object, leading to a more
accurate segmentation of the object's shape. This global context awareness also
makes our model robust to unseen local perturbations during inference, such as
additive noise or motion blur that are prevalent in medical images. When FCSN
is compared with other state-of-the-art models (UNet+, DeepLabV3+, UNETR) on 3
medical image segmentation tasks (ISIC\_2018, RIM\_CUP, RIM\_DISC), FCSN
attains significantly lower Hausdorff scores of 19.14 (6\%), 17.42 (6\%), and
9.16 (14\%) on the 3 tasks, respectively. Moreover, FCSN is lightweight by
discarding the decoder module, which incurs significant computational overhead.
FCSN only requires 22.2M parameters, 82M and 10M fewer parameters than UNETR
and DeepLabV3+. FCSN attains inference and training speeds of 1.6ms/img and
6.3ms/img, that is 8$\times$ and 3$\times$ faster than UNet and UNETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleAM: Perception-Oriented Unsupervised Domain Adaption for Non-reference Image Quality Assessment. (arXiv:2207.14489v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14489">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have shown great potential in non-reference image
quality assessment (NR-IQA). However, the annotation of NR-IQA is
labor-intensive and time-consuming, which severely limits their application
especially for authentic images. To relieve the dependence on quality
annotation, some works have applied unsupervised domain adaptation (UDA) to
NR-IQA. However, the above methods ignore that the alignment space used in
classification is sub-optimal, since the space is not elaborately designed for
perception. To solve this challenge, we propose an effective
perception-oriented unsupervised domain adaptation method StyleAM for NR-IQA,
which transfers sufficient knowledge from label-rich source domain data to
label-free target domain images via Style Alignment and Mixup. Specifically, we
find a more compact and reliable space i.e., feature style space for
perception-oriented UDA based on an interesting/amazing observation, that the
feature style (i.e., the mean and variance) of the deep layer in DNNs is
exactly associated with the quality score in NR-IQA. Therefore, we propose to
align the source and target domains in a more perceptual-oriented space i.e.,
the feature style space, to reduce the intervention from other
quality-irrelevant feature factors. Furthermore, to increase the consistency
between quality score and its feature style, we also propose a novel feature
augmentation strategy Style Mixup, which mixes the feature styles (i.e., the
mean and variance) before the last layer of DNNs together with mixing their
labels. Extensive experimental results on two typical cross-domain settings
(i.e., synthetic to authentic, and multiple distortions to one distortion) have
demonstrated the effectiveness of our proposed StyleAM on NR-IQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conservative Generator, Progressive Discriminator: Coordination of Adversaries in Few-shot Incremental Image Synthesis. (arXiv:2207.14491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14491">
<div class="article-summary-box-inner">
<span><p>The capacity to learn incrementally from an online stream of data is an
envied trait of human learners, as deep neural networks typically suffer from
catastrophic forgetting and stability-plasticity dilemma. Several works have
previously explored incremental few-shot learning, a task with greater
challenges due to data constraint, mostly in classification setting with mild
success. In this work, we study the underrepresented task of generative
incremental few-shot learning. To effectively handle the inherent challenges of
incremental learning and few-shot learning, we propose a novel framework named
ConPro that leverages the two-player nature of GANs. Specifically, we design a
conservative generator that preserves past knowledge in parameter and compute
efficient manner, and a progressive discriminator that learns to reason
semantic distances between past and present task samples, minimizing
overfitting with few data points and pursuing good forward transfer. We present
experiments to validate the effectiveness of ConPro.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reference-Guided Texture and Structure Inference for Image Inpainting. (arXiv:2207.14498v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14498">
<div class="article-summary-box-inner">
<span><p>Existing learning-based image inpainting methods are still in challenge when
facing complex semantic environments and diverse hole patterns. The prior
information learned from the large scale training data is still insufficient
for these situations. Reference images captured covering the same scenes share
similar texture and structure priors with the corrupted images, which offers
new prospects for the image inpainting tasks. Inspired by this, we first build
a benchmark dataset containing 10K pairs of input and reference images for
reference-guided inpainting. Then we adopt an encoder-decoder structure to
separately infer the texture and structure features of the input image
considering their pattern discrepancy of texture and structure during
inpainting. A feature alignment module is further designed to refine these
features of the input image with the guidance of a reference image. Both
quantitative and qualitative evaluations demonstrate the superiority of our
method over the state-of-the-art methods in terms of completing complex holes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Difficulty Based Methods for Long-Tailed Visual Recognition. (arXiv:2207.14499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14499">
<div class="article-summary-box-inner">
<span><p>Long-tailed datasets are very frequently encountered in real-world use cases
where few classes or categories (known as majority or head classes) have higher
number of data samples compared to the other classes (known as minority or tail
classes). Training deep neural networks on such datasets gives results biased
towards the head classes. So far, researchers have come up with multiple
weighted loss and data re-sampling techniques in efforts to reduce the bias.
However, most of such techniques assume that the tail classes are always the
most difficult classes to learn and therefore need more weightage or attention.
Here, we argue that the assumption might not always hold true. Therefore, we
propose a novel approach to dynamically measure the instantaneous difficulty of
each class during the training phase of the model. Further, we use the
difficulty measures of each class to design a novel weighted loss technique
called `class-wise difficulty based weighted (CDB-W) loss' and a novel data
sampling technique called `class-wise difficulty based sampling (CDB-S)'. To
verify the wide-scale usability of our CDB methods, we conducted extensive
experiments on multiple tasks such as image classification, object detection,
instance segmentation and video-action classification. Results verified that
CDB-W loss and CDB-S could achieve state-of-the-art results on many
class-imbalanced datasets such as ImageNet-LT, LVIS and EGTEA, that resemble
real-world use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transfer Learning-Based Approach to Marine Vessel Re-Identification. (arXiv:2207.14500v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14500">
<div class="article-summary-box-inner">
<span><p>Marine vessel re-identification technology is an important component of
intelligent shipping systems and an important part of the visual perception
tasks required for marine surveillance. However, unlike the situation on land,
the maritime environment is complex and variable with fewer samples, and it is
more difficult to perform vessel re-identification at sea. Therefore, this
paper proposes a transfer dynamic alignment algorithm and simulates the swaying
situation of vessels at sea, using a well-camouflaged and similar warship as
the test target to improve the recognition difficulty and thus cope with the
impact caused by complex sea conditions, and discusses the effect of different
types of vessels as transfer objects. The experimental results show that the
improved algorithm improves the mean average accuracy (mAP) by 10.2% and the
first hit rate (Rank1) by 4.9% on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPU-accelerated SIFT-aided source identification of stabilized videos. (arXiv:2207.14507v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14507">
<div class="article-summary-box-inner">
<span><p>Video stabilization is an in-camera processing commonly applied by modern
acquisition devices. While significantly improving the visual quality of the
resulting videos, it has been shown that such operation typically hinders the
forensic analysis of video signals. In fact, the correct identification of the
acquisition source usually based on Photo Response non-Uniformity (PRNU) is
subject to the estimation of the transformation applied to each frame in the
stabilization phase. A number of techniques have been proposed for dealing with
this problem, which however typically suffer from a high computational burden
due to the grid search in the space of inversion parameters. Our work attempts
to alleviate these shortcomings by exploiting the parallelization capabilities
of Graphics Processing Units (GPUs), typically used for deep learning
applications, in the framework of stabilised frames inversion. Moreover, we
propose to exploit SIFT features {to estimate the camera momentum and} %to
identify less stabilized temporal segments, thus enabling a more accurate
identification analysis, and to efficiently initialize the frame-wise parameter
search of consecutive frames. Experiments on a consolidated benchmark dataset
confirm the effectiveness of the proposed approach in reducing the required
computational time and improving the source identification accuracy. {The code
is available at \url{https://github.com/AMontiB/GPU-PRNU-SIFT}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder. (arXiv:2207.14508v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14508">
<div class="article-summary-box-inner">
<span><p>It is common practice to reuse models initially trained on different data to
increase downstream task performance. Especially in the computer vision domain,
ImageNet-pretrained weights have been successfully used for various tasks. In
this work, we investigate the impact of transfer learning for segmentation
problems, being pixel-wise classification problems that can be tackled with
encoder-decoder architectures. We find that transfer learning the decoder does
not help downstream segmentation tasks, while transfer learning the encoder is
truly beneficial. We demonstrate that pretrained weights for a decoder may
yield faster convergence, but they do not improve the overall model performance
as one can obtain equivalent results with randomly initialized decoders.
However, we show that it is more effective to reuse encoder weights trained on
a segmentation or reconstruction task than reusing encoder weights trained on
classification tasks. This finding implicates that using ImageNet-pretrained
encoders for downstream segmentation problems is suboptimal. We also propose a
contrastive self-supervised approach with multiple self-reconstruction tasks,
which provides encoders that are suitable for transfer learning in segmentation
problems in the absence of segmentation labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Driven Action Quality Assessment. (arXiv:2207.14513v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14513">
<div class="article-summary-box-inner">
<span><p>Automatic action quality assessment (AQA) has attracted more interests due to
its wide applications. However, existing AQA methods usually employ the
multi-branch models to generate multiple scores, which is not flexible for
dealing with a variable number of judges. In this paper, we propose a novel
Uncertainty-Driven AQA (UD-AQA) model to generate multiple predictions only
using one single branch. Specifically, we design a CVAE (Conditional
Variational Auto-Encoder) based module to encode the uncertainty, where
multiple scores can be produced by sampling from the learned latent space
multiple times. Moreover, we output the estimation of uncertainty and utilize
the predicted uncertainty to re-weight AQA regression loss, which can reduce
the contributions of uncertain samples for training. We further design an
uncertainty-guided training strategy to dynamically adjust the learning order
of the samples from low uncertainty to high uncertainty. The experiments show
that our proposed method achieves new state-of-the-art results on the Olympic
events MTL-AQA and surgical skill JIGSAWS datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Practicality of Learned Image Compression. (arXiv:2207.14524v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14524">
<div class="article-summary-box-inner">
<span><p>Learned image compression has achieved extraordinary rate-distortion
performance in PSNR and MS-SSIM compared to traditional methods. However, it
suffers from intensive computation, which is intolerable for real-world
applications and leads to its limited industrial application for now. In this
paper, we introduce neural architecture search (NAS) to designing more
efficient networks with lower latency, and leverage quantization to accelerate
the inference process. Meanwhile, efforts in engineering like multi-threading
and SIMD have been made to improve efficiency. Optimized using a hybrid loss of
PSNR and MS-SSIM for better visual quality, we obtain much higher MS-SSIM than
JPEG, JPEG XL and AVIF over all bit rates, and PSNR between that of JPEG XL and
AVIF. Our software implementation of LIC achieves comparable or even faster
inference speed compared to jpeg-turbo while being multiple times faster than
JPEG XL and AVIF. Besides, our implementation of LIC reaches stunning
throughput of 145 fps for encoding and 208 fps for decoding on a Tesla T4 GPU
for 1080p images. On CPU, the latency of our implementation is comparable with
JPEG XL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curriculum Learning for Data-Efficient Vision-Language Alignment. (arXiv:2207.14525v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14525">
<div class="article-summary-box-inner">
<span><p>Aligning image and text encoders from scratch using contrastive learning
requires large amounts of paired image-text data. We alleviate this need by
aligning individually pre-trained language and vision representation models
using a much smaller amount of paired data, augmented with a curriculum
learning algorithm to learn fine-grained vision-language alignments. TOnICS
(Training with Ontology-Informed Contrastive Sampling) initially samples
minibatches whose image-text pairs contain a wide variety of objects to learn
object-level alignment, and progressively samples minibatches where all
image-text pairs contain the same object to learn finer-grained contextual
alignment. Aligning pre-trained BERT and VinVL models to each other using
TOnICS outperforms CLIP on downstream zero-shot image retrieval while using
less than 1% as much training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings. (arXiv:2207.14539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14539">
<div class="article-summary-box-inner">
<span><p>Pre-training trajectory embeddings is a fundamental and critical procedure in
spatial-temporal trajectory mining, and is beneficial for a wide range of
downstream tasks. The key for generating effective trajectory embeddings is to
extract high-level travel semantics from trajectories, including movement
patterns and travel purposes, with consideration of the trajectories' long-term
spatial-temporal correlations. Despite the existing efforts, there are still
major challenges in pre-training trajectory embeddings. First, commonly used
generative pretext tasks are not suitable for extracting high-level semantics
from trajectories. Second, existing data augmentation methods fit badly on
trajectory datasets. Third, current encoder designs fail to fully incorporate
long-term spatial-temporal correlations hidden in trajectories. To tackle these
challenges, we propose a novel Contrastive Spatial-Temporal Trajectory
Embedding (CSTTE) model for learning comprehensive trajectory embeddings. CSTTE
adopts the contrastive learning framework so that its pretext task is robust to
noise. A specially designed data augmentation method for trajectories is
coupled with the contrastive pretext task to preserve the high-level travel
semantics. We also build an efficient spatial-temporal trajectory encoder to
efficiently and comprehensively model the long-term spatial-temporal
correlations in trajectories. Extensive experiments on two downstream tasks and
three real-world datasets prove the superiority of our model compared with the
existing trajectory embedding methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A One-Shot Reparameterization Method for Reducing the Loss of Tile Pruning on DNNs. (arXiv:2207.14545v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14545">
<div class="article-summary-box-inner">
<span><p>Recently, tile pruning has been widely studied to accelerate the inference of
deep neural networks (DNNs). However, we found that the loss due to tile
pruning, which can eliminate important elements together with unimportant
elements, is large on trained DNNs. In this study, we propose a one-shot
reparameterization method, called TileTrans, to reduce the loss of tile
pruning. Specifically, we repermute the rows or columns of the weight matrix
such that the model architecture can be kept unchanged after
reparameterization. This repermutation realizes the reparameterization of the
DNN model without any retraining. The proposed reparameterization method
combines important elements into the same tile; thus, preserving the important
elements after the tile pruning. Furthermore, TileTrans can be seamlessly
integrated into existing tile pruning methods because it is a pre-processing
method executed before pruning, which is orthogonal to most existing methods.
The experimental results demonstrate that our method is essential in reducing
the loss of tile pruning on DNNs. Specifically, the accuracy is improved by up
to 17% for AlexNet while 5% for ResNet-34, where both models are pre-trained on
ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise Perspective for Medical Image Segmentation. (arXiv:2207.14552v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14552">
<div class="article-summary-box-inner">
<span><p>Recently, a variety of vision transformers have been developed as their
capability of modeling long-range dependency. In current transformer-based
backbones for medical image segmentation, convolutional layers were replaced
with pure transformers, or transformers were added to the deepest encoder to
learn global context. However, there are mainly two challenges in a scale-wise
perspective: (1) intra-scale problem: the existing methods lacked in extracting
local-global cues in each scale, which may impact the signal propagation of
small objects; (2) inter-scale problem: the existing methods failed to explore
distinctive information from multiple scales, which may hinder the
representation learning from objects with widely variable size, shape and
location. To address these limitations, we propose a novel backbone, namely
ScaleFormer, with two appealing designs: (1) A scale-wise intra-scale
transformer is designed to couple the CNN-based local features with the
transformer-based global cues in each scale, where the row-wise and column-wise
global dependencies can be extracted by a lightweight Dual-Axis MSA. (2) A
simple and effective spatial-aware inter-scale transformer is designed to
interact among consensual regions in multiple scales, which can highlight the
cross-scale dependency and resolve the complex scale variations. Experimental
results on different benchmarks demonstrate that our Scale-Former outperforms
the current state-of-the-art methods. The code is publicly available at:
https://github.com/ZJUGiveLab/ScaleFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting for Multi-Modal Tracking. (arXiv:2207.14571v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14571">
<div class="article-summary-box-inner">
<span><p>Multi-modal tracking gains attention due to its ability to be more accurate
and robust in complex scenarios compared to traditional RGB-based tracking. Its
key lies in how to fuse multi-modal data and reduce the gap between modalities.
However, multi-modal tracking still severely suffers from data deficiency, thus
resulting in the insufficient learning of fusion modules. Instead of building
such a fusion module, in this paper, we provide a new perspective on
multi-modal tracking by attaching importance to the multi-modal visual prompts.
We design a novel multi-modal prompt tracker (ProTrack), which can transfer the
multi-modal inputs to a single modality by the prompt paradigm. By best
employing the tracking ability of pre-trained RGB trackers learning at scale,
our ProTrack can achieve high-performance multi-modal tracking by only altering
the inputs, even without any extra training on multi-modal data. Extensive
experiments on 5 benchmark datasets demonstrate the effectiveness of the
proposed ProTrack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Augmentation for Satellite Images. (arXiv:2207.14580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14580">
<div class="article-summary-box-inner">
<span><p>This study proposes the use of generative models (GANs) for augmenting the
EuroSAT dataset for the Land Use and Land Cover (LULC) Classification task. We
used DCGAN and WGAN-GP to generate images for each class in the dataset. We
then explored the effect of augmenting the original dataset by about 10% in
each case on model performance. The choice of GAN architecture seems to have no
apparent effect on the model performance. However, a combination of geometric
augmentation and GAN-generated images improved baseline results. Our study
shows that GANs augmentation can improve the generalizability of deep
classification models on satellite images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Prototype via Placeholder for Zero-shot Recognition. (arXiv:2207.14581v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14581">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) aims to recognize unseen classes by exploiting
semantic descriptions shared between seen classes and unseen classes. Current
methods show that it is effective to learn visual-semantic alignment by
projecting semantic embeddings into the visual space as class prototypes.
However, such a projection function is only concerned with seen classes. When
applied to unseen classes, the prototypes often perform suboptimally due to
domain shift. In this paper, we propose to learn prototypes via placeholders,
termed LPL, to eliminate the domain shift between seen and unseen classes.
Specifically, we combine seen classes to hallucinate new classes which play as
placeholders of the unseen classes in the visual and semantic space. Placed
between seen classes, the placeholders encourage prototypes of seen classes to
be highly dispersed. And more space is spared for the insertion of
well-separated unseen ones. Empirically, well-separated prototypes help
counteract visual-semantic misalignment caused by domain shift. Furthermore, we
exploit a novel semantic-oriented fine-tuning to guarantee the semantic
reliability of placeholders. Extensive experiments on five benchmark datasets
demonstrate the significant performance gain of LPL over the state-of-the-art
methods. Code is available at https://github.com/zaiquanyang/LPL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Deformable 3D Caricatures with Learned Shape Control. (arXiv:2207.14593v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14593">
<div class="article-summary-box-inner">
<span><p>A 3D caricature is an exaggerated 3D depiction of a human face. The goal of
this paper is to model the variations of 3D caricatures in a compact parameter
space so that we can provide a useful data-driven toolkit for handling 3D
caricature deformations. To achieve the goal, we propose an MLP-based framework
for building a deformable surface model, which takes a latent code and produces
a 3D surface. In the framework, a SIREN MLP models a function that takes a 3D
position on a fixed template surface and returns a 3D displacement vector for
the input position. We create variations of 3D surfaces by learning a
hypernetwork that takes a latent code and produces the parameters of the MLP.
Once learned, our deformable model provides a nice editing space for 3D
caricatures, supporting label-based semantic editing and point-handle-based
deformation, both of which produce highly exaggerated and natural 3D caricature
shapes. We also demonstrate other applications of our deformable model, such as
automatic 3D caricature creation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WISE: Whitebox Image Stylization by Example-based Learning. (arXiv:2207.14606v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14606">
<div class="article-summary-box-inner">
<span><p>Image-based artistic rendering can synthesize a variety of expressive styles
using algorithmic image filtering. In contrast to deep learning-based methods,
these heuristics-based filtering techniques can operate on high-resolution
images, are interpretable, and can be parameterized according to various design
aspects. However, adapting or extending these techniques to produce new styles
is often a tedious and error-prone task that requires expert knowledge. We
propose a new paradigm to alleviate this problem: implementing algorithmic
image filtering techniques as differentiable operations that can learn
parametrizations aligned to certain reference styles. To this end, we present
WISE, an example-based image-processing system that can handle a multitude of
stylization techniques, such as watercolor, oil or cartoon stylization, within
a common framework. By training parameter prediction networks for global and
local filter parameterizations, we can simultaneously adapt effects to
reference styles and image content, e.g., to enhance facial features. Our
method can be optimized in a style-transfer framework or learned in a
generative-adversarial setting for image-to-image translation. We demonstrate
that jointly training an XDoG filter and a CNN for postprocessing can achieve
comparable results to a state-of-the-art GAN-based method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computational complexity reduction of deep neural networks. (arXiv:2207.14620v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14620">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNN) have been widely used and play a major role in the
field of computer vision and autonomous navigation. However, these DNNs are
computationally complex and their deployment over resource-constrained
platforms is difficult without additional optimizations and customization.
</p>
<p>In this manuscript, we describe an overview of DNN architecture and propose
methods to reduce computational complexity in order to accelerate training and
inference speeds to fit them on edge computing platforms with low computational
resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Graph Theoretic Exploration of Coronary Vascular Trees. (arXiv:2207.14624v1 [cs.DM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14624">
<div class="article-summary-box-inner">
<span><p>The aim of this study was to automate the generation of small coronary
vascular networks from large point clouds that represent the coronary arterial
network. Smaller networks that can be generated in a predictable manner can be
used to assess the impact of network morphometry on, for example, blood flow in
hemodynamic simulations. We develop a set of algorithms for generating coronary
vascular networks from large point clouds. These algorithms sort the point
cloud, simplify its network structure without information loss, and produce
subgraphs based on given, physiologically meaningful parameters. The data were
originally collected from optical fluorescence cryomicrotome images and
processed before their use here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content-Aware Differential Privacy with Conditional Invertible Neural Networks. (arXiv:2207.14625v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14625">
<div class="article-summary-box-inner">
<span><p>Differential privacy (DP) has arisen as the gold standard in protecting an
individual's privacy in datasets by adding calibrated noise to each data
sample. While the application to categorical data is straightforward, its
usability in the context of images has been limited. Contrary to categorical
data the meaning of an image is inherent in the spatial correlation of
neighboring pixels making the simple application of noise infeasible.
Invertible Neural Networks (INN) have shown excellent generative performance
while still providing the ability to quantify the exact likelihood. Their
principle is based on transforming a complicated distribution into a simple one
e.g. an image into a spherical Gaussian. We hypothesize that adding noise to
the latent space of an INN can enable differentially private image
modification. Manipulation of the latent space leads to a modified image while
preserving important details. Further, by conditioning the INN on meta-data
provided with the dataset we aim at leaving dimensions important for downstream
tasks like classification untouched while altering other parts that potentially
contain identifying information. We term our method content-aware differential
privacy (CADP). We conduct experiments on publicly available benchmarking
datasets as well as dedicated medical ones. In addition, we show the
generalizability of our method to categorical data. The source code is publicly
available at https://github.com/Cardio-AI/CADP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models. (arXiv:2207.14626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14626">
<div class="article-summary-box-inner">
<span><p>Image restoration under adverse weather conditions has been of significant
interest for various computer vision applications. Recent successful methods
rely on the current progress in deep neural network architectural designs
(e.g., with vision transformers). Motivated by the recent progress achieved
with state-of-the-art conditional generative models, we present a novel
patch-based image restoration algorithm based on denoising diffusion
probabilistic models. Our patch-based diffusion modeling approach enables
size-agnostic image restoration by using a guided denoising process with
smoothed noise estimates across overlapping patches during inference. We
empirically evaluate our model on benchmark datasets for image desnowing,
combined deraining and dehazing, and raindrop removal. We demonstrate our
approach to achieve state-of-the-art performances on both weather-specific and
multi-weather image restoration, and qualitatively show strong generalization
to real-world test images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data. (arXiv:2207.14650v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14650">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI), machine learning, and deep learning (DL)
methods are becoming increasingly important in the field of biomedical image
analysis. However, to exploit the full potential of such methods, a
representative number of experimentally acquired images containing a
significant number of manually annotated objects is needed as training data.
Here we introduce SYNTA (synthetic data) as a novel approach for the generation
of synthetic, photo-realistic, and highly complex biomedical images as training
data for DL systems. We show the versatility of our approach in the context of
muscle fiber and connective tissue analysis in histological sections. We
demonstrate that it is possible to perform robust and expert-level segmentation
tasks on previously unseen real-world data, without the need for manual
annotations using synthetic training data alone. Being a fully parametric
technique, our approach poses an interpretable and controllable alternative to
Generative Adversarial Networks (GANs) and has the potential to significantly
accelerate quantitative image analysis in a variety of biomedical applications
in microscopy and beyond.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal SuperCon: Classifier for Drivers of Deforestation in Indonesia. (arXiv:2207.14656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14656">
<div class="article-summary-box-inner">
<span><p>Deforestation is one of the contributing factors to climate change. Climate
change has a serious impact on human life, and it occurs due to emission of
greenhouse gases, such as carbon dioxide, to the atmosphere. It is important to
know the causes of deforestation for mitigation efforts, but there is a lack of
data-driven research studies to predict these deforestation drivers. In this
work, we propose a contrastive learning architecture, called Multimodal
SuperCon, for classifying drivers of deforestation in Indonesia using satellite
images obtained from Landsat 8. Multimodal SuperCon is an architecture which
combines contrastive learning and multimodal fusion to handle the available
deforestation dataset. Our proposed model outperforms previous work on driver
classification, giving a 7% improvement in accuracy in comparison to a
state-of-the-art rotation equivariant model for the same task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching with AffNet based rectifications. (arXiv:2207.14660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14660">
<div class="article-summary-box-inner">
<span><p>We consider the problem of two-view matching under significant viewpoint
changes with view synthesis. We propose two novel methods, minimizing the view
synthesis overhead. The first one, named DenseAffNet, uses dense affine shapes
estimates from AffNet, which allows it to partition the image, rectifying each
partition with just a single affine map. The second one, named DepthAffNet,
combines information from depth maps and affine shapes estimates to produce
different sets of rectifying affine maps for different image partitions.
DenseAffNet is faster than the state-of-the-art and more accurate on generic
scenes. DepthAffNet is on par with the state of the art on scenes containing
large planes. The evaluation is performed on 3 public datasets - EVD Dataset,
Strong ViewPoint Changes Dataset and IMC Phototourism Dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling. (arXiv:2207.14663v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14663">
<div class="article-summary-box-inner">
<span><p>Personalised 3D vascular models are valuable for diagnosis, prognosis and
treatment planning in patients with cardiovascular disease. Traditionally, such
models have been constructed with explicit representations such as meshes and
voxel masks, or implicit representations such as radial basis functions or
atomic (tubular) shapes. Here, we propose to represent surfaces by the zero
level set of their signed distance function (SDF) in a differentiable implicit
neural representation (INR). This allows us to model complex vascular
structures with a representation that is implicit, continuous, light-weight,
and easy to integrate with deep learning algorithms. We here demonstrate the
potential of this approach with three practical examples. First, we obtain an
accurate and watertight surface for an abdominal aortic aneurysm (AAA) from CT
images and show robust fitting from as little as 200 points on the surface.
Second, we simultaneously fit nested vessel walls in a single INR without
intersections. Third, we show how 3D models of individual arteries can be
smoothly blended into a single watertight surface. Our results show that INRs
are a flexible representation with potential for minimally interactive
annotation and manipulation of complex vascular structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High Dynamic Range and Super-Resolution from Raw Image Bursts. (arXiv:2207.14671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14671">
<div class="article-summary-box-inner">
<span><p>Photographs captured by smartphones and mid-range cameras have limited
spatial resolution and dynamic range, with noisy response in underexposed
regions and color artefacts in saturated areas. This paper introduces the first
approach (to the best of our knowledge) to the reconstruction of
high-resolution, high-dynamic range color images from raw photographic bursts
captured by a handheld camera with exposure bracketing. This method uses a
physically-accurate model of image formation to combine an iterative
optimization algorithm for solving the corresponding inverse problem with a
learned image representation for robust alignment and a learned natural image
prior. The proposed algorithm is fast, with low memory requirements compared to
state-of-the-art learning-based approaches to image restoration, and features
that are learned end to end from synthetic yet realistic data. Extensive
experiments demonstrate its excellent performance with super-resolution factors
of up to $\times 4$ on real photographs taken in the wild with hand-held
cameras, and high robustness to low-light conditions, noise, camera shake, and
moderate object motion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Laser-Scan Matching with Online Error Estimation for Highway and Tunnel Driving. (arXiv:2207.14674v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14674">
<div class="article-summary-box-inner">
<span><p>Lidar data can be used to generate point clouds for the navigation of
autonomous vehicles or mobile robotics platforms. Scan matching, the process of
estimating the rigid transformation that best aligns two point clouds, is the
basis for lidar odometry, a form of dead reckoning. Lidar odometry is
particularly useful when absolute sensors, like GPS, are not available. Here we
propose the Iterative Closest Ellipsoidal Transform (ICET), a scan matching
algorithm which provides two novel improvements over the current
state-of-the-art Normal Distributions Transform (NDT). Like NDT, ICET
decomposes lidar data into voxels and fits a Gaussian distribution to the
points within each voxel. The first innovation of ICET reduces geometric
ambiguity along large flat surfaces by suppressing the solution along those
directions. The second innovation of ICET is to infer the output error
covariance associated with the position and orientation transformation between
successive point clouds; the error covariance is particularly useful when ICET
is incorporated into a state-estimation routine such as an extended Kalman
filter. We constructed a simulation to compare the performance of ICET and NDT
in 2D space both with and without geometric ambiguity and found that ICET
produces superior estimates while accurately predicting solution accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global-Local Self-Distillation for Visual Representation Learning. (arXiv:2207.14676v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14676">
<div class="article-summary-box-inner">
<span><p>The downstream accuracy of self-supervised methods is tightly linked to the
proxy task solved during training and the quality of the gradients extracted
from it. Richer and more meaningful gradients updates are key to allow
self-supervised methods to learn better and in a more efficient manner. In a
typical self-distillation framework, the representation of two augmented images
are enforced to be coherent at the global level. Nonetheless, incorporating
local cues in the proxy task can be beneficial and improve the model accuracy
on downstream tasks. This leads to a dual objective in which, on the one hand,
coherence between global-representations is enforced and on the other,
coherence between local-representations is enforced. Unfortunately, an exact
correspondence mapping between two sets of local-representations does not exist
making the task of matching local-representations from one augmentation to
another non-trivial. We propose to leverage the spatial information in the
input images to obtain geometric matchings and compare this geometric approach
against previous methods based on similarity matchings. Our study shows that
not only 1) geometric matchings perform better than similarity based matchings
in low-data regimes but also 2) that similarity based matchings are highly
hurtful in low-data regimes compared to the vanilla baseline without local
self-distillation. The code will be released upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlphaVC: High-Performance and Efficient Learned Video Compression. (arXiv:2207.14678v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14678">
<div class="article-summary-box-inner">
<span><p>Recently, learned video compression has drawn lots of attention and show a
rapid development trend with promising results. However, the previous works
still suffer from some criticial issues and have a performance gap with
traditional compression standards in terms of widely used PSNR metric. In this
paper, we propose several techniques to effectively improve the performance.
First, to address the problem of accumulative error, we introduce a
conditional-I-frame as the first frame in the GoP, which stabilizes the
reconstructed quality and saves the bit-rate. Second, to efficiently improve
the accuracy of inter prediction without increasing the complexity of decoder,
we propose a pixel-to-feature motion prediction method at encoder side that
helps us to obtain high-quality motion information. Third, we propose a
probability-based entropy skipping method, which not only brings performance
gain, but also greatly reduces the runtime of entropy coding. With these
powerful techniques, this paper proposes AlphaVC, a high-performance and
efficient learned video compression scheme. To the best of our knowledge,
AlphaVC is the first E2E AI codec that exceeds the latest compression standard
VVC on all common test datasets for both PSNR (-28.2% BD-rate saving) and
MSSSIM (-52.2% BD-rate saving), and has very fast encoding (0.001x VVC) and
decoding (1.69x VVC) speeds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks. (arXiv:2207.14682v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14682">
<div class="article-summary-box-inner">
<span><p>Freely available and easy-to-use audio editing tools make it straightforward
to perform audio splicing. Convincing forgeries can be created by combining
various speech samples from the same person. Detection of such splices is
important both in the public sector when considering misinformation, and in a
legal context to verify the integrity of evidence. Unfortunately, most existing
detection algorithms for audio splicing use handcrafted features and make
specific assumptions. However, criminal investigators are often faced with
audio samples from unconstrained sources with unknown characteristics, which
raises the need for more generally applicable methods.
</p>
<p>With this work, we aim to take a first step towards unconstrained audio
splicing detection to address this need. We simulate various attack scenarios
in the form of post-processing operations that may disguise splicing. We
propose a Transformer sequence-to-sequence (seq2seq) network for splicing
detection and localization. Our extensive evaluation shows that the proposed
method outperforms existing dedicated approaches for splicing detection [3, 10]
as well as the general-purpose networks EfficientNet [28] and RegNet [25].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Replacing the Framingham-based equation for prediction of cardiovascular disease risk and adverse outcome by using artificial intelligence and retinal imaging. (arXiv:2207.14685v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14685">
<div class="article-summary-box-inner">
<span><p>Purpose: To create and evaluate the accuracy of an artificial intelligence
Deep learning platform (ORAiCLE) capable of using only retinal fundus images to
predict both an individuals overall 5 year cardiovascular risk (CVD) and the
relative contribution of the component risk factors that comprise this risk.
Methods: We used 165,907 retinal images from a database of 47,236 patient
visits. Initially, each image was paired with biometric data age, ethnicity,
sex, presence and duration of diabetes a HDL/LDL ratios as well as any CVD
event wtihin 5 years of the retinal image acquisition. A risk score based on
Framingham equations was calculated. The real CVD event rate was also
determined for the individuals and overall population. Finally, ORAiCLE was
trained using only age, ethnicity, sex plus retinal images. Results: Compared
to Framingham-based score, ORAiCLE was up to 12% more accurate in prediciting
cardiovascular event in he next 5-years, especially for the highest risk group
of people. The reliability and accuracy of each of the restrictive models was
suboptimal to ORAiCLE performance ,indicating that it was using data from both
sets of data to derive its final results. Conclusion: Retinal photography is
inexpensive and only minimal training is required to acquire them as fully
automated, inexpensive camera systems are now widely available. As such,
AI-based CVD risk algorithms such as ORAiCLE promise to make CV health
screening more accurate, more afforadable and more accessible for all.
Furthermore, ORAiCLE unique ability to assess the relative contribution of the
components that comprise an individuals overall risk would inform treatment
decisions based on the specific needs of an individual, thereby increasing the
likelihood of positive health outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forensic License Plate Recognition with Compression-Informed Transformers. (arXiv:2207.14686v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14686">
<div class="article-summary-box-inner">
<span><p>Forensic license plate recognition (FLPR) remains an open challenge in legal
contexts such as criminal investigations, where unreadable license plates (LPs)
need to be deciphered from highly compressed and/or low resolution footage,
e.g., from surveillance cameras. In this work, we propose a side-informed
Transformer architecture that embeds knowledge on the input compression level
to improve recognition under strong compression. We show the effectiveness of
Transformers for license plate recognition (LPR) on a low-quality real-world
dataset. We also provide a synthetic dataset that includes strongly degraded,
illegible LP images and analyze the impact of knowledge embedding on it. The
network outperforms existing FLPR methods and standard state-of-the art image
recognition models while requiring less parameters. For the severest degraded
images, we can improve recognition by up to 8.9 percent points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Shuffling Video Benefit Temporal Bias Problem: A Novel Training Framework for Temporal Grounding. (arXiv:2207.14698v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14698">
<div class="article-summary-box-inner">
<span><p>Temporal grounding aims to locate a target video moment that semantically
corresponds to the given sentence query in an untrimmed video. However, recent
works find that existing methods suffer a severe temporal bias problem. These
methods do not reason the target moment locations based on the visual-textual
semantic alignment but over-rely on the temporal biases of queries in training
sets. To this end, this paper proposes a novel training framework for grounding
models to use shuffled videos to address temporal bias problem without losing
grounding accuracy. Our framework introduces two auxiliary tasks, cross-modal
matching and temporal order discrimination, to promote the grounding model
training. The cross-modal matching task leverages the content consistency
between shuffled and original videos to force the grounding model to mine
visual contents to semantically match queries. The temporal order
discrimination task leverages the difference in temporal order to strengthen
the understanding of long-term temporal contexts. Extensive experiments on
Charades-STA and ActivityNet Captions demonstrate the effectiveness of our
method for mitigating the reliance on temporal biases and strengthening the
model's generalization ability against the different temporal distributions.
Code is available at https://github.com/haojc/ShufflingVideosForTSG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Small Lesion Segmentation in CT Scans using Intensity Distribution Supervision: Application to Small Bowel Carcinoid Tumor. (arXiv:2207.14700v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14700">
<div class="article-summary-box-inner">
<span><p>Finding small lesions is very challenging due to lack of noticeable features,
severe class imbalance, as well as the size itself. One approach to improve
small lesion segmentation is to reduce the region of interest and inspect it at
a higher sensitivity rather than performing it for the entire region. It is
usually implemented as sequential or joint segmentation of organ and lesion,
which requires additional supervision on organ segmentation. Instead, we
propose to utilize an intensity distribution of a target lesion at no
additional labeling cost to effectively separate regions where the lesions are
possibly located from the background. It is incorporated into network training
as an auxiliary task. We applied the proposed method to segmentation of small
bowel carcinoid tumors in CT scans. We observed improvements for all metrics
(33.5% $\rightarrow$ 38.2%, 41.3% $\rightarrow$ 47.8%, 30.0% $\rightarrow$
35.9% for the global, per case, and per tumor Dice scores, respectively.)
compared to the baseline method, which proves the validity of our idea. Our
method can be one option for explicitly incorporating intensity distribution
information of a target in network training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Quantitative Susceptibility Mapping via Approximate Message Passing. (arXiv:2207.14709v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14709">
<div class="article-summary-box-inner">
<span><p>Purpose: It has been challenging to recover QSM in the presence of phase
errors, which could be caused by the noise or strong local susceptibility
shifts in cases of brain hemorrhage and calcification. We propose a Bayesian
formulation for QSM where a two-component Gaussian-mixture distribution is used
to model the long-tailed noise (error) distribution, and design an approximate
message passing (AMP) algorithm with automatic and adaptive parameter
estimation.
</p>
<p>Theory: Wavelet coefficients of the susceptibility map follow the Laplace
distribution. The measurement noise follows a two-component Gaussian-mixture
distribution where the second Gaussian component models the noise outliers. The
distribution parameters are treated as unknown variables and jointly recovered
with the susceptibility using AMP.
</p>
<p>Methods: The proposed AMP with parameter estimation (AMP-PE) is compared with
the state-of-the-art nonlinear L1-QSM and MEDI approaches that adopt the
L1-norm and L2-norm data-fidelity terms respectively. The three approaches are
tested on the Sim2Snr1 data from QSM challenge 2.0, the in vivo data from both
healthy and hemorrhage scans.
</p>
<p>Results: On the simulated Sim2Snr1 dataset, AMP-PE achieved the lowest NRMSE
and SSIM, MEDI achieved the lowest HFEN, and each approach also has its own
strong suit when it comes to various local evaluation metrics. On the in vivo
dataset, AMP-PE is better at preserving structural details and removing
streaking artifacts than L1-QSM and MEDI.
</p>
<p>Conclusion: By leveraging a customized Gaussian-mixture noise prior, AMP-PE
achieves better performance on the challenging QSM cases involving hemorrhage
and calcification. It is equipped with built-in parameter estimation, which
avoids subjective bias from the usual visual fine-tuning step of in vivo
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end View Synthesis via NeRF Attention. (arXiv:2207.14741v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14741">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a simple seq2seq formulation for view synthesis
where we take a set of ray points as input and output colors corresponding to
the rays. Directly applying a standard transformer on this seq2seq formulation
has two limitations. First, the standard attention cannot successfully fit the
volumetric rendering procedure, and therefore high-frequency components are
missing in the synthesized views. Second, applying global attention to all rays
and pixels is extremely inefficient. Inspired by the neural radiance field
(NeRF), we propose the NeRF attention (NeRFA) to address the above problems. On
the one hand, NeRFA considers the volumetric rendering equation as a soft
feature modulation procedure. In this way, the feature modulation enhances the
transformers with the NeRF-like inductive bias. On the other hand, NeRFA
performs multi-stage attention to reduce the computational overhead.
Furthermore, the NeRFA model adopts the ray and pixel transformers to learn the
interactions between rays and pixels. NeRFA demonstrates superior performance
over NeRF and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D.
Besides, NeRFA establishes a new state-of-the-art under two settings: the
single-scene view synthesis and the category-centric novel view synthesis. The
code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval. (arXiv:2207.14757v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14757">
<div class="article-summary-box-inner">
<span><p>Image-text matching is gaining a leading role among tasks involving the joint
understanding of vision and language. In literature, this task is often used as
a pre-training objective to forge architectures able to jointly deal with
images and texts. Nonetheless, it has a direct downstream application:
cross-modal retrieval, which consists in finding images related to a given
query text or vice-versa. Solving this task is of critical importance in
cross-modal search engines. Many recent methods proposed effective solutions to
the image-text matching problem, mostly using recent large vision-language (VL)
Transformer networks. However, these models are often computationally
expensive, especially at inference time. This prevents their adoption in
large-scale cross-modal retrieval scenarios, where results should be provided
to the user almost instantaneously. In this paper, we propose to fill in the
gap between effectiveness and efficiency by proposing an ALign And DIstill
Network (ALADIN). ALADIN first produces high-effective scores by aligning at
fine-grained level images and texts. Then, it learns a shared embedding space -
where an efficient kNN search can be performed - by distilling the relevance
scores obtained from the fine-grained alignments. We obtained remarkable
results on MS-COCO, showing that our method can compete with state-of-the-art
VL Transformers while being almost 90 times faster. The code for reproducing
our results is available at https://github.com/mesnico/ALADIN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Quality Assessment: Integrating Model-Centric and Data-Centric Approaches. (arXiv:2207.14769v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14769">
<div class="article-summary-box-inner">
<span><p>Learning-based image quality assessment (IQA) has made remarkable progress in
the past decade, but nearly all consider the two key components - model and
data - in relative isolation. Specifically, model-centric IQA focuses on
developing "better" objective quality methods on fixed and extensively reused
datasets, with a great danger of overfitting. Data-centric IQA involves
conducting psychophysical experiments to construct "better" human-annotated
datasets, which unfortunately ignores current IQA models during dataset
creation. In this paper, we first design a series of experiments to probe
computationally that such isolation of model and data impedes further progress
of IQA. We then describe a computational framework that integrates
model-centric and data-centric IQA. As a specific example, we design
computational modules to quantify the sampling-worthiness of candidate images
based on blind IQA (BIQA) model predictions and deep content-aware features.
Experimental results show that the proposed sampling-worthiness module
successfully spots diverse failures of the examined BIQA models, which are
indeed worthy samples to be included in next-generation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-radiomics: A Research Protocol to Make Radiomics-based Machine Learning Pipelines Reproducible. (arXiv:2207.14776v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14776">
<div class="article-summary-box-inner">
<span><p>The application of artificial intelligence (AI) techniques to medical imaging
data has yielded promising results. As an important branch of AI pipelines in
medical imaging, radiomics faces two major challenges namely reproducibility
and accessibility. In this work, we introduce open-radiomics, a set of
radiomics datasets, and a comprehensive radiomics pipeline that investigates
the effects of radiomics feature extraction settings such as binWidth and image
normalization on the reproducibility of the radiomics results performance. To
make radiomics research more accessible and reproducible, we provide guidelines
for building machine learning (ML) models on radiomics data, introduce
Open-radiomics, an evolving collection of open-source radiomics datasets, and
publish baseline models for the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Multi-modal Data for Improving Generalizability and Explainability of Disease Classification in Radiology. (arXiv:2207.14781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14781">
<div class="article-summary-box-inner">
<span><p>Traditional datasets for the radiological diagnosis tend to only provide the
radiology image alongside the radiology report. However, radiology reading as
performed by radiologists is a complex process, and information such as the
radiologist's eye-fixations over the course of the reading has the potential to
be an invaluable data source to learn from. Nonetheless, the collection of such
data is expensive and time-consuming. This leads to the question of whether
such data is worth the investment to collect. This paper utilizes the recently
published Eye-Gaze dataset to perform an exhaustive study on the impact on
performance and explainability of deep learning (DL) classification in the face
of varying levels of input features, namely: radiology images, radiology report
text, and radiologist eye-gaze data. We find that the best classification
performance of X-ray images is achieved with a combination of radiology report
free-text and radiology image, with the eye-gaze data providing no performance
boost. Nonetheless, eye-gaze data serving as secondary ground truth alongside
the class label results in highly explainable models that generate better
attention maps compared to models trained to do classification and attention
map generation without eye-gaze data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion. (arXiv:2207.14782v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14782">
<div class="article-summary-box-inner">
<span><p>Explicit neural surface representations allow for exact and efficient
extraction of the encoded surface at arbitrary precision, as well as analytic
derivation of differential geometric properties such as surface normal and
curvature. Such desirable properties, which are absent in its implicit
counterpart, makes it ideal for various applications in computer vision,
graphics and robotics. However, SOTA works are limited in terms of the topology
it can effectively describe, distortion it introduces to reconstruct complex
surfaces and model efficiency. In this work, we present Minimal Neural Atlas, a
novel atlas-based explicit neural surface representation. At its core is a
fully learnable parametric domain, given by an implicit probabilistic occupancy
field defined on an open square of the parametric space. In contrast, prior
works generally predefine the parametric domain. The added flexibility enables
charts to admit arbitrary topology and boundary. Thus, our representation can
learn a minimal atlas of 3 charts with distortion-minimal parameterization for
surfaces of arbitrary topology, including closed and open surfaces with
arbitrary connected components. Our experiments support the hypotheses and show
that our reconstructions are more accurate in terms of the overall geometry,
due to the separation of concerns on topology and geometry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognition of Handwritten Chinese Text by Segmentation: A Segment-annotation-free Approach. (arXiv:2207.14801v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14801">
<div class="article-summary-box-inner">
<span><p>Online and offline handwritten Chinese text recognition (HTCR) has been
studied for decades. Early methods adopted oversegmentation-based strategies
but suffered from low speed, insufficient accuracy, and high cost of character
segmentation annotations. Recently, segmentation-free methods based on
connectionist temporal classification (CTC) and attention mechanism, have
dominated the field of HCTR. However, people actually read text character by
character, especially for ideograms such as Chinese. This raises the question:
are segmentation-free strategies really the best solution to HCTR? To explore
this issue, we propose a new segmentation-based method for recognizing
handwritten Chinese text that is implemented using a simple yet efficient fully
convolutional network. A novel weakly supervised learning method is proposed to
enable the network to be trained using only transcript annotations; thus, the
expensive character segmentation annotations required by previous
segmentation-based methods can be avoided. Owing to the lack of context
modeling in fully convolutional networks, we propose a contextual
regularization method to integrate contextual information into the network
during the training stage, which can further improve the recognition
performance. Extensive experiments conducted on four widely used benchmarks,
namely CASIA-HWDB, CASIA-OLHWDB, ICDAR2013, and SCUT-HCCDoc, show that our
method significantly surpasses existing methods on both online and offline
HCTR, and exhibits a considerably higher inference speed than
CTC/attention-based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artifact Identification in X-ray Diffraction Data using Machine Learning Methods. (arXiv:2207.14804v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14804">
<div class="article-summary-box-inner">
<span><p>The in situ synchrotron high-energy X-ray powder diffraction (XRD) technique
is highly utilized by researchers to analyze the crystallographic structures of
materials in functional devices (e.g., battery materials) or in complex sample
environments (e.g., diamond anvil cells or syntheses reactors). An atomic
structure of a material can be identified by its diffraction pattern, along
with detailed analysis such as Rietveld refinement which indicates how the
measured structure deviates from the ideal structure (e.g., internal stresses
or defects). For in situ experiments, a series of XRD images is usually
collected on the same sample at different conditions (e.g., adiabatic
conditions), yielding different states of matter, or simply collected
continuously as a function of time to track the change of a sample over a
chemical or physical process. In situ experiments are usually performed with
area detectors, collecting 2D images composed of diffraction rings for ideal
powders. Depending on the material's form, one may observe different
characteristics other than the typical Debye Scherrer rings for a realistic
sample and its environments, such as textures or preferred orientations and
single crystal diffraction spots in the 2D XRD image. In this work, we present
an investigation of machine learning methods for fast and reliable
identification and separation of the single crystal diffraction spots in XRD
images. The exclusion of artifacts during an XRD image integration process
allows a precise analysis of the powder diffraction rings of interest. We
observe that the gradient boosting method can consistently produce high
accuracy results when it is trained with small subsets of highly diverse
datasets. The method dramatically decreases the amount of time spent on
identifying and separating single crystal spots in comparison to the
conventional method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition. (arXiv:2207.14807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14807">
<div class="article-summary-box-inner">
<span><p>Handwritten Chinese text recognition (HCTR) has been an active research topic
for decades. However, most previous studies solely focus on the recognition of
cropped text line images, ignoring the error caused by text line detection in
real-world applications. Although some approaches aimed at page-level text
recognition have been proposed in recent years, they either are limited to
simple layouts or require very detailed annotations including expensive
line-level and even character-level bounding boxes. To this end, we propose
PageNet for end-to-end weakly supervised page-level HCTR. PageNet detects and
recognizes characters and predicts the reading order between them, which is
more robust and flexible when dealing with complex layouts including
multi-directional and curved text lines. Utilizing the proposed weakly
supervised learning framework, PageNet requires only transcripts to be
annotated for real data; however, it can still output detection and recognition
results at both the character and line levels, avoiding the labor and cost of
labeling bounding boxes of characters and text lines. Extensive experiments
conducted on five datasets demonstrate the superiority of PageNet over existing
weakly supervised and fully supervised page-level methods. These experimental
results may spark further research beyond the realms of existing methods based
on connectionist temporal classification or attention. The source code is
available at https://github.com/shannanyinxiang/PageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleLight: HDR Panorama Generation for Lighting Estimation and Editing. (arXiv:2207.14811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14811">
<div class="article-summary-box-inner">
<span><p>We present a new lighting estimation and editing framework to generate
high-dynamic-range (HDR) indoor panorama lighting from a single limited
field-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras.
Existing lighting estimation methods either directly regress lighting
representation parameters or decompose this problem into LFOV-to-panorama and
LDR-to-HDR lighting generation sub-tasks. However, due to the partial
observation, the high-dynamic-range lighting, and the intrinsic ambiguity of a
scene, lighting estimation remains a challenging task. To tackle this problem,
we propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that
integrates LDR and HDR panorama synthesis into a unified framework. The LDR and
HDR panorama synthesis share a similar generator but have separate
discriminators. During inference, given an LDR LFOV image, we propose a
focal-masked GAN inversion method to find its latent code by the LDR panorama
synthesis branch and then synthesize the HDR panorama by the HDR panorama
synthesis branch. StyleLight takes LFOV-to-panorama and LDR-to-HDR lighting
generation into a unified framework and thus greatly improves lighting
estimation. Extensive experiments demonstrate that our framework achieves
superior performance over state-of-the-art methods on indoor lighting
estimation. Notably, StyleLight also enables intuitive lighting editing on
indoor HDR panoramas, which is suitable for real-world applications. Code is
available at https://style-light.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond. (arXiv:2207.14812v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14812">
<div class="article-summary-box-inner">
<span><p>We show that pre-trained Generative Adversarial Networks (GANs) such as
StyleGAN and BigGAN can be used as a latent bank to improve the performance of
image super-resolution. While most existing perceptual-oriented approaches
attempt to generate realistic outputs through learning with adversarial loss,
our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by
directly leveraging rich and diverse priors encapsulated in a pre-trained GAN.
But unlike prevalent GAN inversion methods that require expensive
image-specific optimization at runtime, our approach only needs a single
forward pass for restoration. GLEAN can be easily incorporated in a simple
encoder-bank-decoder architecture with multi-resolution skip connections.
Employing priors from different generative models allows GLEAN to be applied to
diverse categories (\eg~human faces, cats, buildings, and cars). We further
present a lightweight version of GLEAN, named LightGLEAN, which retains only
the critical components in GLEAN. Notably, LightGLEAN consists of only 21% of
parameters and 35% of FLOPs while achieving comparable image quality. We extend
our method to different tasks including image colorization and blind image
restoration, and extensive experiments show that our proposed models perform
favorably in comparison to existing methods. Codes and models are available at
https://github.com/open-mmlab/mmediting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Data Augmentation for LiDAR based 3D Object Detection. (arXiv:2004.01643v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.01643">
<div class="article-summary-box-inner">
<span><p>In this work, we shed light on different data augmentation techniques
commonly used in Light Detection and Ranging (LiDAR) based 3D Object Detection.
For the bulk of our experiments, we utilize the well known PointPillars
pipeline and the well established KITTI dataset. We investigate a variety of
global and local augmentation techniques, where global augmentation techniques
are applied to the entire point cloud of a scene and local augmentation
techniques are only applied to points belonging to individual objects in the
scene. Our findings show that both types of data augmentation can lead to
performance increases, but it also turns out, that some augmentation
techniques, such as individual object translation, for example, can be
counterproductive and can hurt the overall performance. We show that these
findings transfer and generalize well to other state of the art 3D Object
Detection methods and the challenging STF dataset. On the KITTI dataset we can
gain up to 1.5% and on the STF dataset up to 1.7% in 3D mAP on the moderate car
class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effects of Image Size on Deep Learning. (arXiv:2101.11508v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11508">
<div class="article-summary-box-inner">
<span><p>This paper presents the effects of late gadolinium enhancement (LGE) magnetic
resonance imaging (MRI) image size on deep learning based fully automated
quantification of myocardial infarction (MI). The main objective is to
determine the best size for LGE MRI images in the training dataset to achieve
optimal deep learning training outcomes. To determine the new size of LGE MRI
images of the reference training dataset, non-extra pixel and extra pixel
interpolation algorithms are used. A novel strategy based on thresholding,
median filtering, and subtraction operations is introduced and applied to
remove extra class labels in interpolated ground truth (GT) segmentation masks.
Fully automated quantification is achieved using the expectation maximization,
weighted intensity, a priori information (EWA) algorithm, and the outcome of
automatic semantic segmentation of LGE-MRI images with the convolutional neural
network (CNN). In the experiments, common class metrics are used to evaluate
the quality of semantic segmentation with a CNN architecture of interest
(U-net) against the GT segmentation. Arbitrary threshold, comparison of the
sums, and sums of differences are used to estimate the relationship between
semi-automatic and fully automated quantification of MI results. A close
relationship between semi-automatic and fully automated quantification of MI
results was more identified in the case involving the dataset of bigger LGE MRI
images than in that of the dataset of smaller LGE MRI images, where
quantification results based on the dataset of bigger LGE MRI images were 55.5%
closer the manual or semi-automatic results while quantification results based
on the dataset of smaller LGE MRI images were 22.2% closer the manual results
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization: A Survey. (arXiv:2103.02503v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02503">
<div class="article-summary-box-inner">
<span><p>Generalization to out-of-distribution (OOD) data is a capability natural to
humans yet challenging for machines to reproduce. This is because most learning
algorithms strongly rely on the i.i.d.~assumption on source/target data, which
is often violated in practice due to domain shift. Domain generalization (DG)
aims to achieve OOD generalization by using only source data for model
learning. Over the last ten years, research in DG has made great progress,
leading to a broad spectrum of methodologies, e.g., those based on domain
alignment, meta-learning, data augmentation, or ensemble learning, to name a
few; DG has also been studied in various application areas including computer
vision, speech recognition, natural language processing, medical imaging, and
reinforcement learning. In this paper, for the first time a comprehensive
literature review in DG is provided to summarize the developments over the past
decade. Specifically, we first cover the background by formally defining DG and
relating it to other relevant fields like domain adaptation and transfer
learning. Then, we conduct a thorough review into existing methods and
theories. Finally, we conclude this survey with insights and discussions on
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated liver tissues delineation techniques: A systematic survey on machine learning current trends and future orientations. (arXiv:2103.06384v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06384">
<div class="article-summary-box-inner">
<span><p>Machine learning and computer vision techniques have grown rapidly in recent
years due to their automation, suitability, and ability to generate astounding
results. Hence, in this paper, we survey the key studies that are published
between 2014 and 2022, showcasing the different machine learning algorithms
researchers have used to segment the liver, hepatic tumors, and
hepatic-vasculature structures. We divide the surveyed studies based on the
tissue of interest (hepatic-parenchyma, hepatic-tumors, or hepatic-vessels),
highlighting the studies that tackle more than one task simultaneously.
Additionally, the machine learning algorithms are classified as either
supervised or unsupervised, and they are further partitioned if the amount of
work that falls under a certain scheme is significant. Moreover, different
datasets and challenges found in literature and websites containing masks of
the aforementioned tissues are thoroughly discussed, highlighting the
organizers' original contributions and those of other researchers. Also, the
metrics used excessively in literature are mentioned in our review, stressing
their relevance to the task at hand. Finally, critical challenges and future
directions are emphasized for innovative researchers to tackle, exposing gaps
that need addressing, such as the scarcity of many studies on the vessels'
segmentation challenge and why their absence needs to be dealt with sooner than
later.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Subject Domain Adaptation for Classifying Working Memory Load with Multi-Frame EEG Images. (arXiv:2106.06769v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06769">
<div class="article-summary-box-inner">
<span><p>Working memory (WM), denoting the information temporally stored in the mind,
is a fundamental research topic in the field of human cognition.
Electroencephalograph (EEG), which can monitor the electrical activity of the
brain, has been widely used in measuring the level of WM. However, one of the
critical challenges is that individual differences may cause ineffective
results, especially when the established model meets an unfamiliar subject. In
this work, we propose a cross-subject deep adaptation model with spatial
attention (CS-DASA) to generalize the workload classifications across subjects.
First, we transform EEG time series into multi-frame EEG images incorporating
spatial, spectral, and temporal information. First, the Subject-Shared module
in CS-DASA receives multi-frame EEG image data from both source and target
subjects and learns the common feature representations. Then, in the
subject-specific module, the maximum mean discrepancy is implemented to measure
the domain distribution divergence in a reproducing kernel Hilbert space, which
can add an effective penalty loss for domain adaptation. Additionally, the
subject-to-subject spatial attention mechanism is employed to focus on the
discriminative spatial features from the target image data. Experiments
conducted on a public WM EEG dataset containing 13 subjects show that the
proposed model is capable of achieving better performance than existing
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dive into Deep Learning. (arXiv:2106.11342v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11342">
<div class="article-summary-box-inner">
<span><p>This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Motion Prior for Weakly-Supervised Temporal Action Localization. (arXiv:2108.05607v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05607">
<div class="article-summary-box-inner">
<span><p>Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize
actions in untrimmed videos with only video-level labels. Currently, most
state-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline:
producing snippet-level predictions first and then aggregating to the
video-level prediction. However, we argue that existing methods have overlooked
two important drawbacks: 1) inadequate use of motion information and 2) the
incompatibility of prevailing cross-entropy training loss. In this paper, we
analyze that the motion cues behind the optical flow features are complementary
informative. Inspired by this, we propose to build a context-dependent motion
prior, termed as motionness. Specifically, a motion graph is introduced to
model motionness based on the local motion carrier (e.g., optical flow). In
addition, to highlight more informative video snippets, a motion-guided loss is
proposed to modulate the network training conditioned on motionness scores.
Extensive ablation studies confirm that motionness efficaciously models
action-of-interest, and the motion-guided loss leads to more accurate results.
Besides, our motion-guided loss is a plug-and-play loss function and is
applicable with existing WSTAL methods. Without loss of generality, based on
the standard MIL pipeline, our method achieves new state-of-the-art performance
on three challenging benchmarks, including THUMOS'14, ActivityNet v1.2 and
v1.3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducible radiomics through automated machine learning validated on twelve clinical applications. (arXiv:2108.08618v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08618">
<div class="article-summary-box-inner">
<span><p>Radiomics uses quantitative medical imaging features to predict clinical
outcomes. Currently, in a new clinical application, finding the optimal
radiomics method out of the wide range of available options has to be done
manually through a heuristic trial-and-error process. In this study we propose
a framework for automatically optimizing the construction of radiomics
workflows per application. To this end, we formulate radiomics as a modular
workflow and include a large collection of common algorithms for each
component. To optimize the workflow per application, we employ automated
machine learning using a random search and ensembling. We evaluate our method
in twelve different clinical applications, resulting in the following area
under the curves: 1) liposarcoma (0.83); 2) desmoid-type fibromatosis (0.82);
3) primary liver tumors (0.80); 4) gastrointestinal stromal tumors (0.77); 5)
colorectal liver metastases (0.61); 6) melanoma metastases (0.45); 7)
hepatocellular carcinoma (0.75); 8) mesenteric fibrosis (0.80); 9) prostate
cancer (0.72); 10) glioma (0.71); 11) Alzheimer's disease (0.87); and 12) head
and neck cancer (0.84). We show that our framework has a competitive
performance compared human experts, outperforms a radiomics baseline, and
performs similar or superior to Bayesian optimization and more advanced
ensemble approaches. Concluding, our method fully automatically optimizes the
construction of radiomics workflows, thereby streamlining the search for
radiomics biomarkers in new applications. To facilitate reproducibility and
future research, we publicly release six datasets, the software implementation
of our framework, and the code to reproduce this study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12043">
<div class="article-summary-box-inner">
<span><p>Disentangled representation learning has been proposed as an approach to
learning general representations even in the absence of, or with limited,
supervision. A good general representation can be fine-tuned for new target
tasks using modest amounts of data, or used directly in unseen domains
achieving remarkable performance in the corresponding task. This alleviation of
the data and annotation requirements offers tantalising prospects for
applications in computer vision and healthcare. In this tutorial paper, we
motivate the need for disentangled representations, revisit key concepts, and
describe practical building blocks and criteria for learning such
representations. We survey applications in medical imaging emphasising choices
made in exemplar key works, and then discuss links to computer vision
applications. We conclude by presenting limitations, challenges, and
opportunities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Framework for COVID-19 Identification from a Multicenter Dataset of Chest CT Scans. (arXiv:2109.09241v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09241">
<div class="article-summary-box-inner">
<span><p>The objective of this study is to develop a robust deep learning-based
framework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and
Normal cases based on chest CT scans acquired in different imaging centers
using various protocols, and radiation doses. We showed that while our proposed
model is trained on a relatively small dataset acquired from only one imaging
center using a specific scanning protocol, the model performs well on
heterogeneous test sets obtained by multiple scanners using different technical
parameters. We also showed that the model can be updated via an unsupervised
approach to cope with the data shift between the train and test sets and
enhance the robustness of the model upon receiving a new external dataset from
a different center. We adopted an ensemble architecture to aggregate the
predictions from multiple versions of the model. For initial training and
development purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76
Normal cases was used, which contained volumetric CT scans acquired from one
imaging center using a constant standard radiation dose scanning protocol. To
evaluate the model, we collected four different test sets retrospectively to
investigate the effects of the shifts in the data characteristics on the
model's performance. Among the test cases, there were CT scans with similar
characteristics as the train set as well as noisy low-dose and ultra-low dose
CT scans. In addition, some test CT scans were obtained from patients with a
history of cardiovascular diseases or surgeries. The entire test dataset used
in this study contained 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental
results indicate that our proposed framework performs well on all test sets
achieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity
of 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI:
[76.50-99.19]).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Visual-Auditory Fixation Prediction with Multigranularity Perception. (arXiv:2112.13697v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13697">
<div class="article-summary-box-inner">
<span><p>Thanks to the rapid advances in deep learning techniques and the wide
availability of large-scale training sets, the performance of video saliency
detection models has been improving steadily and significantly. However, deep
learning-based visualaudio fixation prediction is still in its infancy. At
present, only a few visual-audio sequences have been furnished, with real
fixations being recorded in real visual-audio environments. Hence, it would be
neither efficient nor necessary to recollect real fixations under the same
visual-audio circumstances. To address this problem, this paper promotes a
novel approach in a weakly supervised manner to alleviate the demand of
large-scale training sets for visual-audio model training. By using only the
video category tags, we propose the selective class activation mapping (SCAM)
and its upgrade (SCAM+). In the spatial-temporal-audio circumstance, the former
follows a coarse-to-fine strategy to select the most discriminative regions,
and these regions are usually capable of exhibiting high consistency with the
real human-eye fixations. The latter equips the SCAM with an additional
multi-granularity perception mechanism, making the whole process more
consistent with that of the real human visual system. Moreover, we distill
knowledge from these regions to obtain complete new spatial-temporal-audio
(STA) fixation prediction (FP) networks, enabling broad applications in cases
where video tags are not available. Without resorting to any real human-eye
fixation, the performances of these STA FP networks are comparable to those of
fully supervised networks. The code and results are publicly available at
https://github.com/guotaowang/STANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Twenty-thousand Classes using Image-level Supervision. (arXiv:2201.02605v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02605">
<div class="article-summary-box-inner">
<span><p>Current object detectors are limited in vocabulary size due to the small
scale of detection datasets. Image classifiers, on the other hand, reason about
much larger vocabularies, as their datasets are larger and easier to collect.
We propose Detic, which simply trains the classifiers of a detector on image
classification data and thus expands the vocabulary of detectors to tens of
thousands of concepts. Unlike prior work, Detic does not need complex
assignment schemes to assign image labels to boxes based on model predictions,
making it much easier to implement and compatible with a range of detection
architectures and backbones. Our results show that Detic yields excellent
detectors even for classes without box annotations. It outperforms prior work
on both open-vocabulary and long-tail detection benchmarks. Detic provides a
gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the
open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains
41.7 mAP when evaluated on all classes, or only rare classes, hence closing the
gap in performance for object categories with few samples. For the first time,
we train a detector with all the twenty-one-thousand classes of the ImageNet
dataset and show that it generalizes to new datasets without finetuning. Code
is available at \url{https://github.com/facebookresearch/Detic}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02832">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks have demonstrated human-level performance in
the classification of melanoma and other skin lesions, but evident performance
disparities between differing skin tones should be addressed before widespread
deployment. In this work, we propose an efficient yet effective algorithm for
automatically labelling the skin tone of lesion images, and use this to
annotate the benchmark ISIC dataset. We subsequently use these automated labels
as the target for two leading bias unlearning techniques towards mitigating
skin tone bias. Our experimental results provide evidence that our skin tone
detection algorithm outperforms existing solutions and that unlearning skin
tone may improve generalisation and can reduce the performance disparity
between melanoma detection in lighter and darker skin tones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChimeraMix: Image Classification on Small Datasets via Masked Feature Mixing. (arXiv:2202.11616v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11616">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks require large amounts of labeled data
samples. For many real-world applications, this is a major limitation which is
commonly treated by augmentation methods. In this work, we address the problem
of learning deep neural networks on small datasets. Our proposed architecture
called ChimeraMix learns a data augmentation by generating compositions of
instances. The generative model encodes images in pairs, combines the features
guided by a mask, and creates new samples. For evaluation, all methods are
trained from scratch without any additional data. Several experiments on
benchmark datasets, e.g. ciFAIR-10, STL-10, and ciFAIR-100, demonstrate the
superior performance of ChimeraMix compared to current state-of-the-art methods
for classification on small datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human Pose Estimation. (arXiv:2203.07628v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07628">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel Pre-trained Spatial Temporal Many-to-One
(P-STMO) model for 2D-to-3D human pose estimation task. To reduce the
difficulty of capturing spatial and temporal information, we divide this task
into two stages: pre-training (Stage I) and fine-tuning (Stage II). In Stage I,
a self-supervised pre-training sub-task, termed masked pose modeling, is
proposed. The human joints in the input sequence are randomly masked in both
spatial and temporal domains. A general form of denoising auto-encoder is
exploited to recover the original 2D poses and the encoder is capable of
capturing spatial and temporal dependencies in this way. In Stage II, the
pre-trained encoder is loaded to STMO model and fine-tuned. The encoder is
followed by a many-to-one frame aggregator to predict the 3D pose in the
current frame. Especially, an MLP block is utilized as the spatial feature
extractor in STMO, which yields better performance than other methods. In
addition, a temporal downsampling strategy is proposed to diminish data
redundancy. Extensive experiments on two benchmarks show that our method
outperforms state-of-the-art methods with fewer parameters and less
computational overhead. For example, our P-STMO model achieves 42.1mm MPJPE on
Human3.6M dataset when using 2D poses from CPN as inputs. Meanwhile, it brings
a 1.5-7.1 times speedup to state-of-the-art methods. Code is available at
https://github.com/paTRICK-swk/P-STMO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images. (arXiv:2203.08138v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08138">
<div class="article-summary-box-inner">
<span><p>Cryo-electron microscopy (cryo-EM) has become a tool of fundamental
importance in structural biology, helping us understand the basic building
blocks of life. The algorithmic challenge of cryo-EM is to jointly estimate the
unknown 3D poses and the 3D electron scattering potential of a biomolecule from
millions of extremely noisy 2D images. Existing reconstruction algorithms,
however, cannot easily keep pace with the rapidly growing size of cryo-EM
datasets due to their high computational and memory cost. We introduce cryoAI,
an ab initio reconstruction algorithm for homogeneous conformations that uses
direct gradient-based optimization of particle poses and the electron
scattering potential from single-particle cryo-EM data. CryoAI combines a
learned encoder that predicts the poses of each particle image with a
physics-based decoder to aggregate each particle image into an implicit
representation of the scattering potential volume. This volume is stored in the
Fourier domain for computational efficiency and leverages a modern coordinate
network architecture for memory efficiency. Combined with a symmetrized loss
function, this framework achieves results of a quality on par with
state-of-the-art cryo-EM solvers for both simulated and experimental data, one
order of magnitude faster for large datasets and with significantly lower
memory requirements than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting a kNN-based Image Classification System with High-capacity Storage. (arXiv:2204.01186v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01186">
<div class="article-summary-box-inner">
<span><p>In existing image classification systems that use deep neural networks, the
knowledge needed for image classification is implicitly stored in model
parameters. If users want to update this knowledge, then they need to fine-tune
the model parameters. Moreover, users cannot verify the validity of inference
results or evaluate the contribution of knowledge to the results. In this
paper, we investigate a system that stores knowledge for image classification,
such as image feature maps, labels, and original images, not in model
parameters but in external high-capacity storage. Our system refers to the
storage like a database when classifying input images. To increase knowledge,
our system updates the database instead of fine-tuning model parameters, which
avoids catastrophic forgetting in incremental learning scenarios. We revisit a
kNN (k-Nearest Neighbor) classifier and employ it in our system. By analyzing
the neighborhood samples referred by the kNN algorithm, we can interpret how
knowledge learned in the past is used for inference results. Our system
achieves 79.8% top-1 accuracy on the ImageNet dataset without fine-tuning model
parameters after pretraining, and 90.8% accuracy on the Split CIFAR-100 dataset
in the task incremental learning setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Event Camera-based Odometry for Planetary Robots. (arXiv:2204.05880v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05880">
<div class="article-summary-box-inner">
<span><p>Due to their resilience to motion blur and high robustness in low-light and
high dynamic range conditions, event cameras are poised to become enabling
sensors for vision-based exploration on future Mars helicopter missions.
However, existing event-based visual-inertial odometry (VIO) algorithms either
suffer from high tracking errors or are brittle, since they cannot cope with
significant depth uncertainties caused by an unforeseen loss of tracking or
other effects. In this work, we introduce EKLT-VIO, which addresses both
limitations by combining a state-of-the-art event-based frontend with a
filter-based backend. This makes it both accurate and robust to uncertainties,
outperforming event- and frame-based VIO algorithms on challenging benchmarks
by 32%. In addition, we demonstrate accurate performance in hover-like
conditions (outperforming existing event-based methods) as well as high
robustness in newly collected Mars-like and high-dynamic-range sequences, where
existing frame-based methods fail. In doing so, we show that event-based VIO is
the way forward for vision-based exploration on Mars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Grounded Indoor 3D Semantic Segmentation in the Wild. (arXiv:2204.07761v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07761">
<div class="article-summary-box-inner">
<span><p>Recent advances in 3D semantic segmentation with deep neural networks have
shown remarkable success, with rapid performance increase on available
datasets. However, current 3D semantic segmentation benchmarks contain only a
small number of categories -- less than 30 for ScanNet and SemanticKITTI, for
instance, which are not enough to reflect the diversity of real environments
(e.g., semantic image understanding covers hundreds to thousands of classes).
Thus, we propose to study a larger vocabulary for 3D semantic segmentation with
a new extended benchmark on ScanNet data with 200 class categories, an order of
magnitude more than previously studied. This large number of class categories
also induces a large natural class imbalance, both of which are challenging for
existing 3D semantic segmentation methods. To learn more robust 3D features in
this context, we propose a language-driven pre-training method to encourage
learned 3D features that might have limited training examples to lie close to
their pre-trained text embeddings. Extensive experiments show that our approach
consistently outperforms state-of-the-art 3D pre-training for 3D semantic
segmentation on our proposed benchmark (+9% relative mIoU), including
limited-data scenarios with +25% relative mIoU using only 5% annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing. (arXiv:2204.11573v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11573">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the weakly-supervised audio-visual video parsing task,
which aims to recognize all events belonging to each modality and localize
their temporal boundaries. This task is challenging because only overall labels
indicating the video events are provided for training. However, an event might
be labeled but not appear in one of the modalities, which results in a
modality-specific noisy label problem. In this work, we propose a training
strategy to identify and remove modality-specific noisy labels dynamically. It
is motivated by two key observations: 1) networks tend to learn clean samples
first; and 2) a labeled event would appear in at least one modality.
Specifically, we sort the losses of all instances within a mini-batch
individually in each modality, and then select noisy samples according to the
relationships between intra-modal and inter-modal losses. Besides, we also
propose a simple but valid noise ratio estimation method by calculating the
proportion of instances whose confidence is below a preset threshold. Our
method makes large improvements over the previous state of the arts (\eg, from
60.0\% to 63.8\% in segment-level visual metric), which demonstrates the
effectiveness of our approach. Code and trained models are publicly available
at \url{https://github.com/MCG-NJU/JoMoLD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning on rail profiles matching. (arXiv:2205.08687v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08687">
<div class="article-summary-box-inner">
<span><p>Matching the rail cross-section profiles measured on site with the designed
profile is a must to evaluate the wear of the rail, which is very important for
track maintenance and rail safety. So far, the measured rail profiles to be
matched usually have four features, that is, large amount of data, diverse
section shapes, hardware made errors, and human experience needs to be
introduced to solve the complex situation on site during matching process.
However, traditional matching methods based on feature points or feature lines
could no longer meet the requirements. To this end, we first establish the rail
profiles matching dataset composed of 46386 pairs of professional manual
matched data, then propose a general high-precision method for rail profiles
matching using pre-trained convolutional neural network (CNN). This new method
based on deep learning is promising to be the dominant approach for this issue.
Source code is at
https://github.com/Kunqi1994/Deep-learning-on-rail-profile-matching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09479">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Network (GAN) is one of the state-of-the-art
generative models for realistic image synthesis. While training and evaluating
GAN becomes increasingly important, the current GAN research ecosystem does not
provide reliable benchmarks for which the evaluation is conducted consistently
and fairly. Furthermore, because there are few validated GAN implementations,
researchers devote considerable time to reproducing baselines. We study the
taxonomy of GAN approaches and present a new open-source library named
StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4
adversarial losses, 13 regularization modules, 3 differentiable augmentations,
7 evaluation metrics, and 5 evaluation backbones. With our training and
evaluation protocol, we present a large-scale benchmark using various datasets
(CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3
different evaluation backbones (InceptionV3, SwAV, and Swin Transformer).
Unlike other benchmarks used in the GAN community, we train representative
GANs, including BigGAN, StyleGAN2, and StyleGAN3, in a unified training
pipeline and quantify generation performance with 7 evaluation metrics. The
benchmark evaluates other cutting-edge generative models(e.g., StyleGAN-XL,
ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations,
training, and evaluation scripts with the pre-trained weights. StudioGAN is
available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Graph Matching Algorithms in Computer Vision. (arXiv:2207.00291v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00291">
<div class="article-summary-box-inner">
<span><p>The graph matching optimization problem is an essential component for many
tasks in computer vision, such as bringing two deformable objects in
correspondence. Naturally, a wide range of applicable algorithms have been
proposed in the last decades. Since a common standard benchmark has not been
developed, their performance claims are often hard to verify as evaluation on
differing problem instances and criteria make the results incomparable. To
address these shortcomings, we present a comparative study of graph matching
algorithms. We create a uniform benchmark where we collect and categorize a
large set of existing and publicly available computer vision graph matching
problems in a common format. At the same time we collect and categorize the
most popular open-source implementations of graph matching algorithms. Their
performance is evaluated in a way that is in line with the best practices for
comparing optimization algorithms. The study is designed to be reproducible and
extensible to serve as a valuable resource in the future.
</p>
<p>Our study provides three notable insights:
</p>
<p>1.) popular problem instances are exactly solvable in substantially less than
1 second and, therefore, are insufficient for future empirical evaluations;
</p>
<p>2.) the most popular baseline methods are highly inferior to the best
available methods;
</p>
<p>3.) despite the NP-hardness of the problem, instances coming from vision
applications are often solvable in a few seconds even for graphs with more than
500 vertices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview Detection with Cardboard Human Modeling. (arXiv:2207.02013v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02013">
<div class="article-summary-box-inner">
<span><p>Multiview detection uses multiple calibrated cameras with overlapping fields
of views to locate occluded pedestrians. In this field, existing methods
typically adopt a "human modeling - aggregation" strategy. To find robust
pedestrian representations, some intuitively use locations of detected 2D
bounding boxes, while others use entire frame features projected to the ground
plane. However, the former does not consider human appearance and leads to many
ambiguities, and the latter suffers from projection errors due to the lack of
accurate height of the human torso and head. In this paper, we propose a new
pedestrian representation scheme based on human point clouds modeling.
Specifically, using ray tracing for holistic human depth estimation, we model
pedestrians as upright, thin cardboard point clouds on the ground. Then, we
aggregate the point clouds of the pedestrian cardboard across multiple views
for a final decision. Compared with existing representations, the proposed
method explicitly leverages human appearance and reduces projection errors
significantly by relatively accurate height estimation. On two standard
evaluation benchmarks, the proposed method achieves very competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Label Retinal Disease Classification using Transformers. (arXiv:2207.02335v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02335">
<div class="article-summary-box-inner">
<span><p>Early detection of retinal diseases is one of the most important means of
preventing partial or permanent blindness in patients. In this research, a
novel multi-label classification system is proposed for the detection of
multiple retinal diseases, using fundus images collected from a variety of
sources. First, a new multi-label retinal disease dataset, the MuReD dataset,
is constructed, using a number of publicly available datasets for fundus
disease classification. Next, a sequence of post-processing steps is applied to
ensure the quality of the image data and the range of diseases, present in the
dataset. For the first time in fundus multi-label disease classification, a
transformer-based model optimized through extensive experimentation is used for
image analysis and decision making. Numerous experiments are performed to
optimize the configuration of the proposed system. It is shown that the
approach performs better than state-of-the-art works on the same task by 7.9%
and 8.1% in terms of AUC score for disease detection and disease
classification, respectively. The obtained results further support the
potential applications of transformer-based architectures in the medical
imaging field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-based Monocular 3D Reconstruction of Birds: A Contemporary Survey. (arXiv:2207.04512v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04512">
<div class="article-summary-box-inner">
<span><p>In nature, the collective behavior of animals, such as flying birds is
dominated by the interactions between individuals of the same species. However,
the study of such behavior among the bird species is a complex process that
humans cannot perform using conventional visual observational techniques such
as focal sampling in nature. For social animals such as birds, the mechanism of
group formation can help ecologists understand the relationship between social
cues and their visual characteristics over time (e.g., pose and shape). But,
recovering the varying pose and shapes of flying birds is a highly challenging
problem. A widely-adopted solution to tackle this bottleneck is to extract the
pose and shape information from 2D image to 3D correspondence. Recent advances
in 3D vision have led to a number of impressive works on the 3D shape and pose
estimation, each with different pros and cons. To the best of our knowledge,
this work is the first attempt to provide an overview of recent advances in 3D
bird reconstruction based on monocular vision, give both computer vision and
biology researchers an overview of existing approaches, and compare their
characteristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Background Distraction in Video Object Segmentation. (arXiv:2207.06953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06953">
<div class="article-summary-box-inner">
<span><p>Semi-supervised video object segmentation (VOS) aims to densely track certain
designated objects in videos. One of the main challenges in this task is the
existence of background distractors that appear similar to the target objects.
We propose three novel strategies to suppress such distractors: 1) a
spatio-temporally diversified template construction scheme to obtain
generalized properties of the target objects; 2) a learnable distance-scoring
function to exclude spatially-distant distractors by exploiting the temporal
consistency between two consecutive frames; 3) swap-and-attach augmentation to
force each object to have unique features by providing training samples
containing entangled objects. On all public benchmark datasets, our model
achieves a comparable performance to contemporary state-of-the-art approaches,
even with real-time performance. Qualitative results also demonstrate the
superiority of our approach over existing methods. We believe our approach will
be widely used for future VOS research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototype-Guided Continual Adaptation for Class-Incremental Unsupervised Domain Adaptation. (arXiv:2207.10856v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10856">
<div class="article-summary-box-inner">
<span><p>This paper studies a new, practical but challenging problem, called
Class-Incremental Unsupervised Domain Adaptation (CI-UDA), where the labeled
source domain contains all classes, but the classes in the unlabeled target
domain increase sequentially. This problem is challenging due to two
difficulties. First, source and target label sets are inconsistent at each time
step, which makes it difficult to conduct accurate domain alignment. Second,
previous target classes are unavailable in the current step, resulting in the
forgetting of previous knowledge. To address this problem, we propose a novel
Prototype-guided Continual Adaptation (ProCA) method, consisting of two
solution strategies. 1) Label prototype identification: we identify target
label prototypes by detecting shared classes with cumulative prediction
probabilities of target samples. 2) Prototype-based alignment and replay: based
on the identified label prototypes, we align both domains and enforce the model
to retain previous knowledge. With these two strategies, ProCA is able to adapt
the source model to a class-incremental unlabeled target domain effectively.
Extensive experiments demonstrate the effectiveness and superiority of ProCA in
resolving CI-UDA. The source code is available at
https://github.com/Hongbin98/ProCA.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Steganography Network. (arXiv:2207.13867v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13867">
<div class="article-summary-box-inner">
<span><p>Steganography usually modifies cover media to embed secret data. A new
steganographic approach called generative steganography (GS) has emerged
recently, in which stego images (images containing secret data) are generated
from secret data directly without cover media. However, existing GS schemes are
often criticized for their poor performances. In this paper, we propose an
advanced generative steganography network (GSN) that can generate realistic
stego images without using cover images, in which mutual information is firstly
introduced in stego image generation. Our model contains four sub-networks,
i.e., an image generator ($G$), a discriminator ($D$), a steganalyzer ($S$),
and a data extractor ($E$). $D$ and $S$ act as two adversarial discriminators
to ensure the visual and statistical imperceptibility of generated stego
images. $E$ is to extract the hidden secret from generated stego images. The
generator $G$ is flexibly constructed to synthesize either cover or stego
images with different inputs. It facilitates covert communication by hiding the
function of generating stego images in a normal image generator. A module named
secret block is designed delicately to conceal secret data in the feature maps
during image generation, with which high hiding capacity and image fidelity are
achieved. In addition, a novel hierarchical gradient decay skill is developed
to resist steganalysis detection. Experiments demonstrate the superiority of
our work over existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14024">
<div class="article-summary-box-inner">
<span><p>Large-scale deployment of autonomous vehicles has been continually delayed
due to safety concerns. On the one hand, comprehensive scene understanding is
indispensable, a lack of which would result in vulnerability to rare but
complex traffic situations, such as the sudden emergence of unknown objects.
However, reasoning from a global context requires access to sensors of multiple
types and adequate fusion of multi-modal sensor signals, which is difficult to
achieve. On the other hand, the lack of interpretability in learning models
also hampers the safety with unverifiable failure causes. In this paper, we
propose a safety-enhanced autonomous driving framework, named Interpretable
Sensor Fusion Transformer(InterFuser), to fully process and fuse information
from multi-modal multi-view sensors for achieving comprehensive scene
understanding and adversarial event detection. Besides, intermediate
interpretable features are generated from our framework, which provide more
semantics and are exploited to better constrain actions to be within the safe
sets. We conducted extensive experiments on CARLA benchmarks, where our model
outperforms prior methods, ranking the first on the public CARLA Leaderboard.
Our code will be made available at https://github.com/opendilab/InterFuser
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images. (arXiv:2207.14067v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14067">
<div class="article-summary-box-inner">
<span><p>We present Neural Strands, a novel learning framework for modeling accurate
hair geometry and appearance from multi-view image inputs. The learned hair
model can be rendered in real-time from any viewpoint with high-fidelity
view-dependent effects. Our model achieves intuitive shape and style control
unlike volumetric counterparts. To enable these properties, we propose a novel
hair representation based on a neural scalp texture that encodes the geometry
and appearance of individual strands at each texel location. Furthermore, we
introduce a novel neural rendering framework based on rasterization of the
learned hair strands. Our neural rendering is strand-accurate and anti-aliased,
making the rendering view-consistent and photorealistic. Combining appearance
with a multi-view geometric prior, we enable, for the first time, the joint
learning of appearance and explicit hair geometry from a multi-view setup. We
demonstrate the efficacy of our approach in terms of fidelity and efficiency
for various hairstyles.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-08-01 23:10:28.552015420 UTC">2022-08-01 23:10:28 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>