<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-11-02T01:30:00Z">11-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\"om Method. (arXiv:2111.00035v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00035">
<div class="article-summary-box-inner">
<span><p>Transformers are expensive to train due to the quadratic time and space
complexity in the self-attention mechanism. On the other hand, although kernel
machines suffer from the same computation bottleneck in pairwise dot products,
several approximation schemes have been successfully incorporated to
considerably reduce their computational cost without sacrificing too much
accuracy. In this work, we leverage the computation methods for kernel machines
to alleviate the high computational cost and introduce Skyformer, which
replaces the softmax structure with a Gaussian kernel to stabilize the model
training and adapts the Nystr\"om method to a non-positive semidefinite matrix
to accelerate the computation. We further conduct theoretical analysis by
showing that the matrix approximation error of our proposed method is small in
the spectral norm. Experiments on Long Range Arena benchmark show that the
proposed method is sufficient in getting comparable or even better performance
than the full self-attention while requiring fewer computation resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring a Texts Fairness Dimensions Using Machine Learning Based on Social Psychological Factors. (arXiv:2111.00086v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00086">
<div class="article-summary-box-inner">
<span><p>Fairness is a principal social value that can be observed in civilisations
around the world. A manifestations of this is in social agreements, often
described in texts, such as contracts. Yet, despite the prevalence of such, a
fairness metric for texts describing a social act remains wanting. To address
this, we take a step back to consider the problem based on first principals.
Instead of using rules or templates, we utilise social psychology literature to
determine the principal factors that humans use when making a fairness
assessment. We then attempt to digitise these using word embeddings into a
multi-dimensioned sentence level fairness perceptions vector to serve as an
approximation for these fairness perceptions. The method leverages a pro-social
bias within word embeddings, for which we obtain an F1= 81.0. A second
approach, using PCA and ML based on the said fairness approximation vector
produces an F1 score of 86.2. We details improvements that can be made in the
methodology to incorporate the projection of sentence embedding on to a
subspace representation of fairness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Golden Rule as a Heuristic to Measure the Fairness of Texts Using Machine Learning. (arXiv:2111.00107v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00107">
<div class="article-summary-box-inner">
<span><p>To treat others as one would wish to be treated is a common formulation of
the Golden Rule (GR). Yet, despite its prevalence as an axiom throughout
history, no digitisation of the moral philosophy exists. In this paper we
consider how to digitise it so that it may be used to measure sentences such
as: the boy harmed the girl, and categorise them as fair or unfair. A review
and reply to criticisms of the GR is made. We share the code for the
digitisation of the GR, and test it with a list of sentences. Implementing two
approaches, one using the USE, and a second using ALBERT. We find F1 scores of
78.0, 85.0, respectively. A suggestion of how the technology may be implemented
to avoid unfair biases in word embeddings is made - given that individuals
would typically not wish to be on the receiving end of an unfair act, such as
racism, irrespective of whether the corpus being used deems such discrimination
as praiseworthy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransAug: Translate as Augmentation for Sentence Embeddings. (arXiv:2111.00157v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00157">
<div class="article-summary-box-inner">
<span><p>While contrastive learning greatly advances the representation of sentence
embeddings, it is still limited by the size of the existing sentence datasets.
In this paper, we present TransAug (Translate as Augmentation), which provide
the first exploration of utilizing translated sentence pairs as data
augmentation for text, and introduce a two-stage paradigm to advances the
state-of-the-art sentence embeddings. Instead of adopting an encoder trained in
other languages setting, we first distill a Chinese encoder from a SimCSE
encoder (pretrained in English), so that their embeddings are close in semantic
space, which can be regraded as implicit data augmentation. Then, we only
update the English encoder via cross-lingual contrastive learning and frozen
the distilled Chinese encoder. Our approach achieves a new state-of-art on
standard semantic textual similarity (STS), outperforming both SimCSE and
Sentence-T5, and the best performance in corresponding tracks on transfer tasks
evaluated by SentEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models. (arXiv:2111.00160v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00160">
<div class="article-summary-box-inner">
<span><p>Gigantic pre-trained models have become central to natural language
processing (NLP), serving as the starting point for fine-tuning towards a range
of downstream tasks. However, two pain points persist for this paradigm: (a) as
the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the
fine-tuning process can be time-consuming and computationally expensive; (b)
the fine-tuned model has the same size as its starting point by default, which
is neither sensible due to its more specialized functionality, nor practical
since many fine-tuned models will be deployed in resource-constrained
environments. To address these pain points, we propose a framework for
resource- and parameter-efficient fine-tuning by leveraging the sparsity prior
in both weight updates and the final model weights. Our proposed framework,
dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two
key objectives: (i) parameter efficient fine-tuning - by enforcing
sparsity-aware weight updates on top of the pre-trained weights; and (ii)
resource-efficient inference - by encouraging a sparse weight structure towards
the final fine-tuned model. We leverage sparsity in these two directions by
exploiting both unstructured and structured sparse patterns in pre-trained
language models via magnitude-based pruning and $\ell_1$ sparse regularization.
Extensive experiments and in-depth investigations, with diverse network
backbones (i.e., BERT, GPT-2, and DeBERTa) on dozens of datasets, consistently
demonstrate highly impressive parameter-/training-/inference-efficiency, while
maintaining competitive downstream transfer performance. For instance, our
DSEE-BERT obtains about $35\%$ inference FLOPs savings with &lt;1% trainable
parameters and comparable performance to conventional fine-tuning. Codes are
available in https://github.com/VITA-Group/DSEE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Labeling for Massively Multilingual Speech Recognition. (arXiv:2111.00161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00161">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning through pseudo-labeling has become a staple of
state-of-the-art monolingual speech recognition systems. In this work, we
extend pseudo-labeling to massively multilingual speech recognition with 60
languages. We propose a simple pseudo-labeling recipe that works well even with
low-resource languages: train a supervised multilingual model, fine-tune it
with semi-supervised learning on a target language, generate pseudo-labels for
that language, and train a final model using pseudo-labels for all languages,
either from scratch or by fine-tuning. Experiments on the labeled Common Voice
and unlabeled VoxPopuli datasets show that our recipe can yield a model with
better performance for many languages that also transfers well to LibriSpeech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification. (arXiv:2111.00180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00180">
<div class="article-summary-box-inner">
<span><p>Short text classification is a fundamental task in natural language
processing. It is hard due to the lack of context information and labeled data
in practice. In this paper, we propose a new method called SHINE, which is
based on graph neural network (GNN), for short text classification. First, we
model the short text dataset as a hierarchical heterogeneous graph consisting
of word-level component graphs which introduce more semantic and syntactic
information. Then, we dynamically learn a short document graph that facilitates
effective label propagation among similar short texts. Thus, compared with
existing GNN-based methods, SHINE can better exploit interactions between nodes
of the same types and capture similarities between short texts. Extensive
experiments on various benchmark short text datasets show that SHINE
consistently outperforms state-of-the-art methods, especially with fewer
labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How should human translation coexist with NMT? Efficient tool for building high quality parallel corpus. (arXiv:2111.00191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00191">
<div class="article-summary-box-inner">
<span><p>This paper proposes a tool for efficiently constructing high-quality parallel
corpora with minimizing human labor and making this tool publicly available.
Our proposed construction process is based on neural machine translation (NMT)
to allow for it to not only coexist with human translation, but also improve
its efficiency by combining data quality control with human translation in a
data-centric approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Knowledge Augmentation for Generative Commonsense Reasoning. (arXiv:2111.00192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00192">
<div class="article-summary-box-inner">
<span><p>Generative commonsense reasoning is the capability of a language model to
generate a sentence with a given concept-set that is based on commonsense
knowledge. However, generative language models still struggle to provide
outputs, and the training set does not contain patterns that are sufficient for
generative commonsense reasoning. In this paper, we propose a data-centric
method that uses automatic knowledge augmentation to extend commonsense
knowledge using a machine knowledge generator. This method can generate
semi-golden sentences that improve the generative commonsense reasoning of a
language model without architecture modifications. Furthermore, this approach
is a model-agnostic method and does not require human effort for data
construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Pre-trained Models Can Transfer to All. (arXiv:2111.00197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00197">
<div class="article-summary-box-inner">
<span><p>Pre-trained general-purpose language models have been a dominating component
in enabling real-world natural language processing (NLP) applications. However,
a pre-trained model with backdoor can be a severe threat to the applications.
Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by
introducing malicious triggers in the targeted class, thus relying greatly on
the prior knowledge of the fine-tuning task. In this paper, we propose a new
approach to map the inputs containing triggers directly to a predefined output
representation of the pre-trained NLP models, e.g., a predefined output
representation for the classification token in BERT, instead of a target label.
It can thus introduce backdoor to a wide range of downstream tasks without any
prior knowledge. Additionally, in light of the unique properties of triggers in
NLP, we propose two new metrics to measure the performance of backdoor attacks
in terms of both effectiveness and stealthiness. Our experiments with various
types of triggers show that our method is widely applicable to different
fine-tuning tasks (classification and named entity recognition) and to
different models (such as BERT, XLNet, BART), which poses a severe threat.
Furthermore, by collaborating with the popular online model repository Hugging
Face, the threat brought by our method has been confirmed. Finally, we analyze
the factors that may affect the attack performance and share insights on the
causes of the success of our backdoor attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Magic Pyramid: Accelerating Inference with Early Exiting and Token Pruning. (arXiv:2111.00230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00230">
<div class="article-summary-box-inner">
<span><p>Pre-training and then fine-tuning large language models is commonly used to
achieve state-of-the-art performance in natural language processing (NLP)
tasks. However, most pre-trained models suffer from low inference speed.
Deploying such large models to applications with latency constraints is
challenging. In this work, we focus on accelerating the inference via
conditional computations. To achieve this, we propose a novel idea, Magic
Pyramid (MP), to reduce both width-wise and depth-wise computation via token
pruning and early exiting for Transformer-based models, particularly BERT. The
former manages to save the computation via removing non-salient tokens, while
the latter can fulfill the computation reduction by terminating the inference
early before reaching the final layer, if the exiting condition is met. Our
empirical studies demonstrate that compared to previous state of arts, MP is
not only able to achieve a speed-adjustable inference but also to surpass token
pruning and early exiting by reducing up to 70% giga floating point operations
(GFLOPs) with less than 0.5% accuracy drop. Token pruning and early exiting
express distinctive preferences to sequences with different lengths. However,
MP is capable of achieving an average of 8.06x speedup on two popular text
classification tasks, regardless of the sizes of the inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventNarrative: A large-scale Event-centric Dataset for Knowledge Graph-to-Text Generation. (arXiv:2111.00276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00276">
<div class="article-summary-box-inner">
<span><p>We introduce EventNarrative, a knowledge graph-to-text dataset from publicly
available open-world knowledge graphs. Given the recent advances in
event-driven Information Extraction (IE), and that prior research on
graph-to-text only focused on entity-driven KGs, this paper focuses on
event-centric data. However, our data generation system can still be adapted to
other other types of KG data. Existing large-scale datasets in the
graph-to-text area are non-parallel, meaning there is a large disconnect
between the KGs and text. The datasets that have a paired KG and text, are
small scale and manually generated or generated without a rich ontology, making
the corresponding graphs sparse. Furthermore, these datasets contain many
unlinked entities between their KG and text pairs. EventNarrative consists of
approximately 230,000 graphs and their corresponding natural language text, 6
times larger than the current largest parallel dataset. It makes use of a rich
ontology, all of the KGs entities are linked to the text, and our manual
annotations confirm a high data quality. Our aim is two-fold: help break new
ground in event-centric research where data is lacking, and to give researchers
a well-defined, large-scale dataset in order to better evaluate existing and
future knowledge graph-to-text models. We also evaluate two types of baseline
on EventNarrative: a graph-to-text specific model and two state-of-the-art
language models, which previous work has shown to be adaptable to the knowledge
graph-to-text domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmpBot: A T5-based Empathetic Chatbot focusing on Sentiments. (arXiv:2111.00310v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00310">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce EmpBot: an end-to-end empathetic chatbot.
Empathetic conversational agents should not only understand what is being
discussed, but also acknowledge the implied feelings of the conversation
partner and respond appropriately. To this end, we propose a method based on a
transformer pretrained language model (T5). Specifically, during finetuning we
propose to use three objectives: response language modeling, sentiment
understanding, and empathy forcing. The first objective is crucial for
generating relevant and coherent responses, while the next ones are significant
for acknowledging the sentimental state of the conversational partner and for
favoring empathetic responses. We evaluate our model on the EmpatheticDialogues
dataset using both automated metrics and human evaluation. The inclusion of the
sentiment understanding and empathy forcing auxiliary losses favor empathetic
responses, as human evaluation results indicate, comparing with the current
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdvCodeMix: Adversarial Attack on Code-Mixed Data. (arXiv:2111.00350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00350">
<div class="article-summary-box-inner">
<span><p>Research on adversarial attacks are becoming widely popular in the recent
years. One of the unexplored areas where prior research is lacking is the
effect of adversarial attacks on code-mixed data. Therefore, in the present
work, we have explained the first generalized framework on text perturbation to
attack code-mixed classification models in a black-box setting. We rely on
various perturbation techniques that preserve the semantic structures of the
sentences and also obscure the attacks from the perception of a human user. The
present methodology leverages the importance of a token to decide where to
attack by employing various perturbation strategies. We test our strategies on
various sentiment classification models trained on Bengali-English and
Hindi-English code-mixed datasets, and reduce their F1-scores by nearly 51 %
and 53 % respectively, which can be further reduced if a larger number of
tokens are perturbed in a given sentence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientWord-Net: An Open Source Hotword Detection Engine based on One-shot Learning. (arXiv:2111.00379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00379">
<div class="article-summary-box-inner">
<span><p>Voice assistants like Siri, Google Assistant, Alexa etc. are used widely
across the globe for home automation, these require the use of special phrases
also known as hotwords to wake it up and perform an action like "Hey Alexa!",
"Ok Google!" and "Hey Siri!" etc. These hotwords are detected with lightweight
real-time engines whose purpose is to detect the hotwords uttered by the user.
This paper presents the design and implementation of a hotword detection engine
based on one-shot learning which detects the hotword uttered by the user in
real-time with just one or few training samples of the hotword. This approach
is efficient when compared to existing implementations because the process of
adding a new hotword in the existing systems requires enormous amounts of
positive and negative training samples and the model needs to retrain for every
hotword. This makes the existing implementations inefficient in terms of
computation and cost. The architecture proposed in this paper has achieved an
accuracy of 94.51%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FANS: Fusing ASR and NLU for on-device SLU. (arXiv:2111.00400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00400">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) systems translate voice input commands to
semantics which are encoded as an intent and pairs of slot tags and values.
Most current SLU systems deploy a cascade of two neural models where the first
one maps the input audio to a transcript (ASR) and the second predicts the
intent and slots from the transcript (NLU). In this paper, we introduce FANS, a
new end-to-end SLU model that fuses an ASR audio encoder to a multi-task NLU
decoder to infer the intent, slot tags, and slot values directly from a given
input audio, obviating the need for transcription. FANS consists of a shared
audio encoder and three decoders, two of which are seq-to-seq decoders that
predict non null slot tags and slot values in parallel and in an
auto-regressive manner. FANS neural encoder and decoders architectures are
flexible which allows us to leverage different combinations of LSTM,
self-attention, and attenders. Our experiments show compared to the
state-of-the-art end-to-end SLU models, FANS reduces ICER and IRER errors
relatively by 30 % and 7 %, respectively, when tested on an in-house SLU
dataset and by 0.86 % and 2 % absolute when tested on a public SLU dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Emotion Recognition Using Quaternion Convolutional Neural Networks. (arXiv:2111.00404v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00404">
<div class="article-summary-box-inner">
<span><p>Although speech recognition has become a widespread technology, inferring
emotion from speech signals still remains a challenge. To address this problem,
this paper proposes a quaternion convolutional neural network (QCNN) based
speech emotion recognition (SER) model in which Mel-spectrogram features of
speech signals are encoded in an RGB quaternion domain. We show that our QCNN
based SER model outperforms other real-valued methods in the Ryerson
Audio-Visual Database of Emotional Speech and Song (RAVDESS, 8-classes)
dataset, achieving, to the best of our knowledge, state-of-the-art results. The
QCNN also achieves comparable results with the state-of-the-art methods in the
Interactive Emotional Dyadic Motion Capture (IEMOCAP 4-classes) and Berlin
EMO-DB (7-classes) datasets. Specifically, the model achieves an accuracy of
77.87\%, 70.46\%, and 88.78\% for the RAVDESS, IEMOCAP, and EMO-DB datasets,
respectively. In addition, our results show that the quaternion unit structure
is better able to encode internal dependencies to reduce its model size
significantly compared to other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Deep Residual Reasoning for Temporal Moment Localization. (arXiv:2111.00417v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00417">
<div class="article-summary-box-inner">
<span><p>Temporal Moment Localization (TML) in untrimmed videos is a challenging task
in the field of multimedia, which aims at localizing the start and end points
of the activity in the video, described by a sentence query. Existing methods
mainly focus on mining the correlation between video and sentence
representations or investigating the fusion manner of the two modalities. These
works mainly understand the video and sentence coarsely, ignoring the fact that
a sentence can be understood from various semantics, and the dominant words
affecting the moment localization in the semantics are the action and object
reference. Toward this end, we propose a Hierarchical Deep Residual Reasoning
(HDRR) model, which decomposes the video and sentence into multi-level
representations with different semantics to achieve a finer-grained
localization. Furthermore, considering that videos with different resolution
and sentences with different length have different difficulty in understanding,
we design the simple yet effective Res-BiGRUs for feature fusion, which is able
to grasp the useful information in a self-adapting manner. Extensive
experiments conducted on Charades-STA and ActivityNet-Captions datasets
demonstrate the superiority of our HDRR model compared with other
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSC-IITISM at FinCausal 2021: Combining POS tagging with Attention-based Contextual Representations for Identifying Causal Relationships in Financial Documents. (arXiv:2111.00490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00490">
<div class="article-summary-box-inner">
<span><p>Causality detection draws plenty of attention in the field of Natural
Language Processing and linguistics research. It has essential applications in
information retrieval, event prediction, question answering, financial
analysis, and market research. In this study, we explore several methods to
identify and extract cause-effect pairs in financial documents using
transformers. For this purpose, we propose an approach that combines POS
tagging with the BIO scheme, which can be integrated with modern transformer
models to address this challenge of identifying causality in a given text. Our
best methodology achieves an F1-Score of 0.9551, and an Exact Match Score of
0.8777 on the blind test in the FinCausal-2021 Shared Task at the FinCausal
2021 Workshop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualization: the missing factor in Simultaneous Speech Translation. (arXiv:2111.00514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00514">
<div class="article-summary-box-inner">
<span><p>Simultaneous speech translation (SimulST) is the task in which output
generation has to be performed on partial, incremental speech input. In recent
years, SimulST has become popular due to the spread of cross-lingual
application scenarios, like international live conferences and streaming
lectures, in which on-the-fly speech translation can facilitate users' access
to audio-visual content. In this paper, we analyze the characteristics of the
SimulST systems developed so far, discussing their strengths and weaknesses. We
then concentrate on the evaluation framework required to properly assess
systems' effectiveness. To this end, we raise the need for a broader
performance analysis, also including the user experience standpoint. SimulST
systems, indeed, should be evaluated not only in terms of quality/latency
measures, but also via task-oriented metrics accounting, for instance, for the
visualization strategy adopted. In light of this, we highlight which are the
goals achieved by the community and what is still missing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinEAS: Financial Embedding Analysis of Sentiment. (arXiv:2111.00526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00526">
<div class="article-summary-box-inner">
<span><p>We introduce a new language representation model in finance called Financial
Embedding Analysis of Sentiment (FinEAS). In financial markets, news and
investor sentiment are significant drivers of security prices. Thus, leveraging
the capabilities of modern NLP approaches for financial sentiment analysis is a
crucial component in identifying patterns and trends that are useful for market
participants and regulators. In recent years, methods that use transfer
learning from large Transformer-based language models like BERT, have achieved
state-of-the-art results in text classification tasks, including sentiment
analysis using labelled datasets. Researchers have quickly adopted these
approaches to financial texts, but best practices in this domain are not
well-established. In this work, we propose a new model for financial sentiment
analysis based on supervised fine-tuned sentence embeddings from a standard
BERT model. We demonstrate our approach achieves significant improvements in
comparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific
BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Reasoning via Template Filling. (arXiv:2111.00539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00539">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the ability of sequence to sequence models to
perform cross-domain reasoning. Towards this, we present a
prompt-template-filling approach to enable sequence to sequence models to
perform cross-domain reasoning. We also present a case-study with commonsense
and health and well-being domains, where we study how prompt-template-filling
enables pretrained sequence to sequence models across domains. Our experiments
across several pretrained encoder-decoder models show that cross-domain
reasoning is challenging for current models. We also show an in-depth error
analysis and avenues for future research for reasoning across domains
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality Estimation Using Round-trip Translation with Sentence Embeddings. (arXiv:2111.00554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00554">
<div class="article-summary-box-inner">
<span><p>Estimating the quality of machine translation systems has been an ongoing
challenge for researchers in this field. Many previous attempts at using
round-trip translation as a measure of quality have failed, and there is much
disagreement as to whether it can be a viable method of quality estimation. In
this paper, we revisit round-trip translation, proposing a system which aims to
solve the previous pitfalls found with the approach. Our method makes use of
recent advances in language representation learning to more accurately gauge
the similarity between the original and round-trip translated sentences.
Experiments show that while our approach does not reach the performance of
current state of the art methods, it may still be an effective approach for
some language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing and Protecting Labels in Distributed Training. (arXiv:2111.00556v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00556">
<div class="article-summary-box-inner">
<span><p>Distributed learning paradigms such as federated learning often involve
transmission of model updates, or gradients, over a network, thereby avoiding
transmission of private data. However, it is possible for sensitive information
about the training data to be revealed from such gradients. Prior works have
demonstrated that labels can be revealed analytically from the last layer of
certain models (e.g., ResNet), or they can be reconstructed jointly with model
inputs by using Gradients Matching [Zhu et al'19] with additional knowledge
about the current state of the model. In this work, we propose a method to
discover the set of labels of training samples from only the gradient of the
last layer and the id to label mapping. Our method is applicable to a wide
variety of model architectures across multiple domains. We demonstrate the
effectiveness of our method for model training in two domains - image
classification, and automatic speech recognition. Furthermore, we show that
existing reconstruction techniques improve their efficacy when used in
conjunction with our method. Conversely, we demonstrate that gradient
quantization and sparsification can significantly reduce the success of the
attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Approach to Inference-Driven Dialogue Management within a Social Chatbot. (arXiv:2111.00570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00570">
<div class="article-summary-box-inner">
<span><p>We present a chatbot implementing a novel dialogue management approach based
on logical inference. Instead of framing conversation a sequence of response
generation tasks, we model conversation as a collaborative inference process in
which speakers share information to synthesize new knowledge in real time. Our
chatbot pipeline accomplishes this modelling in three broad stages. The first
stage translates user utterances into a symbolic predicate representation. The
second stage then uses this structured representation in conjunction with a
larger knowledge base to synthesize new predicates using efficient graph
matching. In the third and final stage, our bot selects a small subset of
predicates and translates them into an English response. This approach lends
itself to understanding latent semantics of user inputs, flexible initiative
taking, and responses that are novel and coherent with the dialogue context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Went Wrong? Explaining Overall Dialogue Quality through Utterance-Level Impacts. (arXiv:2111.00572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00572">
<div class="article-summary-box-inner">
<span><p>Improving user experience of a dialogue system often requires intensive
developer effort to read conversation logs, run statistical analyses, and
intuit the relative importance of system shortcomings. This paper presents a
novel approach to automated analysis of conversation logs that learns the
relationship between user-system interactions and overall dialogue quality.
Unlike prior work on utterance-level quality prediction, our approach learns
the impact of each interaction from the overall user rating without
utterance-level annotation, allowing resultant model conclusions to be derived
on the basis of empirical evidence and at low cost. Our model identifies
interactions that have a strong correlation with the overall dialogue quality
in a chatbot setting. Experiments show that the automated analysis from our
model agrees with expert judgments, making this work the first to show that
such weakly-supervised learning of utterance-level quality prediction is highly
achievable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Classification for Task-based Source Code Related Questions. (arXiv:2111.00580v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00580">
<div class="article-summary-box-inner">
<span><p>There is a key demand to automatically generate code for small tasks for
developers. Websites such as StackOverflow provide a simplistic way by offering
solutions in small snippets which provide a complete answer to whatever task
question the developer wants to code. Natural Language Processing and
particularly Question-Answering Systems are very helpful in resolving and
working on these tasks. In this paper, we develop a two-fold deep learning
model: Seq2Seq and a binary classifier that takes in the intent (which is in
natural language) and code snippets in Python. We train both the intent and the
code utterances in the Seq2Seq model, where we decided to compare the effect of
the hidden layer embedding from the encoder for representing the intent and
similarly, using the decoder's hidden layer embeddings for the code sequence.
Then we combine both these embeddings and then train a simple binary neural
network classifier model for predicting if the intent is correctly answered by
the predicted code sequence from the seq2seq model. We find that the hidden
state layer's embeddings perform slightly better than regular standard
embeddings from a constructed vocabulary. We experimented with our tests on the
CoNaLa dataset in addition to the StaQC database consisting of simple task-code
snippet-based pairs. We empirically establish that using additional pre-trained
embeddings for code snippets in Python is less context-based in comparison to
using hidden state context vectors from seq2seq models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum Description Length Recurrent Neural Networks. (arXiv:2111.00600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00600">
<div class="article-summary-box-inner">
<span><p>We train neural networks to optimize a Minimum Description Length score,
i.e., to balance between the complexity of the network and its accuracy at a
task. We show that networks trained with this objective function master tasks
involving memory challenges such as counting, including cases that go beyond
context-free languages. These learners master grammars for, e.g., $a^nb^n$,
$a^nb^nc^n$, $a^nb^{2n}$, and $a^nb^mc^{n+m}$, and they perform addition. They
do so with 100% accuracy, sometimes also with 100% confidence. The networks are
also small and their inner workings are transparent. We thus provide formal
proofs that their perfect accuracy holds not only on a given test set, but for
any input sequence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Investigation of Commonsense Understanding in Large Language Models. (arXiv:2111.00607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00607">
<div class="article-summary-box-inner">
<span><p>Large language models have shown impressive performance on many natural
language processing (NLP) tasks in a zero-shot setting. We ask whether these
models exhibit commonsense understanding -- a critical component of NLP
applications -- by evaluating models against four commonsense benchmarks. We
find that the impressive zero-shot performance of large language models is
mostly due to existence of dataset bias in our benchmarks. We also show that
the zero-shot performance is sensitive to the choice of hyper-parameters and
similarity of the benchmark to the pre-training datasets. Moreover, we did not
observe substantial improvements when evaluating models in a few-shot setting.
Finally, in contrast to previous work, we find that leveraging explicit
commonsense knowledge does not yield substantial improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Unreasonable Effectiveness of Machine Learning in Moldavian versus Romanian Dialect Identification. (arXiv:2007.15700v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15700">
<div class="article-summary-box-inner">
<span><p>Motivated by the seemingly high accuracy levels of machine learning models in
Moldavian versus Romanian dialect identification and the increasing research
interest on this topic, we provide a follow-up on the Moldavian versus Romanian
Cross-Dialect Topic Identification (MRC) shared task of the VarDial 2019
Evaluation Campaign. The shared task included two sub-task types: one that
consisted in discriminating between the Moldavian and Romanian dialects and one
that consisted in classifying documents by topic across the two dialects of
Romanian. Participants achieved impressive scores, e.g. the top model for
Moldavian versus Romanian dialect identification obtained a macro F1 score of
0.895. We conduct a subjective evaluation by human annotators, showing that
humans attain much lower accuracy rates compared to machine learning (ML)
models. Hence, it remains unclear why the methods proposed by participants
attain such high accuracy rates. Our goal is to understand (i) why the proposed
methods work so well (by visualizing the discriminative features) and (ii) to
what extent these methods can keep their high accuracy levels, e.g. when we
shorten the text samples to single sentences or when we use tweets at inference
time. A secondary goal of our work is to propose an improved ML model using
ensemble learning. Our experiments show that ML models can accurately identify
the dialects, even at the sentence level and across different domains (news
articles versus tweets). We also analyze the most discriminative features of
the best performing models, providing some explanations behind the decisions
taken by these models. Interestingly, we learn new dialectal patterns
previously unknown to us or to our human annotators. Furthermore, we conduct
experiments showing that the machine learning performance on the MRC shared
task can be improved through an ensemble based on stacking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking for Clues of Language in Multilingual BERT to Improve Cross-lingual Generalization. (arXiv:2010.10041v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.10041">
<div class="article-summary-box-inner">
<span><p>Token embeddings in multilingual BERT (m-BERT) contain both language and
semantic information. We find that the representation of a language can be
obtained by simply averaging the embeddings of the tokens of the language.
Given this language representation, we control the output languages of
multilingual BERT by manipulating the token embeddings, thus achieving
unsupervised token translation. We further propose a computationally cheap but
effective approach to improve the cross-lingual ability of m-BERT based on this
observation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Multiple Choices Question Answering: Start Learning from Basic Knowledge. (arXiv:2010.11003v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11003">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the possibility of almost unsupervised Multiple
Choices Question Answering (MCQA). Starting from very basic knowledge, MCQA
model knows that some choices have higher probabilities of being correct than
the others. The information, though very noisy, guides the training of an MCQA
model. The proposed method is shown to outperform the baseline approaches on
RACE and even comparable with some supervised learning approaches on MC500.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Text Style Transfer: A Survey. (arXiv:2011.00416v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00416">
<div class="article-summary-box-inner">
<span><p>Text style transfer is an important task in natural language generation,
which aims to control certain attributes in the generated text, such as
politeness, emotion, humor, and many others. It has a long history in the field
of natural language processing, and recently has re-gained significant
attention thanks to the promising performance brought by deep neural models. In
this paper, we present a systematic survey of the research on neural text style
transfer, spanning over 100 representative articles since the first neural text
style transfer work in 2017. We discuss the task formulation, existing datasets
and subtasks, evaluation, as well as the rich methodologies in the presence of
parallel and non-parallel data. We also provide discussions on a variety of
important topics regarding the future development of this task. Our curated
paper list is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Portuguese Semantic Role Labeling with Transformers and Transfer Learning. (arXiv:2101.01213v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01213">
<div class="article-summary-box-inner">
<span><p>The Natural Language Processing task of determining "Who did what to whom" is
called Semantic Role Labeling. For English, recent methods based on Transformer
models have allowed for major improvements in this task over the previous state
of the art. However, for low resource languages, like Portuguese, currently
available semantic role labeling models are hindered by scarce training data.
In this paper, we explore a model architecture with only a pre-trained
Transformer-based model, a linear layer, softmax and Viterbi decoding. We
substantially improve the state-of-the-art performance in Portuguese by over 15
F1. Additionally, we improve semantic role labeling results in Portuguese
corpora by exploiting cross-lingual transfer learning using multilingual
pre-trained models, and transfer learning from dependency parsing in
Portuguese, evaluating the various proposed approaches empirically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Extraction of Causal Relations from Natural Language Text. (arXiv:2101.06426v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06426">
<div class="article-summary-box-inner">
<span><p>As an essential component of human cognition, cause-effect relations appear
frequently in text, and curating cause-effect relations from text helps in
building causal networks for predictive tasks. Existing causality extraction
techniques include knowledge-based, statistical machine learning(ML)-based, and
deep learning-based approaches. Each method has its advantages and weaknesses.
For example, knowledge-based methods are understandable but require extensive
manual domain knowledge and have poor cross-domain applicability. Statistical
machine learning methods are more automated because of natural language
processing (NLP) toolkits. However, feature engineering is labor-intensive, and
toolkits may lead to error propagation. In the past few years, deep learning
techniques attract substantial attention from NLP researchers because of its'
powerful representation learning ability and the rapid increase in
computational resources. Their limitations include high computational costs and
a lack of adequate annotated training data. In this paper, we conduct a
comprehensive survey of causality extraction. We initially introduce primary
forms existing in the causality extraction: explicit intra-sentential
causality, implicit causality, and inter-sentential causality. Next, we list
benchmark datasets and modeling assessment methods for causal relation
extraction. Then, we present a structured overview of the three techniques with
their representative systems. Lastly, we highlight existing open challenges
with their potential directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">First Target and Opinion then Polarity: Enhancing Target-opinion Correlation for Aspect Sentiment Triplet Extraction. (arXiv:2102.08549v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08549">
<div class="article-summary-box-inner">
<span><p>Aspect Sentiment Triplet Extraction (ASTE) aims to extract triplets from a
sentence, including target entities, associated sentiment polarities, and
opinion spans which rationalize the polarities. Existing methods are short on
building correlation between target-opinion pairs, and neglect the mutual
interference among different sentiment triplets. To address these issues, we
utilize a two-stage framework to enhance the correlation between targets and
opinions: at stage one, we extract targets and opinions through sequence
tagging; then we append a group of artificial tags named Perceivable Pair,
which indicate the span of a specific target-opinion tuple, to the input
sentence to obtain closer correlated target-opinion pair representation.
Meanwhile, we reduce the negative interference between triplets by restricting
tokens' attention field. Finally, the polarity is identified according to the
representation of the Perceivable Pair. We conduct experiments on four
datasets, and the experimental results show the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELLA: Exploration through Learned Language Abstraction. (arXiv:2103.05825v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05825">
<div class="article-summary-box-inner">
<span><p>Building agents capable of understanding language instructions is critical to
effective and robust human-AI collaboration. Recent work focuses on training
these agents via reinforcement learning in environments with synthetic
language; however, instructions often define long-horizon, sparse-reward tasks,
and learning policies requires many episodes of experience. We introduce ELLA:
Exploration through Learned Language Abstraction, a reward shaping approach
geared towards boosting sample efficiency in sparse reward environments by
correlating high-level instructions with simpler low-level constituents. ELLA
has two key elements: 1) A termination classifier that identifies when agents
complete low-level instructions, and 2) A relevance classifier that correlates
low-level instructions with success on high-level tasks. We learn the
termination classifier offline from pairs of instructions and terminal states.
Notably, in departure from prior work in language and abstraction, we learn the
relevance classifier online, without relying on an explicit decomposition of
high-level instructions to low-level instructions. On a suite of complex BabyAI
environments with varying instruction complexities and reward sparsity, ELLA
shows gains in sample efficiency relative to language-based shaping and
traditional RL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HBert + BiasCorp -- Fighting Racism on the Web. (arXiv:2104.02242v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02242">
<div class="article-summary-box-inner">
<span><p>Subtle and overt racism is still present both in physical and online
communities today and has impacted many lives in different segments of the
society. In this short piece of work, we present how we're tackling this
societal issue with Natural Language Processing. We are releasing BiasCorp, a
dataset containing 139,090 comments and news segment from three specific
sources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually
annotated) is ready for publication. We are currently in the final phase of
manually labeling the remaining dataset using Amazon Mechanical Turk. BERT has
been used widely in several downstream tasks. In this work, we present hBERT,
where we modify certain layers of the pretrained BERT model with the new
Hopfield Layer. hBert generalizes well across different distributions with the
added advantage of a reduced model complexity. We are also releasing a
JavaScript library and a Chrome Extension Application, to help developers make
use of our trained model in web applications (say chat application) and for
users to identify and report racially biased contents on the web respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Achieving Model Robustness through Discrete Adversarial Training. (arXiv:2104.05062v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05062">
<div class="article-summary-box-inner">
<span><p>Discrete adversarial attacks are symbolic perturbations to a language input
that preserve the output label but lead to a prediction error. While such
attacks have been extensively explored for the purpose of evaluating model
robustness, their utility for improving robustness has been limited to offline
augmentation only. Concretely, given a trained model, attacks are used to
generate perturbed (adversarial) examples, and the model is re-trained exactly
once. In this work, we address this gap and leverage discrete attacks for
online augmentation, where adversarial examples are generated at every training
step, adapting to the changing nature of the model. We propose (i) a new
discrete attack, based on best-first search, and (ii) random sampling attacks
that unlike prior work are not based on expensive search-based procedures.
Surprisingly, we find that random sampling leads to impressive gains in
robustness, outperforming the commonly-used offline augmentation, while leading
to a speedup at training time of ~10x. Furthermore, online augmentation with
search-based attacks justifies the higher training cost, significantly
improving robustness on three datasets. Last, we show that our new attack
substantially improves robustness compared to prior methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing small BERTs trained for German NER. (arXiv:2104.11559v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11559">
<div class="article-summary-box-inner">
<span><p>Currently, the most widespread neural network architecture for training
language models is the so called BERT which led to improvements in various
Natural Language Processing (NLP) tasks. In general, the larger the number of
parameters in a BERT model, the better the results obtained in these NLP tasks.
Unfortunately, the memory consumption and the training duration drastically
increases with the size of these models. In this article, we investigate
various training techniques of smaller BERT models: We combine different
methods from other BERT variants like ALBERT, RoBERTa, and relative positional
encoding. In addition, we propose two new fine-tuning modifications leading to
better performance: Class-Start-End tagging and a modified form of Linear Chain
Conditional Random Fields. Furthermore, we introduce Whole-Word Attention which
reduces BERTs memory usage and leads to a small increase in performance
compared to classical Multi-Head-Attention. We evaluate these techniques on
five public German Named Entity Recognition (NER) tasks of which two are
introduced by this article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PREDICT: Persian Reverse Dictionary. (arXiv:2105.00309v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00309">
<div class="article-summary-box-inner">
<span><p>Finding the appropriate words to convey concepts (i.e., lexical access) is
essential for effective communication. Reverse dictionaries fulfill this need
by helping individuals to find the word(s) which could relate to a specific
concept or idea. To the best of our knowledge, this resource has not been
available for the Persian language. In this paper, we compare four different
architectures for implementing a Persian reverse dictionary (PREDICT).
</p>
<p>We evaluate our models using (phrase,word) tuples extracted from the only
Persian dictionaries available online, namely Amid, Moein, and Dehkhoda where
the phrase describes the word. Given the phrase, a model suggests the most
relevant word(s) in terms of the ability to convey the concept. The model is
considered to perform well if the correct word is one of its top suggestions.
</p>
<p>Our experiments show that a model consisting of Long Short-Term Memory (LSTM)
units enhanced by an additive attention mechanism is enough to produce
suggestions comparable to (or in some cases better than) the word in the
original dictionary. The study also reveals that the model sometimes produces
the synonyms of the word as its output which led us to introduce a new metric
for the evaluation of reverse dictionaries called Synonym Accuracy accounting
for the percentage of times the event of producing the word or a synonym of it
occurs. The assessment of the best model using this new metric also indicates
that at least 62% of the times, it produces an accurate result within the top
100 suggestions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual and crosslingual speech recognition using phonological-vector based phone embeddings. (arXiv:2107.05038v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05038">
<div class="article-summary-box-inner">
<span><p>The use of phonological features (PFs) potentially allows language-specific
phones to remain linked in training, which is highly desirable for information
sharing for multilingual and crosslingual speech recognition methods for
low-resourced languages. A drawback suffered by previous methods in using
phonological features is that the acoustic-to-PF extraction in a bottom-up way
is itself difficult. In this paper, we propose to join phonology driven phone
embedding (top-down) and deep neural network (DNN) based acoustic feature
extraction (bottom-up) to calculate phone probabilities. The new method is
called JoinAP (Joining of Acoustics and Phonology). Remarkably, no inversion
from acoustics to phonological features is required for speech recognition. For
each phone in the IPA (International Phonetic Alphabet) table, we encode its
phonological features to a phonological-vector, and then apply linear or
nonlinear transformation of the phonological-vector to obtain the phone
embedding. A series of multilingual and crosslingual (both zero-shot and
few-shot) speech recognition experiments are conducted on the CommonVoice
dataset (German, French, Spanish and Italian) and the AISHLL-1 dataset
(Mandarin), and demonstrate the superiority of JoinAP with nonlinear phone
embeddings over both JoinAP with linear phone embeddings and the traditional
method with flat phone embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01135">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence learning with neural networks has become the de facto
standard for sequence prediction tasks. This approach typically models the
local distribution over the next word with a powerful neural network that can
condition on arbitrary context. While flexible and performant, these models
often require large datasets for training and can fail spectacularly on
benchmarks designed to test for compositional generalization. This work
explores an alternative, hierarchical approach to sequence-to-sequence learning
with quasi-synchronous grammars, where each node in the target tree is
transduced by a node in the source tree. Both the source and target trees are
treated as latent and induced during training. We develop a neural
parameterization of the grammar which enables parameter sharing over the
combinatorial space of derivation rules without the need for manual feature
engineering. We apply this latent neural grammar to various domains -- a
diagnostic language navigation task designed to test for compositional
generalization (SCAN), style transfer, and small-scale machine translation --
and find that it performs respectably compared to standard baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01900">
<div class="article-summary-box-inner">
<span><p>Identifying emotions from text is crucial for a variety of real world tasks.
We consider the two largest now-available corpora for emotion classification:
GoEmotions, with 58k messages labelled by readers, and Vent, with 33M
writer-labelled messages. We design a benchmark and evaluate several feature
spaces and learning algorithms, including two simple yet novel models on top of
BERT that outperform previous strong baselines on GoEmotions. Through an
experiment with human participants, we also analyze the differences between how
writers express emotions and how readers perceive them. Our results suggest
that emotions expressed by writers are harder to identify than emotions that
readers perceive. We share a public web interface for researchers to explore
our models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker-Oriented Latent Structures for Dialogue-Based Relation Extraction. (arXiv:2109.05182v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05182">
<div class="article-summary-box-inner">
<span><p>Dialogue-based relation extraction (DiaRE) aims to detect the structural
information from unstructured utterances in dialogues. Existing relation
extraction models may be unsatisfactory under such a conversational setting,
due to the entangled logic and information sparsity issues in utterances
involving multiple speakers. To this end, we introduce SOLS, a novel model
which can explicitly induce speaker-oriented latent structures for better
DiaRE. Specifically, we learn latent structures to capture the relationships
among tokens beyond the utterance boundaries, alleviating the entangled logic
issue. During the learning process, our speaker-specific regularization method
progressively highlights speaker-related key clues and erases the irrelevant
ones, alleviating the information sparsity issue. Experiments on three public
datasets demonstrate the effectiveness of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08722">
<div class="article-summary-box-inner">
<span><p>Prerequisite chain learning helps people acquire new knowledge efficiently.
While people may quickly determine learning paths over concepts in a domain,
finding such paths in other domains can be challenging. We introduce
Domain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this
cross-domain prerequisite chain learning task efficiently. Our novel model
consists of a variational graph autoencoder (VGAE) and a domain discriminator.
The VGAE is trained to predict concept relations through link prediction, while
the domain discriminator takes both source and target domain data as input and
is trained to predict domain labels. Most importantly, this method only needs
simple homogeneous graphs as input, compared with the current state-of-the-art
model. We evaluate our model on the LectureBankCD dataset, and results show
that our model outperforms recent graph-based benchmarks while using only 1/10
of graph scale and 1/3 computation time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsolved Problems in ML Safety. (arXiv:2109.13916v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13916">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards ("Robustness"),
identifying hazards ("Monitoring"), steering ML systems ("Alignment"), and
reducing hazards in deployment ("External Safety"). Throughout, we clarify each
problem's motivation and provide concrete research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors. (arXiv:2110.04399v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04399">
<div class="article-summary-box-inner">
<span><p>Evaluation metrics are a key ingredient for progress of text generation
systems. In recent years, several BERT-based evaluation metrics have been
proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much
better with human assessment of text generation quality than BLEU or ROUGE,
invented two decades ago. However, little is known what these metrics, which
are based on black-box language model representations, actually capture (it is
typically assumed they model semantic similarity). In this work, we use a
simple regression based global explainability technique to disentangle metric
scores along linguistic factors, including semantics, syntax, morphology, and
lexical overlap. We show that the different metrics capture all aspects to some
degree, but that they are all substantially sensitive to lexical overlap, just
like BLEU and ROUGE. This exposes limitations of these novelly proposed
metrics, which we also highlight in an adversarial test scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Hate Speech Detection in Code Mixed Text using Transformer Based Approaches. (arXiv:2110.09338v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09338">
<div class="article-summary-box-inner">
<span><p>In the recent past, social media platforms have helped people in connecting
and communicating to a wider audience. But this has also led to a drastic
increase in cyberbullying. It is essential to detect and curb hate speech to
keep the sanity of social media platforms. Also, code mixed text containing
more than one language is frequently used on these platforms. We, therefore,
propose automated techniques for hate speech detection in code mixed text from
scraped Twitter. We specifically focus on code mixed English-Hindi text and
transformer-based approaches. While regular approaches analyze the text
independently, we also make use of content text in the form of parent tweets.
We try to evaluate the performances of multilingual BERT and Indic-BERT in
single-encoder and dual-encoder settings. The first approach is to concatenate
the target text and context text using a separator token and get a single
representation from the BERT model. The second approach encodes the two texts
independently using a dual BERT encoder and the corresponding representations
are averaged. We show that the dual-encoder approach using independent
representations yields better performance. We also employ simple ensemble
methods to further improve the performance. Using these methods we report the
best F1 score of 73.07% on the HASOC 2021 ICHCL code mixed data set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NormFormer: Improved Transformer Pretraining with Extra Normalization. (arXiv:2110.09456v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09456">
<div class="article-summary-box-inner">
<span><p>During pretraining, the Pre-LayerNorm transformer suffers from a gradient
magnitude mismatch: gradients at early layers are much larger than at later
layers. These issues can be alleviated by our proposed NormFormer architecture,
which adds three normalization operations to each layer: a Layer Norm after
self attention, head-wise scaling of self-attention outputs, and a Layer Norm
after the first fully connected layer. The extra operations incur negligible
compute cost (+0.4% parameter increase), but improve pretraining perplexity and
downstream task performance for both causal and masked language models ranging
from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on
top of our strongest 1.3B parameter baseline can reach equal perplexity 24%
faster, or converge 0.27 perplexity better in the same compute budget. This
model reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked
language modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on
average. Code to train NormFormer models is available in fairseq
https://github.com/pytorch/fairseq/tree/main/examples/normformer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate and Offensive Speech Detection in Hindi and Marathi. (arXiv:2110.12200v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12200">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is the most basic NLP task to determine the polarity of
text data. There has been a significant amount of work in the area of
multilingual text as well. Still hate and offensive speech detection faces a
challenge due to inadequate availability of data, especially for Indian
languages like Hindi and Marathi. In this work, we consider hate and offensive
speech detection in Hindi and Marathi texts. The problem is formulated as a
text classification task using the state of the art deep learning approaches.
We explore different deep learning architectures like CNN, LSTM, and variations
of BERT like multilingual BERT, IndicBERT, and monolingual RoBERTa. The basic
models based on CNN and LSTM are augmented with fast text word embeddings. We
use the HASOC 2021 Hindi and Marathi hate speech datasets to compare these
algorithms. The Marathi dataset consists of binary labels and the Hindi dataset
consists of binary as well as more-fine grained labels. We show that the
transformer-based models perform the best and even the basic models along with
FastText embeddings give a competitive performance. Moreover, with normal
hyper-parameter tuning, the basic models perform better than BERT-based models
on the fine-grained Hindi dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Entity Representation Model for Reasoning over Knowledge Graphs. (arXiv:2110.13522v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13522">
<div class="article-summary-box-inner">
<span><p>Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique that
can provide efficient querying mechanism over large and incomplete databases.
Current approaches employ spatial geometries such as boxes to learn query
representations that encompass the answer entities and model the logical
operations of projection and intersection. However, their geometry is
restrictive and leads to non-smooth strict boundaries, which further results in
ambiguous answer entities. Furthermore, previous works propose transformation
tricks to handle unions which results in non-closure and, thus, cannot be
chained in a stream. In this paper, we propose a Probabilistic Entity
Representation Model (PERM) to encode entities as a Multivariate Gaussian
density with mean and covariance parameters to capture its semantic position
and smooth decision boundary, respectively. Additionally, we also define the
closed logical operations of projection, intersection, and union that can be
aggregated using an end-to-end objective function. On the logical query
reasoning problem, we demonstrate that the proposed PERM significantly
outperforms the state-of-the-art methods on various public benchmark KG
datasets on standard evaluation metrics. We also evaluate PERM's competence on
a COVID-19 drug-repurposing case study and show that our proposed work is able
to recommend drugs with substantially better F1 than current methods. Finally,
we demonstrate the working of our PERM's query answering process through a
low-dimensional visualization of the Gaussian representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Vagueness Detection with Deep Learning to Identify Fake News. (arXiv:2110.14780v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14780">
<div class="article-summary-box-inner">
<span><p>In this paper, we combine two independent detection methods for identifying
fake news: the algorithm VAGO uses semantic rules combined with NLP techniques
to measure vagueness and subjectivity in texts, while the classifier FAKE-CLF
relies on Convolutional Neural Network classification and supervised deep
learning to classify texts as biased or legitimate. We compare the results of
the two methods on four corpora. We find a positive correlation between the
vagueness and subjectivity measures obtained by VAGO, and the classification of
text as biased by FAKE-CLF. The comparison yields mutual benefits: VAGO helps
explain the results of FAKE-CLF. Conversely FAKE-CLF helps us corroborate and
expand VAGO's database. The use of two complementary techniques (rule-based vs
data-driven) proves a fruitful approach for the challenging problem of
identifying fake news.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Hierarchical Similarity Metric Learning with Noisy Labels. (arXiv:2111.00006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00006">
<div class="article-summary-box-inner">
<span><p>Deep Metric Learning (DML) plays a critical role in various machine learning
tasks. However, most existing deep metric learning methods with binary
similarity are sensitive to noisy labels, which are widely present in
real-world data. Since these noisy labels often cause severe performance
degradation, it is crucial to enhance the robustness and generalization ability
of DML. In this paper, we propose an Adaptive Hierarchical Similarity Metric
Learning method. It considers two noise-insensitive information, \textit{i.e.},
class-wise divergence and sample-wise consistency. Specifically, class-wise
divergence can effectively excavate richer similarity information beyond binary
in modeling by taking advantage of Hyperbolic metric learning, while
sample-wise consistency can further improve the generalization ability of the
model using contrastive augmentation. More importantly, we design an adaptive
strategy to integrate this information in a unified view. It is noteworthy that
the new method can be extended to any pair-based metric loss. Extensive
experimental results on benchmark datasets demonstrate that our method achieves
state-of-the-art performance compared with current deep metric learning
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Agnostic Few-Shot Learning For Document Intelligence. (arXiv:2111.00007v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00007">
<div class="article-summary-box-inner">
<span><p>Few-shot learning aims to generalize to novel classes with only a few samples
with class labels. Research in few-shot learning has borrowed techniques from
transfer learning, metric learning, meta-learning, and Bayesian methods. These
methods also aim to train models from limited training samples, and while
encouraging performance has been achieved, they often fail to generalize to
novel domains. Many of the existing meta-learning methods rely on training data
for which the base classes are sampled from the same domain as the novel
classes used for meta-testing. However, in many applications in the industry,
such as document classification, collecting large samples of data for
meta-learning is infeasible or impossible. While research in the field of the
cross-domain few-shot learning exists, it is mostly limited to computer vision.
To our knowledge, no work yet exists that examines the use of few-shot learning
for classification of semi-structured documents (scans of paper documents)
generated as part of a business workflow (forms, letters, bills, etc.). Here
the domain shift is significant, going from natural images to the
semi-structured documents of interest. In this work, we address the problem of
few-shot document image classification under domain shift. We evaluate our work
by extensive comparisons with existing methods. Experimental results
demonstrate that the proposed method shows consistent improvements on the
few-shot classification performance under domain shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-device Real-time Hand Gesture Recognition. (arXiv:2111.00038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00038">
<div class="article-summary-box-inner">
<span><p>We present an on-device real-time hand gesture recognition (HGR) system,
which detects a set of predefined static gestures from a single RGB camera. The
system consists of two parts: a hand skeleton tracker and a gesture classifier.
We use MediaPipe Hands as the basis of the hand skeleton tracker, improve the
keypoint accuracy, and add the estimation of 3D keypoints in a world metric
space. We create two different gesture classifiers, one based on heuristics and
the other using neural networks (NN).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CvS: Classification via Segmentation For Small Datasets. (arXiv:2111.00042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00042">
<div class="article-summary-box-inner">
<span><p>Deep learning models have shown promising results in a wide range of computer
vision applications across various domains. The success of deep learning
methods relies heavily on the availability of a large amount of data. Deep
neural networks are prone to overfitting when data is scarce. This problem
becomes even more severe for neural network with classification head with
access to only a few data points. However, acquiring large-scale datasets is
very challenging, laborious, or even infeasible in some domains. Hence,
developing classifiers that are able to perform well in small data regimes is
crucial for applications with limited data. This paper presents CvS, a
cost-effective classifier for small datasets that derives the classification
labels from predicting the segmentation maps. We employ the label propagation
method to achieve a fully segmented dataset with only a handful of manually
segmented data. We evaluate the effectiveness of our framework on diverse
problems showing that CvS is able to achieve much higher classification results
compared to previous methods when given only a handful of examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Data Weighting via Class-level Gradient Manipulation. (arXiv:2111.00056v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00056">
<div class="article-summary-box-inner">
<span><p>Label noise and class imbalance are two major issues coexisting in real-world
datasets. To alleviate the two issues, state-of-the-art methods reweight each
instance by leveraging a small amount of clean and unbiased data. Yet, these
methods overlook class-level information within each instance, which can be
further utilized to improve performance. To this end, in this paper, we propose
Generalized Data Weighting (GDW) to simultaneously mitigate label noise and
class imbalance by manipulating gradients at the class level. To be specific,
GDW unrolls the loss gradient to class-level gradients by the chain rule and
reweights the flow of each gradient separately. In this way, GDW achieves
remarkable performance improvement on both issues. Aside from the performance
gain, GDW efficiently obtains class-level weights without introducing any extra
computational cost compared with instance weighting methods. Specifically, GDW
performs a gradient descent step on class-level weights, which only relies on
intermediate gradients. Extensive experiments in various settings verify the
effectiveness of GDW. For example, GDW outperforms state-of-the-art methods by
$2.56\%$ under the $60\%$ uniform noise setting in CIFAR10. Our code is
available at https://github.com/GGchen1997/GDW-NIPS2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polyline Based Generative Navigable Space Segmentation for Autonomous Visual Navigation. (arXiv:2111.00063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00063">
<div class="article-summary-box-inner">
<span><p>Detecting navigable space is a fundamental capability for mobile robots
navigating in unknown or unmapped environments. In this work, we treat the
visual navigable space segmentation as a scene decomposition problem and
propose Polyline Segmentation Variational AutoEncoder Networks (PSV-Nets), a
representation-learning-based framework to enable robots to learn the navigable
space segmentation in an unsupervised manner. Current segmentation techniques
heavily rely on supervised learning strategies which demand a large amount of
pixel-level annotated images. In contrast, the proposed framework leverages a
generative model - Variational AutoEncoder (VAE) and an AutoEncoder (AE) to
learn a polyline representation that compactly outlines the desired navigable
space boundary in an unsupervised way. We also propose a visual receding
horizon planning method that uses the learned navigable space and a Scaled
Euclidean Distance Field (SEDF) to achieve autonomous navigation without an
explicit map. Through extensive experiments, we have validated that the
proposed PSV-Nets can learn the visual navigable space with high accuracy, even
without any single label. We also show that the prediction of the PSV-Nets can
be further improved with a small number of labels (if available) and can
significantly outperform the state-of-the-art fully supervised-learning-based
segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepDoseNet: A Deep Learning model for 3D Dose Prediction in Radiation Therapy. (arXiv:2111.00077v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00077">
<div class="article-summary-box-inner">
<span><p>The DeepDoseNet 3D dose prediction model based on ResNet and Dilated DenseNet
is proposed. The 340 head-and-neck datasets from the 2020 AAPM OpenKBP
challenge were utilized, with 200 for training, 40 for validation, and 100 for
testing. Structures include 56Gy, 63Gy, 70Gy PTVs, and brainstem, spinal cord,
right parotid, left parotid, larynx, esophagus, and mandible OARs. Mean squared
error (MSE) loss, mean absolute error (MAE) loss, and MAE plus dose-volume
histogram (DVH) based loss functions were investigated. Each model's
performance was compared using a 3D dose score, $\bar{S_{D}}$, (mean absolute
difference between ground truth and predicted 3D dose distributions) and a DVH
score, $\bar{S_{DVH}}$ (mean absolute difference between ground truth and
predicted dose-volume metrics).Furthermore, DVH metrics Mean[Gy] and D0.1cc
[Gy] for OARs and D99%, D95%, D1% for PTVs were computed. DeepDoseNet with the
MAE plus DVH-based loss function had the best dose score performance of the
OpenKBP entries. MAE+DVH model had the lowest prediction error (P&lt;0.0001,
Wilcoxon test) on validation and test datasets (validation:
$\bar{S_{D}}$=2.3Gy, $\bar{S_{DVH}}$=1.9Gy; test: $\bar{S_{D}}$=2.0Gy,
$\bar{S_{DVH}}$=1.6Gy) followed by the MAE model (validation:
$\bar{S_{D}}$=3.6Gy, $\bar{S_{DVH}}$=2.4Gy; test: $\bar{S_{D}}$=3.5Gy,
$\bar{S_{DVH}}$=2.3Gy). The MSE model had the highest prediction error
(validation: $\bar{S_{D}}$=3.7Gy, $\bar{S_{DVH}}$=3.2Gy; test:
$\bar{S_{D}}$=3.6Gy, $\bar{S_{DVH}}$=3.0Gy). No significant difference was
found among models in terms of Mean [Gy], but the MAE+DVH model significantly
outperformed the MAE and MSE models in terms of D0.1cc[Gy], particularly for
mandible and parotids on both validation (P&lt;0.01) and test (P&lt;0.0001) datasets.
MAE+DVH outperformed (P&lt;0.0001) in terms of D99%, D95%, D1% for targets.
MAE+DVH reduced $\bar{S_{D}}$ by ~60% and $\bar{S_{DVH}}$ by ~70%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Deterministic Uncertainty for Semantic Segmentation. (arXiv:2111.00079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00079">
<div class="article-summary-box-inner">
<span><p>We extend Deep Deterministic Uncertainty (DDU), a method for uncertainty
estimation using feature space densities, to semantic segmentation. DDU enables
quantifying and disentangling epistemic and aleatoric uncertainty in a single
forward pass through the model. We study the similarity of feature
representations of pixels at different locations for the same class and
conclude that it is feasible to apply DDU location independently, which leads
to a significant reduction in memory consumption compared to pixel dependent
DDU. Using the DeepLab-v3+ architecture on Pascal VOC 2012, we show that DDU
improves upon MC Dropout and Deep Ensembles while being significantly faster to
compute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fetal MRI by robust deep generative prior reconstruction and diffeomorphic registration: application to gestational age prediction. (arXiv:2111.00102v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00102">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging of whole fetal body and placenta is limited by
different sources of motion affecting the womb. Usual scanning techniques
employ single-shot multi-slice sequences where anatomical information in
different slices may be subject to different deformations, contrast variations
or artifacts. Volumetric reconstruction formulations have been proposed to
correct for these factors, but they must accommodate a non-homogeneous and
non-isotropic sampling, so regularization becomes necessary. Thus, in this
paper we propose a deep generative prior for robust volumetric reconstructions
integrated with a diffeomorphic volume to slice registration method.
Experiments are performed to validate our contributions and compare with a
state of the art method in a cohort of $72$ fetal datasets in the range of
$20-36$ weeks gestational age. Results suggest improved image resolution and
more accurate prediction of gestational age at scan when comparing to a state
of the art reconstruction method. In addition, gestational age prediction
results from our volumetric reconstructions compare favourably with existing
brain-based approaches, with boosted accuracy when integrating information of
organs other than the brain. Namely, a mean absolute error of $0.618$ weeks
($R^2=0.958$) is achieved when combining fetal brain and trunk information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FC2T2: The Fast Continuous Convolutional Taylor Transform with Applications in Vision and Graphics. (arXiv:2111.00110v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00110">
<div class="article-summary-box-inner">
<span><p>Series expansions have been a cornerstone of applied mathematics and
engineering for centuries. In this paper, we revisit the Taylor series
expansion from a modern Machine Learning perspective. Specifically, we
introduce the Fast Continuous Convolutional Taylor Transform (FC2T2), a variant
of the Fast Multipole Method (FMM), that allows for the efficient approximation
of low dimensional convolutional operators in continuous space. We build upon
the FMM which is an approximate algorithm that reduces the computational
complexity of N-body problems from O(NM) to O(N+M) and finds application in
e.g. particle simulations. As an intermediary step, the FMM produces a series
expansion for every cell on a grid and we introduce algorithms that act
directly upon this representation. These algorithms analytically but
approximately compute the quantities required for the forward and backward pass
of the backpropagation algorithm and can therefore be employed as (implicit)
layers in Neural Networks. Specifically, we introduce a root-implicit layer
that outputs surface normals and object distances as well as an
integral-implicit layer that outputs a rendering of a radiance field given a 3D
pose. In the context of Machine Learning, $N$ and $M$ can be understood as the
number of model parameters and model evaluations respectively which entails
that, for applications that require repeated function evaluations which are
prevalent in Computer Vision and Graphics, unlike regular Neural Networks, the
techniques introduce in this paper scale gracefully with parameters. For some
applications, this results in a 200x reduction in FLOPs compared to
state-of-the-art approaches at a reasonable or non-existent loss in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of jujube fruit based on several pricing factors using machine learning methods. (arXiv:2111.00112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00112">
<div class="article-summary-box-inner">
<span><p>Jujube is a fruit mainly cultivated in India, China and Iran and has many
health benefits. It is sold both fresh and dried. There are several factors in
jujube pricing such as weight, wrinkles and defections. Some jujube farmers
sell their product all at once, without any proper sorting or classification,
for an average price. Our studies and experiences show that their profit can
increase significantly if their product is sold after the sorting process.
There are some traditional sorting methods for dried jujube fruit but they are
costly, time consuming and can be inaccurate due to human error. Nowadays,
computer vision combined with machine learning methods, is used increasingly in
food industry for sorting and classification purposes and solve many of the
traditional sorting methods' problems. In this paper we are proposing a
computer vision-based method for grading jujube fruits using machine learning
techniques which will take most of the important pricing factors into account
and can be used to increase the profit of farmers. In this method we first
acquire several images from different samples and then extract their visual
features such as color features, shape and size features, texture features,
defection and wrinkle features and then we select the most useful features
using feature selection algorithms like PCA and CFS. A feature vector is
obtained for each sample and we use these vectors to train our classifiers to
be able to specify the corresponding pre-defined group for each of the samples.
We used different classifiers and training methods in order to obtain the best
result and by using decision tree we could reach 98.8% accuracy of the
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Explanations for Convolutional Neural Networks via Latent Traversal. (arXiv:2111.00116v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00116">
<div class="article-summary-box-inner">
<span><p>Lack of explainability in artificial intelligence, specifically deep neural
networks, remains a bottleneck for implementing models in practice. Popular
techniques such as Gradient-weighted Class Activation Mapping (Grad-CAM)
provide a coarse map of salient features in an image, which rarely tells the
whole story of what a convolutional neural network (CNN) learned. Using
COVID-19 chest X-rays, we present a method for interpreting what a CNN has
learned by utilizing Generative Adversarial Networks (GANs). Our GAN framework
disentangles lung structure from COVID-19 features. Using this GAN, we can
visualize the transition of a pair of COVID negative lungs in a chest
radiograph to a COVID positive pair by interpolating in the latent space of the
GAN, which provides fine-grained visualization of how the CNN responds to
varying features within the lungs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Longitudinal Analysis of Mask and No-Mask on Child Face Recognition. (arXiv:2111.00121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00121">
<div class="article-summary-box-inner">
<span><p>Face is one of the most widely employed traits for person recognition, even
in many large-scale applications. Despite technological advancements in face
recognition systems, they still face obstacles caused by pose, expression,
occlusion, and aging variations. Owing to the COVID-19 pandemic, contactless
identity verification has become exceedingly vital. To constrain the pandemic,
people have started using face mask. Recently, few studies have been conducted
on the effect of face mask on adult face recognition systems. However, the
impact of aging with face mask on child subject recognition has not been
adequately explored. Thus, the main objective of this study is analyzing the
child longitudinal impact together with face mask and other covariates on face
recognition systems. Specifically, we performed a comparative investigation of
three top performing publicly available face matchers and a post-COVID-19
commercial-off-the-shelf (COTS) system under child cross-age verification and
identification settings using our generated synthetic mask and no-mask samples.
Furthermore, we investigated the longitudinal consequence of eyeglasses with
mask and no-mask. The study exploited no-mask longitudinal child face dataset
(i.e., extended Indian Child Longitudinal Face Dataset) that contains $26,258$
face images of $7,473$ subjects in the age group of $[2, 18]$ over an average
time span of $3.35$ years. Experimental results showed that problem of face
mask on automated face recognition is compounded by aging variate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Atlantic Multidecadal Variability. (arXiv:2111.00124v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00124">
<div class="article-summary-box-inner">
<span><p>Atlantic Multidecadal Variability (AMV) describes variations of North
Atlantic sea surface temperature with a typical cycle of between 60 and 70
years. AMV strongly impacts local climate over North America and Europe,
therefore prediction of AMV, especially the extreme values, is of great
societal utility for understanding and responding to regional climate change.
This work tests multiple machine learning models to improve the state of AMV
prediction from maps of sea surface temperature, salinity, and sea level
pressure in the North Atlantic region. We use data from the Community Earth
System Model 1 Large Ensemble Project, a state-of-the-art climate model with
3,440 years of data. Our results demonstrate that all of the models we use
outperform the traditional persistence forecast baseline. Predicting the AMV is
important for identifying future extreme temperatures and precipitation, as
well as hurricane activity, in Europe and North America up to 25 years in
advance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three approaches to facilitate DNN generalization to objects in out-of-distribution orientations and illuminations: late-stopping, tuning batch normalization and invariance loss. (arXiv:2111.00131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00131">
<div class="article-summary-box-inner">
<span><p>The training data distribution is often biased towards objects in certain
orientations and illumination conditions. While humans have a remarkable
capability of recognizing objects in out-of-distribution (OoD) orientations and
illuminations, Deep Neural Networks (DNNs) severely suffer in this case, even
when large amounts of training examples are available. In this paper, we
investigate three different approaches to improve DNNs in recognizing objects
in OoD orientations and illuminations. Namely, these are (i) training much
longer after convergence of the in-distribution (InD) validation accuracy,
i.e., late-stopping, (ii) tuning the momentum parameter of the batch
normalization layers, and (iii) enforcing invariance of the neural activity in
an intermediate layer to orientation and illumination conditions. Each of these
approaches substantially improves the DNN's OoD accuracy (more than 20% in some
cases). We report results in four datasets: two datasets are modified from the
MNIST and iLab datasets, and the other two are novel (one of 3D rendered cars
and another of objects taken from various controlled orientations and
illumination conditions). These datasets allow to study the effects of
different amounts of bias and are challenging as DNNs perform poorly in OoD
conditions. Finally, we demonstrate that even though the three approaches focus
on different aspects of DNNs, they all tend to lead to the same underlying
neural mechanism to enable OoD accuracy gains -- individual neurons in the
intermediate layers become more selective to a category and also invariant to
OoD orientations and illuminations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIB-R++: Learning to Predict Lighting and Material with a Hybrid Differentiable Renderer. (arXiv:2111.00140v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00140">
<div class="article-summary-box-inner">
<span><p>We consider the challenging problem of predicting intrinsic object properties
from a single image by exploiting differentiable renderers. Many previous
learning-based approaches for inverse graphics adopt rasterization-based
renderers and assume naive lighting and material models, which often fail to
account for non-Lambertian, specular reflections commonly observed in the wild.
In this work, we propose DIBR++, a hybrid differentiable renderer which
supports these photorealistic effects by combining rasterization and
ray-tracing, taking the advantage of their respective strengths -- speed and
realism. Our renderer incorporates environmental lighting and spatially-varying
material models to efficiently approximate light transport, either through
direct estimation or via spherical basis functions. Compared to more advanced
physics-based differentiable renderers leveraging path tracing, DIBR++ is
highly performant due to its compact and expressive shading model, which
enables easy integration with learning frameworks for geometry, reflectance and
lighting prediction from a single image without requiring any ground-truth. We
experimentally demonstrate that our approach achieves superior material and
lighting disentanglement on synthetic and real data compared to existing
rasterization-based approaches and showcase several artistic applications
including material editing and relighting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIERMATCH: Leveraging Label Hierarchies for Improving Semi-Supervised Learning. (arXiv:2111.00164v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00164">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning approaches have emerged as an active area of
research to combat the challenge of obtaining large amounts of annotated data.
Towards the goal of improving the performance of semi-supervised learning
methods, we propose a novel framework, HIERMATCH, a semi-supervised approach
that leverages hierarchical information to reduce labeling costs and performs
as well as a vanilla semi-supervised learning method. Hierarchical information
is often available as prior knowledge in the form of coarse labels (e.g.,
woodpeckers) for images with fine-grained labels (e.g., downy woodpeckers or
golden-fronted woodpeckers). However, the use of supervision using coarse
category labels to improve semi-supervised techniques has not been explored. In
the absence of fine-grained labels, HIERMATCH exploits the label hierarchy and
uses coarse class labels as a weak supervisory signal. Additionally, HIERMATCH
is a generic-approach to improve any semisupervised learning framework, we
demonstrate this using our results on recent state-of-the-art techniques
MixMatch and FixMatch. We evaluate the efficacy of HIERMATCH on two benchmark
datasets, namely CIFAR-100 and NABirds. HIERMATCH can reduce the usage of
fine-grained labels by 50% on CIFAR-100 with only a marginal drop of 0.59% in
top-1 accuracy as compared to MixMatch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iris Recognition Based on SIFT Features. (arXiv:2111.00176v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00176">
<div class="article-summary-box-inner">
<span><p>Biometric methods based on iris images are believed to allow very high
accuracy, and there has been an explosion of interest in iris biometrics in
recent years. In this paper, we use the Scale Invariant Feature Transformation
(SIFT) for recognition using iris images. Contrarily to traditional iris
recognition systems, the SIFT approach does not rely on the transformation of
the iris pattern to polar coordinates or on highly accurate segmentation,
allowing less constrained image acquisition conditions. We extract
characteristic SIFT feature points in scale space and perform matching based on
the texture information around the feature points using the SIFT operator.
Experiments are done using the BioSec multimodal database, which includes 3,200
iris images from 200 individuals acquired in two different sessions. We
contribute with the analysis of the influence of different SIFT parameters on
the recognition performance. We also show the complementarity between the SIFT
approach and a popular matching approach based on transformation to polar
coordinates and Log-Gabor wavelets. The combination of the two approaches
achieves significantly better performance than either of the individual
schemes, with a performance improvement of 24% in the Equal Error Rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct attacks using fake images in iris verification. (arXiv:2111.00178v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00178">
<div class="article-summary-box-inner">
<span><p>In this contribution, the vulnerabilities of iris-based recognition systems
to direct attacks are studied. A database of fake iris images has been created
from real iris of the BioSec baseline database. Iris images are printed using a
commercial printer and then, presented at the iris sensor. We use for our
experiments a publicly available iris recognition system, which some
modifications to improve the iris segmentation step. Based on results achieved
on different operational scenarios, we show that the system is vulnerable to
direct attacks, pointing out the importance of having countermeasures against
this type of fraudulent actions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-Aware Hierarchical Bayesian Learning on Manifolds. (arXiv:2111.00184v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00184">
<div class="article-summary-box-inner">
<span><p>Bayesian learning with Gaussian processes demonstrates encouraging regression
and classification performances in solving computer vision tasks. However,
Bayesian methods on 3D manifold-valued vision data, such as meshes and point
clouds, are seldom studied. One of the primary challenges is how to effectively
and efficiently aggregate geometric features from the irregular inputs. In this
paper, we propose a hierarchical Bayesian learning model to address this
challenge. We initially introduce a kernel with the properties of
geometry-awareness and intra-kernel convolution. This enables geometrically
reasonable inferences on manifolds without using any specific hand-crafted
feature descriptors. Then, we use a Gaussian process regression to organize the
inputs and finally implement a hierarchical Bayesian network for the feature
aggregation. Furthermore, we incorporate the feature learning of neural
networks with the feature aggregation of Bayesian models to investigate the
feasibility of jointly learning on manifolds. Experimental results not only
show that our method outperforms existing Bayesian methods on manifolds but
also demonstrate the prospect of coupling neural networks with Bayesian
networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging SE(3) Equivariance for Self-Supervised Category-Level Object Pose Estimation. (arXiv:2111.00190v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00190">
<div class="article-summary-box-inner">
<span><p>Category-level object pose estimation aims to find 6D object poses of
previously unseen object instances from known categories without access to
object CAD models. To reduce the huge amount of pose annotations needed for
category-level learning, we propose for the first time a self-supervised
learning framework to estimate category-level 6D object pose from single 3D
point clouds.During training, our method assumes no ground-truth pose
annotations, no CAD models, and no multi-view supervision. The key to our
method is to disentangle shape and pose through an invariant shape
reconstruction module and an equivariant pose estimation module, empowered by
SE(3) equivariant point cloud networks.The invariant shape reconstruction
module learns to perform aligned reconstructions, yielding a category-level
reference frame without using any annotations. In addition, the equivariant
pose estimation module achieves category-level pose estimation accuracy that is
comparable to some fully supervised methods. Extensive experiments demonstrate
the effectiveness of our approach on both complete and partial depth point
clouds from the ModelNet40 benchmark, and on real depth point clouds from the
NOCS-REAL 275 dataset. The project page with code and visualizations can be
found at: https://dragonlong.github.io/equi-pose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M2MRF: Many-to-Many Reassembly of Features for Tiny Lesion Segmentation in Fundus Images. (arXiv:2111.00193v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00193">
<div class="article-summary-box-inner">
<span><p>Feature reassembly is an essential component in modern CNNs-based
segmentation approaches, which includes feature downsampling and upsampling
operators. Existing feature reassembly operators reassemble multiple features
from a small predefined region into one for each target location independently.
This may result in loss of spatial information, which could vanish activations
of tiny lesions particularly when they cluster together. In this paper, we
propose a many-to-many reassembly of features (M2MRF). It reassembles features
in a dimension-reduced feature space and simultaneously aggregates multiple
features inside a large predefined region into multiple target features. In
this way, long range spatial dependencies are captured to maintain activations
on tiny lesions, particularly when multiple lesions coexist. Experimental
results on two lesion segmentation benchmarks, i.e. DDR and IDRiD, show that
our M2MRF outperforms existing feature reassembly operators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Review of Recent Few-Shot Object Detection Algorithms. (arXiv:2111.00201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00201">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection, learning to adapt to the novel classes with a few
labeled data, is an imperative and long-lasting problem due to the inherent
long-tail distribution of real-world data and the urgent demands to cut costs
of data collection and annotation. Recently, some studies have explored how to
use implicit cues in extra datasets without target-domain supervision to help
few-shot detectors refine robust task notions. This survey provides a
comprehensive overview from current classic and latest achievements for
few-shot object detection to future research expectations from manifold
perspectives. In particular, we first propose a data-based taxonomy of the
training data and the form of corresponding supervision which are accessed
during the training stage. Following this taxonomy, we present a significant
review of the formal definition, main challenges, benchmark datasets,
evaluation metrics, and learning strategies. In addition, we present a detailed
investigation of how to interplay the object detection methods to develop this
issue systematically. Finally, we conclude with the current status of few-shot
object detection, along with potential research directions for this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imitating Arbitrary Talking Style for Realistic Audio-DrivenTalking Face Synthesis. (arXiv:2111.00203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00203">
<div class="article-summary-box-inner">
<span><p>People talk with diversified styles. For one piece of speech, different
talking styles exhibit significant differences in the facial and head pose
movements. For example, the "excited" style usually talks with the mouth wide
open, while the "solemn" style is more standardized and seldomly exhibits
exaggerated motions. Due to such huge differences between different styles, it
is necessary to incorporate the talking style into audio-driven talking face
synthesis framework. In this paper, we propose to inject style into the talking
face synthesis framework through imitating arbitrary talking style of the
particular reference video. Specifically, we systematically investigate talking
styles with our collected \textit{Ted-HD} dataset and construct style codes as
several statistics of 3D morphable model~(3DMM) parameters. Afterwards, we
devise a latent-style-fusion~(LSF) model to synthesize stylized talking faces
by imitating talking styles from the style codes. We emphasize the following
novel characteristics of our framework: (1) It doesn't require any annotation
of the style, the talking style is learned in an unsupervised manner from
talking videos in the wild. (2) It can imitate arbitrary styles from arbitrary
videos, and the style codes can also be interpolated to generate new styles.
Extensive experiments demonstrate that the proposed framework has the ability
to synthesize more natural and expressive talking styles compared with baseline
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatchFormer: A Versatile 3D Transformer Based on Patch Attention. (arXiv:2111.00207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00207">
<div class="article-summary-box-inner">
<span><p>The 3D vision community is witnesses a modeling shift from CNNs to
Transformers, where pure Transformer architectures have attained top accuracy
on the major 3D learning benchmarks. However, existing 3D Transformers need to
generate a large attention map, which has quadratic complexity (both in space
and time) with respect to input size. To solve this shortcoming, we introduce
patch-attention to adaptively learn a much smaller set of bases upon which the
attention maps are computed. By a weighted summation upon these bases,
patch-attention not only captures the global shape context but also achieves
linear complexity to input size. In addition, we propose a lightweight
Multi-scale Attention (MSA) block to build attentions among features of
different scales, providing the model with multi-scale features. Based on these
proposed modules, we construct our neural architecture called PatchFormer.
Extensive experiments demonstrate that our network achieves strong accuracy on
general 3D recognition tasks with 7.3x speed-up than previous 3D Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mastering Atari Games with Limited Data. (arXiv:2111.00210v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00210">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning has achieved great success in many applications.
However, sample efficiency remains a key challenge, with prominent methods
requiring millions (or even billions) of environment steps to train. Recently,
there has been significant progress in sample efficient image-based RL
algorithms; however, consistent human-level performance on the Atari game
benchmark remains an elusive goal. We propose a sample efficient model-based
visual RL algorithm built on MuZero, which we name EfficientZero. Our method
achieves 190.4% mean human performance and 116.0% median performance on the
Atari 100k benchmark with only two hours of real-time game experience and
outperforms the state SAC in some tasks on the DMControl 100k benchmark. This
is the first time an algorithm achieves super-human performance on Atari games
with such little data. EfficientZero's performance is also close to DQN's
performance at 200 million frames while we consume 500 times less data.
EfficientZero's low sample complexity and high performance can bring RL closer
to real-world applicability. We implement our algorithm in an
easy-to-understand manner and it is available at
https://github.com/YeWR/EfficientZero. We hope it will accelerate the research
of MCTS-based RL algorithms in the wider community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Learning for High Dynamic Range Image Tone Mapping. (arXiv:2111.00219v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00219">
<div class="article-summary-box-inner">
<span><p>High dynamic range (HDR) photography is becoming increasingly popular and
available by DSLR and mobile-phone cameras. While deep neural networks (DNN)
have greatly impacted other domains of image manipulation, their use for HDR
tone-mapping is limited due to the lack of a definite notion of ground-truth
solution, which is needed for producing training data.
</p>
<p>In this paper we describe a new tone-mapping approach guided by the distinct
goal of producing low dynamic range (LDR) renditions that best reproduce the
visual characteristics of native LDR images. This goal enables the use of an
unpaired adversarial training based on unrelated sets of HDR and LDR images,
both of which are widely available and easy to acquire.
</p>
<p>In order to achieve an effective training under this minimal requirements, we
introduce the following new steps and components: (i) a range-normalizing
pre-process which estimates and applies a different level of curve-based
compression, (ii) a loss that preserves the input content while allowing the
network to achieve its goal, and (iii) the use of a more concise discriminator
network, designed to promote the reproduction of low-level attributes native
LDR possess.
</p>
<p>Evaluation of the resulting network demonstrates its ability to produce
photo-realistic artifact-free tone-mapped images, and state-of-the-art
performance on different image fidelity indices and visual distances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Spatio-Temporal Identity Verification Method for Person-Action Instance Search in Movies. (arXiv:2111.00228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00228">
<div class="article-summary-box-inner">
<span><p>As one of the challenging problems in video search, Person-Action Instance
Search (INS) aims to retrieve shots with specific person carrying out specific
action from massive video shots. Existing methods mainly include two steps:
First, two individual INS branches, i.e., person INS and action INS, are
separately conducted to compute the initial person and action ranking scores;
Second, both scores are directly fused to generate the final ranking list.
However, direct aggregation of two individual INS scores cannot guarantee the
identity consistency between person and action. For example, a shot with "Pat
is standing" and "Ian is sitting on couch" may be erroneously understood as
"Pat is sitting on couch" or "Ian is standing". To address the above identity
inconsistency problem (IIP), we study a spatio-temporal identity verification
method. Specifically, in the spatial dimension, we propose an identity
consistency verification scheme to optimize the direct fusion score of person
INS and action INS. The motivation originates from an observation that face
detection results usually locate in the identity-consistent action bounding
boxes. Moreover, in the temporal dimension, considering the complex filming
condition, we propose an inter-frame detection extension operation to
interpolate missing face/action detection results in successive video frames.
The proposed method is evaluated on the large scale TRECVID INS dataset, and
the experimental results show that our method can effectively mitigate the IIP
and surpass the existing second places in both TRECVID 2019 and 2020 INS tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Heads are Better than One: Geometric-Latent Attention for Point Cloud Classification and Segmentation. (arXiv:2111.00231v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00231">
<div class="article-summary-box-inner">
<span><p>We present an innovative two-headed attention layer that combines geometric
and latent features to segment a 3D scene into semantically meaningful subsets.
Each head combines local and global information, using either the geometric or
latent features, of a neighborhood of points and uses this information to learn
better local relationships. This Geometric-Latent attention layer (Ge-Latto) is
combined with a sub-sampling strategy to capture global features. Our method is
invariant to permutation thanks to the use of shared-MLP layers, and it can
also be used with point clouds with varying densities because the local
attention layer does not depend on the neighbor order. Our proposal is simple
yet robust, which allows it to achieve competitive results in the ShapeNetPart
and ModelNet40 datasets, and the state-of-the-art when segmenting the complex
dataset S3DIS, with 69.2% IoU on Area 5, and 89.7% overall accuracy using
K-fold cross-validation on the 6 areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00232">
<div class="article-summary-box-inner">
<span><p>In visual recognition tasks, few-shot learning requires the ability to learn
object categories with few support examples. Its recent resurgence in light of
the deep learning development is mainly in image classification. This work
focuses on few-shot semantic segmentation, which is still a largely unexplored
field. A few recent advances are often restricted to single-class few-shot
segmentation. In this paper, we first present a novel multi-way encoding and
decoding architecture which effectively fuses multi-scale query information and
multi-class support information into one query-support embedding; multi-class
segmentation is directly decoded upon this embedding. In order for better
feature fusion, a multi-level attention mechanism is proposed within the
architecture, which includes the attention for support feature modulation and
attention for multi-scale combination. Last, to enhance the embedding space
learning, an additional pixel-wise metric learning module is devised with
triplet loss formulated on the pixel-level embedding of the input image.
Extensive experiments on standard benchmarks PASCAL-5^i and COCO-20^i show
clear benefits of our method over the state of the art in few-shot
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Fusion Transformer for Multispectral Object Detection. (arXiv:2111.00273v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00273">
<div class="article-summary-box-inner">
<span><p>Multispectral image pairs can provide the combined information, making object
detection applications more reliable and robust in the open world. To fully
exploit the different modalities, we present a simple yet effective
cross-modality feature fusion approach, named Cross-Modality Fusion Transformer
(CFT) in this paper. Unlike prior CNNs-based works, guided by the transformer
scheme, our network learns long-range dependencies and integrates global
contextual information in the feature extraction stage. More importantly, by
leveraging the self attention of the transformer, the network can naturally
carry out simultaneous intra-modality and inter-modality fusion, and robustly
capture the latent interactions between RGB and Thermal domains, thereby
significantly improving the performance of multispectral object detection.
Extensive experiments and ablation studies on multiple datasets demonstrate
that our approach is effective and achieves state-of-the-art detection
performance. Our code and models will be released soon at
https://github.com/DocF/multispectral-object-detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Get Fooled for the Right Reason: Improving Adversarial Robustness through a Teacher-guided Curriculum Learning Approach. (arXiv:2111.00295v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00295">
<div class="article-summary-box-inner">
<span><p>Current SOTA adversarially robust models are mostly based on adversarial
training (AT) and differ only by some regularizers either at inner maximization
or outer minimization steps. Being repetitive in nature during the inner
maximization step, they take a huge time to train. We propose a non-iterative
method that enforces the following ideas during training. Attribution maps are
more aligned to the actual object in the image for adversarially robust models
compared to naturally trained models. Also, the allowed set of pixels to
perturb an image (that changes model decision) should be restricted to the
object pixels only, which reduces the attack strength by limiting the attack
space. Our method achieves significant performance gains with a little extra
effort (10-20%) over existing AT models and outperforms all other methods in
terms of adversarial as well as natural accuracy. We have performed extensive
experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and
reported results against many popular strong adversarial attacks to prove the
effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A fast accurate fine-grain object detection model based on YOLOv4 deep neural network. (arXiv:2111.00298v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00298">
<div class="article-summary-box-inner">
<span><p>Early identification and prevention of various plant diseases in commercial
farms and orchards is a key feature of precision agriculture technology. This
paper presents a high-performance real-time fine-grain object detection
framework that addresses several obstacles in plant disease detection that
hinder the performance of traditional methods, such as, dense distribution,
irregular morphology, multi-scale object classes, textural similarity, etc. The
proposed model is built on an improved version of the You Only Look Once
(YOLOv4) algorithm. The modified network architecture maximizes both detection
accuracy and speed by including the DenseNet in the back-bone to optimize
feature transfer and reuse, two new residual blocks in the backbone and neck
enhance feature extraction and reduce computing cost; the Spatial Pyramid
Pooling (SPP) enhances receptive field, and a modified Path Aggregation Network
(PANet) preserves fine-grain localized information and improve feature fusion.
Additionally, the use of the Hard-Swish function as the primary activation
improved the model's accuracy due to better nonlinear feature extraction. The
proposed model is tested in detecting four different diseases in tomato plants
under various challenging environments. The model outperforms the existing
state-of-the-art detection models in detection accuracy and speed. At a
detection rate of 70.19 FPS, the proposed model obtained a precision value of
$90.33 \%$, F1-score of $93.64 \%$, and a mean average precision ($mAP$) value
of $96.29 \%$. Current work provides an effective and efficient method for
detecting different plant diseases in complex scenarios that can be extended to
different fruit and crop detection, generic disease detection, and various
automated agricultural detection processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DP3: 3D Scene Perception via Probabilistic Programming. (arXiv:2111.00312v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00312">
<div class="article-summary-box-inner">
<span><p>We present 3DP3, a framework for inverse graphics that uses inference in a
structured generative model of objects, scenes, and images. 3DP3 uses (i) voxel
models to represent the 3D shape of objects, (ii) hierarchical scene graphs to
decompose scenes into objects and the contacts between them, and (iii) depth
image likelihoods based on real-time graphics. Given an observed RGB-D image,
3DP3's inference algorithm infers the underlying latent 3D scene, including the
object poses and a parsimonious joint parametrization of these poses, using
fast bottom-up pose proposals, novel involutive MCMC updates of the scene graph
structure, and, optionally, neural object detectors and pose estimators. We
show that 3DP3 enables scene understanding that is aware of 3D shape,
occlusion, and contact structure. Our results demonstrate that 3DP3 is more
accurate at 6DoF object pose estimation from real images than deep learning
baselines and shows better generalization to challenging scenes with novel
viewpoints, contact, and partial observability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Functional Neural Networks for Parametric Image Restoration Problems. (arXiv:2111.00361v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00361">
<div class="article-summary-box-inner">
<span><p>Almost every single image restoration problem has a closely related
parameter, such as the scale factor in super-resolution, the noise level in
image denoising, and the quality factor in JPEG deblocking. Although recent
studies on image restoration problems have achieved great success due to the
development of deep neural networks, they handle the parameter involved in an
unsophisticated way. Most previous researchers either treat problems with
different parameter levels as independent tasks, and train a specific model for
each parameter level; or simply ignore the parameter, and train a single model
for all parameter levels. The two popular approaches have their own
shortcomings. The former is inefficient in computing and the latter is
ineffective in performance. In this work, we propose a novel system called
functional neural network (FuncNet) to solve a parametric image restoration
problem with a single model. Unlike a plain neural network, the smallest
conceptual element of our FuncNet is no longer a floating-point variable, but a
function of the parameter of the problem. This feature makes it both efficient
and effective for a parametric problem. We apply FuncNet to super-resolution,
image denoising, and JPEG deblocking. The experimental results show the
superiority of our FuncNet on all three parametric image restoration tasks over
the state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Attention Network for Heart Rate and Respiratory Rate Estimation. (arXiv:2111.00390v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00390">
<div class="article-summary-box-inner">
<span><p>Heart rate and respiratory rate measurement is a vital step for diagnosing
many diseases. Non-contact camera based physiological measurement is more
accessible and convenient in Telehealth nowadays than contact instruments such
as fingertip oximeters since non-contact methods reduce risk of infection.
However, remote physiological signal measurement is challenging due to
environment illumination variations, head motion, facial expression, etc. It's
also desirable to have a unified network which could estimate both heart rate
and respiratory rate to reduce system complexity and latency. We propose a
convolutional neural network which leverages spatial attention and channel
attention, which we call it dual attention network (DAN) to jointly estimate
heart rate and respiratory rate with camera video as input. Extensive
experiments demonstrate that our proposed system significantly improves heart
rate and respiratory rate measurement accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A robust single-pixel particle image velocimetry based on fully convolutional networks with cross-correlation embedded. (arXiv:2111.00395v1 [physics.flu-dyn])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00395">
<div class="article-summary-box-inner">
<span><p>Particle image velocimetry (PIV) is essential in experimental fluid dynamics.
In the current work, we propose a new velocity field estimation paradigm, which
achieves a synergetic combination of the deep learning method and the
traditional cross-correlation method. Specifically, the deep learning method is
used to optimize and correct a coarse velocity guess to achieve a
super-resolution calculation. And the cross-correlation method provides the
initial velocity field based on a coarse correlation with a large interrogation
window. As a reference, the coarse velocity guess helps with improving the
robustness of the proposed algorithm. This fully convolutional network with
embedded cross-correlation is named as CC-FCN. CC-FCN has two types of input
layers, one is for the particle images, and the other is for the initial
velocity field calculated using cross-correlation with a coarse resolution.
Firstly, two pyramidal modules extract features of particle images and initial
velocity field respectively. Then the fusion module appropriately fuses these
features. Finally, CC-FCN achieves the super-resolution calculation through a
series of deconvolution layers to obtain the single-pixel velocity field. As
the supervised learning strategy is considered, synthetic data sets including
ground-truth fluid motions are generated to train the network parameters.
Synthetic and real experimental PIV data sets are used to test the trained
neural network in terms of accuracy, precision, spatial resolution and
robustness. The test results show that these attributes of CC-FCN are further
improved compared with those of other tested PIV algorithms. The proposed model
could therefore provide competitive and robust estimations for PIV experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Approach to Image Tilt Correction with Self-Attention MobileNet for Smartphones. (arXiv:2111.00398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00398">
<div class="article-summary-box-inner">
<span><p>The main contributions of our work are two-fold. First, we present a
Self-Attention MobileNet, called SA-MobileNet Network that can model long-range
dependencies between the image features instead of processing the local region
as done by standard convolutional kernels. SA-MobileNet contains self-attention
modules integrated with the inverted bottleneck blocks of the MobileNetV3 model
which results in modeling of both channel-wise attention and spatial attention
of the image features and at the same time introduce a novel self-attention
architecture for low-resource devices. Secondly, we propose a novel training
pipeline for the task of image tilt detection. We treat this problem in a
multi-label scenario where we predict multiple angles for a tilted input image
in a narrow interval of range 1-2 degrees, depending on the dataset used. This
process induces an implicit correlation between labels without any
computational overhead of the second or higher-order methods in multi-label
learning. With the combination of our novel approach and the architecture, we
present state-of-the-art results on detecting the image tilt angle on mobile
devices as compared to the MobileNetV3 model. Finally, we establish that
SA-MobileNet is more accurate than MobileNetV3 on SUN397, NYU-V1, and ADE20K
datasets by 6.42%, 10.51%, and 9.09% points respectively, and faster by at
least 4 milliseconds on Snapdragon 750 Octa-core.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PANet: Perspective-Aware Network with Dynamic Receptive Fields and Self-Distilling Supervision for Crowd Counting. (arXiv:2111.00406v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00406">
<div class="article-summary-box-inner">
<span><p>Crowd counting aims to learn the crowd density distributions and estimate the
number of objects (e.g. persons) in images. The perspective effect, which
significantly influences the distribution of data points, plays an important
role in crowd counting. In this paper, we propose a novel perspective-aware
approach called PANet to address the perspective problem. Based on the
observation that the size of the objects varies greatly in one image due to the
perspective effect, we propose the dynamic receptive fields (DRF) framework.
The framework is able to adjust the receptive field by the dilated convolution
parameters according to the input image, which helps the model to extract more
discriminative features for each local region. Different from most previous
works which use Gaussian kernels to generate the density map as the supervised
information, we propose the self-distilling supervision (SDS) training method.
The ground-truth density maps are refined from the first training stage and the
perspective information is distilled to the model in the second stage. The
experimental results on ShanghaiTech Part_A and Part_B, UCF_QNRF, and UCF_CC_50
datasets demonstrate that our proposed PANet outperforms the state-of-the-art
methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Deep Residual Reasoning for Temporal Moment Localization. (arXiv:2111.00417v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00417">
<div class="article-summary-box-inner">
<span><p>Temporal Moment Localization (TML) in untrimmed videos is a challenging task
in the field of multimedia, which aims at localizing the start and end points
of the activity in the video, described by a sentence query. Existing methods
mainly focus on mining the correlation between video and sentence
representations or investigating the fusion manner of the two modalities. These
works mainly understand the video and sentence coarsely, ignoring the fact that
a sentence can be understood from various semantics, and the dominant words
affecting the moment localization in the semantics are the action and object
reference. Toward this end, we propose a Hierarchical Deep Residual Reasoning
(HDRR) model, which decomposes the video and sentence into multi-level
representations with different semantics to achieve a finer-grained
localization. Furthermore, considering that videos with different resolution
and sentences with different length have different difficulty in understanding,
we design the simple yet effective Res-BiGRUs for feature fusion, which is able
to grasp the useful information in a self-adapting manner. Extensive
experiments conducted on Charades-STA and ActivityNet-Captions datasets
demonstrate the superiority of our HDRR model compared with other
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Loop closure detection using local 3D deep descriptors. (arXiv:2111.00440v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00440">
<div class="article-summary-box-inner">
<span><p>We present a simple yet effective method to address loop closure detection in
simultaneous localisation and mapping using local 3D deep descriptors (L3Ds).
L3Ds are emerging compact representations of patches extracted from point
clouds that are learned from data using a deep learning algorithm. We propose a
novel overlap measure for loop detection by computing the metric error between
points that correspond to mutually-nearest-neighbour descriptors after
registering the loop candidate point cloud by its estimated relative pose. This
novel approach enables us to accurately detect loops and estimate six
degrees-of-freedom poses in the case of small overlaps. We compare our
L3D-based loop closure approach with recent approaches on LiDAR data and
achieve state-of-the-art loop closure detection accuracy. Additionally, we
embed our loop closure approach in RESLAM, a recent edge-based SLAM system, and
perform the evaluation on real-world RGBD-TUM and synthetic ICL datasets. Our
approach enables RESLAM to achieve a better localisation accuracy compared to
its original loop closure strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaussian Kernel Mixture Network for Single Image Defocus Deblurring. (arXiv:2111.00454v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00454">
<div class="article-summary-box-inner">
<span><p>Defocus blur is one kind of blur effects often seen in images, which is
challenging to remove due to its spatially variant amount. This paper presents
an end-to-end deep learning approach for removing defocus blur from a single
image, so as to have an all-in-focus image for consequent vision tasks. First,
a pixel-wise Gaussian kernel mixture (GKM) model is proposed for representing
spatially variant defocus blur kernels in an efficient linear parametric form,
with higher accuracy than existing models. Then, a deep neural network called
GKMNet is developed by unrolling a fixed-point iteration of the GKM-based
deblurring. The GKMNet is built on a lightweight scale-recurrent architecture,
with a scale-recurrent attention module for estimating the mixing coefficients
in GKM for defocus deblurring. Extensive experiments show that the GKMNet not
only noticeably outperforms existing defocus deblurring methods, but also has
its advantages in terms of model complexity and computational efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGCN: Image-to-graph Convolutional Network for 2D/3D Deformable Registration. (arXiv:2111.00484v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00484">
<div class="article-summary-box-inner">
<span><p>Organ shape reconstruction based on a single-projection image during
treatment has wide clinical scope, e.g., in image-guided radiotherapy and
surgical guidance. We propose an image-to-graph convolutional network that
achieves deformable registration of a 3D organ mesh for a single-viewpoint 2D
projection image. This framework enables simultaneous training of two types of
transformation: from the 2D projection image to a displacement map, and from
the sampled per-vertex feature to a 3D displacement that satisfies the
geometrical constraint of the mesh structure. Assuming application to radiation
therapy, the 2D/3D deformable registration performance is verified for multiple
abdominal organs that have not been targeted to date, i.e., the liver, stomach,
duodenum, and kidney, and for pancreatic cancer. The experimental results show
shape prediction considering relationships among multiple organs can be used to
predict respiratory motion and deformation from digitally reconstructed
radiographs with clinically acceptable accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Image Compression with Separate Hyperprior Decoders. (arXiv:2111.00485v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00485">
<div class="article-summary-box-inner">
<span><p>Learned image compression techniques have achieved considerable development
in recent years. In this paper, we find that the performance bottleneck lies in
the use of a single hyperprior decoder, in which case the ternary Gaussian
model collapses to a binary one. To solve this, we propose to use three
hyperprior decoders to separate the decoding process of the mixed parameters in
discrete Gaussian mixture likelihoods, achieving more accurate parameters
estimation. Experimental results demonstrate the proposed method optimized by
MS-SSIM achieves on average 3.36% BD-rate reduction compared with
state-of-the-art approach. The contribution of the proposed method to the
coding time and FLOPs is negligible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart(Sampling)Augment: Optimal and Efficient Data Augmentation for Semantic Segmentation. (arXiv:2111.00487v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00487">
<div class="article-summary-box-inner">
<span><p>Data augmentation methods enrich datasets with augmented data to improve the
performance of neural networks. Recently, automated data augmentation methods
have emerged, which automatically design augmentation strategies. Existing work
focuses on image classification and object detection, whereas we provide the
first study on semantic image segmentation and introduce two new approaches:
\textit{SmartAugment} and \textit{SmartSamplingAugment}. SmartAugment uses
Bayesian Optimization to search over a rich space of augmentation strategies
and achieves a new state-of-the-art performance in all semantic segmentation
tasks we consider. SmartSamplingAugment, a simple parameter-free approach with
a fixed augmentation strategy competes in performance with the existing
resource-intensive approaches and outperforms cheap state-of-the-art data
augmentation methods. Further, we analyze the impact, interaction, and
importance of data augmentation hyperparameters and perform ablation studies,
which confirm our design choices behind SmartAugment and SmartSamplingAugment.
Lastly, we will provide our source code for reproducibility and to facilitate
further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPNET: Dual-Path Network for Efficient Object Detectioj with Lightweight Self-Attention. (arXiv:2111.00500v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00500">
<div class="article-summary-box-inner">
<span><p>Object detection often costs a considerable amount of computation to get
satisfied performance, which is unfriendly to be deployed in edge devices. To
address the trade-off between computational cost and detection accuracy, this
paper presents a dual path network, named DPNet, for efficient object detection
with lightweight self-attention. In backbone, a single input/output lightweight
self-attention module (LSAM) is designed to encode global interactions between
different positions. LSAM is also extended into a multiple-inputs version in
feature pyramid network (FPN), which is employed to capture cross-resolution
dependencies in two paths. Extensive experiments on the COCO dataset
demonstrate that our method achieves state-of-the-art detection results. More
specifically, DPNet obtains 29.0% AP on COCO test-dev, with only 1.14 GFLOPs
and 2.27M model size for a 320x320 image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully convolutional Siamese neural networks for buildings damage assessment from satellite images. (arXiv:2111.00508v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00508">
<div class="article-summary-box-inner">
<span><p>Damage assessment after natural disasters is needed to distribute aid and
forces to recovery from damage dealt optimally. This process involves acquiring
satellite imagery for the region of interest, localization of buildings, and
classification of the amount of damage caused by nature or urban factors to
buildings. In case of natural disasters, this means processing many square
kilometers of the area to judge whether a particular building had suffered from
the damaging factors.
</p>
<p>In this work, we develop a computational approach for an automated comparison
of the same region's satellite images before and after the disaster, and
classify different levels of damage in buildings. Our solution is based on
Siamese neural networks with encoder-decoder architecture. We include an
extensive ablation study and compare different encoders, decoders, loss
functions, augmentations, and several methods to combine two images. The
solution achieved one of the best results in the Computer Vision for Building
Damage Assessment competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRBANET: A Lightweight Dual-Resolution Network for Semantic Segmentation with Boundary Auxiliary. (arXiv:2111.00509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00509">
<div class="article-summary-box-inner">
<span><p>Due to the powerful ability to encode image details and semantics, many
lightweight dual-resolution networks have been proposed in recent years.
However, most of them ignore the benefit of boundary information. This paper
introduces a lightweight dual-resolution network, called DRBANet, aiming to
refine semantic segmentation results with the aid of boundary information.
DRBANet adopts dual parallel architecture, including: high resolution branch
(HRB) and low resolution branch (LRB). Specifically, HRB mainly consists of a
set of Efficient Inverted Bottleneck Modules (EIBMs), which learn feature
representations with larger receptive fields. LRB is composed of a series of
EIBMs and an Extremely Lightweight Pyramid Pooling Module (ELPPM), where ELPPM
is utilized to capture multi-scale context through hierarchical residual
connections. Finally, a boundary supervision head is designed to capture object
boundaries in HRB. Extensive experiments on Cityscapes and CamVid datasets
demonstrate that our method achieves promising trade-off between segmentation
accuracy and running efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrating the Dice loss to handle neural network overconfidence for biomedical image segmentation. (arXiv:2111.00528v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00528">
<div class="article-summary-box-inner">
<span><p>The Dice similarity coefficient (DSC) is both a widely used metric and loss
function for biomedical image segmentation due to its robustness to class
imbalance. However, it is well known that the DSC loss is poorly calibrated,
resulting in overconfident predictions that cannot be usefully interpreted in
biomedical and clinical practice. Performance is often the only metric used to
evaluate segmentations produced by deep neural networks, and calibration is
often neglected. However, calibration is important for translation into
biomedical and clinical practice, providing crucial contextual information to
model predictions for interpretation by scientists and clinicians. In this
study, we identify poor calibration as an emerging challenge of deep learning
based biomedical image segmentation. We provide a simple yet effective
extension of the DSC loss, named the DSC++ loss, that selectively modulates the
penalty associated with overconfident, incorrect predictions. As a standalone
loss function, the DSC++ loss achieves significantly improved calibration over
the conventional DSC loss across five well-validated open-source biomedical
imaging datasets. Similarly, we observe significantly improved when integrating
the DSC++ loss into four DSC-based loss functions. Finally, we use softmax
thresholding to illustrate that well calibrated outputs enable tailoring of
precision-recall bias, an important post-processing technique to adapt the
model predictions to suit the biomedical or clinical task. The DSC++ loss
overcomes the major limitation of the DSC, providing a suitable loss function
for training deep learning segmentation models for use in biomedical and
clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Debiased and Disentangled Representations for Semantic Segmentation. (arXiv:2111.00531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00531">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are susceptible to learn biased models with entangled
feature representations, which may lead to subpar performances on various
downstream tasks. This is particularly true for under-represented classes,
where a lack of diversity in the data exacerbates the tendency. This limitation
has been addressed mostly in classification tasks, but there is little study on
additional challenges that may appear in more complex dense prediction problems
including semantic segmentation. To this end, we propose a model-agnostic and
stochastic training scheme for semantic segmentation, which facilitates the
learning of debiased and disentangled representations. For each class, we first
extract class-specific information from the highly entangled feature map. Then,
information related to a randomly sampled class is suppressed by a feature
selection process in the feature space. By randomly eliminating certain class
information in each training iteration, we effectively reduce feature
dependencies among classes, and the model is able to learn more debiased and
disentangled feature representations. Models trained with our approach
demonstrate strong results on multiple semantic segmentation benchmarks, with
especially notable performance gains on under-represented classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Boundary Uncertainty into loss functions for biomedical image segmentation. (arXiv:2111.00533v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00533">
<div class="article-summary-box-inner">
<span><p>Manual segmentation is used as the gold-standard for evaluating neural
networks on automated image segmentation tasks. Due to considerable
heterogeneity in shapes, colours and textures, demarcating object boundaries is
particularly difficult in biomedical images, resulting in significant inter and
intra-rater variability. Approaches, such as soft labelling and distance
penalty term, apply a global transformation to the ground truth, redefining the
loss function with respect to uncertainty. However, global operations are
computationally expensive, and neither approach accurately reflects the
uncertainty underlying manual annotation. In this paper, we propose the
Boundary Uncertainty, which uses morphological operations to restrict soft
labelling to object boundaries, providing an appropriate representation of
uncertainty in ground truth labels, and may be adapted to enable robust model
training where systematic manual segmentation errors are present. We
incorporate Boundary Uncertainty with the Dice loss, achieving consistently
improved performance across three well-validated biomedical imaging datasets
compared to soft labelling and distance-weighted penalty. Boundary Uncertainty
not only more accurately reflects the segmentation process, but it is also
efficient, robust to segmentation errors and exhibits better generalisation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focal Attention Networks: optimising attention for biomedical image segmentation. (arXiv:2111.00534v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00534">
<div class="article-summary-box-inner">
<span><p>In recent years, there has been increasing interest to incorporate attention
into deep learning architectures for biomedical image segmentation. The modular
design of attention mechanisms enables flexible integration into convolutional
neural network architectures, such as the U-Net. Whether attention is
appropriate to use, what type of attention to use, and where in the network to
incorporate attention modules, are all important considerations that are
currently overlooked. In this paper, we investigate the role of the Focal
parameter in modulating attention, revealing a link between attention in loss
functions and networks. By incorporating a Focal distance penalty term, we
extend the Unified Focal loss framework to include boundary-based losses.
Furthermore, we develop a simple and interpretable, dataset and model-specific
heuristic to integrate the Focal parameter into the Squeeze-and-Excitation
block and Attention Gate, achieving optimal performance with fewer number of
attention modules on three well-validated biomedical imaging datasets,
suggesting judicious use of attention modules results in better performance and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Face to Gait: Weakly-Supervised Learning of Gender Information from Walking Patterns. (arXiv:2111.00538v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00538">
<div class="article-summary-box-inner">
<span><p>Obtaining demographics information from video is valuable for a range of
real-world applications. While approaches that leverage facial features for
gender inference are very successful in restrained environments, they do not
work in most real-world scenarios when the subject is not facing the camera,
has the face obstructed or the face is not clear due to distance from the
camera or poor resolution. We propose a weakly-supervised method for learning
gender information of people based on their manner of walking. We make use of
state-of-the art facial analysis models to automatically annotate front-view
walking sequences and generalise to unseen angles by leveraging gait-based
label propagation. Our results show on par or higher performance with facial
analysis models with an F1 score of 91% and the ability to successfully
generalise to scenarios in which facial analysis is unfeasible due to subjects
not facing the camera or having the face obstructed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Detect Open Carry and Concealed Object with 77GHz Radar. (arXiv:2111.00551v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00551">
<div class="article-summary-box-inner">
<span><p>Detecting harmful carried objects plays a key role in intelligent
surveillance systems and has widespread applications, for example, in airport
security. In this paper, we focus on the relatively unexplored area of using
low-cost 77GHz mmWave radar for the carried objects detection problem. The
proposed system is capable of real-time detecting three classes of objects -
laptop, phone, and knife - under open carry and concealed cases where objects
are hidden with clothes or bags. This capability is achieved by initial signal
processing for localization and generating range-azimuth-elevation image cubes,
followed by a deep learning-based prediction network and a multi-shot
post-processing module for detecting objects. Extensive experiments for
validating the system performance on detecting open carry and concealed objects
have been presented with a self-built radar-camera testbed and dataset.
Additionally, the influence of different input, factors, and parameters on
system performance is analyzed, providing an intuitive understanding of the
system. This system would be the very first baseline for other future works
aiming to detect carried objects using 77GHz radar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TorchXRayVision: A library of chest X-ray datasets and models. (arXiv:2111.00595v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00595">
<div class="article-summary-box-inner">
<span><p>TorchXRayVision is an open source software library for working with chest
X-ray datasets and deep learning models. It provides a common interface and
common pre-processing chain for a wide set of publicly available chest X-ray
datasets. In addition, a number of classification and representation learning
models with different architectures, trained on different data combinations,
are available through the library to serve as baselines or feature extractors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognizing Families In the Wild (RFIW): The 5th Edition. (arXiv:2111.00598v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00598">
<div class="article-summary-box-inner">
<span><p>Recognizing Families In the Wild (RFIW), held as a data challenge in
conjunction with the 16th IEEE International Conference on Automatic Face and
Gesture Recognition (FG), is a large-scale, multi-track visual kinship
recognition evaluation. This is our fifth edition of RFIW, for which we
continue the effort to attract scholars, bring together professionals, publish
new work, and discuss prospects. In this paper, we summarize submissions for
the three tasks of this year's RFIW: specifically, we review the results for
kinship verification, tri-subject verification, and family member search and
retrieval. We take a look at the RFIW problem, as well as share current efforts
and make recommendations for promising future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attack Generation Empowered by Min-Max Optimization. (arXiv:1906.03563v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.03563">
<div class="article-summary-box-inner">
<span><p>The worst-case training principle that minimizes the maximal adversarial
loss, also known as adversarial training (AT), has shown to be a
state-of-the-art approach for enhancing adversarial robustness. Nevertheless,
min-max optimization beyond the purpose of AT has not been rigorously explored
in the adversarial context. In this paper, we show how a general framework of
min-max optimization over multiple domains can be leveraged to advance the
design of different types of adversarial attacks. In particular, given a set of
risk sources, minimizing the worst-case attack loss can be reformulated as a
min-max problem by introducing domain weights that are maximized over the
probability simplex of the domain set. We showcase this unified framework in
three attack generation problems -- attacking model ensembles, devising
universal perturbation under multiple inputs, and crafting attacks resilient to
data transformations. Extensive experiments demonstrate that our approach leads
to substantial attack improvement over the existing heuristic strategies as
well as robustness improvement over state-of-the-art defense methods trained to
be robust against multiple perturbation types. Furthermore, we find that the
self-adjusted domain weights learned from our min-max framework can provide a
holistic tool to explain the difficulty level of attack across domains. Code is
available at https://github.com/wangjksjtu/minmax-adv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Image-generation Enhanced Adaptation for Object Detection in Thermal images. (arXiv:2002.06770v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.06770">
<div class="article-summary-box-inner">
<span><p>Object detection in thermal images is an important computer vision task and
has many applications such as unmanned vehicles, robotics, surveillance and
night vision. Deep learning based detectors have achieved major progress, which
usually need large amount of labelled training data. However, labelled data for
object detection in thermal images is scarce and expensive to collect. How to
take advantage of the large number labelled visible images and adapt them into
thermal image domain, is expected to solve. This paper proposes an unsupervised
image-generation enhanced adaptation method for object detection in thermal
images. To reduce the gap between visible domain and thermal domain, the
proposed method manages to generate simulated fake thermal images that are
similar to the target images, and preserves the annotation information of the
visible source domain. The image generation includes a CycleGAN based
image-to-image translation and an intensity inversion transformation. Generated
fake thermal images are used as renewed source domain. And then the
off-the-shelf Domain Adaptive Faster RCNN is utilized to reduce the gap between
generated intermediate domain and the thermal target domain. Experiments
demonstrate the effectiveness and superiority of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ENSEI: Efficient Secure Inference via Frequency-Domain Homomorphic Convolution for Privacy-Preserving Visual Recognition. (arXiv:2003.05328v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.05328">
<div class="article-summary-box-inner">
<span><p>In this work, we propose ENSEI, a secure inference (SI) framework based on
the frequency-domain secure convolution (FDSC) protocol for the efficient
execution of privacy-preserving visual recognition. Our observation is that,
under the combination of homomorphic encryption and secret sharing, homomorphic
convolution can be obliviously carried out in the frequency domain,
significantly simplifying the related computations. We provide protocol designs
and parameter derivations for number-theoretic transform (NTT) based FDSC. In
the experiment, we thoroughly study the accuracy-efficiency trade-offs between
time- and frequency-domain homomorphic convolution. With ENSEI, compared to the
best known works, we achieve 5--11x online time reduction, up to 33x setup time
reduction, and up to 10x reduction in the overall inference time. A further 33%
of bandwidth reductions can be obtained on binary neural networks with only 1%
of accuracy degradation on the CIFAR-10 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teacher-Class Network: A Neural Network Compression Mechanism. (arXiv:2004.03281v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.03281">
<div class="article-summary-box-inner">
<span><p>To reduce the overwhelming size of Deep Neural Networks (DNN) teacher-student
methodology tries to transfer knowledge from a complex teacher network to a
simple student network. We instead propose a novel method called the
teacher-class network consisting of a single teacher and multiple student
networks (i.e. class of students). Instead of transferring knowledge to one
student only, the proposed method transfers a chunk of knowledge to each
student. Our students are not trained for problem-specific logits, they are
trained to mimic knowledge (dense representation) learned by the teacher
network thus the combined knowledge learned by the class of students can be
used to solve other problems as well. The proposed teacher-class architecture
is evaluated on several benchmark datasets such as MNIST, Fashion MNIST, IMDB
Movie Reviews, CAMVid, CIFAR-10 and ImageNet on multiple tasks including image
classification, sentiment classification and segmentation. Our approach
outperforms the state of-the-art single student approach in terms of accuracy
as well as computational cost while achieving 10-30 times reduction in
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Mid-Level Patterns for Cross-Domain Few-Shot Recognition. (arXiv:2008.03128v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.03128">
<div class="article-summary-box-inner">
<span><p>Existing few-shot learning (FSL) methods usually assume base classes and
novel classes are from the same domain (in-domain setting). However, in
practice, it may be infeasible to collect sufficient training samples for some
special domains to construct base classes. To solve this problem, cross-domain
FSL (CDFSL) is proposed very recently to transfer knowledge from general-domain
base classes to special-domain novel classes. Existing CDFSL works mostly focus
on transferring between near domains, while rarely consider transferring
between distant domains, which is in practical need as any novel classes could
appear in real-world applications, and is even more challenging. In this paper,
we study a challenging subset of CDFSL where the novel classes are in distant
domains from base classes, by revisiting the mid-level features, which are more
transferable yet under-explored in main stream FSL work. To boost the
discriminability of mid-level features, we propose a residual-prediction task
to encourage mid-level features to learn discriminative information of each
sample. Notably, such mechanism also benefits the in-domain FSL and CDFSL in
near domains. Therefore, we provide two types of features for both cross- and
in-domain FSL respectively, under the same training framework. Experiments
under both settings on six public datasets, including two challenging medical
datasets, validate the our rationale and demonstrate state-of-the-art
performance. Code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Encoding Robustness to Image Style via Adversarial Feature Perturbations. (arXiv:2009.08965v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08965">
<div class="article-summary-box-inner">
<span><p>Adversarial training is the industry standard for producing models that are
robust to small adversarial perturbations. However, machine learning
practitioners need models that are robust to other kinds of changes that occur
naturally, such as changes in the style or illumination of input images. Such
changes in input distribution have been effectively modeled as shifts in the
mean and variance of deep image features. We adapt adversarial training by
directly perturbing feature statistics, rather than image pixels, to produce
models that are robust to various unseen distributional shifts. We explore the
relationship between these perturbations and distributional shifts by
visualizing adversarial features. Our proposed method, Adversarial Batch
Normalization (AdvBN), is a single network layer that generates worst-case
feature perturbations during training. By fine-tuning neural networks on
adversarial feature distributions, we observe improved robustness of networks
to various unseen distributional shifts, including style variations and image
corruptions. In addition, we show that our proposed adversarial feature
perturbation can be complementary to existing image space data augmentation
methods, leading to improved performance. The source code and pre-trained
models are released at \url{https://github.com/azshue/AdvBN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Translation for Medical Image Generation -- Ischemic Stroke Lesions. (arXiv:2010.02745v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02745">
<div class="article-summary-box-inner">
<span><p>Deep learning based disease detection and segmentation algorithms promise to
improve many clinical processes. However, such algorithms require vast amounts
of annotated training data, which are typically not available in the medical
context due to data privacy, legal obstructions, and non-uniform data
acquisition protocols. Synthetic databases with annotated pathologies could
provide the required amounts of training data. We demonstrate with the example
of ischemic stroke that an improvement in lesion segmentation is feasible using
deep learning based augmentation. To this end, we train different
image-to-image translation models to synthesize magnetic resonance images of
brain volumes with and without stroke lesions from semantic segmentation maps.
In addition, we train a generative adversarial network to generate synthetic
lesion masks. Subsequently, we combine these two components to build a large
database of synthetic stroke images. The performance of the various models is
evaluated using a U-Net which is trained to segment stroke lesions on a
clinical test set. We report a Dice score of $\mathbf{72.8}$%
[$\mathbf{70.8\pm1.0}$%] for the model with the best performance, which
outperforms the model trained on the clinical images alone $\mathbf{67.3}$%
[$\mathbf{63.2\pm1.9}$%], and is close to the human inter-reader Dice score of
$\mathbf{76.9}$%. Moreover, we show that for a small database of only 10 or 50
clinical cases, synthetic data augmentation yields significant improvement
compared to a setting where no synthetic data is used. To the best of our
knowledge, this presents the first comparative analysis of synthetic data
augmentation based on image-to-image translation, and first application to
ischemic stroke.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NCP-VAE: Variational Autoencoders with Noise Contrastive Priors. (arXiv:2010.02917v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02917">
<div class="article-summary-box-inner">
<span><p>Variational autoencoders (VAEs) are one of the powerful likelihood-based
generative models with applications in various domains. However, they struggle
to generate high-quality images, especially when samples are obtained from the
prior without any tempering. One explanation for VAEs' poor generative quality
is the prior hole problem: the prior distribution fails to match the aggregate
approximate posterior. Due to this mismatch, there exist areas in the latent
space with high density under the prior that do not correspond to any encoded
image. Samples from those areas are decoded to corrupted images. To tackle this
issue, we propose an energy-based prior defined by the product of a base prior
distribution and a reweighting factor, designed to bring the base closer to the
aggregate posterior. We train the reweighting factor by noise contrastive
estimation, and we generalize it to hierarchical VAEs with many latent variable
groups. Our experiments confirm that the proposed noise contrastive priors
improve the generative performance of state-of-the-art VAEs by a large margin
on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariant Representation Learning for Infant Pose Estimation with Small Data. (arXiv:2010.06100v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06100">
<div class="article-summary-box-inner">
<span><p>Infant motion analysis is a topic with critical importance in early childhood
development studies. However, while the applications of human pose estimation
have become more and more broad, models trained on large-scale adult pose
datasets are barely successful in estimating infant poses due to the
significant differences in their body ratio and the versatility of their poses.
Moreover, the privacy and security considerations hinder the availability of
adequate infant pose data required for training of a robust model from scratch.
To address this problem, this paper presents (1) building and publicly
releasing a hybrid synthetic and real infant pose (SyRIP) dataset with small
yet diverse real infant images as well as generated synthetic infant poses and
(2) a multi-stage invariant representation learning strategy that could
transfer the knowledge from the adjacent domains of adult poses and synthetic
infant images into our fine-tuned domain-adapted infant pose (FiDIP) estimation
model. In our ablation study, with identical network structure, models trained
on SyRIP dataset show noticeable improvement over the ones trained on the only
other public infant pose datasets. Integrated with pose estimation backbone
networks with varying complexity, FiDIP performs consistently better than the
fine-tuned versions of those models. One of our best infant pose estimation
performers on the state-of-the-art DarkPose model shows mean average precision
(mAP) of 93.6.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose And Joint-Aware Action Recognition. (arXiv:2010.08164v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08164">
<div class="article-summary-box-inner">
<span><p>Recent progress on action recognition has mainly focused on RGB and optical
flow features. In this paper, we approach the problem of joint-based action
recognition. Unlike other modalities, constellation of joints and their motion
generate models with succinct human motion information for activity
recognition. We present a new model for joint-based action recognition, which
first extracts motion features from each joint separately through a shared
motion encoder before performing collective reasoning. Our joint selector
module re-weights the joint information to select the most discriminative
joints for the task. We also propose a novel joint-contrastive loss that pulls
together groups of joint features which convey the same action. We strengthen
the joint-based representations by using a geometry-aware data augmentation
technique which jitters pose heatmaps while retaining the dynamics of the
action. We show large improvements over the current state-of-the-art
joint-based approaches on JHMDB, HMDB, Charades, AVA action recognition
datasets. A late fusion with RGB and Flow-based approaches yields additional
improvements. Our model also outperforms the existing baseline on Mimetics, a
dataset with out-of-context actions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobustBench: a standardized adversarial robustness benchmark. (arXiv:2010.09670v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09670">
<div class="article-summary-box-inner">
<span><p>As a research community, we are still lacking a systematic understanding of
the progress on adversarial robustness which often makes it hard to identify
the most promising ideas in training robust models. A key challenge in
benchmarking robustness is that its evaluation is often error-prone leading to
robustness overestimation. Our goal is to establish a standardized benchmark of
adversarial robustness, which as accurately as possible reflects the robustness
of the considered models within a reasonable computational budget. To this end,
we start by considering the image classification task and introduce
restrictions (possibly loosened in the future) on the allowed models. We
evaluate adversarial robustness with AutoAttack, an ensemble of white- and
black-box attacks, which was recently shown in a large-scale study to improve
almost all robustness evaluations compared to the original publications. To
prevent overadaptation of new defenses to AutoAttack, we welcome external
evaluations based on adaptive attacks, especially where AutoAttack flags a
potential overestimation of robustness. Our leaderboard, hosted at
https://robustbench.github.io/, contains evaluations of 120+ models and aims at
reflecting the current state of the art in image classification on a set of
well-defined tasks in $\ell_\infty$- and $\ell_2$-threat models and on common
corruptions, with possible extensions in the future. Additionally, we
open-source the library https://github.com/RobustBench/robustbench that
provides unified access to 80+ robust models to facilitate their downstream
applications. Finally, based on the collected models, we analyze the impact of
robustness on the performance on distribution shifts, calibration,
out-of-distribution detection, fairness, privacy leakage, smoothness, and
transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Distinguishing Normal versus Abnormal Chest Radiographs and Generalization to Unseen Diseases. (arXiv:2010.11375v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11375">
<div class="article-summary-box-inner">
<span><p>Chest radiography (CXR) is the most widely-used thoracic clinical imaging
modality and is crucial for guiding the management of cardiothoracic
conditions. The detection of specific CXR findings has been the main focus of
several artificial intelligence (AI) systems. However, the wide range of
possible CXR abnormalities makes it impractical to build specific systems to
detect every possible condition. In this work, we developed and evaluated an AI
system to classify CXRs as normal or abnormal. For development, we used a
de-identified dataset of 248,445 patients from a multi-city hospital network in
India. To assess generalizability, we evaluated our system using 6
international datasets from India, China, and the United States. Of these
datasets, 4 focused on diseases that the AI was not trained to detect: 2
datasets with tuberculosis and 2 datasets with coronavirus disease 2019. Our
results suggest that the AI system generalizes to new patient populations and
abnormalities. In a simulated workflow where the AI system prioritized abnormal
cases, the turnaround time for abnormal cases reduced by 7-28%. These results
represent an important step towards evaluating whether AI can be safely used to
flag cases in a general setting where previously unseen abnormalities exist.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi Scale Identity-Preserving Image-to-Image Translation Network for Low-Resolution Face Recognition. (arXiv:2010.12249v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12249">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep neural network models have reached near perfect face
recognition accuracy rates on controlled high-resolution face images. However,
their performance is drastically degraded when they are tested with very
low-resolution face images. This is particularly critical in surveillance
systems, where a low-resolution probe image is to be matched with
high-resolution gallery images. super-resolution techniques aim at producing
high-resolution face images from low-resolution counterparts. While they are
capable of reconstructing images that are visually appealing, the
identity-related information is not preserved. Here, we propose an
identity-preserving end-to-end image-to-image translation deep neural network
which is capable of super-resolving very low-resolution faces to their
high-resolution counterparts while preserving identity-related information. We
achieved this by training a very deep convolutional encoder-decoder network
with a symmetric contracting path between corresponding layers. This network
was trained with a combination of a reconstruction and an identity-preserving
loss, on multi-scale low-resolution conditions. Extensive quantitative
evaluations of our proposed model demonstrated that it outperforms competing
super-resolution and low-resolution face recognition methods on natural and
artificial low-resolution face data sets and even unseen identities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Casting a BAIT for Offline and Online Source-free Domain Adaptation. (arXiv:2010.12427v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12427">
<div class="article-summary-box-inner">
<span><p>We address the source-free domain adaptation (SFDA) problem, where only the
source model is available during adaptation to the target domain. We consider
two settings: the offline setting where all target data can be visited multiple
times (epochs) to arrive at a prediction for each target sample, and the online
setting where the target data needs to be directly classified upon arrival.
Inspired by diverse classifier based domain adaptation methods, in this paper
we introduce a second classifier, but with another classifier head fixed. When
adapting to the target domain, the additional classifier initialized from
source classifier is expected to find misclassified features. Next, when
updating the feature extractor, those features will be pushed towards the right
side of the source decision boundary, thus achieving source-free domain
adaptation. Experimental results show that the proposed method achieves
competitive results for offline SFDA on several benchmark datasets compared
with existing DA and SFDA methods, and our method surpasses by a large margin
other SFDA methods under online source-free domain adaptation setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Contextual Prediction for Learned Image Compression. (arXiv:2011.09704v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09704">
<div class="article-summary-box-inner">
<span><p>Over the past several years, we have witnessed impressive progress in the
field of learned image compression. Recent learned image codecs are commonly
based on autoencoders, that first encode an image into low-dimensional latent
representations and then decode them for reconstruction purposes. To capture
spatial dependencies in the latent space, prior works exploit hyperprior and
spatial context model to build an entropy model, which estimates the bit-rate
for end-to-end rate-distortion optimization. However, such an entropy model is
suboptimal from two aspects: (1) It fails to capture spatially global
correlations among the latents. (2) Cross-channel relationships of the latents
are still underexplored. In this paper, we propose the concept of separate
entropy coding to leverage a serial decoding process for causal contextual
entropy prediction in the latent space. A causal context model is proposed that
separates the latents across channels and makes use of cross-channel
relationships to generate highly informative contexts. Furthermore, we propose
a causal global prediction model, which is able to find global reference points
for accurate predictions of unknown points. Both these two models facilitate
entropy estimation without the transmission of overhead. In addition, we
further adopt a new separate attention module to build more powerful transform
networks. Experimental results demonstrate that our full image compression
model outperforms standard VVC/H.266 codec on Kodak dataset in terms of both
PSNR and MS-SSIM, yielding the state-of-the-art rate-distortion performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation-Efficient Untrimmed Video Action Recognition. (arXiv:2011.14478v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14478">
<div class="article-summary-box-inner">
<span><p>Deep learning has achieved great success in recognizing video actions, but
the collection and annotation of training data are still quite laborious, which
mainly lies in two aspects: (1) the amount of required annotated data is large;
(2) temporally annotating the location of each action is time-consuming. Works
such as few-shot learning or untrimmed video recognition have been proposed to
handle either one aspect or the other. However, very few existing works can
handle both issues simultaneously. In this paper, we target a new problem,
Annotation-Efficient Video Recognition, to reduce the requirement of
annotations for both large amount of samples and the action location. Such
problem is challenging due to two aspects: (1) the untrimmed videos only have
weak supervision; (2) video segments not relevant to current actions of
interests (background, BG) could contain actions of interests (foreground, FG)
in novel classes, which is a widely existing phenomenon but has rarely been
studied in few-shot untrimmed video recognition. To achieve this goal, by
analyzing the property of BG, we categorize BG into informative BG (IBG) and
non-informative BG (NBG), and we propose (1) an open-set detection based method
to find the NBG and FG, (2) a contrastive learning method to learn IBG and
distinguish NBG in a self-supervised way, and (3) a self-weighting mechanism
for the better distinguishing of IBG and FG. Extensive experiments on
ActivityNet v1.2 and ActivityNet v1.3 verify the rationale and effectiveness of
the proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative label cleaning for transductive and semi-supervised few-shot learning. (arXiv:2012.07962v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07962">
<div class="article-summary-box-inner">
<span><p>Few-shot learning amounts to learning representations and acquiring knowledge
such that novel tasks may be solved with both supervision and data being
limited. Improved performance is possible by transductive inference, where the
entire test set is available concurrently, and semi-supervised learning, where
more unlabeled data is available. Focusing on these two settings, we introduce
a new algorithm that leverages the manifold structure of the labeled and
unlabeled data distribution to predict pseudo-labels, while balancing over
classes and using the loss value distribution of a limited-capacity classifier
to select the cleanest labels, iteratively improving the quality of
pseudo-labels. Our solution surpasses or matches the state of the art results
on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and
CIFAR-FS, while being robust over feature space pre-processing and the quantity
of available data. The publicly available source code can be found in
https://github.com/MichalisLazarou/iLPC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Momentum-Contrastive Pre-Training for Robust Feature Extraction. (arXiv:2012.13154v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13154">
<div class="article-summary-box-inner">
<span><p>Recently proposed adversarial self-supervised learning methods usually
require big batches and long training epochs to extract robust features, which
is not friendly in practical application. In this paper, we present a novel
adversarial momentum-contrastive learning approach that leverages two memory
banks to track the invariant features across different mini-batches. These
memory banks can be efficiently incorporated into each iteration and help the
network to learn more robust feature representations with smaller batches and
far fewer epochs. Furthermore, after fine-tuning on the classification tasks,
the proposed approach can meet or exceed the performance of some
state-of-the-art supervised baselines on real world datasets. Our code is
available at \url{https://github.com/MTandHJ/amoc}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real Masks and Spoof Faces: On the Masked Face Presentation Attack Detection. (arXiv:2103.01546v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01546">
<div class="article-summary-box-inner">
<span><p>Face masks have become one of the main methods for reducing the transmission
of COVID-19. This makes face recognition (FR) a challenging task because masks
hide several discriminative features of faces. Moreover, face presentation
attack detection (PAD) is crucial to ensure the security of FR systems. In
contrast to the growing number of masked FR studies, the impact of face masked
attacks on PAD has not been explored. Therefore, we present novel attacks with
real face masks placed on presentations and attacks with subjects wearing masks
to reflect the current real-world situation. Furthermore, this study
investigates the effect of masked attacks on PAD performance by using seven
state-of-the-art PAD algorithms under different experimental settings. We also
evaluate the vulnerability of FR systems to masked attacks. The experiments
show that real masked attacks pose a serious threat to the operation and
security of FR systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Touchless Palmprint Recognition based on 3D Gabor Template and Block Feature Refinement. (arXiv:2103.02167v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02167">
<div class="article-summary-box-inner">
<span><p>With the growing demand for hand hygiene and convenience of use, palmprint
recognition with touchless manner made a great development recently, providing
an effective solution for person identification. Despite many efforts that have
been devoted to this area, it is still uncertain about the discriminative
ability of the contactless palmprint, especially for large-scale datasets. To
tackle the problem, in this paper, we build a large-scale touchless palmprint
dataset containing 2334 palms from 1167 individuals. To our best knowledge, it
is the largest contactless palmprint image benchmark ever collected with regard
to the number of individuals and palms. Besides, we propose a novel deep
learning framework for touchless palmprint recognition named 3DCPN (3D
Convolution Palmprint recognition Network) which leverages 3D convolution to
dynamically integrate multiple Gabor features. In 3DCPN, a novel variant of
Gabor filter is embedded into the first layer for enhancement of curve feature
extraction. With a well-designed ensemble scheme,low-level 3D features are then
convolved to extract high-level features. Finally on the top, we set a
region-based loss function to strengthen the discriminative ability of both
global and local descriptors. To demonstrate the superiority of our method,
extensive experiments are conducted on our dataset and other popular databases
TongJi and IITD, where the results show the proposed 3DCPN achieves
state-of-the-art or comparable performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Importance of Sampling in Training GCNs: Tighter Analysis and Variance Reduction. (arXiv:2103.02696v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02696">
<div class="article-summary-box-inner">
<span><p>Graph Convolutional Networks (GCNs) have achieved impressive empirical
advancement across a wide variety of semi-supervised node classification tasks.
Despite their great success, training GCNs on large graphs suffers from
computational and memory issues. A potential path to circumvent these obstacles
is sampling-based methods, where at each layer a subset of nodes is sampled.
Although recent studies have empirically demonstrated the effectiveness of
sampling-based methods, these works lack theoretical convergence guarantees
under realistic settings and cannot fully leverage the information of evolving
parameters during optimization. In this paper, we describe and analyze a
general doubly variance reduction schema that can accelerate any sampling
method under the memory budget. The motivating impetus for the proposed schema
is a careful analysis of the variance of sampling methods where it is shown
that the induced variance can be decomposed into node embedding approximation
variance (zeroth-order variance) during forward propagation and
layerwise-gradient variance (first-order variance) during backward propagation.
We theoretically analyze the convergence of the proposed schema and show that
it enjoys an $\mathcal{O}(1/T)$ convergence rate. We complement our theoretical
results by integrating the proposed schema in different sampling methods and
applying them to different large real-world graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised 3D Representation Learning of Dressed Humans from Social Media Videos. (arXiv:2103.03319v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03319">
<div class="article-summary-box-inner">
<span><p>A key challenge of learning a visual representation for the 3D high fidelity
geometry of dressed humans lies in the limited availability of the ground truth
data (e.g., 3D scanned models), which results in the performance degradation of
3D human reconstruction when applying to real-world imagery. We address this
challenge by leveraging a new data resource: a number of social media dance
videos that span diverse appearance, clothing styles, performances, and
identities. Each video depicts dynamic movements of the body and clothes of a
single person while lacking the 3D ground truth geometry. To learn a visual
representation from these videos, we present a new self-supervised learning
method to use the local transformation that warps the predicted local geometry
of the person from an image to that of another image at a different time
instant. This allows self-supervision by enforcing a temporal coherence over
the predictions. In addition, we jointly learn the depths along with the
surface normals that are highly responsive to local texture, wrinkle, and shade
by maximizing their geometric consistency. Our method is end-to-end trainable,
resulting in high fidelity depth estimation that predicts fine geometry
faithful to the input real image. We demonstrate that our method outperforms
the state-of-the-art human depth estimation and human shape recovery approaches
on both real and rendered images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Morphological and Histogram based Text Line Segmentation in the OCR Context. (arXiv:2103.08922v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08922">
<div class="article-summary-box-inner">
<span><p>Text line segmentation is one of the pre-stages of modern optical character
recognition systems. The algorithmic approach proposed by this paper has been
designed for this exact purpose. Its main characteristic is the combination of
two different techniques, morphological image operations and horizontal
histogram projections. The method was developed to be applied on a historic
data collection that commonly features quality issues, such as degraded paper,
blurred text, or presence of noise. For that reason, the segmenter in question
could be of particular interest for cultural institutions, that want access to
robust line bounding boxes for a given historic document. Because of the
promising segmentation results that are joined by low computational cost, the
algorithm was incorporated into the OCR pipeline of the National Library of
Luxembourg, in the context of the initiative of reprocessing their historic
newspaper collection. The general contribution of this paper is to outline the
approach and to evaluate the gains in terms of accuracy and speed, comparing it
to the segmentation algorithm bundled with the used open source OCR software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling Missing Observations with an RNN-based Prediction-Update Cycle. (arXiv:2103.11747v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11747">
<div class="article-summary-box-inner">
<span><p>In tasks such as tracking, time-series data inevitably carry missing
observations. While traditional tracking approaches can handle missing
observations, recurrent neural networks (RNNs) are designed to receive input
data in every step. Furthermore, current solutions for RNNs, like omitting the
missing data or data imputation, are not sufficient to account for the
resulting increased uncertainty. Towards this end, this paper introduces an
RNN-based approach that provides a full temporal filtering cycle for motion
state estimation. The Kalman filter inspired approach, enables to deal with
missing observations and outliers. For providing a full temporal filtering
cycle, a basic RNN is extended to take observations and the associated belief
about its accuracy into account for updating the current state. An RNN
prediction model, which generates a parametrized distribution to capture the
predicted states, is combined with an RNN update model, which relies on the
prediction model output and the current observation. By providing the model
with masking information, binary-encoded missing events, the model can overcome
limitations of standard techniques for dealing with missing input values. The
model abilities are demonstrated on synthetic data reflecting prototypical
pedestrian tracking scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch Craft: Video Denoising by Deep Modeling and Patch Matching. (arXiv:2103.13767v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13767">
<div class="article-summary-box-inner">
<span><p>The non-local self-similarity property of natural images has been exploited
extensively for solving various image processing problems. When it comes to
video sequences, harnessing this force is even more beneficial due to the
temporal redundancy. In the context of image and video denoising, many
classically-oriented algorithms employ self-similarity, splitting the data into
overlapping patches, gathering groups of similar ones and processing these
together somehow. With the emergence of convolutional neural networks (CNN),
the patch-based framework has been abandoned. Most CNN denoisers operate on the
whole image, leveraging non-local relations only implicitly by using a large
receptive field. This work proposes a novel approach for leveraging
self-similarity in the context of video denoising, while still relying on a
regular convolutional architecture. We introduce a concept of patch-craft
frames - artificial frames that are similar to the real ones, built by tiling
matched patches. Our algorithm augments video sequences with patch-craft frames
and feeds them to a CNN. We demonstrate the substantial boost in denoising
performance obtained with the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViViT: A Video Vision Transformer. (arXiv:2103.15691v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15691">
<div class="article-summary-box-inner">
<span><p>We present pure-transformer based models for video classification, drawing
upon the recent success of such models in image classification. Our model
extracts spatio-temporal tokens from the input video, which are then encoded by
a series of transformer layers. In order to handle the long sequences of tokens
encountered in video, we propose several, efficient variants of our model which
factorise the spatial- and temporal-dimensions of the input. Although
transformer-based models are known to only be effective when large training
datasets are available, we show how we can effectively regularise the model
during training and leverage pretrained image models to be able to train on
comparatively small datasets. We conduct thorough ablation studies, and achieve
state-of-the-art results on multiple video classification benchmarks including
Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in
Time, outperforming prior methods based on deep 3D convolutional networks. To
facilitate further research, we release code at
https://github.com/google-research/scenic/tree/main/scenic/projects/vivit
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Radiology Report Labeler Quality on Deep Learning Models for Chest X-Ray Interpretation. (arXiv:2104.00793v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00793">
<div class="article-summary-box-inner">
<span><p>Although deep learning models for chest X-ray interpretation are commonly
trained on labels generated by automatic radiology report labelers, the impact
of improvements in report labeling on the performance of chest X-ray
classification models has not been systematically investigated. We first
compare the CheXpert, CheXbert, and VisualCheXbert labelers on the task of
extracting accurate chest X-ray image labels from radiology reports, reporting
that the VisualCheXbert labeler outperforms the CheXpert and CheXbert labelers.
Next, after training image classification models using labels generated from
the different radiology report labelers on one of the largest datasets of chest
X-rays, we show that an image classification model trained on labels from the
VisualCheXbert labeler outperforms image classification models trained on
labels from the CheXpert and CheXbert labelers. Our work suggests that recent
improvements in radiology report labeling can translate to the development of
higher performing chest X-ray classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributional Robustness Loss for Long-tail Learning. (arXiv:2104.03066v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03066">
<div class="article-summary-box-inner">
<span><p>Real-world data is often unbalanced and long-tailed, but deep models struggle
to recognize rare classes in the presence of frequent classes. To address
unbalanced data, most studies try balancing the data, the loss, or the
classifier to reduce classification bias towards head classes. Far less
attention has been given to the latent representations learned with unbalanced
data. We show that the feature extractor part of deep networks suffers greatly
from this bias. We propose a new loss based on robustness theory, which
encourages the model to learn high-quality representations for both head and
tail classes. While the general form of the robustness loss may be hard to
compute, we further derive an easy-to-compute upper bound that can be minimized
efficiently. This procedure reduces representation bias towards head classes in
the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT,
and iNaturalist long-tail benchmarks. We find that training with robustness
increases recognition accuracy of tail classes while largely maintaining the
accuracy of head classes. The new robustness loss can be combined with various
classifier balancing techniques and can be applied to representations at
several layers of the deep model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks. (arXiv:2104.07085v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07085">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel layer based on fast Walsh-Hadamard
transform (WHT) and smooth-thresholding to replace $1\times 1$ convolution
layers in deep neural networks. In the WHT domain, we denoise the transform
domain coefficients using the new smooth-thresholding non-linearity, a smoothed
version of the well-known soft-thresholding operator. We also introduce a
family of multiplication-free operators from the basic 2$\times$2 Hadamard
transform to implement $3\times 3$ depthwise separable convolution layers.
Using these two types of layers, we replace the bottleneck layers in
MobileNet-V2 to reduce the network's number of parameters with a slight loss in
accuracy. For example, by replacing the final third bottleneck layers, we
reduce the number of parameters from 2.270M to 540K. This reduces the accuracy
from 95.21\% to 92.98\% on the CIFAR-10 dataset. Our approach significantly
improves the speed of data processing. The fast Walsh-Hadamard transform has a
computational complexity of $O(m\log_2 m)$. As a result, it is computationally
more efficient than the $1\times1$ convolution layer. The fast Walsh-Hadamard
layer processes a tensor in $\mathbb{R}^{10\times32\times32\times1024}$ about 2
times faster than $1\times1$ convolution layer on NVIDIA Jetson Nano computer
board.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning for detecting pulmonary tuberculosis via chest radiography: an international study across 10 countries. (arXiv:2105.07540v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07540">
<div class="article-summary-box-inner">
<span><p>Tuberculosis (TB) is a top-10 cause of death worldwide. Though the WHO
recommends chest radiographs (CXRs) for TB screening, the limited availability
of CXR interpretation is a barrier. We trained a deep learning system (DLS) to
detect active pulmonary TB using CXRs from 9 countries across Africa, Asia, and
Europe, and utilized large-scale CXR pretraining, attention pooling, and noisy
student semi-supervised learning. Evaluation was on (1) a combined test set
spanning China, India, US, and Zambia, and (2) an independent mining population
in South Africa. Given WHO targets of 90% sensitivity and 70% specificity, the
DLS's operating point was prespecified to favor sensitivity over specificity.
On the combined test set, the DLS's ROC curve was above all 9 India-based
radiologists, with an AUC of 0.90 (95%CI 0.87-0.92). The DLS's sensitivity
(88%) was higher than the India-based radiologists (75% mean sensitivity),
p&lt;0.001 for superiority; and its specificity (79%) was non-inferior to the
radiologists (84% mean specificity), p=0.004. Similar trends were observed
within HIV positive and sputum smear positive sub-groups, and in the South
Africa test set. We found that 5 US-based radiologists (where TB isn't endemic)
were more sensitive and less specific than the India-based radiologists (where
TB is endemic). The DLS also remained non-inferior to the US-based
radiologists. In simulations, using the DLS as a prioritization tool for
confirmatory testing reduced the cost per positive case detected by 40-80%
compared to using confirmatory testing alone. To conclude, our DLS generalized
to 5 countries, and merits prospective evaluation to assist cost-effective
screening efforts in radiologist-limited settings. Operating point flexibility
may permit customization of the DLS to account for site-specific factors such
as TB prevalence, demographics, clinical resources, and customary practice
patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2106.00908v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00908">
<div class="article-summary-box-inner">
<span><p>Multiple instance learning (MIL) is a powerful tool to solve the weakly
supervised classification in whole slide image (WSI) based pathology diagnosis.
However, the current MIL methods are usually based on independent and identical
distribution hypothesis, thus neglect the correlation among different
instances. To address this problem, we proposed a new framework, called
correlated MIL, and provided a proof for convergence. Based on this framework,
we devised a Transformer based MIL (TransMIL), which explored both
morphological and spatial information. The proposed TransMIL can effectively
deal with unbalanced/balanced and binary/multiple classification with great
visualization and interpretability. We conducted various experiments for three
different computational pathology problems and achieved better performance and
faster convergence compared with state-of-the-art methods. The test AUC for the
binary tumor classification can be up to 93.09% over CAMELYON16 dataset. And
the AUC over the cancer subtypes classification can be up to 96.03% and 98.82%
over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively. Implementation is
available at: https://github.com/szc19990412/TransMIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associating Objects with Transformers for Video Object Segmentation. (arXiv:2106.02638v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02638">
<div class="article-summary-box-inner">
<span><p>This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computing resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
uniformly. In detail, AOT employs an identification mechanism to associate
multiple targets into the same high-dimensional embedding space. Thus, we can
simultaneously process multiple objects' matching and segmentation decoding as
efficiently as processing a single object. For sufficiently modeling
multi-object association, a Long Short-Term Transformer is designed for
constructing hierarchical matching and propagation. We conduct extensive
experiments on both multi-object and single-object benchmarks to examine AOT
variant networks with different complexities. Particularly, our R50-AOT-L
outperforms all the state-of-the-art competitors on three popular benchmarks,
i.e., YouTube-VOS (84.1% J&amp;F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%),
while keeping more than $3\times$ faster multi-object run-time. Meanwhile, our
AOT-T can maintain real-time multi-object speed on the above benchmarks. Based
on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DISCO: accurate Discrete Scale Convolutions. (arXiv:2106.02733v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02733">
<div class="article-summary-box-inner">
<span><p>Scale is often seen as a given, disturbing factor in many vision tasks. When
doing so it is one of the factors why we need more data during learning. In
recent work scale equivariance was added to convolutional neural networks. It
was shown to be effective for a range of tasks. We aim for accurate
scale-equivariant convolutional neural networks (SE-CNNs) applicable for
problems where high granularity of scale and small kernel sizes are required.
Current SE-CNNs rely on weight sharing and kernel rescaling, the latter of
which is accurate for integer scales only. To reach accurate scale
equivariance, we derive general constraints under which scale-convolution
remains equivariant to discrete rescaling. We find the exact solution for all
cases where it exists, and compute the approximation for the rest. The discrete
scale-convolution pays off, as demonstrated in a new state-of-the-art
classification on MNIST-scale and on STL-10 in the supervised learning setting.
With the same SE scheme, we also improve the computational effort of a
scale-equivariant Siamese tracker on OTB-13.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03743">
<div class="article-summary-box-inner">
<span><p>We investigate the reasons for the performance degradation incurred with
batch-independent normalization. We find that the prototypical techniques of
layer normalization and instance normalization both induce the appearance of
failure modes in the neural network's pre-activations: (i) layer normalization
induces a collapse towards channel-wise constant functions; (ii) instance
normalization induces a lack of variability in instance statistics, symptomatic
of an alteration of the expressivity. To alleviate failure mode (i) without
aggravating failure mode (ii), we introduce the technique "Proxy Normalization"
that normalizes post-activations using a proxy distribution. When combined with
layer normalization or group normalization, this batch-independent
normalization emulates batch normalization's behavior and consistently matches
or exceeds its performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethink Transfer Learning in Medical Image Classification. (arXiv:2106.05152v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05152">
<div class="article-summary-box-inner">
<span><p>Transfer learning (TL) with deep convolutional neural networks (DCNNs) is
crucial for modern medical image classification (MIC). However, the current
practice of finetuning the entire pretrained model is puzzling, as most MIC
tasks rely only on low- to mid-level features that are learned by up to mid
layers of DCNNs. To resolve the puzzle, we perform careful empirical
comparisons of several existing deep and shallow models, and propose a novel
truncated TL method that consistently leads to comparable or superior
performance and compact models on two MIC tasks. Our results highlight the
importance of transferring the right level of pretrained visual features
commensurate with the intrinsic complexity of the task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Facet Clustering Variational Autoencoders. (arXiv:2106.05241v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05241">
<div class="article-summary-box-inner">
<span><p>Work in deep clustering focuses on finding a single partition of data.
However, high-dimensional data, such as images, typically feature multiple
interesting characteristics one could cluster over. For example, images of
objects against a background could be clustered over the shape of the object
and separately by the colour of the background. In this paper, we introduce
Multi-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of
variational autoencoders with a hierarchy of latent variables, each with a
Mixture-of-Gaussians prior, that learns multiple clusterings simultaneously,
and is trained fully unsupervised and end-to-end. MFCVAE uses a
progressively-trained ladder architecture which leads to highly stable
performance. We provide novel theoretical results for optimising the ELBO
analytically with respect to the categorical variational posterior
distribution, correcting earlier influential theoretical work. On image
benchmarks, we demonstrate that our approach separates out and clusters over
different aspects of the data in a disentangled manner. We also show other
advantages of our model: the compositionality of its latent space and that it
provides controlled generation of samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-Aware Downsampling with Deep Learning for Scalable Flood Modeling. (arXiv:2106.07218v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07218">
<div class="article-summary-box-inner">
<span><p>Background: Floods are the most common natural disaster in the world,
affecting the lives of hundreds of millions. Flood forecasting is therefore a
vitally important endeavor, typically achieved using physical water flow
simulations, which rely on accurate terrain elevation maps. However, such
simulations, based on solving partial differential equations, are
computationally prohibitive on a large scale. This scalability issue is
commonly alleviated using a coarse grid representation of the elevation map,
though this representation may distort crucial terrain details, leading to
significant inaccuracies in the simulation. Contributions: We train a deep
neural network to perform physics-informed downsampling of the terrain map: we
optimize the coarse grid representation of the terrain maps, so that the flood
prediction will match the fine grid solution. For the learning process to
succeed, we configure a dataset specifically for this task. We demonstrate that
with this method, it is possible to achieve a significant reduction in
computational cost, while maintaining an accurate solution. A reference
implementation accompanies the paper as well as documentation and code for
dataset reproduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data. (arXiv:2106.07807v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07807">
<div class="article-summary-box-inner">
<span><p>Most existing works in few-shot learning rely on meta-learning the network on
a large base dataset which is typically from the same domain as the target
dataset. We tackle the problem of cross-domain few-shot learning where there is
a large shift between the base and target domain. The problem of cross-domain
few-shot recognition with unlabeled target data is largely unaddressed in the
literature. STARTUP was the first method that tackles this problem using
self-training. However, it uses a fixed teacher pretrained on a labeled base
dataset to create soft labels for the unlabeled target samples. As the base
dataset and unlabeled dataset are from different domains, projecting the target
images in the class-domain of the base dataset with a fixed pretrained model
might be sub-optimal. We propose a simple dynamic distillation-based approach
to facilitate unlabeled images from the novel/base dataset. We impose
consistency regularization by calculating predictions from the weakly-augmented
versions of the unlabeled images from a teacher network and matching it with
the strongly augmented versions of the same images from a student network. The
parameters of the teacher network are updated as exponential moving average of
the parameters of the student network. We show that the proposed network learns
representation that can be easily adapted to the target domain even though it
has not been trained with target-specific classes during the pretraining phase.
Our model outperforms the current state-of-the art method by 4.4% for 1-shot
and 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows
competitive performance on traditional in-domain few-shot learning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Navigation with Random Environmental Mixup. (arXiv:2106.07876v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07876">
<div class="article-summary-box-inner">
<span><p>Vision-language Navigation (VLN) tasks require an agent to navigate
step-by-step while perceiving the visual observations and comprehending a
natural language instruction. Large data bias, which is caused by the disparity
ratio between the small data scale and large navigation space, makes the VLN
task challenging. Previous works have proposed various data augmentation
methods to reduce data bias. However, these works do not explicitly reduce the
data bias across different house scenes. Therefore, the agent would overfit to
the seen scenes and achieve poor navigation performance in the unseen scenes.
To tackle this problem, we propose the Random Environmental Mixup (REM) method,
which generates cross-connected house scenes as augmented data via mixuping
environment. Specifically, we first select key viewpoints according to the room
connection graph for each scene. Then, we cross-connect the key views of
different scenes to construct augmented scenes. Finally, we generate augmented
instruction-path pairs in the cross-connected scenes. The experimental results
on benchmark datasets demonstrate that our augmentation data via REM help the
agent reduce its performance gap between the seen and unseen environment and
improve the overall performance, making our model the best existing approach on
the standard VLN benchmark. The code have released:
https://github.com/LCFractal/VLNREM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v3 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08208">
<div class="article-summary-box-inner">
<span><p>Adaptive gradient methods have shown excellent performances for solving many
machine learning problems. Although multiple adaptive methods were recently
studied, they mainly focus on either empirical or theoretical aspects and also
only work for specific problems by using some specific adaptive learning rates.
It is desired to design a universal framework for practical algorithms of
adaptive gradients with theoretical guarantee to solve general problems. To
fill this gap, we propose a faster and universal framework of adaptive
gradients (\emph{i.e.}, SUPER-ADAM) by introducing a universal adaptive matrix
that includes most existing adaptive gradient forms. Moreover, our framework
can flexibly integrate the momentum and variance reduced techniques. In
particular, our novel framework provides the convergence analysis support for
adaptive gradient methods under the nonconvex setting. In theoretical analysis,
we prove that our SUPER-ADAM algorithm can achieve the best known complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms. Code is available at
https://github.com/LIJUNYI95/SuperAdam
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-Net: Towards Unified Image Segmentation. (arXiv:2106.14855v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14855">
<div class="article-summary-box-inner">
<span><p>Semantic, instance, and panoptic segmentations have been addressed using
different and specialized frameworks despite their underlying connections. This
paper presents a unified, simple, and effective framework for these essentially
similar tasks. The framework, named K-Net, segments both instances and semantic
categories consistently by a group of learnable kernels, where each kernel is
responsible for generating a mask for either a potential instance or a stuff
class. To remedy the difficulties of distinguishing various instances, we
propose a kernel update strategy that enables each kernel dynamic and
conditional on its meaningful group in the input image. K-Net can be trained in
an end-to-end manner with bipartite matching, and its training and inference
are naturally NMS-free and box-free. Without bells and whistles, K-Net
surpasses all previous published state-of-the-art single-model results of
panoptic segmentation on MS COCO test-dev split and semantic segmentation on
ADE20K val split with 55.2% PQ and 54.3% mIoU, respectively. Its instance
segmentation performance is also on par with Cascade Mask R-CNN on MS COCO with
60%-90% faster inference speeds. Code and models will be released at
https://github.com/ZwwWayne/K-Net/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Synthetic Training Data for Deep Learning-Based UAV Trajectory Prediction. (arXiv:2107.00422v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00422">
<div class="article-summary-box-inner">
<span><p>Deep learning-based models, such as recurrent neural networks (RNNs), have
been applied to various sequence learning tasks with great success. Following
this, these models are increasingly replacing classic approaches in object
tracking applications for motion prediction. On the one hand, these models can
capture complex object dynamics with less modeling required, but on the other
hand, they depend on a large amount of training data for parameter tuning.
Towards this end, we present an approach for generating synthetic trajectory
data of unmanned-aerial-vehicles (UAVs) in image space. Since UAVs, or rather
quadrotors are dynamical systems, they can not follow arbitrary trajectories.
With the prerequisite that UAV trajectories fulfill a smoothness criterion
corresponding to a minimal change of higher-order motion, methods for planning
aggressive quadrotors flights can be utilized to generate optimal trajectories
through a sequence of 3D waypoints. By projecting these maneuver trajectories,
which are suitable for controlling quadrotors, to image space, a versatile
trajectory data set is realized. To demonstrate the applicability of the
synthetic trajectory data, we show that an RNN-based prediction model solely
trained on the generated data can outperform classic reference models on a
real-world UAV tracking dataset. The evaluation is done on the publicly
available ANTI-UAV dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Per-Pixel Classification is Not All You Need for Semantic Segmentation. (arXiv:2107.06278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06278">
<div class="article-summary-box-inner">
<span><p>Modern approaches typically formulate semantic segmentation as a per-pixel
classification task, while instance-level segmentation is handled with an
alternative mask classification. Our key insight: mask classification is
sufficiently general to solve both semantic- and instance-level segmentation
tasks in a unified manner using the exact same model, loss, and training
procedure. Following this observation, we propose MaskFormer, a simple mask
classification model which predicts a set of binary masks, each associated with
a single global class label prediction. Overall, the proposed mask
classification-based method simplifies the landscape of effective approaches to
semantic and panoptic segmentation tasks and shows excellent empirical results.
In particular, we observe that MaskFormer outperforms per-pixel classification
baselines when the number of classes is large. Our mask classification-based
method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K)
and panoptic segmentation (52.7 PQ on COCO) models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Passive Attention in Artificial Neural Networks Predicts Human Visual Selectivity. (arXiv:2107.07013v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07013">
<div class="article-summary-box-inner">
<span><p>Developments in machine learning interpretability techniques over the past
decade have provided new tools to observe the image regions that are most
informative for classification and localization in artificial neural networks
(ANNs). Are the same regions similarly informative to human observers? Using
data from 79 new experiments and 7,810 participants, we show that passive
attention techniques reveal a significant overlap with human visual selectivity
estimates derived from 6 distinct behavioral tasks including visual
discrimination, spatial localization, recognizability, free-viewing,
cued-object search, and saliency search fixations. We find that input
visualizations derived from relatively simple ANN architectures probed using
guided backpropagation methods are the best predictors of a shared component in
the joint variability of the human measures. We validate these correlational
results with causal manipulations using recognition experiments. We show that
images masked with ANN attention maps were easier for humans to classify than
control masks in a speeded recognition experiment. Similarly, we find that
recognition performance in the same ANN models was likewise influenced by
masking input images using human visual selectivity maps. This work contributes
a new approach to evaluating the biological and psychological validity of
leading ANNs as models of human vision: by examining their similarities and
differences in terms of their visual selectivity to the information contained
in images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rectifying the Shortcut Learning of Background for Few-Shot Learning. (arXiv:2107.07746v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07746">
<div class="article-summary-box-inner">
<span><p>The category gap between training and evaluation has been characterised as
one of the main obstacles to the success of Few-Shot Learning (FSL). In this
paper, we for the first time empirically identify image background, common in
realistic images, as a shortcut knowledge helpful for in-class classification
but ungeneralizable beyond training categories in FSL. A novel framework,
COSOC, is designed to tackle this problem by extracting foreground objects in
images at both training and evaluation without any extra supervision. Extensive
experiments carried on inductive FSL tasks demonstrate the effectiveness of our
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds. (arXiv:2108.04728v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04728">
<div class="article-summary-box-inner">
<span><p>Current 3D single object tracking approaches track the target based on a
feature comparison between the target template and the search area. However,
due to the common occlusion in LiDAR scans, it is non-trivial to conduct
accurate feature comparisons on severe sparse and incomplete shapes. In this
work, we exploit the ground truth bounding box given in the first frame as a
strong cue to enhance the feature description of the target object, enabling a
more accurate feature comparison in a simple yet effective way. In particular,
we first propose the BoxCloud, an informative and robust representation, to
depict an object using the point-to-box relation. We further design an
efficient box-aware feature fusion module, which leverages the aforementioned
BoxCloud for reliable feature matching and embedding. Integrating the proposed
general components into an existing model P2B, we construct a superior
box-aware tracker (BAT). Experiments confirm that our proposed BAT outperforms
the previous state-of-the-art by a large margin on both KITTI and NuScenes
benchmarks, achieving a 15.2% improvement in terms of precision while running
~20% faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Network (CNN) vs Vision Transformer (ViT) for Digital Holography. (arXiv:2108.09147v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09147">
<div class="article-summary-box-inner">
<span><p>In Digital Holography (DH), it is crucial to extract the object distance from
a hologram in order to reconstruct its amplitude and phase. This step is called
auto-focusing and it is conventionally solved by first reconstructing a stack
of images and then by sharpening each reconstructed image using a focus metric
such as entropy or variance. The distance corresponding to the sharpest image
is considered the focal position. This approach, while effective, is
computationally demanding and time-consuming. In this paper, the determination
of the distance is performed by Deep Learning (DL). Two deep learning (DL)
architectures are compared: Convolutional Neural Network (CNN) and Vision
Transformer (ViT). ViT and CNN are used to cope with the problem of
auto-focusing as a classification problem. Compared to a first attempt [11] in
which the distance between two consecutive classes was 100$\mu$m, our proposal
allows us to drastically reduce this distance to 1$\mu$m. Moreover, ViT reaches
similar accuracy and is more robust than CNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AP-10K: A Benchmark for Animal Pose Estimation in the Wild. (arXiv:2108.12617v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12617">
<div class="article-summary-box-inner">
<span><p>Accurate animal pose estimation is an essential step towards understanding
animal behavior, and can potentially benefit many downstream applications, such
as wildlife conservation. Previous works only focus on specific animals while
ignoring the diversity of animal species, limiting the generalization ability.
In this paper, we propose AP-10K, the first large-scale benchmark for mammal
animal pose estimation, to facilitate the research in animal pose estimation.
AP-10K consists of 10,015 images collected and filtered from 23 animal families
and 54 species following the taxonomic rank and high-quality keypoint
annotations labeled and checked manually. Based on AP-10K, we benchmark
representative pose estimation models on the following three tracks: (1)
supervised learning for animal pose estimation, (2) cross-domain transfer
learning from human pose estimation to animal pose estimation, and (3) intra-
and inter-family domain generalization for unseen animals. The experimental
results provide sound empirical evidence on the superiority of learning from
diverse animals species in terms of both accuracy and generalization ability.
It opens new directions for facilitating future research in animal pose
estimation. AP-10k is publicly available at
https://github.com/AlexTheBad/AP10K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04797">
<div class="article-summary-box-inner">
<span><p>Computational fluid dynamics (CFD) is a valuable tool for personalised,
non-invasive evaluation of hemodynamics in arteries, but its complexity and
time-consuming nature prohibit large-scale use in practice. Recently, the use
of deep learning for rapid estimation of CFD parameters like wall shear stress
(WSS) on surface meshes has been investigated. However, existing approaches
typically depend on a hand-crafted re-parametrisation of the surface mesh to
match convolutional neural network architectures. In this work, we propose to
instead use mesh convolutional neural networks that directly operate on the
same finite-element surface mesh as used in CFD. We train and evaluate our
method on two datasets of synthetic coronary artery models with and without
bifurcation, using a ground truth obtained from CFD simulation. We show that
our flexible deep learning model can accurately predict 3D WSS vectors on this
surface mesh. Our method processes new meshes in less than 5 [s], consistently
achieves a normalised mean absolute error of $\leq$ 1.6 [%], and peaks at 90.5
[%] median approximation accuracy over the held-out test set, comparing
favourably to previously published work. This demonstrates the feasibility of
CFD surrogate modelling using mesh convolutional neural networks for
hemodynamic parameter estimation in artery models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSOR: A Scalable Statistical Filter for Removing Falling Snow from LiDAR Point Clouds in Severe Winter Weather. (arXiv:2109.07078v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07078">
<div class="article-summary-box-inner">
<span><p>For autonomous vehicles to viably replace human drivers they must contend
with inclement weather. Falling rain and snow introduce noise in LiDAR returns
resulting in both false positive and false negative object detections. In this
article we introduce the Winter Adverse Driving dataSet (WADS) collected in the
snow belt region of Michigan's Upper Peninsula. WADS is the first multi-modal
dataset featuring dense point-wise labeled sequential LiDAR scans collected in
severe winter weather; weather that would cause an experienced driver to alter
their driving behavior. We have labelled and will make available over 7 GB or
3.6 billion labelled LiDAR points out of over 26 TB of total LiDAR and camera
data collected. We also present the Dynamic Statistical Outlier Removal (DSOR)
filter, a statistical PCL-based filter capable or removing snow with a higher
recall than the state of the art snow de-noising filter while being 28\%
faster. Further, the DSOR filter is shown to have a lower time complexity
compared to the state of the art resulting in an improved scalability.
</p>
<p>Our labeled dataset and DSOR filter will be made available at
https://bitbucket.org/autonomymtu/dsor_filter
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Multi-level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection. (arXiv:2109.07950v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07950">
<div class="article-summary-box-inner">
<span><p>With the increased deployment of face recognition systems in our daily lives,
face presentation attack detection (PAD) is attracting a lot of attention and
playing a key role in securing face recognition systems. Despite the great
performance achieved by the hand-crafted and deep learning based methods in
intra-dataset evaluations, the performance drops when dealing with unseen
scenarios. In this work, we propose a dual-stream convolution neural networks
(CNNs) framework. One stream adapts four learnable frequency filters to learn
features in the frequency domain, which are less influenced variations in
sensors/illuminations. The other stream leverage the RGB images to complement
the features of the frequency domain. Moreover, we propose a hierarchical
attention module integration to join the information from the two streams at
different stages by considering the nature of deep features in different layers
of the CNN. The proposed method is evaluated in the intra-dataset and
cross-dataset setups and the results demonstrates that our proposed approach
enhances the generalizability in most experimental setups in comparison to
state-of-the-art, including the methods designed explicitly for domain
adaption/shift problem. We successfully prove the design of our proposed PAD
solution in a step-wise ablation study that involves our proposed learnable
frequency decomposition, our hierarchical attention module design, and the used
loss function. Training codes and pre-trained models are publicly released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auditing AI models for Verified Deployment under Semantic Specifications. (arXiv:2109.12456v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12456">
<div class="article-summary-box-inner">
<span><p>Auditing trained deep learning (DL) models prior to deployment is vital for
preventing unintended consequences. One of the biggest challenges in auditing
is the lack of human-interpretable specifications for the DL models that are
directly useful to the auditor. We address this challenge through a sequence of
semantically-aligned unit tests, where each unit test verifies whether a
predefined specification (e.g., accuracy over 95%) is satisfied with respect to
controlled and semantically aligned variations in the input space (e.g., in
face recognition, the angle relative to the camera). We enable such unit tests
through variations in a semantically-interpretable latent space of a generative
model. Further, we conduct certified training for the DL model through a shared
latent space representation with the generative model. With evaluations on four
different datasets, covering images of chest X-rays, human faces, ImageNet
classes, and towers, we show how AuditAI allows us to obtain controlled
variations for certified training. Thus, our framework, AuditAI, bridges the
gap between semantically-aligned formal verification and scalability. A blog
post accompanying the paper is at this link
https://developer.nvidia.com/blog/nvidia-research-auditing-ai-models-for-verified-deployment-under-semantic-specifications
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsolved Problems in ML Safety. (arXiv:2109.13916v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13916">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards ("Robustness"),
identifying hazards ("Monitoring"), steering ML systems ("Alignment"), and
reducing hazards in deployment ("External Safety"). Throughout, we clarify each
problem's motivation and provide concrete research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Velocity Mapping Cardiac MRI Coupled with Automated Left Ventricle Segmentation. (arXiv:2110.01304v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01304">
<div class="article-summary-box-inner">
<span><p>Temporal patterns of cardiac motion provide important information for cardiac
disease diagnosis. This pattern could be obtained by three-directional CINE
multi-slice left ventricular myocardial velocity mapping (3Dir MVM), which is a
cardiac MR technique providing magnitude and phase information of the
myocardial motion simultaneously. However, long acquisition time limits the
usage of this technique by causing breathing artifacts, while shortening the
time causes low temporal resolution and may provide an inaccurate assessment of
cardiac motion. In this study, we proposed a frame synthesis algorithm to
increase the temporal resolution of 3Dir MVM data. Our algorithm is featured by
1) three attention-based encoders which accept magnitude images, phase images,
and myocardium segmentation masks respectively as inputs; 2) three decoders
that output the interpolated frames and corresponding myocardium segmentation
results; and 3) loss functions highlighting myocardium pixels. Our algorithm
can not only increase the temporal resolution 3Dir MVMs, but can also generates
the myocardium segmentation results at the same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Out-of-the-Box Frameworks for Contrastive Unpaired Image Translation for Vestibular Schwannoma and Cochlea Segmentation: An approach for the crossMoDA Challenge. (arXiv:2110.01607v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01607">
<div class="article-summary-box-inner">
<span><p>The purpose of this study is to apply and evaluate out-of-the-box deep
learning frameworks for the crossMoDA challenge. We use the CUT model for
domain adaptation from contrast-enhanced T1 MR to high-resolution T2 MR. As
data augmentation, we generated additional images with vestibular schwannomas
with lower signal intensity. For the segmentation task, we use the nnU-Net
framework. Our final submission achieved mean Dice scores of 0.8299 in the
validation phase and 0.8253 in the test phase. Our method ranked 3rd in the
crossMoDA challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03374">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation aims to align a labeled source domain and an
unlabeled target domain, but it requires to access the source data which often
raises concerns in data privacy, data portability and data transmission
efficiency. We study unsupervised model adaptation (UMA), or called
Unsupervised Domain Adaptation without Source Data, an alternative setting that
aims to adapt source-trained models towards target distributions without
accessing source data. To this end, we design an innovative historical
contrastive learning (HCL) technique that exploits historical source hypothesis
to make up for the absence of source data in UMA. HCL addresses the UMA
challenge from two perspectives. First, it introduces historical contrastive
instance discrimination (HCID) that learns from target samples by contrasting
their embeddings which are generated by the currently adapted model and the
historical models. With the historical models, HCID encourages UMA to learn
instance-discriminative target representations while preserving the source
hypothesis. Second, it introduces historical contrastive category
discrimination (HCCD) that pseudo-labels target samples to learn
category-discriminative target representations. Specifically, HCCD re-weights
pseudo labels according to their prediction consistency across the current and
historical models. Extensive experiments show that HCL outperforms and
state-of-the-art methods consistently across a variety of visual tasks and
setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">6D-ViT: Category-Level 6D Object Pose Estimation via Transformer-based Instance Representation Learning. (arXiv:2110.04792v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04792">
<div class="article-summary-box-inner">
<span><p>This paper presents 6D-ViT, a transformer-based instance representation
learning network, which is suitable for highly accurate category-level object
pose estimation on RGB-D images. Specifically, a novel two-stream
encoder-decoder framework is dedicated to exploring complex and powerful
instance representations from RGB images, point clouds and categorical shape
priors. For this purpose, the whole framework consists of two main branches,
named Pixelformer and Pointformer. The Pixelformer contains a pyramid
transformer encoder with an all-MLP decoder to extract pixelwise appearance
representations from RGB images, while the Pointformer relies on a cascaded
transformer encoder and an all-MLP decoder to acquire the pointwise geometric
characteristics from point clouds. Then, dense instance representations (i.e.,
correspondence matrix, deformation field) are obtained from a multi-source
aggregation network with shape priors, appearance and geometric information as
input. Finally, the instance 6D pose is computed by leveraging the
correspondence among dense representations, shape priors, and the instance
point clouds. Extensive experiments on both synthetic and real-world datasets
demonstrate that the proposed 3D instance representation learning framework
achieves state-of-the-art performance on both datasets, and significantly
outperforms all existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMIU: Dataset for Visual Intent Understanding in Multimodal Assistants. (arXiv:2110.06416v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06416">
<div class="article-summary-box-inner">
<span><p>In multimodal assistant, where vision is also one of the input modalities,
the identification of user intent becomes a challenging task as visual input
can influence the outcome. Current digital assistants take spoken input and try
to determine the user intent from conversational or device context. So, a
dataset, which includes visual input (i.e. images or videos for the
corresponding questions targeted for multimodal assistant use cases, is not
readily available. The research in visual question answering (VQA) and visual
question generation (VQG) is a great step forward. However, they do not capture
questions that a visually-abled person would ask multimodal assistants.
Moreover, many times questions do not seek information from external knowledge.
In this paper, we provide a new dataset, MMIU (MultiModal Intent
Understanding), that contains questions and corresponding intents provided by
human annotators while looking at images. We, then, use this dataset for intent
classification task in multimodal digital assistant. We also experiment with
various approaches for combining vision and language features including the use
of multimodal transformer for classification of image-question pairs into 14
intents. We provide the benchmark results and discuss the role of visual and
text features for the intent classification task on our dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Deep Neural Networks with Joint Quantization and Pruning of Weights and Activations. (arXiv:2110.08271v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08271">
<div class="article-summary-box-inner">
<span><p>Quantization and pruning are core techniques used to reduce the inference
costs of deep neural networks. State-of-the-art quantization techniques are
currently applied to both the weights and activations; however, pruning is most
often applied to only the weights of the network. In this work, we jointly
apply novel uniform quantization and unstructured pruning methods to both the
weights and activations of deep neural networks during training. Using our
methods, we empirically evaluate the currently accepted prune-then-quantize
paradigm across a wide range of computer vision tasks and observe a
non-commutative nature when applied to both the weights and activations of deep
neural networks. Informed by these observations, we articulate the
non-commutativity hypothesis: for a given deep neural network being trained for
a specific task, there exists an exact training schedule in which quantization
and pruning can be introduced to optimize network performance. We identify that
this optimal ordering not only exists, but also varies across discriminative
and generative tasks. Using the optimal training schedule within our training
framework, we demonstrate increased performance per memory footprint over
existing solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09113">
<div class="article-summary-box-inner">
<span><p>Salt and pepper noise removal is a common inverse problem in image
processing. Traditional denoising methods have two limitations. First, noise
characteristics are often not described accurately. For example, the noise
location information is often ignored and the sparsity of the salt and pepper
noise is often described by L1 norm, which cannot illustrate the sparse
variables clearly. Second, conventional methods separate the contaminated image
into a recovered image and a noise part, thus resulting in recovering an image
with unsatisfied smooth parts and detail parts. In this study, we introduce a
noise detection strategy to determine the position of the noise, and a
non-convex sparsity regularization depicted by Lp quasi-norm is employed to
describe the sparsity of the noise, thereby addressing the first limitation.
The morphological component analysis framework with stationary Framelet
transform is adopted to decompose the processed image into cartoon, texture,
and noise parts to resolve the second limitation. Then, the alternating
direction method of multipliers (ADMM) is employed to solve the proposed model.
Finally, experiments are conducted to verify the proposed method and compare it
with some current state-of-the-art denoising methods. The experimental results
show that the proposed method can remove salt and pepper noise while preserving
the details of the processed image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning in High Dimension Always Amounts to Extrapolation. (arXiv:2110.09485v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09485">
<div class="article-summary-box-inner">
<span><p>The notion of interpolation and extrapolation is fundamental in various
fields from deep learning to function approximation. Interpolation occurs for a
sample $x$ whenever this sample falls inside or on the boundary of the given
dataset's convex hull. Extrapolation occurs when $x$ falls outside of that
convex hull. One fundamental (mis)conception is that state-of-the-art
algorithms work so well because of their ability to correctly interpolate
training data. A second (mis)conception is that interpolation happens
throughout tasks and datasets, in fact, many intuitions and theories rely on
that assumption. We empirically and theoretically argue against those two
points and demonstrate that on any high-dimensional ($&gt;$100) dataset,
interpolation almost surely never happens. Those results challenge the validity
of our current interpolation/extrapolation definition as an indicator of
generalization performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOS: A Low Latency and Lightweight Framework for Face Detection, Landmark Localization, and Head Pose Estimation. (arXiv:2110.10953v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10953">
<div class="article-summary-box-inner">
<span><p>With the emergence of service robots and surveillance cameras, dynamic face
recognition (DFR) in wild has received much attention in recent years. Face
detection and head pose estimation are two important steps for DFR. Very often,
the pose is estimated after the face detection. However, such sequential
computations lead to higher latency. In this paper, we propose a low latency
and lightweight network for simultaneous face detection, landmark localization
and head pose estimation. Inspired by the observation that it is more
challenging to locate the facial landmarks for faces with large angles, a pose
loss is proposed to constrain the learning. Moreover, we also propose an
uncertainty multi-task loss to learn the weights of individual tasks
automatically. Another challenge is that robots often use low computational
units like ARM based computing core and we often need to use lightweight
networks instead of the heavy ones, which lead to performance drop especially
for small and hard faces. In this paper, we propose online feedback sampling to
augment the training samples across different scales, which increases the
diversity of training data automatically. Through validation in commonly used
WIDER FACE, AFLW and AFLW2000 datasets, the results show that the proposed
method achieves the state-of-the-art performance in low computational
resources. The code and data will be available at
https://github.com/lyp-deeplearning/MOS-Multi-Task-Face-Detect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effect of Wearing a Face Mask on Face Image Quality. (arXiv:2110.11283v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11283">
<div class="article-summary-box-inner">
<span><p>Due to the COVID-19 situation, face masks have become a main part of our
daily life. Wearing mouth-and-nose protection has been made a mandate in many
public places, to prevent the spread of the COVID-19 virus. However, face masks
affect the performance of face recognition, since a large area of the face is
covered. The effect of wearing a face mask on the different components of the
face recognition system in a collaborative environment is a problem that is
still to be fully studied. This work studies, for the first time, the effect of
wearing a face mask on face image quality by utilising state-of-the-art face
image quality assessment methods of different natures. This aims at providing
better understanding on the effect of face masks on the operation of face
recognition as a whole system. In addition, we further studied the effect of
simulated masks on face image utility in comparison to real face masks. We
discuss the correlation between the mask effect on face image quality and that
on the face verification performance by automatic systems and human experts,
indicating a consistent trend between both factors. The evaluation is conducted
on the database containing (1) no-masked faces, (2) real face masks, and (3)
simulated face masks, by synthetically generating digital facial masks on
no-masked faces. Finally, a visual interpretation of the face areas
contributing to the quality score of a selected set of quality assessment
methods is provided to give a deeper insight into the difference of network
decisions in masked and non-masked faces, among other variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parametric Variational Linear Units (PVLUs) in Deep Convolutional Networks. (arXiv:2110.12246v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12246">
<div class="article-summary-box-inner">
<span><p>The Rectified Linear Unit is currently a state-of-the-art activation function
in deep convolutional neural networks. To combat ReLU's dying neuron problem,
we propose the Parametric Variational Linear Unit (PVLU), which adds a
sinusoidal function with trainable coefficients to ReLU. Along with introducing
nonlinearity and non-zero gradients across the entire real domain, PVLU allows
for increased model generalization and robustness when implemented in the
context of transfer learning. On a simple, non-transfer sequential CNN, PVLU
allowed for a relative error decrease of 16.3% and 11.3% (without and with data
augmentation) on CIFAR-10. PVLU is also tested on transfer learning problems.
The VGG-16 and VGG-19 models experience relative error reductions of 9.5% and
10.7% on CIFAR-10, respectively, after the substitution of ReLU with PVLU. When
training on Gaussian-filtered CIFAR-10 images, similar improvements are noted
for the VGG models. Most notably, PVLU fine tuning allows for relative error
reductions up to and exceeding 10% on near state-of-the-art ResNet models for
both CIFAR-10 and CIFAR-100.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logsig-RNN: a novel network for robust and efficient skeleton-based action recognition. (arXiv:2110.13008v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13008">
<div class="article-summary-box-inner">
<span><p>This paper contributes to the challenge of skeleton-based human action
recognition in videos. The key step is to develop a generic network
architecture to extract discriminative features for the spatio-temporal
skeleton data. In this paper, we propose a novel module, namely Logsig-RNN,
which is the combination of the log-signature layer and recurrent type neural
networks (RNNs). The former one comes from the mathematically principled
technology of signatures and log-signatures as representations for streamed
data, which can manage high sample rate streams, non-uniform sampling and time
series of variable length. It serves as an enhancement of the recurrent layer,
which can be conveniently plugged into neural networks. Besides we propose two
path transformation layers to significantly reduce path dimension while
retaining the essential information fed into the Logsig-RNN module. Finally,
numerical results demonstrate that replacing the RNN module by the Logsig-RNN
module in SOTA networks consistently improves the performance on both Chalearn
gesture data and NTU RGB+D 120 action data in terms of accuracy and robustness.
In particular, we achieve the state-of-the-art accuracy on Chalearn2013 gesture
data by combining simple path transformation layers with the Logsig-RNN. Codes
are available at https://github.com/steveliao93/GCN_LogsigRNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BI-GCN: Boundary-Aware Input-Dependent Graph Convolution Network for Biomedical Image Segmentation. (arXiv:2110.14775v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14775">
<div class="article-summary-box-inner">
<span><p>Segmentation is an essential operation of image processing. The convolution
operation suffers from a limited receptive field, while global modelling is
fundamental to segmentation tasks. In this paper, we apply graph convolution
into the segmentation task and propose an improved \textit{Laplacian}.
Different from existing methods, our \textit{Laplacian} is data-dependent, and
we introduce two attention diagonal matrices to learn a better vertex
relationship. In addition, it takes advantage of both region and boundary
information when performing graph-based information propagation. Specifically,
we model and reason about the boundary-aware region-wise correlations of
different classes through learning graph representations, which is capable of
manipulating long range semantic reasoning across various regions with the
spatial enhancement along the object's boundary. Our model is well-suited to
obtain global semantic region information while also accommodates local spatial
boundary characteristics simultaneously. Experiments on two types of
challenging datasets demonstrate that our method outperforms the
state-of-the-art approaches on the segmentation of polyps in colonoscopy images
and of the optic disc and optic cup in colour fundus images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis. (arXiv:2110.15678v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15678">
<div class="article-summary-box-inner">
<span><p>The advancement of generative radiance fields has pushed the boundary of
3D-aware image synthesis. Motivated by the observation that a 3D object should
look realistic from multiple viewpoints, these methods introduce a multi-view
constraint as regularization to learn valid 3D radiance fields from 2D images.
Despite the progress, they often fall short of capturing accurate 3D shapes due
to the shape-color ambiguity, limiting their applicability in downstream tasks.
In this work, we address this ambiguity by proposing a novel shading-guided
generative implicit model that is able to learn a starkly improved shape
representation. Our key insight is that an accurate 3D shape should also yield
a realistic rendering under different lighting conditions. This multi-lighting
constraint is realized by modeling illumination explicitly and performing
shading with various lighting conditions. Gradients are derived by feeding the
synthesized images to a discriminator. To compensate for the additional
computational burden of calculating surface normals, we further devise an
efficient volume rendering strategy via surface tracking, reducing the training
and inference time by 24% and 48%, respectively. Our experiments on multiple
datasets show that the proposed approach achieves photorealistic 3D-aware image
synthesis while capturing accurate underlying 3D shapes. We demonstrate
improved performance of our approach on 3D shape reconstruction against
existing methods, and show its applicability on image relighting. Our code will
be released at https://github.com/XingangPan/ShadeGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generational Frameshifts in Technology: Computer Science and Neurosurgery, The VR Use Case. (arXiv:2110.15719v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15719">
<div class="article-summary-box-inner">
<span><p>We are at a unique moment in history where there is a confluence of
technologies which will synergistically come together to transform the practice
of neurosurgery. These technological transformations will be all-encompassing,
including improved tools and methods for intraoperative performance of
neurosurgery, scalable solutions for asynchronous neurosurgical training and
simulation, as well as broad aggregation of operative data allowing fundamental
changes in quality assessment, billing, outcome measures, and dissemination of
surgical best practices. The ability to perform surgery more safely and more
efficiently while capturing the operative details and parsing each component of
the operation will open an entirely new epoch advancing our field and all
surgical specialties. The digitization of all components within the operating
room will allow us to leverage the various fields within computer and
computational science to obtain new insights that will improve care and
delivery of the highest quality neurosurgery regardless of location. The
democratization of neurosurgery is at hand and will be driven by our
development, extraction, and adoption of these tools of the modern world.
Virtual reality provides a good example of how consumer-facing technologies are
finding a clear role in industry and medicine and serves as a notable example
of the confluence of various computer science technologies creating a novel
paradigm for scaling human ability and interactions. The authors describe the
technology ecosystem that has come and highlight a myriad of computational and
data sciences that will be necessary to enable the operating room of the near
future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Sclerosis Lesions Identification/Segmentation in Magnetic Resonance Imaging using Ensemble CNN and Uncertainty Classification. (arXiv:2108.11791v2 [eess.IV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11791">
<div class="article-summary-box-inner">
<span><p>To date, several automated strategies for identification/segmentation of
Multiple Sclerosis (MS) lesions with the use of Magnetic Resonance Imaging
(MRI) have been presented but they are either outperformed by human experts or
perform differently from them. This is mainly due to the ambiguity originated
by MRI instabilities, peculiar variability of MS and unspecific nature of MRI
with respect to MS. Physicians partially manage the uncertainty generated by
ambiguity relying on their personal radiological/clinical/anatomical background
and experience. We present an automated framework based on three pivotal
concepts to better emulate human reasoning: 1. the modelling of uncertainty; 2.
the proposal of two, separately trained, CNN, one optimized with respect to
lesions themselves and the other to the environment surrounding lesions,
respectively repeated for axial, coronal and sagittal directions; 3. the
definition of an ensemble classifier to merge the information collected by all
CNN. The proposed framework is trained, validated and tested on the 2016 MSSEG
benchmark public data set from a single imaging modality, the FLuid-Attenuated
Inversion Recovery (FLAIR). The comparison, made with the consensus (the
ground-truth) between 7 human raters and with each of the 7 human raters,
proves that there is no significant difference between the automated and the
human raters. The results of our framework concerning the uncertainty are also
reported, even if a comparison with the raters is impossible because they don't
recognize this class.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-11-02 23:02:25.851927240 UTC">2021-11-02 23:02:25 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.6</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>