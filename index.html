<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-28T01:30:00Z">06-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">OPERA: Harmonizing Task-Oriented Dialogs and Information Seeking Experience. (arXiv:2206.12449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12449">
<div class="article-summary-box-inner">
<span><p>Existing studies in conversational AI mostly treat task-oriented dialog (TOD)
and question answering (QA) as separate tasks. Towards the goal of constructing
a conversational agent that can complete user tasks and support information
seeking, it is important to build a system that handles both TOD and QA with
access to various external knowledge. In this work, we propose a new task,
Open-Book TOD (OB-TOD), which combines TOD with QA task and expand external
knowledge sources to include both explicit knowledge sources (e.g., the Web)
and implicit knowledge sources (e.g., pre-trained language models). We create a
new dataset OB-MultiWOZ, where we enrich TOD sessions with QA-like information
seeking experience grounded on external knowledge. We propose a unified model
OPERA (Open-book End-to-end Task-oriented Dialog) which can appropriately
access explicit and implicit external knowledge to tackle the defined task.
Experimental results demonstrate OPERA's superior performance compared to
closed-book baselines and illustrate the value of both knowledge types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion, Age, and Origin from Vocal Bursts. (arXiv:2206.12469v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12469">
<div class="article-summary-box-inner">
<span><p>We present Burst2Vec, our multi-task learning approach to predict emotion,
age, and origin (i.e., native country/language) from vocal bursts. Burst2Vec
utilises pre-trained speech representations to capture acoustic information
from raw waveforms and incorporates the concept of model debiasing via
adversarial training. Our models achieve a relative 30 % performance gain over
baselines using pre-extracted features and score the highest amongst all
participants in the ICML ExVo 2022 Multi-Task Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The syntax-lexicon tradeoff in writing. (arXiv:2206.12485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12485">
<div class="article-summary-box-inner">
<span><p>As speakers turn their thoughts into sentences, they maintain a balance
between the complexity of words and syntax. However, it is unclear whether this
syntax-lexicon tradeoff is unique to the spoken language production that is
under the pressure of rapid online processing. Alternatively, it is possible
that the tradeoff is a basic property of language irrespective of the modality
of production. This work evaluates the relationship between the complexity of
words and syntactic rules in the written language of neurotypical individuals
on three different topics. We found that similar to speaking, constructing
sentences in writing involves a tradeoff between the complexity of the lexical
and syntactic items. We also show that the reduced online processing demands
during writing allows for retrieving more complex words at the cost of
incorporating simpler syntax. This work further highlights the role of
accessibility of the elements of a sentence as the driving force in the
emergence of the syntax-lexicon tradeoff.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DetIE: Multilingual Open Information Extraction Inspired by Object Detection. (arXiv:2206.12514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12514">
<div class="article-summary-box-inner">
<span><p>State of the art neural methods for open information extraction (OpenIE)
usually extract triplets (or tuples) iteratively in an autoregressive or
predicate-based manner in order not to produce duplicates. In this work, we
propose a different approach to the problem that can be equally or more
successful. Namely, we present a novel single-pass method for OpenIE inspired
by object detection algorithms from computer vision. We use an order-agnostic
loss based on bipartite matching that forces unique predictions and a
Transformer-based encoder-only architecture for sequence labeling. The proposed
approach is faster and shows superior or similar performance in comparison with
state of the art models on standard benchmarks in terms of both quality metrics
and inference time. Our model sets the new state of the art performance of
67.7% F1 on CaRB evaluated as OIE2016 while being 3.35x faster at inference
than previous state of the art. We also evaluate the multilingual version of
our model in the zero-shot setting for two languages and introduce a strategy
for generating synthetic multilingual data to fine-tune the model for each
specific language. In this setting, we show performance improvement 15% on
multilingual Re-OIE2016, reaching 75% F1 for both Portuguese and Spanish
languages. Code and models are available at
https://github.com/sberbank-ai/DetIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Causes of Hallucinations in Neural Machine Translations. (arXiv:2206.12529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12529">
<div class="article-summary-box-inner">
<span><p>Hallucination, one kind of pathological translations that bothers Neural
Machine Translation, has recently drawn much attention. In simple terms,
hallucinated translations are fluent sentences but barely related to source
inputs. Arguably, it remains an open problem how hallucination occurs. In this
paper, we propose to use probing methods to investigate the causes of
hallucinations from the perspective of model architecture, aiming to avoid such
problems in future architecture designs. By conducting experiments over various
NMT datasets, we find that hallucination is often accompanied by the deficient
encoder, especially embeddings, and vulnerable cross-attentions, while,
interestingly, cross-attention mitigates some errors caused by the encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConcreteGraph: A Data Augmentation Method Leveraging the Properties of Concept Relatedness Estimation. (arXiv:2206.12556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12556">
<div class="article-summary-box-inner">
<span><p>The concept relatedness estimation (CRE) task is to determine whether two
given concepts are related. Although existing methods for the semantic textual
similarity (STS) task can be easily adapted to this task, the CRE task has some
unique properties that can be leveraged to augment the datasets for addressing
its data scarcity problem. In this paper, we construct a graph named
ConcreteGraph (Concept relatedness estimation Graph) to take advantage of the
CRE properties. For the sampled new concept pairs from the ConcreteGraph, we
add an additional step of filtering out the new concept pairs with low quality
based on simple yet effective quality thresholding. We apply the ConcreteGraph
data augmentation on three Transformer-based models to show its efficacy.
Detailed ablation study for quality thresholding further shows that even a
limited amount of high-quality data is more beneficial than a large quantity of
unthresholded data. This paper is the first one to work on the WORD dataset and
the proposed ConcreteGraph can boost the accuracy of the Transformers by more
than 2%. All three Transformers, with the help of ConcreteGraph, can outperform
the current state-of-theart method, Concept Interaction Graph (CIG), on the
CNSE and CNSS datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Context-aware Style Representation for Expressive Speech Synthesis. (arXiv:2206.12559v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12559">
<div class="article-summary-box-inner">
<span><p>Expressive speech synthesis, like audiobook synthesis, is still challenging
for style representation learning and prediction. Deriving from reference audio
or predicting style tags from text requires a huge amount of labeled data,
which is costly to acquire and difficult to define and annotate accurately. In
this paper, we propose a novel framework for learning style representation from
abundant plain text in a self-supervised manner. It leverages an emotion
lexicon and uses contrastive learning and deep clustering. We further integrate
the style representation as a conditioned embedding in a multi-style
Transformer TTS. Comparing with multi-style TTS by predicting style tags
trained on the same dataset but with human annotations, our method achieves
improved results according to subjective evaluations on both in-domain and
out-of-domain test sets in audiobook speech. Moreover, with implicit
context-aware style representation, the emotion transition of synthesized audio
in a long paragraph appears more natural. The audio samples are available on
the demo web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construct a Sentence with Multiple Specified Words. (arXiv:2206.12565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12565">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates a task to finetune a BART model so it can construct a
sentence from an arbitrary set of words, which used to be a difficult NLP task.
The training task is making sentences with four words, but the trained model
can generate sentences when fewer or more words are provided. The output
sentences have high quality in general. The model can have some real-world
applications, and this task can be used as an evaluation mechanism for any
language model as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Self-Attention for Language Understanding. (arXiv:2206.12608v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12608">
<div class="article-summary-box-inner">
<span><p>An ultimate language system aims at the high generalization and robustness
when adapting to diverse scenarios. Unfortunately, the recent white hope
pre-trained language models (PrLMs) barely escape from stacking excessive
parameters to the over-parameterized Transformer architecture to achieve higher
performances. This paper thus proposes \textit{Adversarial Self-Attention}
mechanism (ASA), which adversarially reconstructs the Transformer attentions
and facilitates model training from contaminated model structures, coupled with
a fast and simple implementation for better PrLM building. We conduct
comprehensive evaluation across a wide range of tasks on both pre-training and
fine-tuning stages. For pre-training, ASA unfolds remarkable performance gain
compared to regular training for longer periods. For fine-tuning, ASA-empowered
models consistently outweigh naive models by a large margin considering both
generalization and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as Knowledge Embeddings. (arXiv:2206.12617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12617">
<div class="article-summary-box-inner">
<span><p>Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding
entities and relations into continuous vector spaces. Existing methods are
mainly structure-based or description-based. Structure-based methods learn
representations that preserve the inherent structure of KGs. They cannot well
represent abundant long-tail entities in real-world KGs with limited structural
information. Description-based methods leverage textual information and
language models. Prior approaches in this direction barely outperform
structure-based ones, and suffer from problems like expensive negative sampling
and restrictive description demand. In this paper, we propose LMKE, which
adopts Language Models to derive Knowledge Embeddings, aiming at both enriching
representations of long-tail entities and solving problems of prior
description-based methods. We formulate description-based KE learning with a
contrastive learning framework to improve efficiency in training and
evaluation. Experimental results show that LMKE achieves state-of-the-art
performance on KE benchmarks of link prediction and triple classification,
especially for long-tail entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling a Pretrained Language Model to a Multilingual ASR Model. (arXiv:2206.12638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12638">
<div class="article-summary-box-inner">
<span><p>Multilingual speech data often suffer from long-tailed language distribution,
resulting in performance degradation. However, multilingual text data is much
easier to obtain, yielding a more useful general language model. Hence, we are
motivated to distill the rich knowledge embedded inside a well-trained teacher
text model to the student speech model. We propose a novel method called the
Distilling a Language model to a Speech model (Distill-L2S), which aligns the
latent representations of two different modalities. The subtle differences are
handled by the shrinking mechanism, nearest-neighbor interpolation, and a
learnable linear projection layer. We demonstrate the effectiveness of our
distillation method by applying it to the multilingual automatic speech
recognition (ASR) task. We distill the transformer-based cross-lingual language
model (InfoXLM) while fine-tuning the large-scale multilingual ASR model
(XLSR-wav2vec 2.0) for each language. We show the superiority of our method on
20 low-resource languages of the CommonVoice dataset with less than 100 hours
of speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis with R: Natural Language Processing for Semi-Automated Assessments of Qualitative Data. (arXiv:2206.12649v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12649">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is a sub-discipline in the field of natural language
processing and computational linguistics and can be used for automated or
semi-automated analyses of text documents. One of the aims of these analyses is
to recognize an expressed attitude as positive or negative as it can be
contained in comments on social media platforms or political documents and
speeches as well as fictional and nonfictional texts. Regarding analyses of
comments on social media platforms, this is an extension of the previous
tutorial on semi-automated screenings of social media network data. A
longitudinal perspective regarding social media comments as well as
cross-sectional perspectives regarding fictional and nonfictional texts, e.g.
entire books and libraries, can lead to extensive text documents. Their
analyses can be simplified and accelerated by using sentiment analysis with
acceptable inter-rater reliability. Therefore, this tutorial introduces the
basic functions for performing a sentiment analysis with R and explains how
text documents can be analysed step by step - regardless of their underlying
formatting. All prerequisites and steps are described in detail and associated
codes are available on GitHub. A comparison of two political speeches
illustrates a possible use case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations. (arXiv:2206.12662v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12662">
<div class="article-summary-box-inner">
<span><p>We formulated non-speech vocalization (NSV) modeling as a text-to-speech task
and verified its viability. Specifically, we evaluated the phonetic
expressivity of HUBERT speech units on NSVs and verified our model's ability to
control over speaker timbre even though the training data is speaker few-shot.
In addition, we substantiated that the heterogeneity in recording conditions is
the major obstacle for NSV modeling. Finally, we discussed five improvements
over our method for future research. Audio samples of synthesized NSVs are
available on our demo page: https://resemble-ai.github.io/reLaugh.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Semantic Answer Similarity Metrics. (arXiv:2206.12664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12664">
<div class="article-summary-box-inner">
<span><p>There are several issues with the existing general machine translation or
natural language generation evaluation metrics, and question-answering (QA)
systems are indifferent in that context. To build robust QA systems, we need
the ability to have equivalently robust evaluation systems to verify whether
model predictions to questions are similar to ground-truth annotations. The
ability to compare similarity based on semantics as opposed to pure string
overlap is important to compare models fairly and to indicate more realistic
acceptance criteria in real-life applications. We build upon the first to our
knowledge paper that uses transformer-based model metrics to assess semantic
answer similarity and achieve higher correlations to human judgement in the
case of no lexical overlap. We propose cross-encoder augmented bi-encoder and
BERTScore models for semantic answer similarity, trained on a new dataset
consisting of name pairs of US-American public figures. As far as we are
concerned, we provide the first dataset of co-referent name string pairs along
with their similarities, which can be used for training.
</p>
<p>Machine Learning &amp; Applications 4th International Conference on Machine
Learning &amp; Applications (CMLA 2022) June 25~26, 2022, Copenhagen, Denmark
Volume Editors : David C. Wyld, Dhinaharan Nagamalai (Eds) ISBN :
978-1-925953-69-5
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEVR: Improving Speech Recognition by Token Entropy Variance Reduction. (arXiv:2206.12693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12693">
<div class="article-summary-box-inner">
<span><p>This paper presents TEVR, a speech recognition model designed to minimize the
variation in token entropy w.r.t. to the language model. This takes advantage
of the fact that if the language model will reliably and accurately predict a
token anyway, then the acoustic model doesn't need to be accurate in
recognizing it. We train German ASR models with 900 million parameters and show
that on CommonVoice German, TEVR scores a very competitive 3.64% word error
rate, which outperforms the best reported results by a relative 16.89%
reduction in word error rate. We hope that releasing our fully trained speech
recognition pipeline to the community will lead to privacy-preserving offline
virtual assistants in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Protoformer: Embedding Prototypes for Transformers. (arXiv:2206.12710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12710">
<div class="article-summary-box-inner">
<span><p>Transformers have been widely applied in text classification. Unfortunately,
real-world data contain anomalies and noisy labels that cause challenges for
state-of-art Transformers. This paper proposes Protoformer, a novel
self-learning framework for Transformers that can leverage problematic samples
for text classification. Protoformer features a selection mechanism for
embedding samples that allows us to efficiently extract and utilize anomalies
prototypes and difficult class prototypes. We demonstrated such capabilities on
datasets with diverse textual structures (e.g., Twitter, IMDB, ArXiv). We also
applied the framework to several models. The results indicate that Protoformer
can improve current Transformers in various empirical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective. (arXiv:2206.12759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12759">
<div class="article-summary-box-inner">
<span><p>Accented speech recognition and accent classification are relatively
under-explored research areas in speech technology. Recently, deep
learning-based methods and Transformer-based pretrained models have achieved
superb performances in both areas. However, most accent classification tasks
focused on classifying different kinds of English accents and little attention
was paid to geographically-proximate accent classification, especially under a
low-resource setting where forensic speech science tasks usually encounter. In
this paper, we explored three main accent modelling methods combined with two
different classifiers based on 105 speaker recordings retrieved from five urban
varieties in Northern England. Although speech representations generated from
pretrained models generally have better performances in downstream
classification, traditional methods like Mel Frequency Cepstral Coefficients
(MFCCs) and formant measurements are equipped with specific strengths. These
results suggest that in forensic phonetics scenario where data are relatively
scarce, a simple modelling method and classifier could be competitive with
state-of-the-art pretrained speech models as feature extractors, which could
enhance a sooner estimation for the accent information in practices. Besides,
our findings also cross-validated a new methodology in quantifying
sociophonetic changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Auxiliary Learning for Low-resource Spoken Language Understanding. (arXiv:2206.12774v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12774">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) treats automatic speech recognition (ASR)
and natural language understanding (NLU) as a unified task and usually suffers
from data scarcity. We exploit an ASR and NLU joint training method based on
meta auxiliary learning to improve the performance of low-resource SLU task by
only taking advantage of abundant manual transcriptions of speech data. One
obvious advantage of such method is that it provides a flexible framework to
implement a low-resource SLU training task without requiring access to any
further semantic annotations. In particular, a NLU model is taken as label
generation network to predict intent and slot tags from texts; a multi-task
network trains ASR task and SLU task synchronously from speech; and the
predictions of label generation network are delivered to the multi-task network
as semantic targets. The efficiency of the proposed algorithm is demonstrated
with experiments on the public CATSLU dataset, which produces more suitable ASR
hypotheses for the downstream NLU task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Guided Multi-View Multi-Domain Fake News Detection. (arXiv:2206.12808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12808">
<div class="article-summary-box-inner">
<span><p>The wide spread of fake news is increasingly threatening both individuals and
society. Great efforts have been made for automatic fake news detection on a
single domain (e.g., politics). However, correlations exist commonly across
multiple news domains, and thus it is promising to simultaneously detect fake
news of multiple domains. Based on our analysis, we pose two challenges in
multi-domain fake news detection: 1) domain shift, caused by the discrepancy
among domains in terms of words, emotions, styles, etc. 2) domain labeling
incompleteness, stemming from the real-world categorization that only outputs
one single domain label, regardless of topic diversity of a news piece. In this
paper, we propose a Memory-guided Multi-view Multi-domain Fake News Detection
Framework (M$^3$FEND) to address these two challenges. We model news pieces
from a multi-view perspective, including semantics, emotion, and style.
Specifically, we propose a Domain Memory Bank to enrich domain information
which could discover potential domain labels based on seen news pieces and
model domain characteristics. Then, with enriched domain information as input,
a Domain Adapter could adaptively aggregate discriminative information from
multiple views for news in various domains. Extensive offline experiments on
English and Chinese datasets demonstrate the effectiveness of M$^3$FEND, and
online tests verify its superiority in practice. Our code is available at
https://github.com/ICTMCG/M3FEND.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Comparison of Encoders for Attention based End to End Speech Recognition in Standalone and Rescoring Mode. (arXiv:2206.12829v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12829">
<div class="article-summary-box-inner">
<span><p>The streaming automatic speech recognition (ASR) models are more popular and
suitable for voice-based applications. However, non-streaming models provide
better performance as they look at the entire audio context. To leverage the
benefits of the non-streaming model in streaming applications like voice
search, it is commonly used in second pass re-scoring mode. The candidate
hypothesis generated using steaming models is re-scored using a non-streaming
model. In this work, we evaluate the non-streaming attention-based end-to-end
ASR models on the Flipkart voice search task in both standalone and re-scoring
modes. These models are based on Listen-Attend-Spell (LAS) encoder-decoder
architecture. We experiment with different encoder variations based on LSTM,
Transformer, and Conformer. We compare the latency requirements of these models
along with their performance. Overall we show that the Transformer model offers
acceptable WER with the lowest latency requirements. We report a relative WER
improvement of around 16% with the second pass LAS re-scoring with latency
overhead under 5ms. We also highlight the importance of CNN front-end with
Transformer architecture to achieve comparable word error rates (WER).
Moreover, we observe that in the second pass re-scoring mode all the encoders
provide similar benefits whereas the difference in performance is prominent in
standalone text generation mode.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Your Autoregressive Generative Model Can be Better If You Treat It as an Energy-Based One. (arXiv:2206.12840v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12840">
<div class="article-summary-box-inner">
<span><p>Autoregressive generative models are commonly used, especially for those
tasks involving sequential data. They have, however, been plagued by a slew of
inherent flaws due to the intrinsic characteristics of chain-style conditional
modeling (e.g., exposure bias or lack of long-range coherence), severely
limiting their ability to model distributions properly. In this paper, we
propose a unique method termed E-ARM for training autoregressive generative
models that takes advantage of a well-designed energy-based learning objective.
By leveraging the extra degree of freedom of the softmax operation, we are
allowed to make the autoregressive model itself be an energy-based model for
measuring the likelihood of input without introducing any extra parameters.
Furthermore, we show that E-ARM can be trained efficiently and is capable of
alleviating the exposure bias problem and increase temporal coherence for
autoregressive generative models. Extensive empirical results, covering
benchmarks like language modeling, neural machine translation, and image
generation, demonstrate the effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual embedding and model weighting by fusing domain knowledge on Biomedical Question Answering. (arXiv:2206.12866v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12866">
<div class="article-summary-box-inner">
<span><p>Biomedical Question Answering aims to obtain an answer to the given question
from the biomedical domain. Due to its high requirement of biomedical domain
knowledge, it is difficult for the model to learn domain knowledge from limited
training data. We propose a contextual embedding method that combines
open-domain QA model \aoa and \biobert model pre-trained on biomedical domain
data. We adopt unsupervised pre-training on large biomedical corpus and
supervised fine-tuning on biomedical question answering dataset. Additionally,
we adopt an MLP-based model weighting layer to automatically exploit the
advantages of two models to provide the correct answer. The public dataset
\biomrc constructed from PubMed corpus is used to evaluate our method.
Experimental results show that our model outperforms state-of-the-art system by
a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Dementia Detection in Spoken Language. (arXiv:2206.12879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12879">
<div class="article-summary-box-inner">
<span><p>Dementia is a growing problem as our society ages, and detection methods are
often invasive and expensive. Recent deep-learning techniques can offer a
faster diagnosis and have shown promising results. However, they require large
amounts of labelled data which is not easily available for the task of dementia
detection. One effective solution to sparse data problems is data augmentation,
though the exact methods need to be selected carefully. To date, there has been
no empirical study of data augmentation on Alzheimer's disease (AD) datasets
for NLP and speech processing. In this work, we investigate data augmentation
techniques for the task of AD detection and perform an empirical evaluation of
the different approaches on two kinds of models for both the text and audio
domains. We use a transformer-based model for both domains, and SVM and Random
Forest models for the text and audio domains, respectively. We generate
additional samples using traditional as well as deep learning based methods and
show that data augmentation improves performance for both the text- and
audio-based models and that such results are comparable to state-of-the-art
results on the popular ADReSS set, with carefully crafted architectures and
features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotated Speech Corpus for Low Resource Indian Languages: Awadhi, Bhojpuri, Braj and Magahi. (arXiv:2206.12931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12931">
<div class="article-summary-box-inner">
<span><p>In this paper we discuss an in-progress work on the development of a speech
corpus for four low-resource Indo-Aryan languages -- Awadhi, Bhojpuri, Braj and
Magahi using the field methods of linguistic data collection. The total size of
the corpus currently stands at approximately 18 hours (approx. 4-5 hours each
language) and it is transcribed and annotated with grammatical information such
as part-of-speech tags, morphological features and Universal dependency
relationships. We discuss our methodology for data collection in these
languages, most of which was done in the middle of the COVID-19 pandemic, with
one of the aims being to generate some additional income for low-income groups
speaking these languages. In the paper, we also discuss the results of the
baseline experiments for automatic speech recognition system in these
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Training Recipe for a Robust Conformer-based Hybrid Model. (arXiv:2206.12955v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12955">
<div class="article-summary-box-inner">
<span><p>Speaker adaptation is important to build robust automatic speech recognition
(ASR) systems. In this work, we investigate various methods for speaker
adaptive training (SAT) based on feature-space approaches for a conformer-based
acoustic model (AM) on the Switchboard 300h dataset. We propose a method,
called Weighted-Simple-Add, which adds weighted speaker information vectors to
the input of the multi-head self-attention module of the conformer AM. Using
this method for SAT, we achieve 3.5% and 4.5% relative improvement in terms of
WER on the CallHome part of Hub5'00 and Hub5'01 respectively. Moreover, we
build on top of our previous work where we proposed a novel and competitive
training recipe for a conformer-based hybrid AM. We extend and improve this
recipe where we achieve 11% relative improvement in terms of word-error-rate
(WER) on Switchboard 300h Hub5'00 dataset. We also make this recipe efficient
by reducing the total number of parameters by 34% relative.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable and High-Performance Hate and Offensive Speech Detection. (arXiv:2206.12983v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12983">
<div class="article-summary-box-inner">
<span><p>The spread of information through social media platforms can create
environments possibly hostile to vulnerable communities and silence certain
groups in society. To mitigate such instances, several models have been
developed to detect hate and offensive speech. Since detecting hate and
offensive speech in social media platforms could incorrectly exclude
individuals from social media platforms, which can reduce trust, there is a
need to create explainable and interpretable models. Thus, we build an
explainable and interpretable high performance model based on the XGBoost
algorithm, trained on Twitter data. For unbalanced Twitter data, XGboost
outperformed the LSTM, AutoGluon, and ULMFiT models on hate speech detection
with an F1 score of 0.75 compared to 0.38 and 0.37, and 0.38 respectively. When
we down-sampled the data to three separate classes of approximately 5000
tweets, XGBoost performed better than LSTM, AutoGluon, and ULMFiT; with F1
scores for hate speech detection of 0.79 vs 0.69, 0.77, and 0.66 respectively.
XGBoost also performed better than LSTM, AutoGluon, and ULMFiT in the
down-sampled version for offensive speech detection with F1 score of 0.83 vs
0.88, 0.82, and 0.79 respectively. We use Shapley Additive Explanations (SHAP)
on our XGBoost models' outputs to makes it explainable and interpretable
compared to LSTM, AutoGluon and ULMFiT that are black-box models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Intention Understanding in a Head-final Language: A Disambiguation Utilizing Intonation-dependency. (arXiv:1811.04231v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1811.04231">
<div class="article-summary-box-inner">
<span><p>For a large portion of real-life utterances, the intention cannot be solely
decided by either their semantic or syntactic characteristics. Although not all
the sociolinguistic and pragmatic information can be digitized, at least
phonetic features are indispensable in understanding the spoken language.
Especially in head-final languages such as Korean, sentence-final prosody has
great importance in identifying the speaker's intention. This paper suggests a
system which identifies the inherent intention of a spoken utterance given its
transcript, in some cases using auxiliary acoustic features. The main point
here is a separate distinction for cases where discrimination of intention
requires an acoustic cue. Thus, the proposed classification system decides
whether the given utterance is a fragment, statement, question, command, or a
rhetorical question/command, utilizing the intonation-dependency coming from
the head-finality. Based on an intuitive understanding of the Korean language
that is engaged in the data annotation, we construct a network which identifies
the intention of a speech, and validate its utility with the test sentences.
The system, if combined with up-to-date speech recognizers, is expected to be
flexibly inserted into various language understanding modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions Attributes. (arXiv:2007.06954v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.06954">
<div class="article-summary-box-inner">
<span><p>This paper describes a large global dataset on people's discourse and
responses to the COVID-19 pandemic over the Twitter platform. From 28 January
2020 to 1 June 2022, we collected and processed over 252 million Twitter posts
from more than 29 million unique users using four keywords: "corona", "wuhan",
"nCov" and "covid". Leveraging probabilistic topic modelling and pre-trained
machine learning-based emotion recognition algorithms, we labelled each tweet
with seventeen attributes, including a) ten binary attributes indicating the
tweet's relevance (1) or irrelevance (0) to the top ten detected topics, b)
five quantitative emotion attributes indicating the degree of intensity of the
valence or sentiment (from 0: extremely negative to 1: extremely positive) and
the degree of intensity of fear, anger, sadness and happiness emotions (from 0:
not at all to 1: extremely intense), and c) two categorical attributes
indicating the sentiment (very negative, negative, neutral or mixed, positive,
very positive) and the dominant emotion (fear, anger, sadness, happiness, no
specific emotion) the tweet is mainly expressing. We discuss the technical
validity and report the descriptive statistics of these attributes, their
temporal distribution, and geographic representation. The paper concludes with
a discussion of the dataset's usage in communication, psychology, public
health, economics, and epidemiology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing conversational agents' overconfidence through linguistic calibration. (arXiv:2012.14983v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14983">
<div class="article-summary-box-inner">
<span><p>While improving neural dialogue agents' factual accuracy is the object of
much research, another important aspect of communication, less studied in the
setting of neural dialogue, is transparency about ignorance. In this work, we
analyze to what extent state-of-the-art chit-chat models are linguistically
calibrated in the sense that their verbalized expression of doubt (or
confidence) matches the likelihood that the model's responses are factually
incorrect (or correct). We find that these models are poorly calibrated, yet we
show that likelihood of correctness can accurately be predicted. By
incorporating such metacognitive features into the training of a controllable
generation model, we obtain a dialogue agent with greatly improved linguistic
calibration. While improving neural dialogue agents' factual accuracy is the
object of much research, another important aspect of communication, less
studied in the setting of neural dialogue, is transparency about ignorance. In
this work, we analyze to what extent state-of-the-art chit-chat models are
linguistically calibrated in the sense that their verbalized expression of
doubt (or confidence) matches the likelihood that the model's responses are
factually incorrect (or correct). We find that these models are poorly
calibrated, yet we show that likelihood of correctness can accurately be
predicted. By incorporating such metacognitive features into the training of a
controllable generation model, we obtain a dialogue agent with greatly improved
linguistic calibration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06450">
<div class="article-summary-box-inner">
<span><p>We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on Image to Sequence
architecture, it can extract text present in an image and then sequence it
correctly without imposing any constraints regarding orientation, layout and
size of text and non-text. Further, it can also be trained to generate
auxiliary markup related to formatting, layout and content. We use character
level vocabulary, thereby enabling language and terminology of any subject. The
model achieves a new state-of-art in paragraph level recognition on the IAM
dataset. When evaluated on scans of real world handwritten free form test
answers - beset with curved and slanted lines, drawings, tables, math,
chemistry and other symbols - it performs better than all commercially
available HTR cloud APIs. It is deployed in production as part of a commercial
web application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Groundedness in Dialogue Systems: The BEGIN Benchmark. (arXiv:2105.00071v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00071">
<div class="article-summary-box-inner">
<span><p>Knowledge-grounded dialogue systems powered by large language models often
generate responses that, while fluent, are not attributable to a relevant
source of information. Progress towards models that do not exhibit this issue
requires evaluation metrics that can quantify its prevalence. To this end, we
introduce the Benchmark for Evaluation of Grounded INteraction (BEGIN),
comprised of 12k dialogue turns generated by neural dialogue systems trained on
three knowledge-grounded dialogue corpora. We collect human annotations
assessing the extent to which the models' responses can be attributed to the
given background information. We then use BEGIN to analyze eight evaluation
metrics. We find that these metrics rely on spurious correlations, do not
reliably distinguish attributable abstractive responses from unattributable
ones, and perform substantially worse when the knowledge source is longer. Our
findings underscore the need for more sophisticated and robust evaluation
metrics for knowledge-grounded dialogue. We make BEGIN publicly available at
https://github.com/google/BEGIN-dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07847">
<div class="article-summary-box-inner">
<span><p>While unbiased machine learning models are essential for many applications,
bias is a human-defined concept that can vary across tasks. Given only
input-label pairs, algorithms may lack sufficient information to distinguish
stable (causal) features from unstable (spurious) features. However, related
tasks often share similar biases -- an observation we may leverage to develop
stable classifiers in the transfer setting. In this work, we explicitly inform
the target classifier about unstable features in the source tasks.
Specifically, we derive a representation that encodes the unstable features by
contrasting different data environments in the source task. We achieve
robustness by clustering data of the target task according to this
representation and minimizing the worst-case risk across these clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task for both synthetically generated environments and real-world environments.
Our code is available at https://github.com/YujiaBao/Tofu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">gaBERT -- an Irish Language Model. (arXiv:2107.12930v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12930">
<div class="article-summary-box-inner">
<span><p>The BERT family of neural language models have become highly popular due to
their ability to provide sequences of text with rich context-sensitive token
encodings which are able to generalise well to many NLP tasks. We introduce
gaBERT, a monolingual BERT model for the Irish language. We compare our gaBERT
model to multilingual BERT and the monolingual Irish WikiBERT, and we show that
gaBERT provides better representations for a downstream parsing task. We also
show how different filtering criteria, vocabulary size and the choice of
subword tokenisation model affect downstream performance. We compare the
results of fine-tuning a gaBERT model with an mBERT model for the task of
identifying verbal multiword expressions, and show that the fine-tuned gaBERT
model also performs better at this task. We release gaBERT and related code to
the community
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution. (arXiv:2108.13530v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13530">
<div class="article-summary-box-inner">
<span><p>We consider the task of document-level entity linking (EL), where it is
important to make consistent decisions for entity mentions over the full
document jointly. We aim to leverage explicit "connections" among mentions
within the document itself: we propose to join the EL task with that of
coreference resolution (coref). This is complementary to related works that
exploit either (i) implicit document information (e.g., latent relations among
entity mentions, or general language models) or (ii) connections between the
candidate links (e.g, as inferred from the external knowledge base).
Specifically, we cluster mentions that are linked via coreference, and enforce
a single EL for all of the clustered mentions together. The latter constraint
has the added benefit of increased coverage by joining EL candidate lists for
the thus clustered mentions. We formulate the coref+EL problem as a structured
prediction task over directed trees and use a globally normalized model to
solve it. Experimental results on two datasets show a boost of up to +5%
F1-score on both coref and EL tasks, compared to their standalone counterparts.
For a subset of hard cases, with individual mentions lacking the correct EL in
their candidate entity list, we obtain a +50% increase in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese. (arXiv:2109.09701v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09701">
<div class="article-summary-box-inner">
<span><p>We present BARTpho with two versions, BARTpho-syllable and BARTpho-word,
which are the first public large-scale monolingual sequence-to-sequence models
pre-trained for Vietnamese. BARTpho uses the "large" architecture and the
pre-training scheme of the sequence-to-sequence denoising autoencoder BART,
thus it is especially suitable for generative NLP tasks. We conduct experiments
to compare our BARTpho with its competitor mBART on a downstream task of
Vietnamese text summarization and show that: in both automatic and human
evaluations, BARTpho outperforms the strong baseline mBART and improves the
state-of-the-art. We further evaluate and compare BARTpho and mBART on the
Vietnamese capitalization and punctuation restoration tasks and also find that
BARTpho is more effective than mBART on these two tasks. We publicly release
BARTpho to facilitate future research and applications of generative Vietnamese
NLP tasks. Our BARTpho models are available at
https://github.com/VinAIResearch/BARTpho
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation Approaches in Natural Language Processing: A Survey. (arXiv:2110.01852v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01852">
<div class="article-summary-box-inner">
<span><p>As an effective strategy, data augmentation (DA) alleviates data scarcity
scenarios where deep learning techniques may fail. It is widely applied in
computer vision then introduced to natural language processing and achieves
improvements in many tasks. One of the main focuses of the DA methods is to
improve the diversity of training data, thereby helping the model to better
generalize to unseen testing data. In this survey, we frame DA methods into
three categories based on the diversity of augmented data, including
paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods
in detail according to the above categories. Further, we also introduce their
applications in NLP tasks as well as the challenges. Some helpful resources are
provided in the appendix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTC Variations Through New WFST Topologies. (arXiv:2110.03098v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03098">
<div class="article-summary-box-inner">
<span><p>This paper presents novel Weighted Finite-State Transducer (WFST) topologies
to implement Connectionist Temporal Classification (CTC)-like algorithms for
automatic speech recognition. Three new CTC variants are proposed: (1) the
"compact-CTC", in which direct transitions between units are replaced with
&lt;epsilon&gt; back-off transitions; (2) the "minimal-CTC", that only adds &lt;blank&gt;
self-loops when used in WFST-composition; and (3) the "selfless-CTC" variants,
which disallows self-loop for non-blank units. Compact-CTC allows for 1.5 times
smaller WFST decoding graphs and reduces memory consumption by two times when
training CTC models with the LF-MMI objective without hurting the recognition
accuracy. Minimal-CTC reduces graph size and memory consumption by two and four
times for the cost of a small accuracy drop. Using selfless-CTC can improve the
accuracy for wide context window models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition. (arXiv:2110.05354v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05354">
<div class="article-summary-box-inner">
<span><p>Text-only adaptation of an end-to-end (E2E) model remains a challenging task
for automatic speech recognition (ASR). Language model (LM) fusion-based
approaches require an additional external LM during inference, significantly
increasing the computation cost. To overcome this, we propose an internal LM
adaptation (ILMA) of the E2E model using text-only data. Trained with
audio-transcript pairs, an E2E model implicitly learns an internal LM that
characterizes the token sequence probability which is approximated by the E2E
model output after zeroing out the encoder contribution. During ILMA, we
fine-tune the internal LM, i.e., the E2E components excluding the encoder, to
minimize a cross-entropy loss. To make ILMA effective, it is essential to train
the E2E model with an internal LM loss besides the standard E2E loss.
Furthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler
divergence between the output distributions of the adapted and unadapted
internal LMs. ILMA is the most effective when we update only the last linear
layer of the joint network. ILMA enables a fast text-only adaptation of the E2E
model without increasing the run-time computational cost. Experimented with
30K-hour trained transformer transducer models, ILMA achieves up to 34.9%
relative word error rate reduction from the unadapted baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ITA: Image-Text Alignments for Multi-Modal Named Entity Recognition. (arXiv:2112.06482v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06482">
<div class="article-summary-box-inner">
<span><p>Recently, Multi-modal Named Entity Recognition (MNER) has attracted a lot of
attention. Most of the work utilizes image information through region-level
visual representations obtained from a pretrained object detector and relies on
an attention mechanism to model the interactions between image and text
representations. However, it is difficult to model such interactions as image
and text representations are trained separately on the data of their respective
modality and are not aligned in the same space. As text representations take
the most important role in MNER, in this paper, we propose {\bf I}mage-{\bf
t}ext {\bf A}lignments (ITA) to align image features into the textual space, so
that the attention mechanism in transformer-based pretrained textual embeddings
can be better utilized. ITA first aligns the image into regional object tags,
image-level captions and optical characters as visual contexts, concatenates
them with the input texts as a new cross-modal input, and then feeds it into a
pretrained textual embedding model. This makes it easier for the attention
module of a pretrained textual embedding model to model the interaction between
the two modalities since they are both represented in the textual space. ITA
further aligns the output distributions predicted from the cross-modal input
and textual input views so that the MNER model can be more practical in dealing
with text-only inputs and robust to noises from images. In our experiments, we
show that ITA models can achieve state-of-the-art accuracy on multi-modal Named
Entity Recognition datasets, even without image information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black-Box Tuning for Language-Model-as-a-Service. (arXiv:2201.03514v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03514">
<div class="article-summary-box-inner">
<span><p>Extremely large pre-trained language models (PTMs) such as GPT-3 are usually
released as a service. It allows users to design task-specific prompts to query
the PTMs through some black-box APIs. In such a scenario, which we call
Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually
unavailable. Can we optimize the task prompts by only accessing the model
inference APIs? This paper proposes the black-box tuning framework to optimize
the continuous prompt prepended to the input text via derivative-free
optimization. Instead of optimizing in the original high-dimensional prompt
space, which is intractable for traditional derivative-free optimization, we
perform optimization in a randomly generated subspace due to the low intrinsic
dimensionality of large PTMs. The experimental results show that the black-box
tuning with RoBERTa on a few labeled samples not only significantly outperforms
manual prompt and GPT-3's in-context learning, but also surpasses the
gradient-based counterparts, i.e., prompt tuning and full model tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVSS Corpus and Massively Multilingual Speech-to-Speech Translation. (arXiv:2201.03713v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03713">
<div class="article-summary-box-inner">
<span><p>We introduce CVSS, a massively multilingual-to-English speech-to-speech
translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21
languages into English. CVSS is derived from the Common Voice speech corpus and
the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the
translation text from CoVoST 2 into speech using state-of-the-art TTS systems.
Two versions of translation speeches are provided: 1) CVSS-C: All the
translation speeches are in a single high-quality canonical voice; 2) CVSS-T:
The translation speeches are in voices transferred from the corresponding
source speeches. In addition, CVSS provides normalized translation text which
matches the pronunciation in the translation speech. On each version of CVSS,
we built baseline multilingual direct S2ST models and cascade S2ST models,
verifying the effectiveness of the corpus. To build strong cascade S2ST
baselines, we trained an ST model on CoVoST 2, which outperforms the previous
state-of-the-art trained on the corpus without extra data by 5.8 BLEU.
Nevertheless, the performance of the direct S2ST models approaches the strong
cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU
difference on ASR transcribed translation when initialized from matching ST
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation. (arXiv:2201.05955v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05955">
<div class="article-summary-box-inner">
<span><p>A recurring challenge of crowdsourcing NLP datasets at scale is that human
writers often rely on repetitive patterns when crafting examples, leading to a
lack of linguistic diversity. We introduce a novel approach for dataset
creation based on worker and AI collaboration, which brings together the
generative strength of language models and the evaluative strength of humans.
Starting with an existing dataset, MultiNLI for natural language inference
(NLI), our approach uses dataset cartography to automatically identify examples
that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose
new examples with similar patterns. Machine generated examples are then
automatically filtered, and finally revised and labeled by human crowdworkers.
The resulting dataset, WANLI, consists of 107,885 NLI examples and presents
unique empirical strengths over existing NLI datasets. Remarkably, training a
model on WANLI instead of MultiNLI (which is $4$ times larger) improves
performance on seven out-of-domain test sets we consider, including by 11% on
HANS and 9% on Adversarial NLI. Moreover, combining MultiNLI with WANLI is more
effective than combining it with other NLI augmentation sets. Our results
demonstrate the potential of natural language generation techniques to curate
NLP datasets of enhanced quality and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Quality in Linear Time. (arXiv:2202.10447v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10447">
<div class="article-summary-box-inner">
<span><p>We revisit the design choices in Transformers, and propose methods to address
their weaknesses in handling long sequences. First, we propose a simple layer
named gated attention unit, which allows the use of a weaker single-head
attention with minimal quality loss. We then propose a linear approximation
method complementary to this new layer, which is accelerator-friendly and
highly competitive in quality. The resulting model, named FLASH, matches the
perplexity of improved Transformers over both short (512) and long (8K) context
lengths, achieving training speedups of up to 4.9$\times$ on Wiki-40B and
12.1$\times$ on PG-19 for auto-regressive language modeling, and 4.8$\times$ on
C4 for masked language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition. (arXiv:2203.00545v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00545">
<div class="article-summary-box-inner">
<span><p>The MultiCoNER shared task aims at detecting semantically ambiguous and
complex named entities in short and low-context settings for multiple
languages. The lack of contexts makes the recognition of ambiguous named
entities challenging. To alleviate this issue, our team DAMO-NLP proposes a
knowledge-based system, where we build a multilingual knowledge base based on
Wikipedia to provide related context information to the named entity
recognition (NER) model. Given an input sentence, our system effectively
retrieves related contexts from the knowledge base. The original input
sentences are then augmented with such context information, allowing
significantly better contextualized token representations to be captured. Our
system wins 10 out of 13 tracks in the MultiCoNER shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses. (arXiv:2203.10750v5 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10750">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop a new multi-singer Chinese neural singing voice
synthesis (SVS) system named WeSinger. To improve the accuracy and naturalness
of synthesized singing voice, we design several specifical modules and
techniques: 1) A deep bi-directional LSTM-based duration model with multi-scale
rhythm loss and post-processing step; 2) A Transformer-alike acoustic model
with progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet
neural vocoder to produce high-quality singing waveforms; 4) A novel data
augmentation method with multi-singer pre-training for stronger robustness and
naturalness. To our knowledge, WeSinger is the first SVS system to adopt 24 kHz
LPCNet and multi-singer pre-training simultaneously. Both quantitative and
qualitative evaluation results demonstrate the effectiveness of WeSinger in
terms of accuracy and naturalness, and WeSinger achieves state-of-the-art
performance on the recent public Chinese singing corpus
Opencpop\footnote{https://wenet.org.cn/opencpop/}. Some synthesized singing
samples are available online\footnote{https://zzw922cn.github.io/wesinger/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning. (arXiv:2203.13628v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13628">
<div class="article-summary-box-inner">
<span><p>Inspired by the recent progress in self-supervised learning for computer
vision, in this paper we introduce DeLoRes, a new general-purpose audio
representation learning approach. Our main objective is to make our network
learn representations in a resource-constrained setting (both data and
compute), that can generalize well across a diverse set of downstream tasks.
Inspired from the Barlow Twins objective function, we propose to learn
embeddings that are invariant to distortions of an input audio sample, while
making sure that they contain non-redundant information about the sample. To
achieve this, we measure the cross-correlation matrix between the outputs of
two identical networks fed with distorted versions of an audio segment sampled
from an audio file and make it as close to the identity matrix as possible. We
use a combination of a small subset of the large-scale AudioSet dataset and
FSD50K for self-supervised learning and are able to learn with less than half
the parameters compared to state-of-the-art algorithms. For evaluation, we
transfer these learned representations to 9 downstream classification tasks,
including speech, music, and animal sounds, and show competitive results under
different evaluation setups. In addition to being simple and intuitive, our
pre-training algorithm is amenable to compute through its inherent nature of
construction and does not require careful implementation details to avoid
trivial or degenerate solutions. Furthermore, we conduct ablation studies on
our results and make all our code and pre-trained models publicly available
https://github.com/Speech-Lab-IITM/DeLoRes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15081">
<div class="article-summary-box-inner">
<span><p>We present a method for visually-grounded spoken term discovery. After
training either a HuBERT or wav2vec2.0 model to associate spoken captions with
natural images, we show that powerful word segmentation and clustering
capability emerges within the model's self-attention heads. Our experiments
reveal that this ability is not present to nearly the same extent in the base
HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a
crucial component of the word discovery capability we observe. We also evaluate
our method on the Buckeye word segmentation and ZeroSpeech spoken term
discovery tasks, where we outperform all currently published methods on several
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Contrast Stretching on Target Feature for Speech Enhancement. (arXiv:2203.17152v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17152">
<div class="article-summary-box-inner">
<span><p>Speech enhancement (SE) performance has improved considerably owing to the
use of deep learning models as a base function. Herein, we propose a perceptual
contrast stretching (PCS) approach to further improve SE performance. The PCS
is derived based on the critical band importance function and is applied to
modify the targets of the SE model. Specifically, the contrast of target
features is stretched based on perceptual importance, thereby improving the
overall SE performance. Compared with post-processing-based implementations,
incorporating PCS into the training phase preserves performance and reduces
online computation. Notably, PCS can be combined with different SE model
architectures and training criteria. Furthermore, PCS does not affect the
causality or convergence of SE model training. Experimental results on the
VoiceBank-DEMAND dataset show that the proposed method can achieve
state-of-the-art performance on both causal (PESQ score = 3.07) and noncausal
(PESQ score = 3.35) SE tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00763">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline
setting or through human evaluation. The evaluation is often limited to
single-turn or very time-intensive. As an alternative, user simulators that
mimic user behavior allow us to consider a broad set of user goals to generate
human-like conversations for simulated evaluation. Employing existing user
simulators to evaluate TDSs is challenging as user simulators are primarily
designed to optimize dialogue policies for TDSs and have limited evaluation
capabilities. Moreover, the evaluation of user simulators is an open challenge.
In this work, we proposes a metaphorical user simulator for end-to-end TDS
evaluation, where we define a simulator to be metaphorical if it simulates
user's analogical thinking in interactions with systems. We also propose a
tester-based evaluation framework to generate variants, i.e., dialogue systems
with different capabilities. Our user simulator constructs a metaphorical user
model that assists the simulator in reasoning by referring to prior knowledge
when encountering new items. We estimate the quality of simulators by checking
the simulated interactions between simulators and variants. Our experiments are
conducted using three TDS datasets. The metaphorical user simulator
demonstrates better consistency with manual evaluation than an agenda-based
simulator and a seq2seq model on three datasets; our tester framework
demonstrates efficiency, and better generalization and scalability because it
can be adapted for dialogues in multiple domains and for multiple tasks, such
as conversational recommendation and e-commerce dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment. (arXiv:2204.04016v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04016">
<div class="article-summary-box-inner">
<span><p>Speech intelligibility assessment plays an important role in the therapy of
patients suffering from pathological speech disorders. Automatic and objective
measures are desirable to assist therapists in their traditionally subjective
and labor-intensive assessments. In this work, we investigate a novel approach
for obtaining such a measure using the divergence in disentangled latent speech
representations of a parallel utterance pair, obtained from a healthy reference
and a pathological speaker. Experiments on an English database of Cerebral
Palsy patients, using all available utterances per speaker, show high and
significant correlation values (R = -0.9) with subjective intelligibility
measures, while having only minimal deviation (+-0.01) across four different
reference speaker pairs. We also demonstrate the robustness of the proposed
method (R = -0.89 deviating +-0.02 over 1000 iterations) by considering a
significantly smaller amount of utterances per speaker. Our results are among
the first to show that disentangled speech representations can be used for
automatic pathological speech intelligibility assessment, resulting in a
reference speaker pair invariant method, applicable in scenarios with only few
utterances available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Language Model Size in Cross-Device Federated Learning. (arXiv:2204.09715v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09715">
<div class="article-summary-box-inner">
<span><p>Most studies in cross-device federated learning focus on small models, due to
the server-client communication and on-device computation bottlenecks. In this
work, we leverage various techniques for mitigating these bottlenecks to train
larger language models in cross-device federated learning. With systematic
applications of partial model training, quantization, efficient transfer
learning, and communication-efficient optimizers, we are able to train a $21$M
parameter Transformer and $20.2$M parameter Conformer that achieve the same or
better perplexity as that of a similarly sized LSTM with $\sim10\times$ smaller
client-to-server communication cost and $11\%$ lower perplexity than smaller
LSTMs commonly studied in literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Training of Neural Transducer for Speech Recognition. (arXiv:2204.10586v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10586">
<div class="article-summary-box-inner">
<span><p>As one of the most popular sequence-to-sequence modeling approaches for
speech recognition, the RNN-Transducer has achieved evolving performance with
more and more sophisticated neural network models of growing size and
increasing training epochs. While strong computation resources seem to be the
prerequisite of training superior models, we try to overcome it by carefully
designing a more efficient training pipeline. In this work, we propose an
efficient 3-stage progressive training pipeline to build highly-performing
neural transducer models from scratch with very limited computation resources
in a reasonable short time period. The effectiveness of each stage is
experimentally verified on both Librispeech and Switchboard corpora. The
proposed pipeline is able to train transducer models approaching
state-of-the-art performance with a single GPU in just 2-3 weeks. Our best
conformer transducer achieves 4.1% WER on Librispeech test-other with only 35
epochs of training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?. (arXiv:2204.12765v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12765">
<div class="article-summary-box-inner">
<span><p>Recently, self-supervised learning (SSL) has demonstrated strong performance
in speaker recognition, even if the pre-training objective is designed for
speech recognition. In this paper, we study which factor leads to the success
of self-supervised learning on speaker-related tasks, e.g. speaker verification
(SV), through a series of carefully designed experiments. Our empirical results
on the Voxceleb-1 dataset suggest that the benefit of SSL to SV task is from a
combination of mask speech prediction loss, data scale, and model size, while
the SSL quantizer has a minor impact. We further employ the integrated
gradients attribution method and loss landscape visualization to understand the
effectiveness of self-supervised learning for speaker recognition performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Parallelize in a Shared-Memory Environment with Transformers. (arXiv:2204.12835v3 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12835">
<div class="article-summary-box-inner">
<span><p>In past years, the world has switched to many-core and multi-core shared
memory architectures. As a result, there is a growing need to utilize these
architectures by introducing shared memory parallelization schemes to software
applications. OpenMP is the most comprehensive API that implements such
schemes, characterized by a readable interface. Nevertheless, introducing
OpenMP into code is challenging due to pervasive pitfalls in management of
parallel shared memory. To facilitate the performance of this task, many
source-to-source (S2S) compilers have been created over the years, tasked with
inserting OpenMP directives into code automatically. In addition to having
limited robustness to their input format, these compilers still do not achieve
satisfactory coverage and precision in locating parallelizable code and
generating appropriate directives. In this work, we propose leveraging recent
advances in ML techniques, specifically in natural language processing (NLP),
to replace S2S compilers altogether. We create a database (corpus), Open-OMP,
specifically for this goal. Open-OMP contains over 28,000 code snippets, half
of which contain OpenMP directives while the other half do not need
parallelization at all with high probability. We use the corpus to train
systems to automatically classify code segments in need of parallelization, as
well as suggest individual OpenMP clauses. We train several transformer models,
named PragFormer, for these tasks, and show that they outperform
statistically-trained baselines and automatic S2S parallelization compilers in
both classifying the overall need for an OpenMP directive and the introduction
of private and reduction clauses.
</p>
<p>Our source code and database are available at:
https://github.com/Scientific-Computing-Lab-NRCN/PragFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding. (arXiv:2205.00693v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00693">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) is an essential task for machines to
understand human speech for better interactions. However, errors from the
automatic speech recognizer (ASR) usually hurt the understanding performance.
In reality, ASR systems may not be easy to adjust for the target scenarios.
Therefore, this paper focuses on learning utterance representations that are
robust to ASR errors using a contrastive objective, and further strengthens the
generalization ability by combining supervised contrastive learning and
self-distillation in model fine-tuning. Experiments on three benchmark datasets
demonstrate the effectiveness of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Word Embeddings in Hyperbolic Space. (arXiv:2205.01907v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01907">
<div class="article-summary-box-inner">
<span><p>Cross-lingual word embeddings can be applied to several natural language
processing applications across multiple languages. Unlike prior works that use
word embeddings based on the Euclidean space, this short paper presents a
simple and effective cross-lingual Word2Vec model that adapts to the Poincar\'e
ball model of hyperbolic space to learn unsupervised cross-lingual word
representations from a German-English parallel corpus. It has been shown that
hyperbolic embeddings can capture and preserve hierarchical relationships. We
evaluate the model on both hypernymy and analogy tasks. The proposed model
achieves comparable performance with the vanilla Word2Vec model on the
cross-lingual analogy task, the hypernymy task shows that the cross-lingual
Poincar\'e Word2Vec model can capture latent hierarchical structure from free
text across languages, which are absent from the Euclidean-based Word2Vec
representations. Our results show that by preserving the latent hierarchical
information, hyperbolic spaces can offer better representations for
cross-lingual embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifiers are Better Experts for Controllable Text Generation. (arXiv:2205.07276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07276">
<div class="article-summary-box-inner">
<span><p>This paper proposes a simple method for controllable text generation based on
weighting logits with a free-form classifier, namely CAIF sampling. Using an
arbitrary text classifier, we adjust a small part of a language model's logits
and guide text generation towards or away from classifier prediction. We
experimented with toxicity avoidance and sentiment control tasks and showed
that the proposed method significantly outperforms recent PPLM, GeDi, and
DExperts on PPL and task accuracy metrics based on the external classifier of
generated texts. In addition, compared to other approaches, it is easier to
implement and tune and has significantly fewer restrictions and requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Connectivity Reveals Generalization Strategies. (arXiv:2205.12411v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12411">
<div class="article-summary-box-inner">
<span><p>It is widely accepted in the mode connectivity literature that when two
neural networks are trained similarly on the same data, they are connected by a
path through parameter space over which test set accuracy is maintained. Under
some circumstances, including transfer learning from pretrained models, these
paths are presumed to be linear. In contrast to existing results, we find that
among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of
finetuned models have large barriers of increasing loss on the linear paths
between them. On each task, we find distinct clusters of models which are
linearly connected on the test loss surface, but are disconnected from models
outside the cluster -- models that occupy separate basins on the surface. By
measuring performance on specially-crafted diagnostic datasets, we find that
these clusters correspond to different generalization strategies: one cluster
behaves like a bag of words model under domain shift, while another cluster
uses syntactic heuristics. Our work demonstrates how the geometry of the loss
surface can guide models towards different heuristic functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rites de Passage: Elucidating Displacement to Emplacement of Refugees on Twitter. (arXiv:2206.03248v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03248">
<div class="article-summary-box-inner">
<span><p>Social media deliberations allow to explore refugee-related is-sues. AI-based
studies have investigated refugee issues mostly around a specific event and
considered unimodal approaches. Contrarily, we have employed a multimodal
architecture for probing the refugee journeys from their home to host nations.
We draw insights from Arnold van Gennep's anthropological work 'Les Rites de
Passage', which systematically analyzed an individual's transition from one
group or society to another. Based on Gennep's
separation-transition-incorporation framework, we have identified four phases
of refugee journeys: Arrival of Refugees, Temporal stay at Asylums,
Rehabilitation, and Integration of Refugees into the host nation. We collected
0.23 million multimodal tweets from April 2020 to March 2021 for testing this
proposed frame-work. We find that a combination of transformer-based language
models and state-of-the-art image recognition models, such as fusion of
BERT+LSTM and InceptionV4, can out-perform unimodal models. Subsequently, to
test the practical implication of our proposed model in real-time, we have
considered 0.01 million multimodal tweets related to the 2022 Ukrainian refugee
crisis. An F1-score of 71.88 % for this 2022 crisis confirms the
generalizability of our proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Zero Oracle Word Error Rate on the Switchboard Benchmark. (arXiv:2206.06192v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06192">
<div class="article-summary-box-inner">
<span><p>The "Switchboard benchmark" is a very well-known test set in automatic speech
recognition (ASR) research, establishing record-setting performance for systems
that claim human-level transcription accuracy. This work highlights
lesser-known practical considerations of this evaluation, demonstrating major
improvements in word error rate (WER) by correcting the reference
transcriptions and deviating from the official scoring methodology. In this
more detailed and reproducible scheme, even commercial ASR systems can score
below 5% WER and the established record for a research system is lowered to
2.3%. An alternative metric of transcript precision is proposed, which does not
penalize deletions and appears to be more discriminating for human vs. machine
performance. While commercial ASR systems are still below this threshold, a
research system is shown to clearly surpass the accuracy of commercial human
speech recognition. This work also explores using standardized scoring tools to
compute oracle WER by selecting the best among a list of alternatives. A phrase
alternatives representation is compared to utterance-level N-best lists and
word-level data structures; using dense lattices and adding out-of-vocabulary
words, this achieves an oracle WER of 0.18%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Narratives through Dimensions of Analogy. (arXiv:2206.07167v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07167">
<div class="article-summary-box-inner">
<span><p>Analogical reasoning is a powerful qualitative reasoning tool that enables
humans to connect two situations, and to generalize their knowledge from
familiar to novel situations. Cognitive Science research provides valuable
insights into the richness and complexity of analogical reasoning, together
with implementations of expressive analogical reasoners with limited
scalability. Modern scalable AI techniques with the potential to reason by
analogy have been only applied to the special case of proportional analogy, and
not to understanding higher-order analogies. In this paper, we aim to bridge
the gap by: 1) formalizing six dimensions of analogy based on mature insights
from Cognitive Science research, 2) annotating a corpus of fables with each of
these dimensions, and 3) defining four tasks with increasing complexity that
enable scalable evaluation of AI techniques. Experiments with language models
and neuro-symbolic AI reasoners on these tasks reveal that state-of-the-art
methods can be applied to reason by analogy with a limited success, motivating
the need for further research towards comprehensive and scalable analogical
reasoning by AI. We make all our code and data available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What can Speech and Language Tell us About the Working Alliance in Psychotherapy. (arXiv:2206.08835v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08835">
<div class="article-summary-box-inner">
<span><p>We are interested in the problem of conversational analysis and its
application to the health domain. Cognitive Behavioral Therapy is a structured
approach in psychotherapy, allowing the therapist to help the patient to
identify and modify the malicious thoughts, behavior, or actions. This
cooperative effort can be evaluated using the Working Alliance Inventory
Observer-rated Shortened - a 12 items inventory covering task, goal, and
relationship - which has a relevant influence on therapeutic outcomes. In this
work, we investigate the relation between this alliance inventory and the
spoken conversations (sessions) between the patient and the psychotherapist. We
have delivered eight weeks of e-therapy, collected their audio and video call
sessions, and manually transcribed them. The spoken conversations have been
annotated and evaluated with WAI ratings by professional therapists. We have
investigated speech and language features and their association with WAI items.
The feature types include turn dynamics, lexical entrainment, and
conversational descriptors extracted from the speech and language signals. Our
findings provide strong evidence that a subset of these features are strong
indicators of working alliance. To the best of our knowledge, this is the first
and a novel study to exploit speech and language for characterising working
alliance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting a French Dictionary from the Beginning of the 20th Century to Wikidata. (arXiv:2206.11022v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11022">
<div class="article-summary-box-inner">
<span><p>The \textit{Petit Larousse illustr\'e} is a French dictionary first published
in 1905. Its division in two main parts on language and on history and
geography corresponds to a major milestone in French lexicography as well as a
repository of general knowledge from this period. Although the value of many
entries from 1905 remains intact, some descriptions now have a dimension that
is more historical than contemporary. They are nonetheless significant to
analyze and understand cultural representations from this time. A comparison
with more recent information or a verification of these entries would require a
tedious manual work. In this paper, we describe a new lexical resource, where
we connected all the dictionary entries of the history and geography part to
current data sources. For this, we linked each of these entries to a wikidata
identifier. Using the wikidata links, we can automate more easily the
identification, comparison, and verification of historically-situated
representations. We give a few examples on how to process wikidata identifiers
and we carried out a small analysis of the entities described in the dictionary
to outline possible applications. The resource, i.e. the annotation of 20,245
dictionary entries with wikidata links, is available from GitHub
url{https://github.com/pnugues/petit_larousse_1905/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of Programming Exercises and Code Explanations using Large Language Models. (arXiv:2206.11861v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11861">
<div class="article-summary-box-inner">
<span><p>This article explores the natural language generation capabilities of large
language models with application to the production of two types of learning
resources common in programming courses. Using OpenAI Codex as the large
language model, we create programming exercises (including sample solutions and
test cases) and code explanations, assessing these qualitatively and
quantitatively. Our results suggest that the majority of the automatically
generated content is both novel and sensible, and in some cases ready to use as
is. When creating exercises we find that it is remarkably easy to influence
both the programming concepts and the contextual themes they contain, simply by
supplying keywords as input to the model. Our analysis suggests that there is
significant value in massive generative machine learning models as a tool for
instructors, although there remains a need for some oversight to ensure the
quality of the generated content before it is delivered to students. We further
discuss the implications of OpenAI Codex and similar tools for introductory
programming education and highlight future research streams that have the
potential to improve the quality of the educational experience for both
teachers and students alike.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Independent evaluation of state-of-the-art deep networks for mammography. (arXiv:2206.12407v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12407">
<div class="article-summary-box-inner">
<span><p>Deep neural models have shown remarkable performance in image recognition
tasks, whenever large datasets of labeled images are available. The largest
datasets in radiology are available for screening mammography. Recent reports,
including in high impact journals, document performance of deep models at or
above that of trained radiologists. What is not yet known is whether
performance of these trained models is robust and replicates across datasets.
Here we evaluate performance of five published state-of-the-art models on four
publicly available mammography datasets. The limited size of public datasets
precludes retraining the model and so we are limited to evaluate those models
that have been made available with pre-trained parameters. Where test data was
available, we replicated published results. However, the trained models
performed poorly on out-of-sample data, except when based on all four standard
views of a mammographic exam. We conclude that future progress will depend on a
concerted effort to make more diverse and larger mammography datasets publicly
available. Meanwhile, results that are not accompanied by a release of trained
models for independent validation should be judged cautiously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep embedded clustering algorithm for clustering PACS repositories. (arXiv:2206.12417v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12417">
<div class="article-summary-box-inner">
<span><p>Creating large datasets of medical radiology images from several sources can
be challenging because of the differences in the acquisition and storage
standards. One possible way of controlling and/or assessing the image selection
process is through medical image clustering. This, however, requires an
efficient method for learning latent image representations. In this paper, we
tackle the problem of fully-unsupervised clustering of medical images using
pixel data only. We test the performance of several contemporary approaches,
built on top of a convolutional autoencoder (CAE) - convolutional deep embedded
clustering (CDEC) and convolutional improved deep embedded clustering (CIDEC) -
and three approaches based on preset feature extraction - histogram of oriented
gradients (HOG), local binary pattern (LBP) and principal component analysis
(PCA). CDEC and CIDEC are end-to-end clustering solutions, involving
simultaneous learning of latent representations and clustering assignments,
whereas the remaining approaches rely on k-means clustering from fixed
embeddings. We train the models on 30,000 images, and test them using a
separate test set consisting of 8,000 images. We sampled the data from the PACS
repository archive of the Clinical Hospital Centre Rijeka. For evaluation, we
use silhouette score, homogeneity score and normalised mutual information (NMI)
on two target parameters, closely associated with commonly occurring DICOM tags
- Modality and anatomical region (adjusted BodyPartExamined tag). CIDEC attains
an NMI score of 0.473 with respect to anatomical region, and CDEC attains an
NMI score of 0.645 with respect to the tag Modality - both outperforming other
commonly used feature descriptors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ev-NeRF: Event Based Neural Radiance Field. (arXiv:2206.12455v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12455">
<div class="article-summary-box-inner">
<span><p>We present Ev-NeRF, a Neural Radiance Field derived from event data. While
event cameras can measure subtle brightness changes in high frame rates, the
measurements in low lighting or extreme motion suffer from significant domain
discrepancy with complex noise. As a result, the performance of event-based
vision tasks does not transfer to challenging environments, where the event
cameras are expected to thrive over normal cameras. We find that the multi-view
consistency of NeRF provides a powerful self-supervision signal for eliminating
the spurious measurements and extracting the consistent underlying structure
despite highly noisy input. Instead of posed images of the original NeRF, the
input to Ev-NeRF is the event measurements accompanied by the movements of the
sensors. Using the loss function that reflects the measurement model of the
sensor, Ev-NeRF creates an integrated neural volume that summarizes the
unstructured and sparse data points captured for about 2-4 seconds. The
generated neural volume can also produce intensity images from novel views with
reasonable depth estimates, which can serve as a high-quality input to various
vision-based tasks. Our results show that Ev-NeRF achieves competitive
performance for intensity image reconstruction under extreme noise conditions
and high-dynamic-range imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag of Tricks for Long-Tail Visual Recognition of Animal Species in Camera Trap Images. (arXiv:2206.12458v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12458">
<div class="article-summary-box-inner">
<span><p>Camera traps are a strategy for monitoring wildlife that collects a large
number of pictures. The number of images collected from each species usually
follows a long-tail distribution, i.e., a few classes have a large number of
instances while a lot of species have just a small percentage. Although in most
cases these rare species are the classes of interest to ecologists, they are
often neglected when using deep learning models because these models require a
large number of images for the training. In this work, we systematically
evaluate recently proposed techniques - namely, square-root re-sampling,
class-balanced focal loss, and balanced group softmax - to address the
long-tail visual recognition of animal species in camera trap images. To
achieve a more general conclusion, we evaluated the selected methods on four
families of computer vision models (ResNet, MobileNetV3, EfficientNetV2, and
Swin Transformer) and four camera trap datasets with different characteristics.
Initially, we prepared a robust baseline with the most recent training tricks
and then we applied the methods for improving long-tail recognition. Our
experiments show that the Swin transformer can reach high performance for rare
classes without applying any additional method for handling imbalance, with an
overall accuracy of 88.76% for WCS dataset and 94.97% for Snapshot Serengeti,
considering a location-based train/test split. In general, the square-root
sampling was the method that most improved the performance for minority classes
by around 10%, but at the cost of reducing the majority classes accuracy at
least 4%. These results motivated us to propose a simple and effective approach
using an ensemble combining square-root sampling and the baseline. The proposed
approach achieved the best trade-off between the performance of the tail class
and the cost of the head classes' accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Estimation for Large Displacements and Deformations. (arXiv:2206.12464v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12464">
<div class="article-summary-box-inner">
<span><p>Large displacement optical flow is an integral part of many computer vision
tasks. Variational optical flow techniques based on a coarse-to-fine scheme
interpolate sparse matches and locally optimize an energy model conditioned on
colour, gradient and smoothness, making them sensitive to noise in the sparse
matches, deformations, and arbitrarily large displacements. This paper
addresses this problem and presents HybridFlow, a variational motion estimation
framework for large displacements and deformations. A multi-scale hybrid
matching approach is performed on the image pairs. Coarse-scale clusters formed
by classifying pixels according to their feature descriptors are matched using
the clusters' context descriptors. We apply a multi-scale graph matching on the
finer-scale superpixels contained within each matched pair of coarse-scale
clusters. Small clusters that cannot be further subdivided are matched using
localized feature matching. Together, these initial matches form the flow,
which is propagated by an edge-preserving interpolation and variational
refinement. Our approach does not require training and is robust to substantial
displacements and rigid and non-rigid transformations due to motion in the
scene, making it ideal for large-scale imagery such as Wide-Area Motion Imagery
(WAMI). More notably, HybridFlow works on directed graphs of arbitrary topology
representing perceptual groups, which improves motion estimation in the
presence of significant deformations. We demonstrate HybridFlow's superior
performance to state-of-the-art variational techniques on two benchmark
datasets and report comparable results with state-of-the-art
deep-learning-based techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-Guided Autoencoder for Automated Progression Prediction of Subjective Cognitive Decline with Structural MRI. (arXiv:2206.12480v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12480">
<div class="article-summary-box-inner">
<span><p>Subjective cognitive decline (SCD) is a preclinical stage of Alzheimer's
disease (AD) which occurs even before mild cognitive impairment (MCI).
Progressive SCD will convert to MCI with the potential of further evolving to
AD. Therefore, early identification of progressive SCD with neuroimaging
techniques (e.g., structural MRI) is of great clinical value for early
intervention of AD. However, existing MRI-based machine/deep learning methods
usually suffer the small-sample-size problem which poses a great challenge to
related neuroimaging analysis. The central question we aim to tackle in this
paper is how to leverage related domains (e.g., AD/NC) to assist the
progression prediction of SCD. Meanwhile, we are concerned about which brain
areas are more closely linked to the identification of progressive SCD. To this
end, we propose an attention-guided autoencoder model for efficient
cross-domain adaptation which facilitates the knowledge transfer from AD to
SCD. The proposed model is composed of four key components: 1) a feature
encoding module for learning shared subspace representations of different
domains, 2) an attention module for automatically locating discriminative brain
regions of interest defined in brain atlases, 3) a decoding module for
reconstructing the original input, 4) a classification module for
identification of brain diseases. Through joint training of these four modules,
domain invariant features can be learned. Meanwhile, the brain disease related
regions can be highlighted by the attention mechanism. Extensive experiments on
the publicly available ADNI dataset and a private CLAS dataset have
demonstrated the effectiveness of the proposed method. The proposed model is
straightforward to train and test with only 5-10 seconds on CPUs and is
suitable for medical tasks with small datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Approach For Analysis of Distributed Acoustic Sensing System Based on Deep Transfer Learning. (arXiv:2206.12484v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12484">
<div class="article-summary-box-inner">
<span><p>Distributed acoustic sensors (DAS) are effective apparatus which are widely
used in many application areas for recording signals of various events with
very high spatial resolution along the optical fiber. To detect and recognize
the recorded events properly, advanced signal processing algorithms with high
computational demands are crucial. Convolutional neural networks are highly
capable tools for extracting spatial information and very suitable for event
recognition applications in DAS. Long-short term memory (LSTM) is an effective
instrument for processing sequential data. In this study, we proposed a
multi-input multi-output, two stage feature extraction methodology that
combines the capabilities of these neural network architectures with transfer
learning to classify vibrations applied to an optical fiber by a piezo
transducer. First, we extracted the differential amplitude and phase
information from the Phase-OTDR recordings and stored them in a
temporal-spatial data matrix. Then, we used a state-of-the-art pre-trained CNN
without dense layers as a feature extractor in the first stage. In the second
stage, we used LSTMs to further analyze the features extracted by the CNN.
Finally, we used a dense layer to classify the extracted features. To observe
the effect of the utilized CNN architecture, we tested our model with five
state-of-the art pre-trained models (VGG-16, ResNet-50, DenseNet-121, MobileNet
and Inception-v3). The results show that using the VGG-16 architecture in our
framework manages to obtain 100% classification accuracy in 50 trainings and
got the best results on our Phase-OTDR dataset. Outcomes of this study indicate
that the pre-trained CNNs combined with LSTM are very suitable for the analysis
of differential amplitude and phase information, represented in a temporal
spatial data matrix which is promising for event recognition operations in DAS
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal and Robust Category-level Perception: Object Pose and Shape Estimation from 2D and 3D Semantic Keypoints. (arXiv:2206.12498v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12498">
<div class="article-summary-box-inner">
<span><p>We consider a category-level perception problem, where one is given 2D or 3D
sensor data picturing an object of a given category (e.g., a car), and has to
reconstruct the 3D pose and shape of the object despite intra-class variability
(i.e., different car models have different shapes). We consider an active shape
model, where -- for an object category -- we are given a library of potential
CAD models describing objects in that category, and we adopt a standard
formulation where pose and shape are estimated from 2D or 3D keypoints via
non-convex optimization. Our first contribution is to develop PACE3D* and
PACE2D*, the first certifiably optimal solvers for pose and shape estimation
using 3D and 2D keypoints, respectively. Both solvers rely on the design of
tight (i.e., exact) semidefinite relaxations. Our second contribution is to
develop outlier-robust versions of both solvers, named PACE3D# and PACE2D#.
Towards this goal, we propose ROBIN, a general graph-theoretic framework to
prune outliers, which uses compatibility hypergraphs to model measurements'
compatibility. We show that in category-level perception problems these
hypergraphs can be built from winding orders of the keypoints (in 2D) or their
convex hulls (in 3D), and many outliers can be pruned via maximum hyperclique
computation. The last contribution is an extensive experimental evaluation.
Besides providing an ablation study on simulated datasets and on the PASCAL
dataset, we combine our solver with a deep keypoint detector, and show that
PACE3D# improves over the state of the art in vehicle pose estimation in the
ApolloScape datasets, and its runtime is compatible with practical
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stain based contrastive co-training for histopathological image analysis. (arXiv:2206.12505v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12505">
<div class="article-summary-box-inner">
<span><p>We propose a novel semi-supervised learning approach for classification of
histopathology images. We employ strong supervision with patch-level
annotations combined with a novel co-training loss to create a semi-supervised
learning framework. Co-training relies on multiple conditionally independent
and sufficient views of the data. We separate the hematoxylin and eosin
channels in pathology images using color deconvolution to create two views of
each slide that can partially fulfill these requirements. Two separate CNNs are
used to embed the two views into a joint feature space. We use a contrastive
loss between the views in this feature space to implement co-training. We
evaluate our approach in clear cell renal cell and prostate carcinomas, and
demonstrate improvement over state-of-the-art semi-supervised learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FetReg2021: A Challenge on Placental Vessel Segmentation and Registration in Fetoscopy. (arXiv:2206.12512v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12512">
<div class="article-summary-box-inner">
<span><p>Fetoscopy laser photocoagulation is a widely adopted procedure for treating
Twin-to-Twin Transfusion Syndrome (TTTS). The procedure involves
photocoagulation pathological anastomoses to regulate blood exchange among
twins. The procedure is particularly challenging due to the limited field of
view, poor manoeuvrability of the fetoscope, poor visibility, and variability
in illumination. These challenges may lead to increased surgery time and
incomplete ablation. Computer-assisted intervention (CAI) can provide surgeons
with decision support and context awareness by identifying key structures in
the scene and expanding the fetoscopic field of view through video mosaicking.
Research in this domain has been hampered by the lack of high-quality data to
design, develop and test CAI algorithms. Through the Fetoscopic Placental
Vessel Segmentation and Registration (FetReg2021) challenge, which was
organized as part of the MICCAI2021 Endoscopic Vision challenge, we released
the first largescale multicentre TTTS dataset for the development of
generalized and robust semantic segmentation and video mosaicking algorithms.
For this challenge, we released a dataset of 2060 images, pixel-annotated for
vessels, tool, fetus and background classes, from 18 in-vivo TTTS fetoscopy
procedures and 18 short video clips. Seven teams participated in this challenge
and their model performance was assessed on an unseen test dataset of 658
pixel-annotated images from 6 fetoscopic procedures and 6 short clips. The
challenge provided an opportunity for creating generalized solutions for
fetoscopic scene understanding and mosaicking. In this paper, we present the
findings of the FetReg2021 challenge alongside reporting a detailed literature
review for CAI in TTTS fetoscopy. Through this challenge, its analysis and the
release of multi-centre fetoscopic data, we provide a benchmark for future
research in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Shallow to Deep: Compositional Reasoning over Graphs for Visual Question Answering. (arXiv:2206.12533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12533">
<div class="article-summary-box-inner">
<span><p>In order to achieve a general visual question answering (VQA) system, it is
essential to learn to answer deeper questions that require compositional
reasoning on the image and external knowledge. Meanwhile, the reasoning process
should be explicit and explainable to understand the working mechanism of the
model. It is effortless for human but challenging for machines. In this paper,
we propose a Hierarchical Graph Neural Module Network (HGNMN) that reasons over
multi-layer graphs with neural modules to address the above issues.
Specifically, we first encode the image by multi-layer graphs from the visual,
semantic and commonsense views since the clues that support the answer may
exist in different modalities. Our model consists of several well-designed
neural modules that perform specific functions over graphs, which can be used
to conduct multi-step reasoning within and between different graphs. Compared
to existing modular networks, we extend visual reasoning from one graph to more
graphs. We can explicitly trace the reasoning process according to module
weights and graph attentions. Experiments show that our model not only achieves
state-of-the-art performance on the CRIC dataset but also obtains explicit and
explainable reasoning procedures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLIC: Self-Supervised Learning with Iterative Clustering for Human Action Videos. (arXiv:2206.12534v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12534">
<div class="article-summary-box-inner">
<span><p>Self-supervised methods have significantly closed the gap with end-to-end
supervised learning for image classification. In the case of human action
videos, however, where both appearance and motion are significant factors of
variation, this gap remains significant. One of the key reasons for this is
that sampling pairs of similar video clips, a required step for many
self-supervised contrastive learning methods, is currently done conservatively
to avoid false positives. A typical assumption is that similar clips only occur
temporally close within a single video, leading to insufficient examples of
motion similarity. To mitigate this, we propose SLIC, a clustering-based
self-supervised contrastive learning method for human action videos. Our key
contribution is that we improve upon the traditional intra-video positive
sampling by using iterative clustering to group similar video instances. This
enables our method to leverage pseudo-labels from the cluster assignments to
sample harder positives and negatives. SLIC outperforms state-of-the-art video
retrieval baselines by +15.4% on top-1 recall on UCF101 and by +5.7% when
directly transferred to HMDB51. With end-to-end finetuning for action
classification, SLIC achieves 83.2% top-1 accuracy (+0.8%) on UCF101 and 54.5%
on HMDB51 (+1.6%). SLIC is also competitive with the state-of-the-art in action
classification after self-supervised pretraining on Kinetics400.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LFPS-Net: a lightweight fast pulse simulation network for BVP estimation. (arXiv:2206.12558v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12558">
<div class="article-summary-box-inner">
<span><p>Heart rate estimation based on remote photoplethysmography plays an important
role in several specific scenarios, such as health monitoring and fatigue
detection. Existing well-established methods are committed to taking the
average of the predicted HRs of multiple overlapping video clips as the final
results for the 30-second facial video. Although these methods with hundreds of
layers and thousands of channels are highly accurate and robust, they require
enormous computational budget and a 30-second wait time, which greatly limits
the application of the algorithms to scale. Under these cicumstacnces, We
propose a lightweight fast pulse simulation network (LFPS-Net), pursuing the
best accuracy within a very limited computational and time budget, focusing on
common mobile platforms, such as smart phones. In order to suppress the noise
component and get stable pulse in a short time, we design a multi-frequency
modal signal fusion mechanism, which exploits the theory of time-frequency
domain analysis to separate multi-modal information from complex signals. It
helps proceeding network learn the effective fetures more easily without adding
any parameter. In addition, we design a oversampling training strategy to solve
the problem caused by the unbalanced distribution of dataset. For the 30-second
facial videos, our proposed method achieves the best results on most of the
evaluation metrics for estimating heart rate or heart rate variability compared
to the best available papers. The proposed method can still obtain very
competitive results by using a short-time (~15-second) facail video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CV 3315 Is All You Need : Semantic Segmentation Competition. (arXiv:2206.12571v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12571">
<div class="article-summary-box-inner">
<span><p>This competition focus on Urban-Sense Segmentation based on the vehicle
camera view. Class highly unbalanced Urban-Sense images dataset challenge the
existing solutions and further studies. Deep Conventional neural network-based
semantic segmentation methods such as encoder-decoder architecture and
multi-scale and pyramid-based approaches become flexible solutions applicable
to real-world applications. In this competition, we mainly review the
literature and conduct experiments on transformer-driven methods especially
SegFormer, to achieve an optimal trade-off between performance and efficiency.
For example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,
and the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple
factors, including individual case failure analysis, individual class
performance, training pressure and efficiency estimation, the final candidate
model for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU
evaluated on the testing set. Checkout our code implementation at
https://vmv.re/cv3315.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RSTAM: An Effective Black-Box Impersonation Attack on Face Recognition using a Mobile and Compact Printer. (arXiv:2206.12590v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12590">
<div class="article-summary-box-inner">
<span><p>Face recognition has achieved considerable progress in recent years thanks to
the development of deep neural networks, but it has recently been discovered
that deep neural networks are vulnerable to adversarial examples. This means
that face recognition models or systems based on deep neural networks are also
susceptible to adversarial examples. However, the existing methods of attacking
face recognition models or systems with adversarial examples can effectively
complete white-box attacks but not black-box impersonation attacks, physical
attacks, or convenient attacks, particularly on commercial face recognition
systems. In this paper, we propose a new method to attack face recognition
models or systems called RSTAM, which enables an effective black-box
impersonation attack using an adversarial mask printed by a mobile and compact
printer. First, RSTAM enhances the transferability of the adversarial masks
through our proposed random similarity transformation strategy. Furthermore, we
propose a random meta-optimization strategy for ensembling several pre-trained
face models to generate more general adversarial masks. Finally, we conduct
experiments on the CelebA-HQ, LFW, Makeup Transfer (MT), and CASIA-FaceV5
datasets. The performance of the attacks is also evaluated on state-of-the-art
commercial face recognition systems: Face++, Baidu, Aliyun, Tencent, and
Microsoft. Extensive experiments show that RSTAM can effectively perform
black-box impersonation attacks on face recognition models or systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asymmetric Transfer Hashing with Adaptive Bipartite Graph Learning. (arXiv:2206.12592v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12592">
<div class="article-summary-box-inner">
<span><p>Thanks to the efficient retrieval speed and low storage consumption, learning
to hash has been widely used in visual retrieval tasks. However, existing
hashing methods assume that the query and retrieval samples lie in homogeneous
feature space within the same domain. As a result, they cannot be directly
applied to heterogeneous cross-domain retrieval. In this paper, we propose a
Generalized Image Transfer Retrieval (GITR) problem, which encounters two
crucial bottlenecks: 1) the query and retrieval samples may come from different
domains, leading to an inevitable {domain distribution gap}; 2) the features of
the two domains may be heterogeneous or misaligned, bringing up an additional
{feature gap}. To address the GITR problem, we propose an Asymmetric Transfer
Hashing (ATH) framework with its unsupervised/semi-supervised/supervised
realizations. Specifically, ATH characterizes the domain distribution gap by
the discrepancy between two asymmetric hash functions, and minimizes the
feature gap with the help of a novel adaptive bipartite graph constructed on
cross-domain data. By jointly optimizing asymmetric hash functions and the
bipartite graph, not only can knowledge transfer be achieved but information
loss caused by feature alignment can also be avoided. Meanwhile, to alleviate
negative transfer, the intrinsic geometrical structure of single-domain data is
preserved by involving a domain affinity graph. Extensive experiments on both
single-domain and cross-domain benchmarks under different GITR subtasks
indicate the superiority of our ATH method in comparison with the
state-of-the-art hashing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning. (arXiv:2206.12596v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12596">
<div class="article-summary-box-inner">
<span><p>Deformable image registration is a crucial step in medical image analysis for
finding a non-linear spatial transformation between a pair of fixed and moving
images. Deep registration methods based on Convolutional Neural Networks (CNNs)
have been widely used as they can perform image registration in a fast and
end-to-end manner. However, these methods usually have limited performance for
image pairs with large deformations. Recently, iterative deep registration
methods have been used to alleviate this limitation, where the transformations
are iteratively learned in a coarse-to-fine manner. However, iterative methods
inevitably prolong the registration runtime, and tend to learn separate image
features for each iteration, which hinders the features from being leveraged to
facilitate the registration at later iterations. In this study, we propose a
Non-Iterative Coarse-to-finE registration Network (NICE-Net) for deformable
image registration. In the NICE-Net, we propose: (i) a Single-pass Deep
Cumulative Learning (SDCL) decoder that can cumulatively learn coarse-to-fine
transformations within a single pass (iteration) of the network, and (ii) a
Selectively-propagated Feature Learning (SFL) encoder that can learn common
image features for the whole coarse-to-fine registration process and
selectively propagate the features as needed. Extensive experiments on six
public datasets of 3D brain Magnetic Resonance Imaging (MRI) show that our
proposed NICE-Net can outperform state-of-the-art iterative deep registration
methods while only requiring similar runtime to non-iterative methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn to Predict How Humans Manipulate Large-sized Objects from Interactive Motions. (arXiv:2206.12612v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12612">
<div class="article-summary-box-inner">
<span><p>Understanding human intentions during interactions has been a long-lasting
theme, that has applications in human-robot interaction, virtual reality and
surveillance. In this study, we focus on full-body human interactions with
large-sized daily objects and aim to predict the future states of objects and
humans given a sequential observation of human-object interaction. As there is
no such dataset dedicated to full-body human interactions with large-sized
daily objects, we collected a large-scale dataset containing thousands of
interactions for training and evaluation purposes. We also observe that an
object's intrinsic physical properties are useful for the object motion
prediction, and thus design a set of object dynamic descriptors to encode such
intrinsic properties. We treat the object dynamic descriptors as a new modality
and propose a graph neural network, HO-GCN, to fuse motion data and dynamic
descriptors for the prediction task. We show the proposed network that consumes
dynamic descriptors can achieve state-of-the-art prediction results and help
the network better generalize to unseen objects. We also demonstrate the
predicted results are useful for human-robot collaborations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BokehMe: When Neural Rendering Meets Classical Rendering. (arXiv:2206.12614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12614">
<div class="article-summary-box-inner">
<span><p>We propose BokehMe, a hybrid bokeh rendering framework that marries a neural
renderer with a classical physically motivated renderer. Given a single image
and a potentially imperfect disparity map, BokehMe generates high-resolution
photo-realistic bokeh effects with adjustable blur size, focal plane, and
aperture shape. To this end, we analyze the errors from the classical
scattering-based method and derive a formulation to calculate an error map.
Based on this formulation, we implement the classical renderer by a
scattering-based method and propose a two-stage neural renderer to fix the
erroneous areas from the classical renderer. The neural renderer employs a
dynamic multi-scale scheme to efficiently handle arbitrary blur sizes, and it
is trained to handle imperfect disparity input. Experiments show that our
method compares favorably against previous methods on both synthetic image data
and real image data with predicted disparity. A user study is further conducted
to validate the advantage of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAT: Self-adaptive training for fashion compatibility prediction. (arXiv:2206.12622v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12622">
<div class="article-summary-box-inner">
<span><p>This paper presents a self-adaptive training (SAT) model for fashion
compatibility prediction. It focuses on the learning of some hard items, such
as those that share similar color, texture, and pattern features but are
considered incompatible due to the aesthetics or temporal shifts. Specifically,
we first design a method to define hard outfits and a difficulty score (DS) is
defined and assigned to each outfit based on the difficulty in recommending an
item for it. Then, we propose a self-adaptive triplet loss (SATL), where the DS
of the outfit is considered. Finally, we propose a very simple conditional
similarity network combining the proposed SATL to achieve the learning of hard
items in the fashion compatibility prediction. Experiments on the publicly
available Polyvore Outfits and Polyvore Outfits-D datasets demonstrate our
SAT's effectiveness in fashion compatibility prediction. Besides, our SATL can
be easily extended to other conditional similarity networks to improve their
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverted Semantic-Index for Image Retrieval. (arXiv:2206.12623v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12623">
<div class="article-summary-box-inner">
<span><p>This paper addresses the construction of inverted index for large-scale image
retrieval. The inverted index proposed by J. Sivic brings a significant
acceleration by reducing distance computations with only a small fraction of
the database. The state-of-the-art inverted indices aim to build finer
partitions that produce a concise and accurate candidate list. However,
partitioning in these frameworks is generally achieved by unsupervised
clustering methods which ignore the semantic information of images. In this
paper, we replace the clustering method with image classification, during the
construction of codebook. We then propose a merging and splitting method to
solve the problem that the number of partitions is unchangeable in the inverted
semantic-index. Next, we combine our semantic-index with the product
quantization (PQ) so as to alleviate the accuracy loss caused by PQ
compression. Finally, we evaluate our model on large-scale image retrieval
benchmarks. Experiment results demonstrate that our model can significantly
improve the retrieval accuracy by generating high-quality candidate lists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SC-Transformer++: Structured Context Transformer for Generic Event Boundary Detection. (arXiv:2206.12634v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12634">
<div class="article-summary-box-inner">
<span><p>This report presents the algorithm used in the submission of Generic Event
Boundary Detection (GEBD) Challenge at CVPR 2022. In this work, we improve the
existing Structured Context Transformer (SC-Transformer) method for GEBD.
Specifically, a transformer decoder module is added after transformer encoders
to extract high quality frame features. The final classification is performed
jointly on the results of the original binary classifier and a newly introduced
multi-class classifier branch. To enrich motion information, optical flow is
introduced as a new modality. Finally, model ensemble is used to further boost
performance. The proposed method achieves 86.49% F1 score on Kinetics-GEBD test
set. which improves 2.86% F1 score compared to the previous SOTA method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BIMS-PU: Bi-Directional and Multi-Scale Point Cloud Upsampling. (arXiv:2206.12648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12648">
<div class="article-summary-box-inner">
<span><p>The learning and aggregation of multi-scale features are essential in
empowering neural networks to capture the fine-grained geometric details in the
point cloud upsampling task. Most existing approaches extract multi-scale
features from a point cloud of a fixed resolution, hence obtain only a limited
level of details. Though an existing approach aggregates a feature hierarchy of
different resolutions from a cascade of upsampling sub-network, the training is
complex with expensive computation. To address these issues, we construct a new
point cloud upsampling pipeline called BIMS-PU that integrates the feature
pyramid architecture with a bi-directional up and downsampling path.
Specifically, we decompose the up/downsampling procedure into several
up/downsampling sub-steps by breaking the target sampling factor into smaller
factors. The multi-scale features are naturally produced in a parallel manner
and aggregated using a fast feature fusion method. Supervision signal is
simultaneously applied to all upsampled point clouds of different scales.
Moreover, we formulate a residual block to ease the training of our model.
Extensive quantitative and qualitative experiments on different datasets show
that our method achieves superior results to state-of-the-art approaches. Last
but not least, we demonstrate that point cloud upsampling can improve robot
perception by ameliorating the 3D data quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis with R: Natural Language Processing for Semi-Automated Assessments of Qualitative Data. (arXiv:2206.12649v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12649">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is a sub-discipline in the field of natural language
processing and computational linguistics and can be used for automated or
semi-automated analyses of text documents. One of the aims of these analyses is
to recognize an expressed attitude as positive or negative as it can be
contained in comments on social media platforms or political documents and
speeches as well as fictional and nonfictional texts. Regarding analyses of
comments on social media platforms, this is an extension of the previous
tutorial on semi-automated screenings of social media network data. A
longitudinal perspective regarding social media comments as well as
cross-sectional perspectives regarding fictional and nonfictional texts, e.g.
entire books and libraries, can lead to extensive text documents. Their
analyses can be simplified and accelerated by using sentiment analysis with
acceptable inter-rater reliability. Therefore, this tutorial introduces the
basic functions for performing a sentiment analysis with R and explains how
text documents can be analysed step by step - regardless of their underlying
formatting. All prerequisites and steps are described in detail and associated
codes are available on GitHub. A comparison of two political speeches
illustrates a possible use case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning-based Biological Ageing Estimation Technologies: A Survey. (arXiv:2206.12650v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12650">
<div class="article-summary-box-inner">
<span><p>In recent years, there are various methods of estimating Biological Age (BA)
have been developed. Especially with the development of machine learning (ML),
there are more and more types of BA predictions, and the accuracy has been
greatly improved. The models for the estimation of BA play an important role in
monitoring healthy aging, and could provide new tools to detect health status
in the general population and give warnings to sub-healthy people. We will
mainly review three age prediction methods by using ML. They are based on blood
biomarkers, facial images, and structural neuroimaging features. For now, the
model using blood biomarkers is the simplest, most direct, and most accurate
method. The face image method is affected by various aspects such as race,
environment, etc., the prediction accuracy is not very good, which cannot make
a great contribution to the medical field. In summary, we are here to track the
way forward in the era of big data for us and other potential general
populations and show ways to leverage the vast amounts of data available today.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Review on Social Behavior Analysis of Laboratory Animals: From Methodologies to Applications. (arXiv:2206.12651v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12651">
<div class="article-summary-box-inner">
<span><p>As the bridge between genetic and physiological aspects, animal behaviour
analysis is one of the most significant topics in biology and ecological
research. However, identifying, tracking and recording animal behaviour are
labour intensive works that require professional knowledge. To mitigate the
spend for annotating data, researchers turn to computer vision techniques for
automatic label algorithms, since most of the data are recorded visually. In
this work, we explore a variety of behaviour detection algorithms, covering
traditional vision methods, statistical methods and deep learning methods. The
objective of this work is to provide a thorough investigation of related work,
furnishing biologists with a scratch of efficient animal behaviour detection
methods. Apart from that, we also discuss the strengths and weaknesses of those
algorithms to provide some insights for those who already delve into this
field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diagnostic Communication and Visual System based on Vehicle UDS Protocol. (arXiv:2206.12653v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12653">
<div class="article-summary-box-inner">
<span><p>Unified Diagnostic Services (UDS) is a diagnostic communication protocol used
in electronic control units (ECUs) within automotive electronics, which is
specified in the ISO 14229-1. It is derived from ISO 14230-3 (KWP2000) and the
now obsolete ISO 15765-3 (Diagnostic Communication over Controller Area Network
(DoCAN). 'Unified' in this context means that it is an international and not a
company-specific standard. By now this communication protocol is used in all
new ECUs made by Tier 1 suppliers of Original Equipment Manufacturer (OEM), and
is incorporated into other standards, such as AUTOSAR. The ECUs in modern
vehicles control nearly all functions, including electronic fuel injection
(EFI), engine control, the transmission, anti-lock braking system, door locks,
braking, window operation, and more.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Deep Animation Video Interpolation. (arXiv:2206.12657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12657">
<div class="article-summary-box-inner">
<span><p>Existing learning-based frame interpolation algorithms extract consecutive
frames from high-speed natural videos to train the model. Compared to natural
videos, cartoon videos are usually in a low frame rate. Besides, the motion
between consecutive cartoon frames is typically nonlinear, which breaks the
linear motion assumption of interpolation algorithms. Thus, it is unsuitable
for generating a training set directly from cartoon videos. For better adapting
frame interpolation algorithms from nature video to animation video, we present
AutoFI, a simple and effective method to automatically render training data for
deep animation video interpolation. AutoFI takes a layered architecture to
render synthetic data, which ensures the assumption of linear motion.
Experimental results show that AutoFI performs favorably in training both DAIN
and ANIN. However, most frame interpolation algorithms will still fail in
error-prone areas, such as fast motion or large occlusion. Besides AutoFI, we
also propose a plug-and-play sketch-based post-processing module, named SktFI,
to refine the final results using user-provided sketches manually. With AutoFI
and SktFI, the interpolated animation frames show high perceptual quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Infer 3D Shape Programs with Differentiable Renderer. (arXiv:2206.12675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12675">
<div class="article-summary-box-inner">
<span><p>Given everyday artifacts, such as tables and chairs, humans recognize
high-level regularities within them, such as the symmetries of a table, the
repetition of its legs, while possessing low-level priors of their geometries,
e.g., surfaces are smooth and edges are sharp. This kind of knowledge
constitutes an important part of human perceptual understanding and reasoning.
Representations of and how to reason in such knowledge, and the acquisition
thereof, are still open questions in artificial intelligence (AI) and cognitive
science. Building on the previous proposal of the \emph{3D shape programs}
representation alone with the accompanying neural generator and executor from
\citet{tian2019learning}, we propose an analytical yet differentiable executor
that is more faithful and controllable in interpreting shape programs
(particularly in extrapolation) and more sample efficient (requires no
training). These facilitate the generator's learning when ground truth programs
are not available, and should be especially useful when new shape-program
components are enrolled either by human designers or -- in the context of
library learning -- algorithms themselves. Preliminary experiments on using it
for adaptation illustrate the aforesaid advantages of the proposed module,
encouraging similar methods being explored in building machines that learn to
reason with the kind of knowledge described above, and even learn this
knowledge itself.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UltraMNIST Classification: A Benchmark to Train CNNs for Very Large Images. (arXiv:2206.12681v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12681">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) approaches available in the current
literature are designed to work primarily with low-resolution images. When
applied on very large images, challenges related to GPU memory, smaller
receptive field than needed for semantic correspondence and the need to
incorporate multi-scale features arise. The resolution of input images can be
reduced, however, with significant loss of critical information. Based on the
outlined issues, we introduce a novel research problem of training CNN models
for very large images, and present 'UltraMNIST dataset', a simple yet
representative benchmark dataset for this task. UltraMNIST has been designed
using the popular MNIST digits with additional levels of complexity added to
replicate well the challenges of real-world problems. We present two variants
of the problem: 'UltraMNIST classification' and 'Budget-aware UltraMNIST
classification'. The standard UltraMNIST classification benchmark is intended
to facilitate the development of novel CNN training methods that make the
effective use of the best available GPU resources. The budget-aware variant is
intended to promote development of methods that work under constrained GPU
memory. For the development of competitive solutions, we present several
baseline models for the standard benchmark and its budget-aware variant. We
study the effect of reducing resolution on the performance and present results
for baseline models involving pretrained backbones from among the popular
state-of-the-art models. Finally, with the presented benchmark dataset and the
baselines, we hope to pave the ground for a new generation of CNN methods
suitable for handling large images in an efficient and resource-light manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defense against adversarial attacks on deep convolutional neural networks through nonlocal denoising. (arXiv:2206.12685v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12685">
<div class="article-summary-box-inner">
<span><p>Despite substantial advances in network architecture performance, the
susceptibility of adversarial attacks makes deep learning challenging to
implement in safety-critical applications. This paper proposes a data-centric
approach to addressing this problem. A nonlocal denoising method with different
luminance values has been used to generate adversarial examples from the
Modified National Institute of Standards and Technology database (MNIST) and
Canadian Institute for Advanced Research (CIFAR-10) data sets. Under
perturbation, the method provided absolute accuracy improvements of up to 9.3%
in the MNIST data set and 13% in the CIFAR-10 data set. Training using
transformed images with higher luminance values increases the robustness of the
classifier. We have shown that transfer learning is disadvantageous for
adversarial machine learning. The results indicate that simple adversarial
examples can improve resilience and make deep learning easier to apply in
various applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RandStainNA: Learning Stain-Agnostic Features from Histology Slides by Bridging Stain Augmentation and Normalization. (arXiv:2206.12694v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12694">
<div class="article-summary-box-inner">
<span><p>Stain variations often decrease the generalization ability of deep learning
based approaches in digital histopathology analysis. Two separate proposals,
namely stain normalization (SN) and stain augmentation (SA), have been
spotlighted to reduce the generalization error, where the former alleviates the
stain shift across different medical centers using template image and the
latter enriches the accessible stain styles by the simulation of more stain
variations. However, their applications are bounded by the selection of
template images and the construction of unrealistic styles. To address the
problems, we unify SN and SA with a novel RandStainNA scheme, which constrains
variable stain styles in a practicable range to train a stain agnostic deep
learning model. The RandStainNA is applicable to stain normalization in a
collection of color spaces i.e. HED, HSV, LAB. Additionally, we propose a
random color space selection scheme to gain extra performance improvement. We
evaluate our method by two diagnostic tasks i.e. tissue subtype classification
and nuclei segmentation, with various network backbones. The performance
superiority over both SA and SN yields that the proposed RandStainNA can
consistently improve the generalization ability, that our models can cope with
more incoming clinical datasets with unpredicted stain styles. The codes is
available at https://github.com/yiqings/RandStainNA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays. (arXiv:2206.12704v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12704">
<div class="article-summary-box-inner">
<span><p>Creating a large-scale dataset of abnormality annotation on medical images is
a labor-intensive and costly task. Leveraging weak supervision from readily
available data such as radiology reports can compensate lack of large-scale
data for anomaly detection methods. However, most of the current methods only
use image-level pathological observations, failing to utilize the relevant
anatomy mentions in reports. Furthermore, Natural Language Processing
(NLP)-mined weak labels are noisy due to label sparsity and linguistic
ambiguity. We propose an Anatomy-Guided chest X-ray Network (AGXNet) to address
these issues of weak annotation. Our framework consists of a cascade of two
networks, one responsible for identifying anatomical abnormalities and the
second responsible for pathological observations. The critical component in our
framework is an anatomy-guided attention module that aids the downstream
observation network in focusing on the relevant anatomical regions generated by
the anatomy network. We use Positive Unlabeled (PU) learning to account for the
fact that lack of mention does not necessarily mean a negative label. Our
quantitative and qualitative results on the MIMIC-CXR dataset demonstrate the
effectiveness of AGXNet in disease and anatomical abnormality localization.
Experiments on the NIH Chest X-ray dataset show that the learned feature
representations are transferable and can achieve the state-of-the-art
performances in disease classification and competitive disease localization
results. Our code is available at https://github.com/batmanlab/AGXNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">p-Meta: Towards On-device Deep Model Adaptation. (arXiv:2206.12705v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12705">
<div class="article-summary-box-inner">
<span><p>Data collected by IoT devices are often private and have a large diversity
across users. Therefore, learning requires pre-training a model with available
representative data samples, deploying the pre-trained model on IoT devices,
and adapting the deployed model on the device with local data. Such an
on-device adaption for deep learning empowered applications demands data and
memory efficiency. However, existing gradient-based meta learning schemes fail
to support memory-efficient adaptation. To this end, we propose p-Meta, a new
meta learning method that enforces structure-wise partial parameter updates
while ensuring fast generalization to unseen tasks. Evaluations on few-shot
image classification and reinforcement learning tasks show that p-Meta not only
improves the accuracy but also substantially reduces the peak dynamic memory by
a factor of 2.5 on average compared to state-of-the-art few-shot adaptation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Multimodal Fusion Models against Single-Source Adversaries. (arXiv:2206.12714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12714">
<div class="article-summary-box-inner">
<span><p>Beyond achieving high performance across many vision tasks, multimodal models
are expected to be robust to single-source faults due to the availability of
redundant information between modalities. In this paper, we investigate the
robustness of multimodal neural networks against worst-case (i.e., adversarial)
perturbations on a single modality. We first show that standard multimodal
fusion models are vulnerable to single-source adversaries: an attack on any
single modality can overcome the correct information from multiple unperturbed
modalities and cause the model to fail. This surprising vulnerability holds
across diverse multimodal tasks and necessitates a solution. Motivated by this
finding, we propose an adversarially robust fusion strategy that trains the
model to compare information coming from all the input sources, detect
inconsistencies in the perturbed modality compared to the other modalities, and
only allow information from the unperturbed modalities to pass through. Our
approach significantly improves on state-of-the-art methods in single-source
robustness, achieving gains of 7.8-25.2% on action recognition, 19.7-48.2% on
object detection, and 1.6-6.7% on sentiment analysis, without degrading
performance on unperturbed (i.e., clean) data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Evaluation of Physical Adversarial Patch Attacks Against Overhead Object Detection Models. (arXiv:2206.12725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12725">
<div class="article-summary-box-inner">
<span><p>Adversarial patches are images designed to fool otherwise well-performing
neural network-based computer vision models. Although these attacks were
initially conceived of and studied digitally, in that the raw pixel values of
the image were perturbed, recent work has demonstrated that these attacks can
successfully transfer to the physical world. This can be accomplished by
printing out the patch and adding it into scenes of newly captured images or
video footage. In this work we further test the efficacy of adversarial patch
attacks in the physical world under more challenging conditions. We consider
object detection models trained on overhead imagery acquired through aerial or
satellite cameras, and we test physical adversarial patches inserted into
scenes of a desert environment. Our main finding is that it is far more
difficult to successfully implement the adversarial patch attacks under these
conditions than in the previously considered conditions. This has important
implications for AI safety as the real-world threat posed by adversarial
examples may be overstated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised 3D Monocular Object Detection by Recycling Bounding Boxes. (arXiv:2206.12738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12738">
<div class="article-summary-box-inner">
<span><p>Modern object detection architectures are moving towards employing
self-supervised learning (SSL) to improve performance detection with related
pretext tasks. Pretext tasks for monocular 3D object detection have not yet
been explored yet in literature. The paper studies the application of
established self-supervised bounding box recycling by labeling random windows
as the pretext task. The classifier head of the 3D detector is trained to
classify random windows containing different proportions of the ground truth
objects, thus handling the foreground-background imbalance. We evaluate the
pretext task using the RTM3D detection model as baseline, with and without the
application of data augmentation. We demonstrate improvements of between 2-3 %
in mAP 3D and 0.9-1.5 % BEV scores using SSL over the baseline scores. We
propose the inverse class frequency re-weighted (ICFW) mAP score that
highlights improvements in detection for low frequency classes in a class
imbalanced dataset with long tails. We demonstrate improvements in ICFW both
mAP 3D and BEV scores to take into account the class imbalance in the KITTI
validation dataset. We see 4-5 % increase in ICFW metric with the pretext task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi Visual Modality Fall Detection Dataset. (arXiv:2206.12740v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12740">
<div class="article-summary-box-inner">
<span><p>Falls are one of the leading cause of injury-related deaths among the elderly
worldwide. Effective detection of falls can reduce the risk of complications
and injuries. Fall detection can be performed using wearable devices or ambient
sensors; these methods may struggle with user compliance issues or false
alarms. Video cameras provide a passive alternative; however, regular RGB
cameras are impacted by changing lighting conditions and privacy concerns. From
a machine learning perspective, developing an effective fall detection system
is challenging because of the rarity and variability of falls. Many existing
fall detection datasets lack important real-world considerations, such as
varied lighting, continuous activities of daily living (ADLs), and camera
placement. The lack of these considerations makes it difficult to develop
predictive models that can operate effectively in the real world. To address
these limitations, we introduce a novel multi-modality dataset (MUVIM) that
contains four visual modalities: infra-red, depth, RGB and thermal cameras.
These modalities offer benefits such as obfuscated facial features and improved
performance in low-light conditions. We formulated fall detection as an anomaly
detection problem, in which a customized spatio-temporal convolutional
autoencoder was trained only on ADLs so that a fall would increase the
reconstruction error. Our results showed that infra-red cameras provided the
highest level of performance (AUC ROC=0.94), followed by thermal (AUC
ROC=0.87), depth (AUC ROC=0.86) and RGB (AUC ROC=0.83). This research provides
a unique opportunity to analyze the utility of camera modalities in detecting
falls in a home setting while balancing performance, passiveness, and privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential image recovery using joint hierarchical Bayesian learning. (arXiv:2206.12745v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12745">
<div class="article-summary-box-inner">
<span><p>Recovering temporal image sequences (videos) based on indirect, noisy, or
incomplete data is an essential yet challenging task. We specifically consider
the case where each data set is missing vital information, which prevents the
accurate recovery of the individual images. Although some recent (variational)
methods have demonstrated high-resolution image recovery based on jointly
recovering sequential images, there remain robustness issues due to parameter
tuning and restrictions on the type of the sequential images. Here, we present
a method based on hierarchical Bayesian learning for the joint recovery of
sequential images that incorporates prior intra- and inter-image information.
Our method restores the missing information in each image by "borrowing" it
from the other images. As a result, \emph{all} of the individual
reconstructions yield improved accuracy. Our method can be used for various
data acquisitions and allows for uncertainty quantification. Some preliminary
results indicate its potential use for sequential deblurring and magnetic
resonance imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatiotemporal Data Mining: A Survey. (arXiv:2206.12753v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12753">
<div class="article-summary-box-inner">
<span><p>Spatiotemporal data mining aims to discover interesting, useful but
non-trivial patterns in big spatial and spatiotemporal data. They are used in
various application domains such as public safety, ecology, epidemiology, earth
science, etc. This problem is challenging because of the high societal cost of
spurious patterns and exorbitant computational cost. Recent surveys of
spatiotemporal data mining need update due to rapid growth. In addition, they
did not adequately survey parallel techniques for spatiotemporal data mining.
This paper provides a more up-to-date survey of spatiotemporal data mining
methods. Furthermore, it has a detailed survey of parallel formulations of
spatiotemporal data mining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Your Sparse Neural Network Better with Any Mask. (arXiv:2206.12755v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12755">
<div class="article-summary-box-inner">
<span><p>Pruning large neural networks to create high-quality, independently trainable
sparse masks, which can maintain similar performance to their dense
counterparts, is very desirable due to the reduced space and time complexity.
As research effort is focused on increasingly sophisticated pruning methods
that leads to sparse subnetworks trainable from the scratch, we argue for an
orthogonal, under-explored theme: improving training techniques for pruned
sub-networks, i.e. sparse training. Apart from the popular belief that only the
quality of sparse masks matters for sparse training, in this paper we
demonstrate an alternative opportunity: one can \textit{carefully customize the
sparse training techniques to deviate from the default dense network training
protocols}, consisting of introducing ``ghost" neurons and skip connections at
the early stage of training, and strategically modifying the initialization as
well as labels. Our new sparse training recipe is generally applicable to
improving training from scratch with various sparse masks. By adopting our
newly curated techniques, we demonstrate significant performance gains across
various popular datasets (CIFAR-10, CIFAR-100, TinyImageNet), architectures
(ResNet-18/32/104, Vgg16, MobileNet), and sparse mask options (lottery ticket,
SNIP/GRASP, SynFlow, or even randomly pruning), compared to the default
training protocols, especially at high sparsity levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation. (arXiv:2206.12772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12772">
<div class="article-summary-box-inner">
<span><p>We present a simple yet effective self-supervised framework for audio-visual
representation learning, to localize the sound source in videos. To understand
what enables to learn useful representations, we systematically investigate the
effects of data augmentations, and reveal that (1) composition of data
augmentations plays a critical role, {\em i.e.}~explicitly encouraging the
audio-visual representations to be invariant to various transformations~({\em
transformation invariance}); (2) enforcing geometric consistency substantially
improves the quality of learned representations, {\em i.e.}~the detected sound
source should follow the same transformation applied on input video
frames~({\em transformation equivariance}). Extensive experiments demonstrate
that our model significantly outperforms previous methods on two sound
localization benchmarks, namely, Flickr-SoundNet and VGG-Sound. Additionally,
we also evaluate audio retrieval and cross-modal retrieval tasks. In both
cases, our self-supervised models demonstrate superior retrieval performances,
even competitive with the supervised approach in audio retrieval. This reveals
the proposed framework learns strong multi-modal representations that are
beneficial to sound localisation and generalization to further applications.
\textit{All codes will be available}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation with Representative Teacher Keys Based on Attention Mechanism for Image Classification Model Compression. (arXiv:2206.12788v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12788">
<div class="article-summary-box-inner">
<span><p>With the improvement of AI chips (e.g., GPU, TPU, and NPU) and the fast
development of internet of things (IoTs), some powerful deep neural networks
(DNNs) are usually composed of millions or even hundreds of millions of
parameters, which may not be suitable to be directly deployed on low
computation and low capacity units (e.g., edge devices). Recently, knowledge
distillation (KD) has been recognized as one of the effective method of model
compression to decrease the model parameters. The main concept of KD is to
extract useful information from the feature maps of a large model (i.e.,
teacher model) as a reference to successfully train a small model (i.e.,
student model) which model size is much smaller than the teacher one. Although
many KD-based methods have been proposed to utilize the information from the
feature maps of intermediate layers in teacher model, however, most of them did
not consider the similarity of feature maps between teacher model and student
model, which may let student model learn useless information. Inspired by
attention mechanism, we propose a novel KD method called representative teacher
key (RTK) that not only consider the similarity of feature maps but also filter
out the useless information to improve the performance of the target student
model. In the experiments, we validate our proposed method with several
backbone networks (e.g., ResNet and WideResNet) and datasets (e.g., CIFAR10,
CIFAR100, SVHN, and CINIC10). The results show that our proposed RTK can
effectively improve the classification accuracy of the state-of-the-art
attention-based KD method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTMQ: Cyclic Training of Convolutional Neural Networks with Multiple Quantization Steps. (arXiv:2206.12794v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12794">
<div class="article-summary-box-inner">
<span><p>This paper proposes a training method having multiple cyclic training for
achieving enhanced performance in low-bit quantized convolutional neural
networks (CNNs). Quantization is a popular method for obtaining lightweight
CNNs, where the initialization with a pretrained model is widely used to
overcome degraded performance in low-resolution quantization. However, large
quantization errors between real values and their low-bit quantized ones cause
difficulties in achieving acceptable performance for complex networks and large
datasets. The proposed training method softly delivers the knowledge of
pretrained models to low-bit quantized models in multiple quantization steps.
In each quantization step, the trained weights of a model are used to
initialize the weights of the next model with the quantization bit depth
reduced by one. With small change of the quantization bit depth, the
performance gap can be bridged, thus providing better weight initialization. In
cyclic training, after training a low-bit quantized model, its trained weights
are used in the initialization of its accurate model to be trained. By using
better training ability of the accurate model in an iterative manner, the
proposed method can produce enhanced trained weights for the low-bit quantized
model in each cycle. Notably, the training method can advance Top-1 and Top-5
accuracies of the binarized ResNet-18 on the ImageNet dataset by 5.80% and
6.85%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Instance Learning with Mixed Supervision in Gleason Grading. (arXiv:2206.12798v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12798">
<div class="article-summary-box-inner">
<span><p>With the development of computational pathology, deep learning methods for
Gleason grading through whole slide images (WSIs) have excellent prospects.
Since the size of WSIs is extremely large, the image label usually contains
only slide-level label or limited pixel-level labels. The current mainstream
approach adopts multi-instance learning to predict Gleason grades. However,
some methods only considering the slide-level label ignore the limited
pixel-level labels containing rich local information. Furthermore, the method
of additionally considering the pixel-level labels ignores the inaccuracy of
pixel-level labels. To address these problems, we propose a mixed supervision
Transformer based on the multiple instance learning framework. The model
utilizes both slide-level label and instance-level labels to achieve more
accurate Gleason grading at the slide level. The impact of inaccurate
instance-level labels is further reduced by introducing an efficient random
masking strategy in the mixed supervision training process. We achieve the
state-of-the-art performance on the SICAPv2 dataset, and the visual analysis
shows the accurate prediction results of instance level. The source code is
available at https://github.com/bianhao123/Mixed_supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparison of AIS, X-Band Marine Radar Systems and Camera Surveillance Systems in the Collection of Tracking Data. (arXiv:2206.12809v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12809">
<div class="article-summary-box-inner">
<span><p>Maritime traffic has increased in recent years, especially in terms of
seaborne trade. To ensure safety, security, and protection of the marine
environment, several systems have been deployed. To overcome some of their
inconveniences, the collected data is typically fused. The fused data is used
for various purposes, one of our interest is target tracking. The most relevant
systems in that context are AIS and X-band marine radar. Many works consider
that visual data provided by camera surveillance systems enable additional
advantages. Therefore, many tracking algorithms using visual data (images) have
been developed. Yet, there is little emphasis on the reasons making the
integration of camera systems important. Thus, our main aim in this paper is to
analyze the aforementioned surveillance systems for target tracking and
conclude some of the maritime security improvements resulted from the
integration of cameras to the overall maritime surveillance system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breast Cancer Classification using Deep Learned Features Boosted with Handcrafted Features. (arXiv:2206.12815v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12815">
<div class="article-summary-box-inner">
<span><p>Breast cancer is one of the leading causes of death among women across the
globe. It is difficult to treat if detected at advanced stages, however, early
detection can significantly increase chances of survival and improves lives of
millions of women. Given the widespread prevalence of breast cancer, it is of
utmost importance for the research community to come up with the framework for
early detection, classification and diagnosis. Artificial intelligence research
community in coordination with medical practitioners are developing such
frameworks to automate the task of detection. With the surge in research
activities coupled with availability of large datasets and enhanced
computational powers, it expected that AI framework results will help even more
clinicians in making correct predictions. In this article, a novel framework
for classification of breast cancer using mammograms is proposed. The proposed
framework combines robust features extracted from novel Convolutional Neural
Network (CNN) features with handcrafted features including HOG (Histogram of
Oriented Gradients) and LBP (Local Binary Pattern). The obtained results on
CBIS-DDSM dataset exceed state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer. (arXiv:2206.12837v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12837">
<div class="article-summary-box-inner">
<span><p>This paper reports our solution for MultiMedia ViCo 2022 Conversational Head
Generation Challenge, which aims to generate vivid face-to-face conversation
videos based on audio and reference images. Our solution focuses on training a
generalized audio-to-head driver using regularization and assembling a high
visual quality renderer. We carefully tweak the audio-to-behavior model and
post-process the generated video using our foreground-background fusion module.
We get first place in the listening head generation track and second place in
the talking head generation track in the official ranking. Our code will be
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval. (arXiv:2206.12845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12845">
<div class="article-summary-box-inner">
<span><p>Seas of videos are uploaded daily with the popularity of social channels;
thus, retrieving the most related video contents with user textual queries
plays a more crucial role. Most methods consider only one joint embedding space
between global visual and textual features without considering the local
structures of each modality. Some other approaches consider multiple embedding
spaces consisting of global and local features separately, ignoring rich
inter-modality correlations.
</p>
<p>We propose a novel mixture-of-expert transformer RoME that disentangles the
text and the video into three levels; the roles of spatial contexts, temporal
contexts, and object contexts. We utilize a transformer-based attention
mechanism to fully exploit visual and text embeddings at both global and local
levels with mixture-of-experts for considering inter-modalities and structures'
correlations. The results indicate that our method outperforms the
state-of-the-art methods on the YouCook2 and MSR-VTT datasets, given the same
visual backbone without pre-training. Finally, we conducted extensive ablation
studies to elucidate our design choices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Role Aware Correlation Transformer for Text to Video Retrieval. (arXiv:2206.12849v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12849">
<div class="article-summary-box-inner">
<span><p>With the emergence of social media, voluminous video clips are uploaded every
day, and retrieving the most relevant visual content with a language query
becomes critical. Most approaches aim to learn a joint embedding space for
plain textual and visual contents without adequately exploiting their
intra-modality structures and inter-modality correlations. This paper proposes
a novel transformer that explicitly disentangles the text and video into
semantic roles of objects, spatial contexts and temporal contexts with an
attention scheme to learn the intra- and inter-role correlations among the
three roles to discover discriminative features for matching at different
levels. The preliminary results on popular YouCook2 indicate that our approach
surpasses a current state-of-the-art method, with a high margin in all metrics.
It also overpasses two SOTA methods in terms of two metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Aesthetics Assessment Using Graph Attention Network. (arXiv:2206.12869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12869">
<div class="article-summary-box-inner">
<span><p>Aspect ratio and spatial layout are two of the principal factors determining
the aesthetic value of a photograph. But, incorporating these into the
traditional convolution-based frameworks for the task of image aesthetics
assessment is problematic. The aspect ratio of the photographs gets distorted
while they are resized/cropped to a fixed dimension to facilitate training
batch sampling. On the other hand, the convolutional filters process
information locally and are limited in their ability to model the global
spatial layout of a photograph. In this work, we present a two-stage framework
based on graph neural networks and address both these problems jointly. First,
we propose a feature-graph representation in which the input image is modelled
as a graph, maintaining its original aspect ratio and resolution. Second, we
propose a graph neural network architecture that takes this feature-graph and
captures the semantic relationship between the different regions of the input
image using visual attention. Our experiments show that the proposed framework
advances the state-of-the-art results in aesthetic score regression on the
Aesthetic Visual Analysis (AVA) benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FingerGAN: A Constrained Fingerprint Generation Scheme for Latent Fingerprint Enhancement. (arXiv:2206.12885v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12885">
<div class="article-summary-box-inner">
<span><p>Latent fingerprint enhancement is an essential pre-processing step for latent
fingerprint identification. Most latent fingerprint enhancement methods try to
restore corrupted gray ridges/valleys. In this paper, we propose a new method
that formulates the latent fingerprint enhancement as a constrained fingerprint
generation problem within a generative adversarial network (GAN) framework. We
name the proposed network as FingerGAN. It can enforce its generated
fingerprint (i.e, enhanced latent fingerprint) indistinguishable from the
corresponding ground-truth instance in terms of the fingerprint skeleton map
weighted by minutia locations and the orientation field regularized by the
FOMFE model. Because minutia is the primary feature for fingerprint recognition
and minutia can be retrieved directly from the fingerprint skeleton map, we
offer a holistic framework which can perform latent fingerprint enhancement in
the context of directly optimizing minutia information. This will help improve
latent fingerprint identification performance significantly. Experimental
results on two public latent fingerprint databases demonstrate that our method
outperforms the state of the arts significantly. The codes will be available
for non-commercial purposes from
\url{https://github.com/HubYZ/LatentEnhancement}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Woodscape Fisheye Object Detection for Autonomous Driving -- CVPR 2022 OmniCV Workshop Challenge. (arXiv:2206.12912v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12912">
<div class="article-summary-box-inner">
<span><p>Object detection is a comprehensively studied problem in autonomous driving.
However, it has been relatively less explored in the case of fisheye cameras.
The strong radial distortion breaks the translation invariance inductive bias
of Convolutional Neural Networks. Thus, we present the WoodScape fisheye object
detection challenge for autonomous driving which was held as part of the CVPR
2022 Workshop on Omnidirectional Computer Vision (OmniCV). This is one of the
first competitions focused on fisheye camera object detection. We encouraged
the participants to design models which work natively on fisheye images without
rectification. We used CodaLab to host the competition based on the publicly
available WoodScape fisheye dataset. In this paper, we provide a detailed
analysis on the competition which attracted the participation of 120 global
teams and a total of 1492 submissions. We briefly discuss the details of the
winning methods and analyze their qualitative and quantitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Anomaly Detection via Prediction Network with Enhanced Spatio-Temporal Memory Exchange. (arXiv:2206.12914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12914">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection is a challenging task because most anomalies are
scarce and non-deterministic. Many approaches investigate the reconstruction
difference between normal and abnormal patterns, but neglect that anomalies do
not necessarily correspond to large reconstruction errors. To address this
issue, we design a Convolutional LSTM Auto-Encoder prediction framework with
enhanced spatio-temporal memory exchange using bi-directionalilty and a
higher-order mechanism. The bi-directional structure promotes learning the
temporal regularity through forward and backward predictions. The unique
higher-order mechanism further strengthens spatial information interaction
between the encoder and the decoder. Considering the limited receptive fields
in Convolutional LSTMs, we also introduce an attention module to highlight
informative features for prediction. Anomalies are eventually identified by
comparing the frames with their corresponding predictions. Evaluations on three
popular benchmarks show that our framework outperforms most existing
prediction-based anomaly detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parametric Style Transfer. (arXiv:2206.12921v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12921">
<div class="article-summary-box-inner">
<span><p>Recent feed-forward neural methods of arbitrary image style transfer mainly
utilized encoded feature map upto its second-order statistics, i.e., linearly
transformed the encoded feature map of a content image to have the same mean
and variance (or covariance) of a target style feature map. In this work, we
extend the second-order statistical feature matching into a general
distribution matching based on the understanding that style of an image is
represented by the distribution of responses from receptive fields. For this
generalization, first, we propose a new feature transform layer that exactly
matches the feature map distribution of content image into that of target style
image. Second, we analyze the recent style losses consistent with our new
feature transform layer to train a decoder network which generates a style
transferred image from the transformed feature map. Based on our experimental
results, it is proven that the stylized images obtained with our method are
more similar with the target style images in all existing style measures
without losing content clearness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Activity Localisation with Uncertainties in Temporal Boundary. (arXiv:2206.12923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12923">
<div class="article-summary-box-inner">
<span><p>Current methods for video activity localisation over time assume implicitly
that activity temporal boundaries labelled for model training are determined
and precise. However, in unscripted natural videos, different activities mostly
transit smoothly, so that it is intrinsically ambiguous to determine in
labelling precisely when an activity starts and ends over time. Such
uncertainties in temporal labelling are currently ignored in model training,
resulting in learning mis-matched video-text correlation with poor
generalisation in test. In this work, we solve this problem by introducing
Elastic Moment Bounding (EMB) to accommodate flexible and adaptive activity
temporal boundaries towards modelling universally interpretable video-text
correlation with tolerance to underlying temporal uncertainties in pre-fixed
annotations. Specifically, we construct elastic boundaries adaptively by mining
and discovering frame-wise temporal endpoints that can maximise the alignment
between video segments and query sentences. To enable both more robust matching
(segment content attention) and more accurate localisation (segment elastic
boundaries), we optimise the selection of frame-wise endpoints subject to
segment-wise contents by a novel Guided Attention mechanism. Extensive
experiments on three video activity localisation benchmarks demonstrate
compellingly the EMB's advantages over existing methods without modelling
uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer for Contrastive Clustering. (arXiv:2206.12925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12925">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT) has shown its advantages over the convolutional
neural network (CNN) with its ability to capture global long-range dependencies
for visual representation learning. Besides ViT, contrastive learning is
another popular research topic recently. While previous contrastive learning
works are mostly based on CNNs, some latest studies have attempted to jointly
model the ViT and the contrastive learning for enhanced self-supervised
learning. Despite the considerable progress, these combinations of ViT and
contrastive learning mostly focus on the instance-level contrastiveness, which
often overlook the contrastiveness of the global clustering structures and also
lack the ability to directly learn the clustering result (e.g., for images). In
view of this, this paper presents an end-to-end deep image clustering approach
termed Vision Transformer for Contrastive Clustering (VTCC), which for the
first time, to the best of our knowledge, unifies the Transformer and the
contrastive learning for the image clustering task. Specifically, with two
random augmentations performed on each image in a mini-batch, we utilize a ViT
encoder with two weight-sharing views as the backbone to learn the
representations for the augmented samples. To remedy the potential instability
of the ViT, we incorporate a convolutional stem, which uses multiple stacked
small convolutions instead of a big convolution in the patch projection layer,
to split each augmented sample into a sequence of patches. With representations
learned via the backbone, an instance projector and a cluster projector are
further utilized for the instance-level contrastive learning and the global
clustering structure learning, respectively. Extensive experiments on eight
image datasets demonstrate the stability (during the training-from-scratch) and
the superiority (in clustering performance) of VTCC over the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVBR-NET: A Non-Blind Spatially Varying Defocus Blur Removal Network. (arXiv:2206.12930v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12930">
<div class="article-summary-box-inner">
<span><p>Defocus blur is a physical consequence of the optical sensors used in most
cameras. Although it can be used as a photographic style, it is commonly viewed
as an image degradation modeled as the convolution of a sharp image with a
spatially-varying blur kernel. Motivated by the advance of blur estimation
methods in the past years, we propose a non-blind approach for image deblurring
that can deal with spatially-varying kernels. We introduce two encoder-decoder
sub-networks that are fed with the blurry image and the estimated blur map,
respectively, and produce as output the deblurred (deconvolved) image. Each
sub-network presents several skip connections that allow data propagation from
layers spread apart, and also inter-subnetwork skip connections that ease the
communication between the modules. The network is trained with synthetically
blur kernels that are augmented to emulate blur maps produced by existing blur
estimation methods, and our experimental results show that our method works
well when combined with a variety of blur estimation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Detection and Tracking with Autonomous UAV. (arXiv:2206.12941v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12941">
<div class="article-summary-box-inner">
<span><p>In this paper, a combat Unmanned Air Vehicle (UAV) is modeled in the
simulation environment. The rotary wing UAV is successfully performed various
tasks such as locking on the targets, tracking, and sharing the relevant data
with surrounding vehicles. Different software technologies such as API
communication, ground control station configuration, autonomous movement
algorithms, computer vision, and deep learning are employed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Feature Augmentation with Adaptive Class Activation Mapping. (arXiv:2206.12943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12943">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end-trainable feature augmentation module built for
image classification that extracts and exploits multi-view local features to
boost model performance. Different from using global average pooling (GAP) to
extract vectorized features from only the global view, we propose to sample and
ensemble diverse multi-view local features to improve model robustness. To
sample class-representative local features, we incorporate a simple auxiliary
classifier head (comprising only one 1$\times$1 convolutional layer) which
efficiently and adaptively attends to class-discriminative local regions of
feature maps via our proposed AdaCAM (Adaptive Class Activation Mapping).
Extensive experiments demonstrate consistent and noticeable performance gains
achieved by our multi-view feature augmentation module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFT-VO: Asynchronous Fusion Transformers for Multi-View Visual Odometry Estimation. (arXiv:2206.12946v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12946">
<div class="article-summary-box-inner">
<span><p>Motion estimation approaches typically employ sensor fusion techniques, such
as the Kalman Filter, to handle individual sensor failures. More recently, deep
learning-based fusion approaches have been proposed, increasing the performance
and requiring less model-specific implementations. However, current deep fusion
approaches often assume that sensors are synchronised, which is not always
practical, especially for low-cost hardware. To address this limitation, in
this work, we propose AFT-VO, a novel transformer-based sensor fusion
architecture to estimate VO from multiple sensors. Our framework combines
predictions from asynchronous multi-view cameras and accounts for the time
discrepancies of measurements coming from different sources.
</p>
<p>Our approach first employs a Mixture Density Network (MDN) to estimate the
probability distributions of the 6-DoF poses for every camera in the system.
Then a novel transformer-based fusion module, AFT-VO, is introduced, which
combines these asynchronous pose estimations, along with their confidences.
More specifically, we introduce Discretiser and Source Encoding techniques
which enable the fusion of multi-source asynchronous signals.
</p>
<p>We evaluate our approach on the popular nuScenes and KITTI datasets. Our
experiments demonstrate that multi-view fusion for VO estimation provides
robust and accurate trajectories, outperforming the state of the art in both
challenging weather and lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonwatertight Mesh Reconstruction. (arXiv:2206.12952v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12952">
<div class="article-summary-box-inner">
<span><p>Reconstructing 3D non-watertight mesh from an unoriented point cloud is an
unexplored area in computer vision and computer graphics. In this project, we
tried to tackle this problem by extending the learning-based watertight mesh
reconstruction pipeline presented in the paper 'Shape as Points'. The core of
our approach is to cast the problem as a semantic segmentation problem that
identifies the region in the 3D volume where the mesh surface lies and extracts
the surfaces from the detected regions. Our approach achieves compelling
results compared to the baseline techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Szloca: towards a framework for full 3D tracking through a single camera in context of interactive arts. (arXiv:2206.12958v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12958">
<div class="article-summary-box-inner">
<span><p>Realtime virtual data of objects and human presence in a large area holds a
valuable key in enabling many experiences and applications in various
industries and with exponential rise in the technological development of
artificial intelligence, computer vision has expanded the possibilities of
tracking and classifying things through just video inputs, which is also
surpassing the limitations of most popular and common hardware setups known
traditionally to detect human pose and position, such as low field of view and
limited tracking capacity. The benefits of using computer vision in application
development is large as it augments traditional input sources (like video
streams) and can be integrated in many environments and platforms. In the
context of new media interactive arts, based on physical movements and
expanding over large areas or gallaries, this research presents a novel way and
a framework towards obtaining data and virtual representation of objects/people
- such as three-dimensional positions, skeltons/pose and masks from a single
rgb camera. Looking at the state of art through some recent developments and
building on prior research in the field of computer vision, the paper also
proposes an original method to obtain three dimensional position data from
monocular images, the model does not rely on complex training of computer
vision systems but combines prior computer vision research and adds a capacity
to represent z depth, ieto represent a world position in 3 axis from a 2d input
source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic PolarGMM: Unsupervised Cluster Learning of Very Noisy Projection Images of Unknown Pose. (arXiv:2206.12959v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12959">
<div class="article-summary-box-inner">
<span><p>A crucial step in single particle analysis (SPA) of cryogenic electron
microscopy (Cryo-EM), 2D classification and alignment takes a collection of
noisy particle images to infer orientations and group similar images together.
Averaging these aligned and clustered noisy images produces a set of clean
images, ready for further analysis such as 3D reconstruction. Fourier-Bessel
steerable principal component analysis (FBsPCA) enables an efficient,
adaptable, low-rank rotation operator. We extend the FBsPCA to additionally
handle translations. In this extended FBsPCA representation, we use a
probabilistic polar-coordinate Gaussian mixture model to learn soft clusters in
an unsupervised fashion using an expectation maximization (EM) algorithm. The
obtained rotational clusters are thus additionally robust to the presence of
pairwise alignment imperfections. Multiple benchmarks from simulated Cryo-EM
datasets show probabilistic PolarGMM's improved performance in comparisons with
standard single-particle Cryo-EM tools, EMAN2 and RELION, in terms of various
clustering metrics and alignment errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Healing Robust Neural Networks via Closed-Loop Control. (arXiv:2206.12963v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12963">
<div class="article-summary-box-inner">
<span><p>Despite the wide applications of neural networks, there have been increasing
concerns about their vulnerability issue. While numerous attack and defense
techniques have been developed, this work investigates the robustness issue
from a new angle: can we design a self-healing neural network that can
automatically detect and fix the vulnerability issue by itself? A typical
self-healing mechanism is the immune system of a human body. This
biology-inspired idea has been used in many engineering designs but is rarely
investigated in deep learning. This paper considers the post-training
self-healing of a neural network, and proposes a closed-loop control
formulation to automatically detect and fix the errors caused by various
attacks or perturbations. We provide a margin-based analysis to explain how
this formulation can improve the robustness of a classifier. To speed up the
inference of the proposed self-healing network, we solve the control problem
via improving the Pontryagin Maximum Principle-based solver. Lastly, we present
an error estimation of the proposed framework for neural networks with
nonlinear activation functions. We validate the performance on several network
architectures against various perturbations. Since the self-healing method does
not need a-priori information about data perturbations/attacks, it can handle a
broad class of unforeseen perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLCap: Vision-Language with Contrastive Learning for Coherent Video Paragraph Captioning. (arXiv:2206.12972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12972">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage the human perceiving process, that involves vision
and language interaction, to generate a coherent paragraph description of
untrimmed videos. We propose vision-language (VL) features consisting of two
modalities, i.e., (i) vision modality to capture global visual content of the
entire scene and (ii) language modality to extract scene elements description
of both human and non-human objects (e.g. animals, vehicles, etc), visual and
non-visual elements (e.g. relations, activities, etc). Furthermore, we propose
to train our proposed VLCap under a contrastive learning VL loss. The
experiments and ablation studies on ActivityNet Captions and YouCookII datasets
show that our VLCap outperforms existing SOTA methods on both accuracy and
diversity metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Schizophrenia with 3D Structural Brain MRI Using Deep Learning. (arXiv:2206.12980v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12980">
<div class="article-summary-box-inner">
<span><p>Schizophrenia is a chronic neuropsychiatric disorder that causes distinct
structural alterations within the brain. We hypothesize that deep learning
applied to a structural neuroimaging dataset could detect disease-related
alteration and improve classification and diagnostic accuracy. We tested this
hypothesis using a single, widely available, and conventional T1-weighted MRI
scan, from which we extracted the 3D whole-brain structure using standard
post-processing methods. A deep learning model was then developed, optimized,
and evaluated on three open datasets with T1-weighted MRI scans of patients
with schizophrenia. Our proposed model outperformed the benchmark model, which
was also trained with structural MR images using a 3D CNN architecture. Our
model is capable of almost perfectly (area under the ROC curve = 0.987)
distinguishing schizophrenia patients from healthy controls on unseen
structural MRI scans. Regional analysis localized subcortical regions and
ventricles as the most predictive brain regions. Subcortical structures serve a
pivotal role in cognitive, affective, and social functions in humans, and
structural abnormalities of these regions have been associated with
schizophrenia. Our finding corroborates that schizophrenia is associated with
widespread alterations in subcortical brain structure and the subcortical
structural information provides prominent features in diagnostic
classification. Together, these results further demonstrate the potential of
deep learning to improve schizophrenia diagnosis and identify its structural
neuroimaging signatures from a single, standard T1-weighted brain MRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-point dimensionality reduction to improve projection layout reliability. (arXiv:2101.06224v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06224">
<div class="article-summary-box-inner">
<span><p>In ordinary Dimensionality Reduction (DR), each data instance in a high
dimensional space (original space), or on a distance matrix denoting original
space distances, is mapped to (projected onto) one point in a low dimensional
space (visual space), building a layout of projected points trying to preserve
as much as possible some property of data such as distances, neighbourhood
relationships, and/or topology structures, with the ultimate goal of
approximating semantic properties of data with preserved geometric properties
or topology structures in visual space. In this paper, the concept of
Multi-point Dimensionality Reduction is elaborated on where each data instance
can be mapped to (projected onto) possibly more than one point in visual space
by providing the first general solution (algorithm) for it as a move in the
direction of improving reliablity, usability and interpretability of
dimensionality reduction. Furthermore by allowing the points in visual space to
be split into two layers while maintaining the possibility of having more than
one projection (mapping) per data instance , the benefit of separating more
reliable points from less reliable points is dicussed notwithstanding the
effort to improve less reliable points. The proposed solution (algorithm) in
this paper, named Layered Vertex Splitting Data Embedding (LVSDE), is built
upon and extends a combination of ordinary DR and graph drawing techniques.
Based on the experiments of this paper on some data sets, the particular
proposed algorithm (LVSDE) practically outperforms popular ordinary DR methods
visually (semantics, group separation, subgroup detection or combinational
group detection) in a way that is easily explainable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06450">
<div class="article-summary-box-inner">
<span><p>We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on Image to Sequence
architecture, it can extract text present in an image and then sequence it
correctly without imposing any constraints regarding orientation, layout and
size of text and non-text. Further, it can also be trained to generate
auxiliary markup related to formatting, layout and content. We use character
level vocabulary, thereby enabling language and terminology of any subject. The
model achieves a new state-of-art in paragraph level recognition on the IAM
dataset. When evaluated on scans of real world handwritten free form test
answers - beset with curved and slanted lines, drawings, tables, math,
chemistry and other symbols - it performs better than all commercially
available HTR cloud APIs. It is deployed in production as part of a commercial
web application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Metric Learning for Few-Shot Image Classification: A Review of Recent Developments. (arXiv:2105.08149v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08149">
<div class="article-summary-box-inner">
<span><p>Few-shot image classification is a challenging problem that aims to achieve
the human level of recognition based only on a small number of training images.
One main solution to few-shot image classification is deep metric learning.
These methods, by classifying unseen samples according to their distances to
few seen samples in an embedding space learned by powerful deep neural
networks, can avoid overfitting to few training images in few-shot image
classification and have achieved the state-of-the-art performance. In this
paper, we provide an up-to-date review of deep metric learning methods for
few-shot image classification from 2018 to 2022 and categorize them into three
groups according to three stages of metric learning, namely learning feature
embeddings, learning class representations, and learning distance measures.
With this taxonomy, we identify the novelties of different methods and problems
they face. We conclude this review with a discussion on current challenges and
future trends in few-shot image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01100">
<div class="article-summary-box-inner">
<span><p>During lung radiotherapy, the position of infrared reflective objects on the
chest can be recorded to estimate the tumor location. However, radiotherapy
systems have a latency inherent to robot control limitations that impedes the
radiation delivery precision. Prediction with online learning of recurrent
neural networks (RNN) allows for adaptation to non-stationary respiratory
signals, but classical methods such as RTRL and truncated BPTT are respectively
slow and biased. This study investigates the capabilities of unbiased online
recurrent optimization (UORO) to forecast respiratory motion and enhance safety
in lung radiotherapy.
</p>
<p>We used 9 observation records of the 3D position of 3 external markers on the
chest and abdomen of healthy individuals breathing during intervals from 73s to
222s. The sampling frequency was 10Hz, and the amplitudes of the recorded
trajectories range from 6mm to 40mm in the superior-inferior direction. We
forecast the 3D location of each marker simultaneously with a horizon value
between 0.1s and 2.0s, using an RNN trained with UORO. We compare its
performance with an RNN trained with RTRL, LMS, and offline linear regression.
We provide closed-form expressions for quantities involved in the loss gradient
calculation in UORO, thereby making its implementation efficient. Training and
cross-validation were performed during the first minute of each sequence.
</p>
<p>On average over the horizon values considered and the 9 sequences, UORO
achieves the lowest root-mean-square (RMS) error and maximum error among the
compared algorithms. These errors are respectively equal to 1.3mm and 8.8mm,
and the prediction time per time step was lower than 2.8ms (Dell Intel core
i9-9900K 3.60 GHz). Linear regression has the lowest RMS error for the horizon
values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,
and UORO for horizon values greater than 0.6s.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Scale Feature Aggregation by Cross-Scale Pixel-to-Region Relation Operation for Semantic Segmentation. (arXiv:2106.01744v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01744">
<div class="article-summary-box-inner">
<span><p>Exploiting multi-scale features has shown great potential in tackling
semantic segmentation problems. The aggregation is commonly done with sum or
concatenation (concat) followed by convolutional (conv) layers. However, it
fully passes down the high-level context to the following hierarchy without
considering their interrelation. In this work, we aim to enable the low-level
feature to aggregate the complementary context from adjacent high-level feature
maps by a cross-scale pixel-to-region relation operation. We leverage
cross-scale context propagation to make the long-range dependency capturable
even by the high-resolution low-level features. To this end, we employ an
efficient feature pyramid network to obtain multi-scale features. We propose a
Relational Semantics Extractor (RSE) and Relational Semantics Propagator (RSP)
for context extraction and propagation respectively. Then we stack several RSP
into an RSP head to achieve the progressive top-down distribution of the
context. Experiment results on two challenging datasets Cityscapes and COCO
demonstrate that the RSP head performs competitively on both semantic
segmentation and panoptic segmentation with high efficiency. It outperforms
DeeplabV3 [1] by 0.7% with 75% fewer FLOPs (multiply-adds) in the semantic
segmentation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06959">
<div class="article-summary-box-inner">
<span><p>The discovery of the disentanglement properties of the latent space in GANs
motivated a lot of research to find the semantically meaningful directions on
it. In this paper, we suggest that the disentanglement property is closely
related to the geometry of the latent space. In this regard, we propose an
unsupervised method for finding the semantic-factorizing directions on the
intermediate latent space of GANs based on the local geometry. Intuitively, our
proposed method, called Local Basis, finds the principal variation of the
latent space in the neighborhood of the base latent variable. Experimental
results show that the local principal variation corresponds to the semantic
factorization and traversing along it provides strong robustness to image
traversal. Moreover, we suggest an explanation for the limited success in
finding the global traversal directions in the latent space, especially W-space
of StyleGAN2. We show that W-space is warped globally by comparing the local
geometry, discovered from Local Basis, through the metric on Grassmannian
Manifold. The global warpage implies that the latent space is not well-aligned
globally and therefore the global traversal directions are bound to show
limited success on it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07847">
<div class="article-summary-box-inner">
<span><p>While unbiased machine learning models are essential for many applications,
bias is a human-defined concept that can vary across tasks. Given only
input-label pairs, algorithms may lack sufficient information to distinguish
stable (causal) features from unstable (spurious) features. However, related
tasks often share similar biases -- an observation we may leverage to develop
stable classifiers in the transfer setting. In this work, we explicitly inform
the target classifier about unstable features in the source tasks.
Specifically, we derive a representation that encodes the unstable features by
contrasting different data environments in the source task. We achieve
robustness by clustering data of the target task according to this
representation and minimizing the worst-case risk across these clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task for both synthetically generated environments and real-world environments.
Our code is available at https://github.com/YujiaBao/Tofu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07933">
<div class="article-summary-box-inner">
<span><p>Unprecedented access to multi-temporal satellite imagery has opened new
perspectives for a variety of Earth observation tasks. Among them,
pixel-precise panoptic segmentation of agricultural parcels has major economic
and environmental implications. While researchers have explored this problem
for single images, we argue that the complex temporal patterns of crop
phenology are better addressed with temporal sequences of images. In this
paper, we present the first end-to-end, single-stage method for panoptic
segmentation of Satellite Image Time Series (SITS). This module can be combined
with our novel image sequence encoding network which relies on temporal
self-attention to extract rich and adaptive multi-scale spatio-temporal
features. We also introduce PASTIS, the first open-access SITS dataset with
panoptic annotations. We demonstrate the superiority of our encoder for
semantic segmentation against multiple competing architectures, and set up the
first state-of-the-art of panoptic segmentation of SITS. Our implementation and
PASTIS are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeoCLR: Georeference Contrastive Learning for Efficient Seafloor Image Interpretation. (arXiv:2108.06421v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06421">
<div class="article-summary-box-inner">
<span><p>This paper describes Georeference Contrastive Learning of visual
Representation (GeoCLR) for efficient training of deep-learning Convolutional
Neural Networks (CNNs). The method leverages georeference information by
generating a similar image pair using images taken of nearby locations, and
contrasting these with an image pair that is far apart. The underlying
assumption is that images gathered within a close distance are more likely to
have similar visual appearance, where this can be reasonably satisfied in
seafloor robotic imaging applications where image footprints are limited to
edge lengths of a few metres and are taken so that they overlap along a
vehicle's trajectory, whereas seafloor substrates and habitats have patch sizes
that are far larger. A key advantage of this method is that it is
self-supervised and does not require any human input for CNN training. The
method is computationally efficient, where results can be generated between
dives during multi-day AUV missions using computational resources that would be
accessible during most oceanic field trials. We apply GeoCLR to habitat
classification on a dataset that consists of ~86k images gathered using an
Autonomous Underwater Vehicle (AUV). We demonstrate how the latent
representations generated by GeoCLR can be used to efficiently guide human
annotation efforts, where the semi-supervised framework improves classification
accuracy by an average of 10.2% compared to the state-of-the-art SimCLR using
the same CNN and equivalent number of human annotations for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNetFormer: A UNet-like Transformer for Efficient Semantic Segmentation of Remote Sensing Urban Scene Imagery. (arXiv:2109.08937v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08937">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of remotely sensed urban scene images is required in a
wide range of practical applications, such as land cover mapping, urban change
detection, environmental protection, and economic assessment.Driven by rapid
developments in deep learning technologies, the convolutional neural network
(CNN) has dominated semantic segmentation for many years. CNN adopts
hierarchical feature representation, demonstrating strong capabilities for
local information extraction. However, the local property of the convolution
layer limits the network from capturing the global context. Recently, as a hot
topic in the domain of computer vision, Transformer has demonstrated its great
potential in global information modelling, boosting many vision-related tasks
such as image classification, object detection, and particularly semantic
segmentation. In this paper, we propose a Transformer-based decoder and
construct a UNet-like Transformer (UNetFormer) for real-time urban scene
segmentation. For efficient segmentation, the UNetFormer selects the
lightweight ResNet18 as the encoder and develops an efficient global-local
attention mechanism to model both global and local information in the decoder.
Extensive experiments reveal that our method not only runs faster but also
produces higher accuracy compared with state-of-the-art lightweight models.
Specifically, the proposed UNetFormer achieved 67.8% and 52.4% mIoU on the
UAVid and LoveDA datasets, respectively, while the inference speed can achieve
up to 322.4 FPS with a 512x512 input on a single NVIDIA GTX 3090 GPU. In
further exploration, the proposed Transformer-based decoder combined with a
Swin Transformer encoder also achieves the state-of-the-art result (91.3% F1
and 84.1% mIoU) on the Vaihingen dataset. The source code will be freely
available at https://github.com/WangLibo1995/GeoSeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural network relief: a pruning algorithm based on neural activity. (arXiv:2109.10795v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10795">
<div class="article-summary-box-inner">
<span><p>Current deep neural networks (DNNs) are overparameterized and use most of
their neuronal connections during inference for each task. The human brain,
however, developed specialized regions for different tasks and performs
inference with a small fraction of its neuronal connections. We propose an
iterative pruning strategy introducing a simple importance-score metric that
deactivates unimportant connections, tackling overparameterization in DNNs and
modulating the firing patterns. The aim is to find the smallest number of
connections that is still capable of solving a given task with comparable
accuracy, i.e. a simpler subnetwork. We achieve comparable performance for
LeNet architectures on MNIST, and significantly higher parameter compression
than state-of-the-art algorithms for VGG and ResNet architectures on
CIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two
different optimizers considered -- Adam and SGD. The algorithm is not designed
to minimize FLOPs when considering current hardware and software
implementations, although it performs reasonably when compared to the state of
the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRACER: Extreme Attention Guided Salient Object Tracing Network. (arXiv:2112.07380v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07380">
<div class="article-summary-box-inner">
<span><p>Existing studies on salient object detection (SOD) focus on extracting
distinct objects with edge information and aggregating multi-level features to
improve SOD performance. To achieve satisfactory performance, the methods
employ refined edge information and low multi-level discrepancy. However, both
performance gain and computational efficiency cannot be attained, which has
motivated us to study the inefficiencies in existing encoder-decoder structures
to avoid this trade-off. We propose TRACER, which detects salient objects with
explicit edges by incorporating attention guided tracing modules. We employ a
masked edge attention module at the end of the first encoder using a fast
Fourier transform to propagate the refined edge information to the downstream
feature extraction. In the multi-level aggregation phase, the union attention
module identifies the complementary channel and important spatial information.
To improve the decoder performance and computational efficiency, we minimize
the decoder block usage with object attention module. This module extracts
undetected objects and edge information from refined channels and spatial
representations. Subsequently, we propose an adaptive pixel intensity loss
function to deal with the relatively important pixels unlike conventional loss
functions which treat all pixels equally. A comparison with 13 existing methods
reveals that TRACER achieves state-of-the-art performance on five benchmark
datasets. We have released TRACER at https://github.com/Karel911/TRACER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing radiologists' gaze and saliency maps generated by interpretability methods for chest x-rays. (arXiv:2112.11716v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11716">
<div class="article-summary-box-inner">
<span><p>The interpretability of medical image analysis models is considered a key
research field. We use a dataset of eye-tracking data from five radiologists to
compare the outputs of interpretability methods and the heatmaps representing
where radiologists looked. We conduct a class-independent analysis of the
saliency maps generated by two methods selected from the literature: Grad-CAM
and attention maps from an attention-gated model. For the comparison, we use
shuffled metrics, which avoid biases from fixation locations. We achieve scores
comparable to an interobserver baseline in one shuffled metric, highlighting
the potential of saliency maps from Grad-CAM to mimic a radiologist's attention
over an image. We also divide the dataset into subsets to evaluate in which
cases similarities are higher.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervision and Multi-Task Learning: Challenges in Fine-Grained COVID-19 Multi-Class Classification from Chest X-rays. (arXiv:2201.06052v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06052">
<div class="article-summary-box-inner">
<span><p>Quick and accurate diagnosis is of paramount importance to mitigate the
effects of COVID-19 infection, particularly for severe cases. Enormous effort
has been put towards developing deep learning methods to classify and detect
COVID-19 infections from chest radiography images. However, recently some
questions have been raised surrounding the clinical viability and effectiveness
of such methods. In this work, we investigate the impact of multi-task learning
(classification and segmentation) on the ability of CNNs to differentiate
between various appearances of COVID-19 infections in the lung. We also employ
self-supervised pre-training approaches, namely MoCo and inpainting-CXR, to
eliminate the dependence on expensive ground truth annotations for COVID-19
classification. Finally, we conduct a critical evaluation of the models to
assess their deploy-readiness and provide insights into the difficulties of
fine-grained COVID-19 multi-class classification from chest X-rays.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs. (arXiv:2201.06374v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06374">
<div class="article-summary-box-inner">
<span><p>Blind face restoration is to recover a high-quality face image from unknown
degradations. As face image contains abundant contextual information, we
propose a method, RestoreFormer, which explores fully-spatial attentions to
model contextual information and surpasses existing works that use local
operators. RestoreFormer has several benefits compared to prior arts. First,
unlike the conventional multi-head self-attention in previous Vision
Transformers (ViTs), RestoreFormer incorporates a multi-head cross-attention
layer to learn fully-spatial interactions between corrupted queries and
high-quality key-value pairs. Second, the key-value pairs in ResotreFormer are
sampled from a reconstruction-oriented high-quality dictionary, whose elements
are rich in high-quality facial features specifically aimed for face
reconstruction, leading to superior restoration results. Third, RestoreFormer
outperforms advanced state-of-the-art methods on one synthetic dataset and
three real-world datasets, as well as produces images with better visual
quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swin-Pose: Swin Transformer Based Human Pose Estimation. (arXiv:2201.07384v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07384">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have been widely utilized in many
computer vision tasks. However, CNNs have a fixed reception field and lack the
ability of long-range perception, which is crucial to human pose estimation.
Due to its capability to capture long-range dependencies between pixels,
transformer architecture has been adopted to computer vision applications
recently and is proven to be a highly effective architecture. We are interested
in exploring its capability in human pose estimation, and thus propose a novel
model based on transformer architecture, enhanced with a feature pyramid fusion
structure. More specifically, we use pre-trained Swin Transformer as our
backbone and extract features from input images, we leverage a feature pyramid
structure to extract feature maps from different stages. By fusing the features
together, our model predicts the keypoint heatmap. The experiment results of
our study have demonstrated that the proposed transformer-based model can
achieve better performance compared to the state-of-the-art CNN-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RePaint: Inpainting using Denoising Diffusion Probabilistic Models. (arXiv:2201.09865v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09865">
<div class="article-summary-box-inner">
<span><p>Free-form inpainting is the task of adding new content to an image in the
regions specified by an arbitrary binary mask. Most existing approaches train
for a certain distribution of masks, which limits their generalization
capabilities to unseen mask types. Furthermore, training with pixel-wise and
perceptual losses often leads to simple textural extensions towards the missing
areas instead of semantically meaningful generation. In this work, we propose
RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting
approach that is applicable to even extreme masks. We employ a pretrained
unconditional DDPM as the generative prior. To condition the generation
process, we only alter the reverse diffusion iterations by sampling the
unmasked regions using the given image information. Since this technique does
not modify or condition the original DDPM network itself, the model produces
high-quality and diverse output images for any inpainting form. We validate our
method for both faces and general-purpose image inpainting using standard and
extreme masks.
</p>
<p>RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for
at least five out of six mask distributions.
</p>
<p>Github Repository: git.io/RePaint
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Quantum-Classical Algorithm for Robust Fitting. (arXiv:2201.10110v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10110">
<div class="article-summary-box-inner">
<span><p>Fitting geometric models onto outlier contaminated data is provably
intractable. Many computer vision systems rely on random sampling heuristics to
solve robust fitting, which do not provide optimality guarantees and error
bounds. It is therefore critical to develop novel approaches that can bridge
the gap between exact solutions that are costly, and fast heuristics that offer
no quality assurances. In this paper, we propose a hybrid quantum-classical
algorithm for robust fitting. Our core contribution is a novel robust fitting
formulation that solves a sequence of integer programs and terminates with a
global solution or an error bound. The combinatorial subproblems are amenable
to a quantum annealer, which helps to tighten the bound efficiently. While our
usage of quantum computing does not surmount the fundamental intractability of
robust fitting, by providing error bounds our algorithm is a practical
improvement over randomised heuristics. Moreover, our work represents a
concrete application of quantum computing in computer vision. We present
results obtained using an actual quantum computer (D-Wave Advantage) and via
simulation. Source code: https://github.com/dadung/HQC-robust-fitting
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SURDS: Self-Supervised Attention-guided Reconstruction and Dual Triplet Loss for Writer Independent Offline Signature Verification. (arXiv:2201.10138v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10138">
<div class="article-summary-box-inner">
<span><p>Offline Signature Verification (OSV) is a fundamental biometric task across
various forensic, commercial and legal applications. The underlying task at
hand is to carefully model fine-grained features of the signatures to
distinguish between genuine and forged ones, which differ only in minute
deformities. This makes OSV more challenging compared to other verification
problems. In this work, we propose a two-stage deep learning framework that
leverages self-supervised representation learning as well as metric learning
for writer-independent OSV. First, we train an image reconstruction network
using an encoder-decoder architecture that is augmented by a 2D spatial
attention mechanism using signature image patches. Next, the trained encoder
backbone is fine-tuned with a projector head using a supervised metric learning
framework, whose objective is to optimize a novel dual triplet loss by sampling
negative samples from both within the same writer class as well as from other
writers. The intuition behind this is to ensure that a signature sample lies
closer to its positive counterpart compared to negative samples from both
intra-writer and cross-writer sets. This results in robust discriminative
learning of the embedding space. To the best of our knowledge, this is the
first work of using self-supervised learning frameworks for OSV. The proposed
two-stage framework has been evaluated on two publicly available offline
signature datasets and compared with various state-of-the-art methods. It is
noted that the proposed method provided promising results outperforming several
existing pieces of work. The code is publicly available at:
https://github.com/soumitri2001/SURDS-SSL-OSV
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RNGDet: Road Network Graph Detection by Transformer in Aerial Images. (arXiv:2202.07824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07824">
<div class="article-summary-box-inner">
<span><p>Road network graphs provide critical information for autonomous-vehicle
applications, such as drivable areas that can be used for motion planning
algorithms. To find road network graphs, manually annotation is usually
inefficient and labor-intensive. Automatically detecting road network graphs
could alleviate this issue, but existing works still have some limitations. For
example, segmentation-based approaches could not ensure satisfactory topology
correctness, and graph-based approaches could not present precise enough
detection results. To provide a solution to these problems, we propose a novel
approach based on transformer and imitation learning in this paper. In view of
that high-resolution aerial images could be easily accessed all over the world
nowadays, we make use of aerial images in our approach. Taken as input an
aerial image, our approach iteratively generates road network graphs
vertex-by-vertex. Our approach can handle complicated intersection points with
various numbers of incident road segments. We evaluate our approach on a
publicly available dataset. The superiority of our approach is demonstrated
through the comparative experiments. Our work is accompanied with a
demonstration video which is available at
\url{https://tonyxuqaq.github.io/projects/RNGDet/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-scalable Transformer for Medical Image Segmentation: Architecture, Model Efficiency, and Benchmark. (arXiv:2203.00131v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00131">
<div class="article-summary-box-inner">
<span><p>Transformer, as a new generation of neural architecture, has demonstrated
remarkable performance in natural language processing and computer vision.
However, existing vision Transformers struggle to learn with limited medical
data and are unable to generalize on diverse medical image tasks. To tackle
these challenges, we present MedFormer as a data-scalable Transformer towards
generalizable medical image segmentation. The key designs incorporate desirable
inductive bias, hierarchical modeling with linear-complexity attention, and
multi-scale feature fusion in a spatially and semantically global manner.
MedFormer can learn across tiny- to large-scale data without pre-training.
Extensive experiments demonstrate the potential of MedFormer as a general
segmentation backbone, outperforming CNNs and vision Transformers on three
public datasets with multiple modalities (e.g., CT and MRI) and diverse medical
targets (e.g., healthy organ, diseased tissue, and tumor). We make the models
and evaluation pipeline publicly available, offering solid baselines and
unbiased comparisons for promoting a wide range of downstream clinical
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods. (arXiv:2203.02486v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02486">
<div class="article-summary-box-inner">
<span><p>In many object recognition applications, the set of possible categories is an
open set, and the deployed recognition system will encounter novel objects
belonging to categories unseen during training. Detecting such ``novel
category'' objects is usually formulated as an anomaly detection problem.
Anomaly detection algorithms for feature-vector data identify anomalies as
outliers, but outlier detection has not worked well in deep learning. Instead,
methods based on the computed logits of visual object classifiers give
state-of-the-art performance. This paper proposes the Familiarity Hypothesis
that these methods succeed because they are detecting the absence of familiar
learned features rather than the presence of novelty. This distinction is
important, because familiarity-based detection will fail in many situations
where novelty is present. For example when an image contains both a novel
object and a familiar one, the familiarity score will be high, so the novel
object will not be noticed. The paper reviews evidence from the literature and
presents additional evidence from our own experiments that provide strong
support for this hypothesis. The paper concludes with a discussion of whether
familiarity-based detection is an inevitable consequence of representation
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stepwise Feature Fusion: Local Guides Global. (arXiv:2203.03635v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03635">
<div class="article-summary-box-inner">
<span><p>Colonoscopy, currently the most efficient and recognized colon polyp
detection technology, is necessary for early screening and prevention of
colorectal cancer. However, due to the varying size and complex morphological
features of colonic polyps as well as the indistinct boundary between polyps
and mucosa, accurate segmentation of polyps is still challenging. Deep learning
has become popular for accurate polyp segmentation tasks with excellent
results. However, due to the structure of polyps image and the varying shapes
of polyps, it easy for existing deep learning models to overfitting the current
dataset. As a result, the model may not process unseen colonoscopy data. To
address this, we propose a new State-Of-The-Art model for medical image
segmentation, the SSFormer, which uses a pyramid Transformer encoder to improve
the generalization ability of models. Specifically, our proposed Progressive
Locality Decoder can be adapted to the pyramid Transformer backbone to
emphasize local features and restrict attention dispersion. The SSFormer
achieves statet-of-the-art performance in both learning and generalization
assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving. (arXiv:2203.05056v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05056">
<div class="article-summary-box-inner">
<span><p>Surround-view cameras are a primary sensor for automated driving, used for
near-field perception. It is one of the most commonly used sensors in
commercial vehicles primarily used for parking visualization and automated
parking. Four fisheye cameras with a 190{\deg} field of view cover the
360{\deg} around the vehicle. Due to its high radial distortion, the standard
algorithms do not extend easily. Previously, we released the first public
fisheye surround-view dataset named WoodScape. In this work, we release a
synthetic version of the surround-view dataset, covering many of its weaknesses
and extending it. Firstly, it is not possible to obtain ground truth for
pixel-wise optical flow and depth. Secondly, WoodScape did not have all four
cameras annotated simultaneously in order to sample diverse frames. However,
this means that multi-camera algorithms cannot be designed to obtain a unified
output in birds-eye space, which is enabled in the new dataset. We implemented
surround-view fisheye geometric projections in CARLA Simulator matching
WoodScape's configuration and created SynWoodScape.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in the Black Box? The False Negative Mechanisms Inside Object Detectors. (arXiv:2203.07662v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07662">
<div class="article-summary-box-inner">
<span><p>In object detection, false negatives arise when a detector fails to detect a
target object. To understand why object detectors produce false negatives, we
identify five 'false negative mechanisms', where each mechanism describes how a
specific component inside the detector architecture failed. Focusing on
two-stage and one-stage anchor-box object detector architectures, we introduce
a framework for quantifying these false negative mechanisms. Using this
framework, we investigate why Faster R-CNN and RetinaNet fail to detect objects
in benchmark vision datasets and robotics datasets. We show that a detector's
false negative mechanisms differ significantly between computer vision
benchmark datasets and robotics deployment scenarios. This has implications for
the translation of object detectors developed for benchmark datasets to
robotics applications. Code is publicly available at
https://github.com/csiro-robotics/fn_mechanisms
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-Driven Deep Learning for Computational Magnetic Resonance Imaging. (arXiv:2203.12215v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12215">
<div class="article-summary-box-inner">
<span><p>Physics-driven deep learning methods have emerged as a powerful tool for
computational magnetic resonance imaging (MRI) problems, pushing reconstruction
performance to new limits. This article provides an overview of the recent
developments in incorporating physics information into learning-based MRI
reconstruction. We consider inverse problems with both linear and non-linear
forward models for computational MRI, and review the classical approaches for
solving these. We then focus on physics-driven deep learning approaches,
covering physics-driven loss functions, plug-and-play methods, generative
models, and unrolled networks. We highlight domain-specific challenges such as
real- and complex-valued building blocks of neural networks, and translational
applications in MRI with linear and non-linear forward models. Finally, we
discuss common issues and open challenges, and draw connections to the
importance of physics-driven learning when combined with other downstream tasks
in the medical imaging pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAT: Mask-Aware Transformer for Large Hole Image Inpainting. (arXiv:2203.15270v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15270">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown the importance of modeling long-range interactions
in the inpainting problem. To achieve this goal, existing approaches exploit
either standalone attention techniques or transformers, but usually under a low
resolution in consideration of computational cost. In this paper, we present a
novel transformer-based model for large hole inpainting, which unifies the
merits of transformers and convolutions to efficiently process high-resolution
images. We carefully design each component of our framework to guarantee the
high fidelity and diversity of recovered images. Specifically, we customize an
inpainting-oriented transformer block, where the attention module aggregates
non-local information only from partial valid tokens, indicated by a dynamic
mask. Extensive experiments demonstrate the state-of-the-art performance of the
new model on multiple benchmark datasets. Code is released at
https://github.com/fenglinglwb/MAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift. (arXiv:2203.16291v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16291">
<div class="article-summary-box-inner">
<span><p>We introduce AmsterTime: a challenging dataset to benchmark visual place
recognition (VPR) in presence of a severe domain shift. AmsterTime offers a
collection of 2,500 well-curated images matching the same scene from a street
view matched to historical archival image data from Amsterdam city. The image
pairs capture the same place with different cameras, viewpoints, and
appearances. Unlike existing benchmark datasets, AmsterTime is directly
crowdsourced in a GIS navigation platform (Mapillary). We evaluate various
baselines, including non-learning, supervised and self-supervised methods,
pre-trained on different relevant datasets, for both verification and retrieval
tasks. Our result credits the best accuracy to the ResNet-101 model pre-trained
on the Landmarks dataset for both verification and retrieval tasks by 84% and
24%, respectively. Additionally, a subset of Amsterdam landmarks is collected
for feature evaluation in a classification task. Classification labels are
further used to extract the visual explanations using Grad-CAM for inspection
of the learned similar visuals in a deep metric learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating High Fidelity Data from Low-density Regions using Diffusion Models. (arXiv:2203.17260v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17260">
<div class="article-summary-box-inner">
<span><p>Our work focuses on addressing sample deficiency from low-density regions of
data manifold in common image datasets. We leverage diffusion process based
generative models to synthesize novel images from low-density regions. We
observe that uniform sampling from diffusion models predominantly samples from
high-density regions of the data manifold. Therefore, we modify the sampling
process to guide it towards low-density regions while simultaneously
maintaining the fidelity of synthetic data. We rigorously demonstrate that our
process successfully generates novel high fidelity samples from low-density
regions. We further examine generated samples and show that the model does not
memorize low-density data and indeed learns to generate novel samples from
low-density regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proper Reuse of Image Classification Features Improves Object Detection. (arXiv:2204.00484v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00484">
<div class="article-summary-box-inner">
<span><p>A common practice in transfer learning is to initialize the downstream model
weights by pre-training on a data-abundant upstream task. In object detection
specifically, the feature backbone is typically initialized with Imagenet
classifier weights and fine-tuned on the object detection task. Recent works
show this is not strictly necessary under longer training regimes and provide
recipes for training the backbone from scratch. We investigate the opposite
direction of this end-to-end training trend: we show that an extreme form of
knowledge preservation -- freezing the classifier-initialized backbone --
consistently improves many different detection models, and leads to
considerable resource savings. We hypothesize and corroborate experimentally
that the remaining detector components capacity and structure is a crucial
factor in leveraging the frozen backbone. Immediate applications of our
findings include performance improvements on hard cases like detection of
long-tail object classes and computational and memory resource savings that
contribute to making the field more accessible to researchers with access to
fewer computational resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Global Shutter: Learn to Restore Video from a Rolling Shutter Camera with Global Reset Feature. (arXiv:2204.00974v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00974">
<div class="article-summary-box-inner">
<span><p>Most computer vision systems assume distortion-free images as inputs. The
widely used rolling-shutter (RS) image sensors, however, suffer from geometric
distortion when the camera and object undergo motion during capture. Extensive
researches have been conducted on correcting RS distortions. However, most of
the existing work relies heavily on the prior assumptions of scenes or motions.
Besides, the motion estimation steps are either oversimplified or
computationally inefficient due to the heavy flow warping, limiting their
applicability. In this paper, we investigate using rolling shutter with a
global reset feature (RSGR) to restore clean global shutter (GS) videos. This
feature enables us to turn the rectification problem into a deblur-like one,
getting rid of inaccurate and costly explicit motion estimation. First, we
build an optic system that captures paired RSGR/GS videos. Second, we develop a
novel algorithm incorporating spatial and temporal designs to correct the
spatial-varying RSGR distortion. Third, we demonstrate that existing
image-to-image translation algorithms can recover clean GS videos from
distorted RSGR inputs, yet our algorithm achieves the best performance with the
specific designs. Our rendered results are not only visually appealing but also
beneficial to downstream tasks. Compared to the state-of-the-art RS solution,
our RSGR solution is superior in both effectiveness and efficiency. Considering
it is easy to realize without changing the hardware, we believe our RSGR
solution can potentially replace the RS solution in taking distortion-free
videos with low noise and low budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Generalizable Dexterous Manipulation from Human Grasp Affordance. (arXiv:2204.02320v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02320">
<div class="article-summary-box-inner">
<span><p>Dexterous manipulation with a multi-finger hand is one of the most
challenging problems in robotics. While recent progress in imitation learning
has largely improved the sample efficiency compared to Reinforcement Learning,
the learned policy can hardly generalize to manipulate novel objects, given
limited expert demonstrations. In this paper, we propose to learn dexterous
manipulation using large-scale demonstrations with diverse 3D objects in a
category, which are generated from a human grasp affordance model. This
generalizes the policy to novel object instances within the same category. To
train the policy, we propose a novel imitation learning objective jointly with
a geometric representation learning objective using our demonstrations. By
experimenting with relocating diverse objects in simulation, we show that our
approach outperforms baselines with a large margin when manipulating novel
objects. We also ablate the importance on 3D object representation learning for
manipulation. We include videos, code, and additional information on the
project website - https://kristery.github.io/ILAD/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Anticipate Future with Dynamic Context Removal. (arXiv:2204.02587v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02587">
<div class="article-summary-box-inner">
<span><p>Anticipating future events is an essential feature for intelligent systems
and embodied AI. However, compared to the traditional recognition task, the
uncertainty of future and reasoning ability requirement make the anticipation
task very challenging and far beyond solved. In this filed, previous methods
usually care more about the model architecture design or but few attention has
been put on how to train an anticipation model with a proper learning policy.
To this end, in this work, we propose a novel training scheme called Dynamic
Context Removal (DCR), which dynamically schedules the visibility of observed
future in the learning procedure. It follows the human-like curriculum learning
process, i.e., gradually removing the event context to increase the
anticipation difficulty till satisfying the final anticipation target. Our
learning scheme is plug-and-play and easy to integrate any reasoning model
including transformer and LSTM, with advantages in both effectiveness and
efficiency. In extensive experiments, the proposed method achieves
state-of-the-art on four widely-used benchmarks. Our code and models are
publicly released at https://github.com/AllenXuuu/DCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neglectable effect of brain MRI data preprocessing for tumor segmentation. (arXiv:2204.05278v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05278">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) data is heterogeneous due to the differences
in device manufacturers, scanning protocols, and inter-subject variability. A
conventional way to mitigate MR image heterogeneity is to apply preprocessing
transformations, such as anatomy alignment, voxel resampling, signal intensity
equalization, image denoising, and localization of regions of interest (ROI).
Although preprocessing pipeline standardizes image appearance, its influence on
the quality of image segmentation and other downstream tasks on deep neural
networks (DNN) has never been rigorously studied.
</p>
<p>Here we report a comprehensive study of multimodal MRI brain cancer image
segmentation on TCIA-GBM open-source dataset. Our results demonstrate that most
popular standardization steps add no value to artificial neural network
performance; moreover, preprocessing can hamper model performance. We suggest
that image intensity normalization approaches do not contribute to model
accuracy because of the reduction of signal variance with image
standardization. Finally, we show the contribution of scull-stripping in data
preprocessing is almost negligible if measured in terms of clinically relevant
metrics.
</p>
<p>We show that the only essential transformation for accurate analysis is the
unification of voxel spacing across the dataset. In contrast, anatomy alignment
in form of non-rigid atlas registration is not necessary and most intensity
equalization steps do not improve model productiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DConvCaps: 3DUnet with Convolutional Capsule Encoder for Medical Image Segmentation. (arXiv:2205.09299v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09299">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) have achieved promising results in
medical image segmentation. However, CNNs require lots of training data and are
incapable of handling pose and deformation of objects. Furthermore, their
pooling layers tend to discard important information such as positions as well
as CNNs are sensitive to rotation and affine transformation. Capsule network is
a recent new architecture that has achieved better robustness in part-whole
representation learning by replacing pooling layers with dynamic routing and
convolutional strides, which has shown potential results on popular tasks such
as digit classification and object segmentation. In this paper, we propose a 3D
encoder-decoder network with Convolutional Capsule Encoder (called 3DConvCaps)
to learn lower-level features (short-range attention) with convolutional layers
while modeling the higher-level features (long-range dependence) with capsule
layers. Our experiments on multiple datasets including iSeg-2017, Hippocampus,
and Cardiac demonstrate that our 3D 3DConvCaps network considerably outperforms
previous capsule networks and 3D-UNets. We further conduct ablation studies of
network efficiency and segmentation performance under various configurations of
convolution layers and capsule layers at both contracting and expanding paths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Landslide4Sense: Reference Benchmark Data and Deep Learning Models for Landslide Detection. (arXiv:2206.00515v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00515">
<div class="article-summary-box-inner">
<span><p>This study introduces \textit{Landslide4Sense}, a reference benchmark for
landslide detection from remote sensing. The repository features 3,799 image
patches fusing optical layers from Sentinel-2 sensors with the digital
elevation model and slope layer derived from ALOS PALSAR. The added
topographical information facilitates an accurate detection of landslide
borders, which recent researches have shown to be challenging using optical
data alone. The extensive data set supports deep learning (DL) studies in
landslide detection and the development and validation of methods for the
systematic update of landslide inventories. The benchmark data set has been
collected at four different times and geographical locations: Iburi (September
2018), Kodagu (August 2018), Gorkha (April 2015), and Taiwan (August 2009).
Each image pixel is labelled as belonging to a landslide or not, incorporating
various sources and thorough manual annotation. We then evaluate the landslide
detection performance of 11 state-of-the-art DL segmentation models: U-Net,
ResU-Net, PSPNet, ContextNet, DeepLab-v2, DeepLab-v3+, FCN-8s, LinkNet, FRRN-A,
FRRN-B, and SQNet. All models were trained from scratch on patches from one
quarter of each study area and tested on independent patches from the other
three quarters. Our experiments demonstrate that ResU-Net outperformed the
other models for the landslide detection task. We make the multi-source
landslide benchmark data (Landslide4Sense) and the tested DL models publicly
available at \url{www.landslide4sense.org}, establishing an important resource
for remote sensing, computer vision, and machine learning communities in
studies of image classification in general and applications to landslide
detection in particular.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supernet Training for Federated Image Classification under System Heterogeneity. (arXiv:2206.01366v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01366">
<div class="article-summary-box-inner">
<span><p>Efficient deployment of deep neural networks across many devices and resource
constraints, especially on edge devices, is one of the most challenging
problems in the presence of data-privacy preservation issues. Conventional
approaches have evolved to either improve a single global model while keeping
each local training data decentralized (i.e., data-heterogeneity) or to train a
once-for-all network that supports diverse architectural settings to address
heterogeneous systems equipped with different computational capabilities (i.e.,
model-heterogeneity). However, little research has considered both directions
simultaneously. In this work, we propose a novel framework to consider both
scenarios, namely Federation of Supernet Training (FedSup), where clients send
and receive a supernet whereby it contains all possible architectures sampled
from itself. It is inspired by how averaging parameters in the model
aggregation stage of Federated Learning (FL) is similar to weight-sharing in
supernet training. Specifically, in the FedSup framework, a weight-sharing
approach widely used in the training single shot model is combined with the
averaging of Federated Learning (FedAvg). Under our framework, we present an
efficient algorithm (E-FedSup) by sending the sub-model to clients in the
broadcast stage for reducing communication costs and training overhead. We
demonstrate several strategies to enhance supernet training in the FL
environment and conduct extensive empirical evaluations. The resulting
framework is shown to pave the way for the robustness of both data- and
model-heterogeneity on several standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drawing out of Distribution with Neuro-Symbolic Generative Models. (arXiv:2206.01829v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01829">
<div class="article-summary-box-inner">
<span><p>Learning general-purpose representations from perceptual inputs is a hallmark
of human intelligence. For example, people can write out numbers or characters,
or even draw doodles, by characterizing these tasks as different instantiations
of the same generic underlying process -- compositional arrangements of
different forms of pen strokes. Crucially, learning to do one task, say
writing, implies reasonable competence at another, say drawing, on account of
this shared process. We present Drawing out of Distribution (DooD), a
neuro-symbolic generative model of stroke-based drawing that can learn such
general-purpose representations. In contrast to prior work, DooD operates
directly on images, requires no supervision or expensive test-time inference,
and performs unsupervised amortised inference with a symbolic stroke model that
better enables both interpretability and generalization. We evaluate DooD on
its ability to generalise across both data and tasks. We first perform
zero-shot transfer from one dataset (e.g. MNIST) to another (e.g. Quickdraw),
across five different datasets, and show that DooD clearly outperforms
different baselines. An analysis of the learnt representations further
highlights the benefits of adopting a symbolic stroke model. We then adopt a
subset of the Omniglot challenge tasks, and evaluate its ability to generate
new exemplars (both unconditionally and conditionally), and perform one-shot
classification, showing that DooD matches the state of the art. Taken together,
we demonstrate that DooD does indeed capture general-purpose representations
across both data and task, and takes a further step towards building general
and robust concept-learning systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image. (arXiv:2206.01856v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01856">
<div class="article-summary-box-inner">
<span><p>Image enhancement approaches often assume that the noise is signal
independent, and approximate the degradation model as zero-mean additive
Gaussian. However, this assumption does not hold for biomedical imaging systems
where sensor-based sources of noise are proportional to signal strengths, and
the noise is better represented as a Poisson process. In this work, we explore
a sparsity and dictionary learning-based approach and present a novel
self-supervised learning method for single-image denoising where the noise is
approximated as a Poisson process, requiring no clean ground-truth data.
Specifically, we approximate traditional iterative optimization algorithms for
image denoising with a recurrent neural network that enforces sparsity with
respect to the weights of the network. Since the sparse representations are
based on the underlying image, it is able to suppress the spurious components
(noise) in the image patches, thereby introducing implicit regularization for
denoising tasks through the network structure. Experiments on two bio-imaging
datasets demonstrate that our method outperforms the state-of-the-art
approaches in terms of PSNR and SSIM. Our qualitative results demonstrate that,
in addition to higher performance on standard quantitative metrics, we are able
to recover much more subtle details than other compared approaches. Our code is
made publicly available at https://github.com/tacalvin/Poisson2Sparse
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Domain Adaptation in Crowd Counting. (arXiv:2206.03431v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03431">
<div class="article-summary-box-inner">
<span><p>Self-training crowd counting has not been attentively explored though it is
one of the important challenges in computer vision. In practice, the fully
supervised methods usually require an intensive resource of manual annotation.
In order to address this challenge, this work introduces a new approach to
utilize existing datasets with ground truth to produce more robust predictions
on unlabeled datasets, named domain adaptation, in crowd counting. While the
network is trained with labeled data, samples without labels from the target
domain are also added to the training process. In this process, the entropy map
is computed and minimized in addition to the adversarial training process
designed in parallel. Experiments on Shanghaitech, UCF_CC_50, and UCF-QNRF
datasets prove a more generalized improvement of our method over the other
state-of-the-arts in the cross-domain setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CO^3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving. (arXiv:2206.04028v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04028">
<div class="article-summary-box-inner">
<span><p>Unsupervised contrastive learning for indoor-scene point clouds has achieved
great successes. However, unsupervised learning point clouds in outdoor scenes
remains challenging because previous methods need to reconstruct the whole
scene and capture partial views for the contrastive objective. This is
infeasible in outdoor scenes with moving objects, obstacles, and sensors. In
this paper, we propose CO^3, namely Cooperative Contrastive Learning and
Contextual Shape Prediction, to learn 3D representation for outdoor-scene point
clouds in an unsupervised manner. CO^3 has several merits compared to existing
methods. (1) It utilizes LiDAR point clouds from vehicle-side and
infrastructure-side to build views that differ enough but meanwhile maintain
common semantic information for contrastive learning, which are more
appropriate than views built by previous methods. (2) Alongside the contrastive
objective, shape context prediction is proposed as pre-training goal and brings
more task-relevant information for unsupervised 3D point cloud representation
learning, which are beneficial when transferring the learned representation to
downstream detection tasks. (3) As compared to previous methods, representation
learned by CO^3 is able to be transferred to different outdoor scene dataset
collected by different type of LiDAR sensors. (4) CO^3 improves current
state-of-the-art methods on both Once and KITTI datasets by up to 2.58 mAP.
Codes and models will be released. We believe CO^3 will facilitate
understanding LiDAR point clouds in outdoor scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04046">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) aims at learning generalizable models under
distribution shifts to avoid redundantly overfitting massive training data.
Previous works with complex loss design and gradient constraint have not yet
led to empirical success on large-scale benchmarks. In this work, we reveal the
mixture-of-experts (MoE) model's generalizability on DG by leveraging to
distributively handle multiple aspects of the predictive features across
domains. To this end, we propose Sparse Fusion Mixture-of-Experts (SF-MoE),
which incorporates sparsity and fusion mechanisms into the MoE framework to
keep the model both sparse and predictive. SF-MoE has two dedicated modules: 1)
sparse block and 2) fusion block, which disentangle and aggregate the diverse
learned signals of an object, respectively. Extensive experiments demonstrate
that SF-MoE is a domain-generalizable learner on large-scale benchmarks. It
outperforms state-of-the-art counterparts by more than 2% across 5 large-scale
DG datasets (e.g., DomainNet), with the same or even lower computational costs.
We further reveal the internal mechanism of SF-MoE from distributed
representation perspective (e.g., visual attributes). We hope this framework
could facilitate future research to push generalizable object recognition to
the real world. Code and models are released at
https://github.com/Luodian/SF-MoE-DG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of COVID-19 in Chest X-ray Images Using Fusion of Deep Features and LightGBM. (arXiv:2206.04548v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04548">
<div class="article-summary-box-inner">
<span><p>The COVID-19 disease was first discovered in Wuhan, China, and spread quickly
worldwide. After the COVID-19 pandemic, many researchers have begun to identify
a way to diagnose the COVID-19 using chest X-ray images. The early diagnosis of
this disease can significantly impact the treatment process. In this article,
we propose a new technique that is faster and more accurate than the other
methods reported in the literature. The proposed method uses a combination of
DenseNet169 and MobileNet Deep Neural Networks to extract the features of the
patient's X-ray images. Using the univariate feature selection algorithm, we
refined the features for the most important ones. Then we applied the selected
features as input to the LightGBM (Light Gradient Boosting Machine) algorithm
for classification. To assess the effectiveness of the proposed method, the
ChestX-ray8 dataset, which includes 1125 X-ray images of the patient's chest,
was used. The proposed method achieved 98.54% and 91.11% accuracies in the
two-class (COVID-19, Healthy) and multi-class (COVID-19, Healthy, Pneumonia)
classification problems, respectively. It is worth mentioning that we have used
Gradient-weighted Class Activation Mapping (Grad-CAM) for further analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images. (arXiv:2206.06665v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06665">
<div class="article-summary-box-inner">
<span><p>Developing an AI-assisted gland segmentation method from histology images is
critical for automatic cancer diagnosis and prognosis; however, the high cost
of pixel-level annotations hinders its applications to broader diseases.
Existing weakly-supervised semantic segmentation methods in computer vision
achieve degenerative results for gland segmentation, since the characteristics
and problems of glandular datasets are different from general object datasets.
We observe that, unlike natural images, the key problem with histology images
is the confusion of classes owning to morphological homogeneity and low color
contrast among different tissues. To this end, we propose a novel method Online
Easy Example Mining (OEEM) that encourages the network to focus on credible
supervision signals rather than noisy signals, therefore mitigating the
influence of inevitable false predictions in pseudo-masks. According to the
characteristics of glandular datasets, we design a strong framework for gland
segmentation. Our results exceed many fully-supervised methods and
weakly-supervised methods for gland segmentation over 4.4% and 6.04% at mIoU,
respectively. Code is available at https://github.com/xmed-lab/OEEM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Transformer Variational Autoencoders for Multi-Action Motion Synthesis. (arXiv:2206.06741v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06741">
<div class="article-summary-box-inner">
<span><p>We consider the problem of synthesizing multi-action human motion sequences
of arbitrary lengths. Existing approaches have mastered motion sequence
generation in single action scenarios, but fail to generalize to multi-action
and arbitrary-length sequences. We fill this gap by proposing a novel efficient
approach that leverages expressiveness of Recurrent Transformers and generative
richness of conditional Variational Autoencoders. The proposed iterative
approach is able to generate smooth and realistic human motion sequences with
an arbitrary number of actions and frames while doing so in linear space and
time. We train and evaluate the proposed approach on PROX and Charades
datasets, where we augment PROX with ground-truth action labels and Charades
with human mesh annotations. Experimental evaluation shows significant
improvements in FID score and semantic consistency metrics compared to the
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites. (arXiv:2206.06813v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06813">
<div class="article-summary-box-inner">
<span><p>In clinical practice, a segmentation network is often required to continually
learn on a sequential data stream from multiple sites rather than a
consolidated set, due to the storage cost and privacy restriction. However,
during the continual learning process, existing methods are usually restricted
in either network memorizability on previous sites or generalizability on
unseen sites. This paper aims to tackle the challenging problem of Synchronous
Memorizability and Generalizability (SMG) and to simultaneously improve
performance on both previous and unseen sites, with a novel proposed
SMG-learning framework. First, we propose a Synchronous Gradient Alignment
(SGA) objective, which not only promotes the network memorizability by
enforcing coordinated optimization for a small exemplar set from previous sites
(called replay buffer), but also enhances the generalizability by facilitating
site-invariance under simulated domain shift. Second, to simplify the
optimization of SGA objective, we design a Dual-Meta algorithm that
approximates the SGA objective as dual meta-objectives for optimization without
expensive computation overhead. Third, for efficient rehearsal, we configure
the replay buffer comprehensively considering additional inter-site diversity
to reduce redundancy. Experiments on prostate MRI data sequentially acquired
from six institutes demonstrate that our method can simultaneously achieve
higher memorizability and generalizability over state-of-the-art methods. Code
is available at https://github.com/jingyzhang/SMG-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation. (arXiv:2206.07117v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07117">
<div class="article-summary-box-inner">
<span><p>3D hand pose estimation methods have made significant progress recently.
However, the estimation accuracy is often far from sufficient for specific
real-world applications, and thus there is significant room for improvement.
This paper proposes TriHorn-Net, a novel model that uses specific innovations
to improve hand pose estimation accuracy on depth images. The first innovation
is the decomposition of the 3D hand pose estimation into the estimation of 2D
joint locations in the depth image space (UV), and the estimation of their
corresponding depths aided by two complementary attention maps. This
decomposition prevents depth estimation, which is a more difficult task, from
interfering with the UV estimations at both the prediction and feature levels.
The second innovation is PixDropout, which is, to the best of our knowledge,
the first appearance-based data augmentation method for hand depth images.
Experimental results demonstrate that the proposed model outperforms the
state-of-the-art methods on three public benchmark datasets. Our implementation
is available at https://github.com/mrezaei92/TriHorn-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursive Neural Programs: Variational Learning of Image Grammars and Part-Whole Hierarchies. (arXiv:2206.08462v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08462">
<div class="article-summary-box-inner">
<span><p>Human vision involves parsing and representing objects and scenes using
structured representations based on part-whole hierarchies. Computer vision and
machine learning researchers have recently sought to emulate this capability
using capsule networks, reference frames and active predictive coding, but a
generative model formulation has been lacking. We introduce Recursive Neural
Programs (RNPs), which, to our knowledge, is the first neural generative model
to address the part-whole hierarchy learning problem. RNPs model images as
hierarchical trees of probabilistic sensory-motor programs that recursively
reuse learned sensory-motor primitives to model an image within different
reference frames, forming recursive image grammars. We express RNPs as
structured variational autoencoders (sVAEs) for inference and sampling, and
demonstrate parts-based parsing, sampling and one-shot transfer learning for
MNIST, Omniglot and Fashion-MNIST datasets, demonstrating the model's
expressive power. Our results show that RNPs provide an intuitive and
explainable way of composing objects and scenes, allowing rich compositionality
and intuitive interpretations of objects in terms of part-whole hierarchies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot AutoML with Pretrained Models. (arXiv:2206.08476v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08476">
<div class="article-summary-box-inner">
<span><p>Given a new dataset D and a low compute budget, how should we choose a
pre-trained model to fine-tune to D, and set the fine-tuning hyperparameters
without risking overfitting, particularly if D is small? Here, we extend
automated machine learning (AutoML) to best make these choices. Our
domain-independent meta-learning approach learns a zero-shot surrogate model
which, at test time, allows to select the right deep learning (DL) pipeline
(including the pre-trained model and fine-tuning hyperparameters) for a new
dataset D given only trivial meta-features describing D such as image
resolution or the number of classes. To train this zero-shot model, we collect
performance data for many DL pipelines on a large collection of datasets and
meta-train on this data to minimize a pairwise ranking objective. We evaluate
our approach under the strict time limit of the vision track of the ChaLearn
AutoDL challenge benchmark, clearly outperforming all challenge contenders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images. (arXiv:2206.08549v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08549">
<div class="article-summary-box-inner">
<span><p>Evaluation metrics in image synthesis play a key role to measure performances
of generative models. However, most metrics mainly focus on image fidelity.
Existing diversity metrics are derived by comparing distributions, and thus
they cannot quantify the diversity or rarity degree of each generated image. In
this work, we propose a new evaluation metric, called `rarity score', to
measure the individual rarity of each image synthesized by generative models.
We first show empirical observation that common samples are close to each other
and rare samples are far from each other in nearest-neighbor distances of
feature space. We then use our metric to demonstrate that the extent to which
different generative models produce rare images can be effectively compared. We
also propose a method to compare rarities between datasets that share the same
concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in
different designs of feature spaces to better understand the relationship
between feature spaces and resulting sparse images. Code will be publicly
available online for the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is Where by Looking: Weakly-Supervised Open-World Phrase-Grounding without Text Inputs. (arXiv:2206.09358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09358">
<div class="article-summary-box-inner">
<span><p>Given an input image, and nothing else, our method returns the bounding boxes
of objects in the image and phrases that describe the objects. This is achieved
within an open world paradigm, in which the objects in the input image may not
have been encountered during the training of the localization mechanism.
Moreover, training takes place in a weakly supervised setting, where no
bounding boxes are provided. To achieve this, our method combines two
pre-trained networks: the CLIP image-to-text matching score and the BLIP image
captioning tool. Training takes place on COCO images and their captions and is
based on CLIP. Then, during inference, BLIP is used to generate a hypothesis
regarding various regions of the current image. Our work generalizes weakly
supervised segmentation and phrase grounding and is shown empirically to
outperform the state of the art in both domains. It also shows very convincing
results in the novel task of weakly-supervised open-world purely visual
phrase-grounding presented in our work. For example, on the datasets used for
benchmarking phrase-grounding, our method results in a very modest degradation
in comparison to methods that employ human captions as an additional input. Our
code is available at https://github.com/talshaharabany/what-is-where-by-looking
and a live demo can be found at
https://replicate.com/talshaharabany/what-is-where-by-looking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Generalizable Person Re-identification with a Bi-stream Generative Model. (arXiv:2206.09362v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09362">
<div class="article-summary-box-inner">
<span><p>Generalizable person re-identification (re-ID) has attracted growing
attention due to its powerful adaptation capability in the unseen data domain.
However, existing solutions often neglect either crossing cameras (e.g.,
illumination and resolution differences) or pedestrian misalignments (e.g.,
viewpoint and pose discrepancies), which easily leads to poor generalization
capability when adapted to the new domain. In this paper, we formulate these
difficulties as: 1) Camera-Camera (CC) problem, which denotes the various human
appearance changes caused by different cameras; 2) Camera-Person (CP) problem,
which indicates the pedestrian misalignments caused by the same identity person
under different camera viewpoints or changing pose. To solve the above issues,
we propose a Bi-stream Generative Model (BGM) to learn the fine-grained
representations fused with camera-invariant global feature and
pedestrian-aligned local feature, which contains an encoding network and two
stream decoding sub-networks. Guided by original pedestrian images, one stream
is employed to learn a camera-invariant global feature for the CC problem via
filtering cross-camera interference factors. For the CP problem, another stream
learns a pedestrian-aligned local feature for pedestrian alignment using
information-complete densely semantically aligned part maps. Moreover, a
part-weighted loss function is presented to reduce the influence of missing
parts on pedestrian alignment. Extensive experiments demonstrate that our
method outperforms the state-of-the-art methods on the large-scale
generalizable re-ID benchmarks, involving domain generalization setting and
cross-domain setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C-SENN: Contrastive Self-Explaining Neural Network. (arXiv:2206.09575v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09575">
<div class="article-summary-box-inner">
<span><p>In this study, we use a self-explaining neural network (SENN), which learns
unsupervised concepts, to acquire concepts that are easy for people to
understand automatically. In concept learning, the hidden layer retains
verbalizable features relevant to the output, which is crucial when adapting to
real-world environments where explanations are required. However, it is known
that the interpretability of concepts output by SENN is reduced in general
settings, such as autonomous driving scenarios. Thus, this study combines
contrastive learning with concept learning to improve the readability of
concepts and the accuracy of tasks. We call this model Contrastive
Self-Explaining Neural Network (C-SENN).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology. (arXiv:2206.09769v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09769">
<div class="article-summary-box-inner">
<span><p>Histopathology whole slide images (WSIs) can reveal significant
inter-hospital variability such as illumination, color or optical artifacts.
These variations, caused by the use of different scanning protocols across
medical centers (staining, scanner), can strongly harm algorithms
generalization on unseen protocols. This motivates development of new methods
to limit such drop of performances. In this paper, to enhance robustness on
unseen target protocols, we propose a new test-time data augmentation based on
multi domain image-to-image translation. It allows to project images from
unseen protocol into each source domain before classifying them and ensembling
the predictions. This test-time augmentation method results in a significant
boost of performances for domain generalization. To demonstrate its
effectiveness, our method has been evaluated on 2 different histopathology
tasks where it outperforms conventional domain generalization, standard H&amp;E
specific color augmentation/normalization and standard test-time augmentation
techniques. Our code is publicly available at
https://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voxel-MAE: Masked Autoencoders for Pre-training Large-scale Point Clouds. (arXiv:2206.09900v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09900">
<div class="article-summary-box-inner">
<span><p>Mask-based pre-training has achieved great success for self-supervised
learning in image, video, and language, without manually annotated supervision.
However, it has not yet been studied about large-scale point clouds with
redundant spatial information in autonomous driving. As the number of
large-scale point clouds is huge, it is impossible to reconstruct the input
point clouds. In this paper, we propose a mask voxel classification network for
large-scale point clouds pre-training. Our key idea is to divide the point
clouds into voxel representations and classify whether the voxel contains point
clouds. This simple strategy makes the network to be voxel-aware of the object
shape, thus improving the performance of the downstream tasks, such as 3D
object detection. Our Voxel-MAE with even a 90% masking ratio can still learn
representative features for the high spatial redundancy of large-scale point
clouds. We also validate the effectiveness of Voxel-MAE in unsupervised domain
adaptative tasks, which proves the generalization ability of Voxel-MAE. Our
Voxel-MAE proves that it is feasible to pre-train large-scale point clouds
without data annotations to enhance the perception ability of the autonomous
vehicle. Extensive experiments show great effectiveness of our pre-trained
model with 3D object detectors (SECOND, CenterPoint, and PV-RCNN) on two
popular datasets (KITTI, Waymo). Codes are publicly available at
https://github.com/chaytonmin/Voxel-MAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ORFD: A Dataset and Benchmark for Off-Road Freespace Detection. (arXiv:2206.09907v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09907">
<div class="article-summary-box-inner">
<span><p>Freespace detection is an essential component of autonomous driving
technology and plays an important role in trajectory planning. In the last
decade, deep learning-based free space detection methods have been proved
feasible. However, these efforts were focused on urban road environments and
few deep learning-based methods were specifically designed for off-road free
space detection due to the lack of off-road benchmarks. In this paper, we
present the ORFD dataset, which, to our knowledge, is the first off-road free
space detection dataset. The dataset was collected in different scenes
(woodland, farmland, grassland, and countryside), different weather conditions
(sunny, rainy, foggy, and snowy), and different light conditions (bright light,
daylight, twilight, darkness), which totally contains 12,198 LiDAR point cloud
and RGB image pairs with the traversable area, non-traversable area and
unreachable area annotated in detail. We propose a novel network named OFF-Net,
which unifies Transformer architecture to aggregate local and global
information, to meet the requirement of large receptive fields for free space
detection tasks. We also propose the cross-attention to dynamically fuse LiDAR
and RGB image information for accurate off-road free space detection. Dataset
and code are publicly available athttps://github.com/chaytonmin/OFF-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test Time Transform Prediction for Open Set Histopathological Image Recognition. (arXiv:2206.10033v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10033">
<div class="article-summary-box-inner">
<span><p>Tissue typology annotation in Whole Slide histological images is a complex
and tedious, yet necessary task for the development of computational pathology
models. We propose to address this problem by applying Open Set Recognition
techniques to the task of jointly classifying tissue that belongs to a set of
annotated classes, e.g. clinically relevant tissue categories, while rejecting
in test time Open Set samples, i.e. images that belong to categories not
present in the training set. To this end, we introduce a new approach for Open
Set histopathological image recognition based on training a model to accurately
identify image categories and simultaneously predict which data augmentation
transform has been applied. In test time, we measure model confidence in
predicting this transform, which we expect to be lower for images in the Open
Set. We carry out comprehensive experiments in the context of colorectal cancer
assessment from histological images, which provide evidence on the strengths of
our approach to automatically identify samples from unknown categories. Code is
released at https://github.com/agaldran/t3po .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders. (arXiv:2206.10207v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10207">
<div class="article-summary-box-inner">
<span><p>Recently, significant progress has been made in masked image modeling to
catch up to masked language modeling. However, unlike words in NLP, the lack of
semantic decomposition of images still makes masked autoencoding (MAE)
different between vision and language. In this paper, we explore a potential
visual analogue of words, i.e., semantic parts, and we integrate semantic
information into the training process of MAE by proposing a Semantic-Guided
Masking strategy. Compared to widely adopted random masking, our masking
strategy can gradually guide the network to learn various information, i.e.,
from intra-part patterns to inter-part relations. In particular, we achieve
this in two steps. 1) Semantic part learning: we design a self-supervised part
learning method to obtain semantic parts by leveraging and refining the
multi-head attention of a ViT-based encoder. 2) Semantic-guided MAE (SemMAE)
training: we design a masking strategy that varies from masking a portion of
patches in each part to masking a portion of (whole) parts in an image.
Extensive experiments on various vision tasks show that SemMAE can learn better
image representation by integrating semantic information. In particular, SemMAE
achieves 84.5% fine-tuning accuracy on ImageNet-1k, which outperforms the
vanilla MAE by 1.4%. In the semantic segmentation and fine-grained recognition
tasks, SemMAE also brings significant improvements and yields the
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose Estimation. (arXiv:2206.10892v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10892">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the Intra- and Inter-Human Relation Networks
(I^2R-Net) for Multi-Person Pose Estimation. It involves two basic modules.
First, the Intra-Human Relation Module operates on a single person and aims to
capture Intra-Human dependencies. Second, the Inter-Human Relation Module
considers the relation between multiple instances and focuses on capturing
Inter-Human interactions. The Inter-Human Relation Module can be designed very
lightweight by reducing the resolution of feature map, yet learn useful
relation information to significantly boost the performance of the Intra-Human
Relation Module. Even without bells and whistles, our method can compete or
outperform current competition winners. We conduct extensive experiments on
COCO, CrowdPose, and OCHuman datasets. The results demonstrate that the
proposed model surpasses all the state-of-the-art methods. Concretely, the
proposed method achieves 77.4% AP on CrowPose dataset and 67.8% AP on OCHuman
dataset respectively, outperforming existing methods by a large margin.
Additionally, the ablation study and visualization analysis also prove the
effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer. (arXiv:2206.11053v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11053">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) in surgery is largely unexplored. Expert
surgeons are scarce and are often overloaded with clinical and academic
workloads. This overload often limits their time answering questionnaires from
patients, medical students or junior residents related to surgical procedures.
At times, students and junior residents also refrain from asking too many
questions during classes to reduce disruption. While computer-aided simulators
and recording of past surgical procedures have been made available for them to
observe and improve their skills, they still hugely rely on medical experts to
answer their questions. Having a Surgical-VQA system as a reliable 'second
opinion' could act as a backup and ease the load on the medical experts in
answering these questions. The lack of annotated medical data and the presence
of domain-specific terms has limited the exploration of VQA for surgical
procedures. In this work, we design a Surgical-VQA task that answers
questionnaires on surgical procedures based on the surgical scene. Extending
the MICCAI endoscopic vision challenge 2018 dataset and workflow recognition
dataset further, we introduce two Surgical-VQA datasets with classification and
sentence-based answers. To perform Surgical-VQA, we employ vision-text
transformers models. We further introduce a residual MLP-based VisualBert
encoder model that enforces interaction between visual and text tokens,
improving performance in classification-based answering. Furthermore, we study
the influence of the number of input image patches and temporal visual features
on the model performance in both classification and sentence-based answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entropy-driven Sampling and Training Scheme for Conditional Diffusion Generation. (arXiv:2206.11474v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11474">
<div class="article-summary-box-inner">
<span><p>Denoising Diffusion Probabilistic Model (DDPM) is able to make flexible
conditional image generation from prior noise to real data, by introducing an
independent noise-aware classifier to provide conditional gradient guidance at
each time step of denoising process. However, due to the ability of classifier
to easily discriminate an incompletely generated image only with high-level
structure, the gradient, which is a kind of class information guidance, tends
to vanish early, leading to the collapse from conditional generation process
into the unconditional process. To address this problem, we propose two simple
but effective approaches from two perspectives. For sampling procedure, we
introduce the entropy of predicted distribution as the measure of guidance
vanishing level and propose an entropy-aware scaling method to adaptively
recover the conditional semantic guidance. For training stage, we propose the
entropy-aware optimization objectives to alleviate the overconfident prediction
for noisy data.On ImageNet1000 256x256, with our proposed sampling scheme and
trained classifier, the pretrained conditional and unconditional DDPM model can
achieve 10.89% (4.59 to 4.09) and 43.5% (12 to 6.78) FID improvement
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel adversarial learning strategy for medical image classification. (arXiv:2206.11501v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11501">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) techniques have been extensively utilized for medical
image classification. Most DL-based classification networks are generally
structured hierarchically and optimized through the minimization of a single
loss function measured at the end of the networks. However, such a single loss
design could potentially lead to optimization of one specific value of interest
but fail to leverage informative features from intermediate layers that might
benefit classification performance and reduce the risk of overfitting.
Recently, auxiliary convolutional neural networks (AuxCNNs) have been employed
on top of traditional classification networks to facilitate the training of
intermediate layers to improve classification performance and robustness. In
this study, we proposed an adversarial learning-based AuxCNN to support the
training of deep neural networks for medical image classification. Two main
innovations were adopted in our AuxCNN classification framework. First, the
proposed AuxCNN architecture includes an image generator and an image
discriminator for extracting more informative image features for medical image
classification, motivated by the concept of generative adversarial network
(GAN) and its impressive ability in approximating target data distribution.
Second, a hybrid loss function is designed to guide the model training by
incorporating different objectives of the classification network and AuxCNN to
reduce overfitting. Comprehensive experimental studies demonstrated the
superior classification performance of the proposed model. The effect of the
network-related factors on classification performance was investigated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022). (arXiv:2206.11610v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11610">
<div class="article-summary-box-inner">
<span><p>This report presents the methods of the winning entry of the RxR-Habitat
Competition in CVPR 2022. The competition addresses the problem of
Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
requires an agent to follow step-by-step natural language instructions to reach
a target. We present a modular plan-and-control approach for the task. Our
model consists of three modules: the candidate waypoints predictor (CWP), the
history enhanced planner and the tryout controller. In each decision loop, CWP
first predicts a set of candidate waypoints based on depth observations from
multiple views. It can reduce the complexity of the action space and facilitate
planning. Then, a history-enhanced planner is adopted to select one of the
candidate waypoints as the subgoal. The planner additionally encodes historical
memory to track the navigation progress, which is especially effective for
long-horizon navigation. Finally, we propose a non-parametric heuristic
controller named tryout to execute low-level actions to reach the planned
subgoal. It is based on the trial-and-error mechanism which can help the agent
to avoid obstacles and escape from getting stuck. All three modules work
hierarchically until the agent stops. We further take several recent advances
of Vision-and-Language Navigation (VLN) to improve the performance such as
pretraining based on large-scale synthetic in-domain dataset, environment-level
data augmentation and snapshot model ensemble. Our model won the RxR-Habitat
Competition 2022, with 48% and 90% relative improvements over existing methods
on NDTW and SR metrics respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evidence fusion with contextual discounting for multi-modality medical image segmentation. (arXiv:2206.11739v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11739">
<div class="article-summary-box-inner">
<span><p>As information sources are usually imperfect, it is necessary to take into
account their reliability in multi-source information fusion tasks. In this
paper, we propose a new deep framework allowing us to merge multi-MR image
segmentation results using the formalism of Dempster-Shafer theory while taking
into account the reliability of different modalities relative to different
classes. The framework is composed of an encoder-decoder feature extraction
module, an evidential segmentation module that computes a belief function at
each voxel for each modality, and a multi-modality evidence fusion module,
which assigns a vector of discount rates to each modality evidence and combines
the discounted evidence using Dempster's rule. The whole framework is trained
by minimizing a new loss function based on a discounted Dice index to increase
segmentation accuracy and reliability. The method was evaluated on the BraTs
2021 database of 1251 patients with brain tumors. Quantitative and qualitative
results show that our method outperforms the state of the art, and implements
an effective new idea for merging multi-information within deep neural
networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need. (arXiv:2206.11804v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11804">
<div class="article-summary-box-inner">
<span><p>Data diversity and volume are crucial to the success of training deep
learning models, while in the medical imaging field, the difficulty and cost of
data collection and annotation are especially huge. Specifically in robotic
surgery, data scarcity and imbalance have heavily affected the model accuracy
and limited the design and deployment of deep learning-based surgical
applications such as surgical instrument segmentation. Considering this, in
this paper, we rethink the surgical instrument segmentation task and propose a
one-to-many data generation solution that gets rid of the complicated and
expensive process of data collection and annotation from robotic surgery. In
our method, we only utilize a single surgical background tissue image and a few
open-source instrument images as the seed images and apply multiple
augmentations and blending techniques to synthesize amounts of image
variations. In addition, we also introduce the chained augmentation mixing
during training to further enhance the data diversities. The proposed approach
is evaluated on the real datasets of the EndoVis-2018 and EndoVis-2017 surgical
scene segmentation. Our empirical analysis suggests that without the high cost
of data collection and annotation, we can achieve decent surgical instrument
segmentation performance. Moreover, we also observe that our method can deal
with novel instrument prediction in the deployment domain. We hope our
inspiring results will encourage researchers to emphasize data-centric methods
to overcome demanding deep learning limitations besides data shortage, such as
class imbalance, domain adaptation, and incremental learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QReg: On Regularization Effects of Quantization. (arXiv:2206.12372v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12372">
<div class="article-summary-box-inner">
<span><p>In this paper we study the effects of quantization in DNN training. We
hypothesize that weight quantization is a form of regularization and the amount
of regularization is correlated with the quantization level (precision). We
confirm our hypothesis by providing analytical study and empirical results. By
modeling weight quantization as a form of additive noise to weights, we explore
how this noise propagates through the network at training time. We then show
that the magnitude of this noise is correlated with the level of quantization.
To confirm our analytical study, we performed an extensive list of experiments
summarized in this paper in which we show that the regularization effects of
quantization can be seen in various vision tasks and models, over various
datasets. Based on our study, we propose that 8-bit quantization provides a
reliable form of regularization in different vision tasks and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Driven Stylization of Video Objects. (arXiv:2206.12396v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12396">
<div class="article-summary-box-inner">
<span><p>We tackle the task of stylizing video objects in an intuitive and semantic
manner following a user-specified text prompt. This is a challenging task as
the resulting video must satisfy multiple properties: (1) it has to be
temporally consistent and avoid jittering or similar artifacts, (2) the
resulting stylization must preserve both the global semantics of the object and
its fine-grained details, and (3) it must adhere to the user-specified text
prompt. To this end, our method stylizes an object in a video according to two
target texts. The first target text prompt describes the global semantics and
the second target text prompt describes the local semantics. To modify the
style of an object, we harness the representational power of CLIP to get a
similarity score between (1) the local target text and a set of local stylized
views, and (2) a global target text and a set of stylized global views. We use
a pretrained atlas decomposition network to propagate the edits in a temporally
consistent manner. We demonstrate that our method can generate consistent style
changes over time for a variety of objects and videos, that adhere to the
specification of the target texts. We also show how varying the specificity of
the target texts and augmenting the texts with a set of prefixes results in
stylizations with different levels of detail. Full results are given on our
project webpage:
https://sloeschcke.github.io/Text-Driven-Stylization-of-Video-Objects/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counting Phases and Faces Using Bayesian Thermodynamic Integration. (arXiv:2206.07494v1 [cond-mat.stat-mech] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07494">
<div class="article-summary-box-inner">
<span><p>We introduce a new approach to reconstruction of the thermodynamic functions
and phase boundaries in two-parametric statistical mechanics systems. Our
method is based on expressing the Fisher metric in terms of the posterior
distributions over a space of external parameters and approximating the metric
field by a Hessian of a convex function. We use the proposed approach to
accurately reconstruct the partition functions and phase diagrams of the Ising
model and the exactly solvable non-equilibrium TASEP without any a priori
knowledge about microscopic rules of the models. We also demonstrate how our
approach can be used to visualize the latent space of StyleGAN models and
evaluate the variability of the generated images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sense The Physical, Walkthrough The Virtual, Manage The Metaverse: A Data-centric Perspective. (arXiv:2206.10326v1 [cs.HC] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10326">
<div class="article-summary-box-inner">
<span><p>In the Metaverse, the physical space and the virtual space co-exist, and
interact simultaneously. While the physical space is virtually enhanced with
information, the virtual space is continuously refreshed with real-time,
real-world information. To allow users to process and manipulate information
seamlessly between the real and digital spaces, novel technologies must be
developed. These include smart interfaces, new augmented realities, efficient
storage and data management and dissemination techniques. In this paper, we
first discuss some promising co-space applications. These applications offer
experiences and opportunities that neither of the spaces can realize on its
own. We then argue that the database community has much to offer to this field.
Finally, we present several challenges that we, as a community, can contribute
towards managing the Metaverse.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-28 23:07:55.345944338 UTC">2022-06-28 23:07:55 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>