<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-23T01:30:00Z">06-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Foundation Models Talk Causality?. (arXiv:2206.10591v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10591">
<div class="article-summary-box-inner">
<span><p>Foundation models are subject to an ongoing heated debate, leaving open the
question of progress towards AGI and dividing the community into two camps: the
ones who see the arguably impressive results as evidence to the scaling
hypothesis, and the others who are worried about the lack of interpretability
and reasoning capabilities. By investigating to which extent causal
representations might be captured by these large scale language models, we make
a humble efforts towards resolving the ongoing philosophical conflicts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10658">
<div class="article-summary-box-inner">
<span><p>We introduce ART, a new corpus-level autoencoding approach for training dense
retrieval models that does not require any labeled training data. Dense
retrieval is a central challenge for open-domain tasks, such as Open QA, where
state-of-the-art methods typically require large supervised datasets with
custom hard-negative mining and denoising of positive examples. ART, in
contrast, only requires access to unpaired inputs and outputs (e.g. questions
and potential answer documents). It uses a new document-retrieval autoencoding
scheme, where (1) an input question is used to retrieve a set of evidence
documents, and (2) the documents are then used to compute the probability of
reconstructing the original question. Training for retrieval based on question
reconstruction enables effective unsupervised learning of both document and
question encoders, which can be later incorporated into complete Open QA
systems without any further finetuning. Extensive experiments demonstrate that
ART obtains state-of-the-art results on multiple QA retrieval benchmarks with
only generic initialization from a pre-trained language model, removing the
need for labeled data and task-specific losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BenchCLAMP: A Benchmark for Evaluating Language Models on Semantic Parsing. (arXiv:2206.10668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10668">
<div class="article-summary-box-inner">
<span><p>We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model
Parsing, which produces semantic outputs based on the analysis of input text
through constrained decoding of a prompted or fine-tuned language model.
Developers of pretrained language models currently benchmark on classification,
span extraction and free-text generation tasks. Semantic parsing is neglected
in language model evaluation because of the complexity of handling
task-specific architectures and representations. Recent work has shown that
generation from a prompted or fine-tuned language model can perform well at
semantic parsing when the output is constrained to be a valid semantic
representation. BenchCLAMP includes context-free grammars for six semantic
parsing datasets with varied output meaning representations, as well as a
constrained decoding interface to generate outputs covered by these grammars.
We provide low, medium, and high resource splits for each dataset, allowing
accurate comparison of various language models under different data regimes.
Our benchmark supports both prompt-based learning as well as fine-tuning, and
provides an easy-to-use toolkit for language model developers to evaluate on
semantic parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the case for audience design in conversational AI: Rapport expectations and language ideologies in a task-oriented chatbot. (arXiv:2206.10694v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10694">
<div class="article-summary-box-inner">
<span><p>Chatbots are more and more prevalent in commercial and science contexts. They
help customers complain about a product or service or support them to find the
best travel deals. Other bots provide mental health support or help book
medical appointments. This paper argues that insights into users' language
ideologies and their rapport expectations can be used to inform the audience
design of the bot's language and interaction patterns and ensure equitable
access to the services provided by bots. The argument is underpinned by three
kinds of data: simulated user interactions with a chatbot facilitating health
appointment bookings, users' introspective comments on their interactions and
users' qualitative survey comments post engagement with the booking bot. In
closing, I will define audience design for conversational AI and discuss how
user-centred analyses of chatbot interactions and sociolinguistically informed
theoretical approaches, such as rapport management, can be used to support
audience design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TraSE: Towards Tackling Authorial Style from a Cognitive Science Perspective. (arXiv:2206.10706v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10706">
<div class="article-summary-box-inner">
<span><p>Stylistic analysis of text is a key task in research areas ranging from
authorship attribution to forensic analysis and personality profiling. The
existing approaches for stylistic analysis are plagued by issues like topic
influence, lack of discriminability for large number of authors and the
requirement for large amounts of diverse data. In this paper, the source of
these issues are identified along with the necessity for a cognitive
perspective on authorial style in addressing them. A novel feature
representation, called Trajectory-based Style Estimation (TraSE), is introduced
to support this purpose. Authorship attribution experiments with over 27,000
authors and 1.4 million samples in a cross-domain scenario resulted in 90%
attribution accuracy suggesting that the feature representation is immune to
such negative influences and an excellent candidate for stylistic analysis.
Finally, a qualitative analysis is performed on TraSE using physical human
characteristics, like age, to validate its claim on capturing cognitive traits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information. (arXiv:2206.10744v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10744">
<div class="article-summary-box-inner">
<span><p>The representations in large language models contain multiple types of gender
information. We focus on two types of such signals in English texts: factual
gender information, which is a grammatical or semantic property, and gender
bias, which is the correlation between a word and specific gender. We can
disentangle the model's embeddings and identify components encoding both types
of information with probing. We aim to diminish the stereotypical bias in the
representations while preserving the factual gender signal. Our filtering
method shows that it is possible to decrease the bias of gender-neutral
profession names without significant deterioration of language modeling
capabilities. The findings can be applied to language generation to mitigate
reliance on stereotypes while preserving gender agreement in coreferences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient and effective training of language and graph neural network models. (arXiv:2206.10781v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10781">
<div class="article-summary-box-inner">
<span><p>Can we combine heterogenous graph structure with text to learn high-quality
semantic and behavioural representations? Graph neural networks (GNN)s encode
numerical node attributes and graph structure to achieve impressive performance
in a variety of supervised learning tasks. Current GNN approaches are
challenged by textual features, which typically need to be encoded to a
numerical vector before provided to the GNN that may incur some information
loss. In this paper, we put forth an efficient and effective framework termed
language model GNN (LM-GNN) to jointly train large-scale language models and
graph neural networks. The effectiveness in our framework is achieved by
applying stage-wise fine-tuning of the BERT model first with heterogenous graph
information and then with a GNN model. Several system and design optimizations
are proposed to enable scalable and efficient training. LM-GNN accommodates
node and edge classification as well as link prediction tasks. We evaluate the
LM-GNN framework in different datasets performance and showcase the
effectiveness of the proposed approach. LM-GNN provides competitive results in
an Amazon query-purchase-product application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities. (arXiv:2206.10883v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10883">
<div class="article-summary-box-inner">
<span><p>With the advent of large language models, methods for abstractive
summarization have made great strides, creating potential for use in
applications to aid knowledge workers processing unwieldy document collections.
One such setting is the Civil Rights Litigation Clearinghouse (CRLC)
(https://clearinghouse.net),which posts information about large-scale civil
rights lawsuits, serving lawyers, scholars, and the general public. Today,
summarization in the CRLC requires extensive training of lawyers and law
students who spend hours per case understanding multiple relevant documents in
order to produce high-quality summaries of key events and outcomes. Motivated
by this ongoing real-world summarization effort, we introduce Multi-LexSum, a
collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.
Multi-LexSum presents a challenging multi-document summarization task given the
length of the source documents, often exceeding two hundred pages per case.
Furthermore, Multi-LexSum is distinct from other datasets in its multiple
target summaries, each at a different granularity (ranging from one-sentence
"extreme" summaries to multi-paragraph narrations of over five hundred words).
We present extensive analysis demonstrating that despite the high-quality
summaries in the training data (adhering to strict content and style
guidelines), state-of-the-art summarization models perform poorly on this task.
We release Multi-LexSum for further research in summarization methods as well
as to facilitate development of applications to assist in the CRLC's mission at
https://multilexsum.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Template-based Approach to Zero-shot Intent Recognition. (arXiv:2206.10914v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10914">
<div class="article-summary-box-inner">
<span><p>The recent advances in transfer learning techniques and pre-training of large
contextualized encoders foster innovation in real-life applications, including
dialog assistants. Practical needs of intent recognition require effective data
usage and the ability to constantly update supported intents, adopting new
ones, and abandoning outdated ones. In particular, the generalized zero-shot
paradigm, in which the model is trained on the seen intents and tested on both
seen and unseen intents, is taking on new importance. In this paper, we explore
the generalized zero-shot setup for intent recognition. Following best
practices for zero-shot text classification, we treat the task with a sentence
pair modeling approach. We outperform previous state-of-the-art f1-measure by
up to 16\% for unseen intents, using intent labels and user utterances and
without accessing external sources (such as knowledge bases). Further
enhancement includes lexicalization of intent labels, which improves
performance by up to 7\%. By using task transferring from other sentence pair
tasks, such as Natural Language Inference, we gain additional improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Formulaic Language in Human and Machine Translation: Insight from a Parliamentary Corpus. (arXiv:2206.10919v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10919">
<div class="article-summary-box-inner">
<span><p>A recent study has shown that, compared to human translations, neural machine
translations contain more strongly-associated formulaic sequences made of
relatively high-frequency words, but far less strongly-associated formulaic
sequences made of relatively rare words. These results were obtained on the
basis of translations of quality newspaper articles in which human translations
can be thought to be not very literal. The present study attempts to replicate
this research using a parliamentary corpus. The text were translated from
French to English by three well-known neural machine translation systems:
DeepL, Google Translate and Microsoft Translator. The results confirm the
observations on the news corpus, but the differences are less strong. They
suggest that the use of text genres that usually result in more literal
translations, such as parliamentary corpora, might be preferable when comparing
human and machine translations. Regarding the differences between the three
neural machine systems, it appears that Google translations contain fewer
highly collocational bigrams, identified by the CollGram technique, than Deepl
and Microsoft translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Networking Cipher Algorithms with Natural Language. (arXiv:2206.10924v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10924">
<div class="article-summary-box-inner">
<span><p>This work provides a survey of several networking cipher algorithms and
proposes a method for integrating natural language processing (NLP) as a
protective agent for them. Two main proposals are covered for the use of NLP in
networking. First, NLP is considered as the weakest link in a networking
encryption model; and, second, as a hefty deterrent when combined as an extra
layer over what could be considered a strong type of encryption -- the stream
cipher. This paper summarizes how languages can be integrated into symmetric
encryption as a way to assist in the encryption of vulnerable streams that may
be found under attack due to the natural frequency distribution of letters or
words in a local language stream.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Embedding Models for Automatic Extraction and Classification of Acknowledged Entities in Scientific Documents. (arXiv:2206.10939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10939">
<div class="article-summary-box-inner">
<span><p>Acknowledgments in scientific papers may give an insight into aspects of the
scientific community, such as reward systems, collaboration patterns, and
hidden research trends. The aim of the paper is to evaluate the performance of
different embedding models for the task of automatic extraction and
classification of acknowledged entities from the acknowledgment text in
scientific papers. We trained and implemented a named entity recognition (NER)
task using the Flair NLP-framework. The training was conducted using three
default Flair NER models with two differently-sized corpora. The Flair
Embeddings model trained on the larger training corpus showed the best accuracy
of 0.77. Our model is able to recognize six entity types: funding agency, grant
number, individuals, university, corporation and miscellaneous. The model works
more precise for some entity types than the others, thus, individuals and grant
numbers showed very good F1-Score over 0.9. Most of the previous works on
acknowledgement analysis were limited by the manual evaluation of data and
therefore by the amount of processed data. This model can be applied for the
comprehensive analysis of the acknowledgement texts and may potentially make a
great contribution to the field of automated acknowledgement analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward An Optimal Selection of Dialogue Strategies: A Target-Driven Approach for Intelligent Outbound Robots. (arXiv:2206.10953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10953">
<div class="article-summary-box-inner">
<span><p>With the growth of the economy and society, enterprises, especially in the
FinTech industry, have increasing demands of outbound calls for customers such
as debt collection, marketing, anti-fraud calls, and so on. But a large amount
of repetitive and mechanical work occupies most of the time of human agents, so
the cost of equipment and labor for enterprises is increasing accordingly. At
the same time, with the development of artificial intelligence technology in
the past few decades, it has become quite common for companies to use new
technologies such as Big Data and artificial intelligence to empower outbound
call businesses. The intelligent outbound robot is a typical application of the
artificial intelligence technology in the field of outbound call businesses. It
is mainly used to communicate with customers in order to accomplish a certain
target. It has the characteristics of low cost, high reuse, and easy
compliance, which has attracted more attention from the industry.
</p>
<p>At present, there are two kinds of intelligent outbound robots in the
industry but both of them still leave large room for improvement. One kind of
them is based on a finite state machine relying on the configuration of jump
conditions and corresponding nodes based on manual experience. This kind of
intelligent outbound robot is also called a flow-based robot. For example, the
schematic diagram of the working model of a flow-based robot for debt
collection is shown in Fig.\ref{fig:label}. In each round, the robot will reply
to the user with the words corresponding to each node.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting a French Dictionary from the Beginning of the 20th Century to Wikidata. (arXiv:2206.11022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11022">
<div class="article-summary-box-inner">
<span><p>The \textit{Petit Larousse illustr\'e} is a French dictionary first published
in 1905. Its division in two main parts on language and on history and
geography corresponds to a major milestone in French lexicography as well as a
repository of general knowledge from this period. Although the value of many
entries from 1905 remains intact, some descriptions now have a dimension that
is more historical than contemporary. They are nonetheless significant to
analyze and understand cultural representations from this time. A comparison
with more recent information or a verification of these entries would require a
tedious manual work. In this paper, we describe a new lexical resource, where
we connected all the dictionary entries of the history and geography part to
current data sources. For this, we linked each of these entries to a wikidata
identifier. Using the wikidata links, we can automate more easily the
identification, comparison, and verification of historically-situated
representations. We give a few examples on how to process wikidata identifiers
and we carried out a small analysis of the entities described in the dictionary
to outline possible applications. The resource, i.e. the annotation of 20,245
dictionary entries with wikidata links, is available from GitHub
(\url{https://github.com/pnugues/petit_larousse_1905/})
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answer Fast: Accelerating BERT on the Tensor Streaming Processor. (arXiv:2206.11062v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11062">
<div class="article-summary-box-inner">
<span><p>Transformers have become a predominant machine learning workload, they are
not only the de-facto standard for natural language processing tasks, but they
are also being deployed in other domains such as vision and speech recognition.
Many of the transformer-based applications are real-time systems such as
machine translation and web search. These real time systems often come with
strict end-to-end inference latency requirements. Unfortunately, while the
majority of the transformer computation comes from matrix multiplications,
transformers also include several non-linear components that tend to become the
bottleneck during an inference. In this work, we accelerate the inference of
BERT models on the tensor streaming processor. By carefully fusing all the
nonlinear components with the matrix multiplication components, we are able to
efficiently utilize the on-chip matrix multiplication units resulting in a
deterministic tail latency of 130 $\mu$s for a batch-1 inference through
BERT-base, which is 6X faster than the current state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Benefits of Free-Form Rationales. (arXiv:2206.11083v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11083">
<div class="article-summary-box-inner">
<span><p>Free-form rationales aim to aid model interpretability by supplying the
background knowledge that can help understand model decisions. Crowdsourced
rationales are provided for commonsense QA instances in popular datasets such
as CoS-E and ECQA, but their utility remains under-investigated. We present
human studies which show that ECQA rationales indeed provide additional
background information to understand a decision, while over 88% of CoS-E
rationales do not. Inspired by this finding, we ask: can the additional context
provided by free-form rationales benefit models, similar to human users? We
investigate the utility of rationales as an additional source of supervision,
by varying the quantity and quality of rationales during training. After
controlling for instances where rationales leak the correct answer while not
providing additional background knowledge, we find that incorporating only 5%
of rationales during training can boost model performance by 47.22% for CoS-E
and 57.14% for ECQA during inference. Moreover, we also show that rationale
quality matters: compared to crowdsourced rationales, T5-generated rationales
provide not only weaker supervision to models, but are also not helpful for
humans in aiding model interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing Multimodal Pre-training into Multilingual via Language Acquisition. (arXiv:2206.11091v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11091">
<div class="article-summary-box-inner">
<span><p>English-based Vision-Language Pre-training (VLP) has achieved great success
in various downstream tasks. Some efforts have been taken to generalize this
success to non-English languages through Multilingual Vision-Language
Pre-training (M-VLP). However, due to the large number of languages, M-VLP
models often require huge computing resources and cannot be flexibly extended
to new languages. In this work, we propose a \textbf{M}ulti\textbf{L}ingual
\textbf{A}cquisition (MLA) framework that can easily generalize a monolingual
Vision-Language Pre-training model into multilingual. Specifically, we design a
lightweight language acquisition encoder based on state-of-the-art monolingual
VLP models. We further propose a two-stage training strategy to optimize the
language acquisition encoder, namely the Native Language Transfer stage and the
Language Exposure stage. With much less multilingual training data and
computing resources, our model achieves state-of-the-art performance on
multilingual image-text and video-text retrieval benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Clustering for Open Knowledge Base Canonicalization. (arXiv:2206.11130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11130">
<div class="article-summary-box-inner">
<span><p>Open information extraction (OIE) methods extract plenty of OIE triples &lt;noun
phrase, relation phrase, noun phrase&gt; from unstructured text, which compose
large open knowledge bases (OKBs). Noun phrases and relation phrases in such
OKBs are not canonicalized, which leads to scattered and redundant facts. It is
found that two views of knowledge (i.e., a fact view based on the fact triple
and a context view based on the fact triple's source context) provide
complementary information that is vital to the task of OKB canonicalization,
which clusters synonymous noun phrases and relation phrases into the same group
and assigns them unique identifiers. However, these two views of knowledge have
so far been leveraged in isolation by existing works. In this paper, we propose
CMVC, a novel unsupervised framework that leverages these two views of
knowledge jointly for canonicalizing OKBs without the need of manually
annotated labels. To achieve this goal, we propose a multi-view CH K-Means
clustering algorithm to mutually reinforce the clustering of view-specific
embeddings learned from each view by considering their different clustering
qualities. In order to further enhance the canonicalization performance, we
propose a training data optimization strategy in terms of data quantity and
data quality respectively in each particular view to refine the learned
view-specific embeddings in an iterative manner. Additionally, we propose a
Log-Jump algorithm to predict the optimal number of clusters in a data-driven
way without requiring any labels. We demonstrate the superiority of our
framework through extensive experiments on multiple real-world OKB data sets
against state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Emergent Lexicon Formation with a Self-Reinforcing Stochastic Process. (arXiv:2206.11146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11146">
<div class="article-summary-box-inner">
<span><p>We introduce FiLex, a self-reinforcing stochastic process which models finite
lexicons in emergent language experiments. The central property of FiLex is
that it is a self-reinforcing process, parallel to the intuition that the more
a word is used in a language, the more its use will continue. As a theoretical
model, FiLex serves as a way to both explain and predict the behavior of the
emergent language system. We empirically test FiLex's ability to capture the
relationship between the emergent language's hyperparameters and the lexicon's
Shannon entropy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">reStructured Pre-training. (arXiv:2206.11147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11147">
<div class="article-summary-box-inner">
<span><p>In this work, we try to decipher the internal connection of NLP technology
development in the past decades, searching for essence, which rewards us with a
(potential) new learning paradigm for NLP tasks, dubbed as reStructured
Pre-training (RST). In such a paradigm, the role of data will be re-emphasized,
and model pre-training and fine-tuning of downstream tasks are viewed as a
process of data storing and accessing. Based on that, we operationalize the
simple principle that a good storage mechanism should not only have the ability
to cache a large amount of data but also consider the ease of access. We
achieve this by pre-training models over restructured data that consist of a
variety of valuable information instead of raw data after overcoming several
engineering challenges. Experimentally, RST models not only surpass strong
competitors (e.g., T0) on 52/55 popular datasets from a variety of NLP tasks,
but also achieve superior performance in National College Entrance Examination
- English (Gaokao-English),the most authoritative examination in China.
Specifically, the proposed system Qin achieves 40 points higher than the
average scores made by students and 15 points higher than GPT3 with 1/16
parameters. In particular, Qin gets a high score of 138.5 (the full mark is
150) in the 2018 English exam (national paper III). We have released the Gaokao
Benchmark with an online submission platform.
</p>
<p>In addition, we test our model in the 2022 College Entrance Examination
English that happened a few days ago (2022.06.08), and it gets a total score of
134 (v.s. GPT3's 108).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Then and Now: Quantifying the Longitudinal Validity of Self-Disclosed Depression Diagnoses. (arXiv:2206.11155v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11155">
<div class="article-summary-box-inner">
<span><p>Self-disclosed mental health diagnoses, which serve as ground truth
annotations of mental health status in the absence of clinical measures,
underpin the conclusions behind most computational studies of mental health
language from the last decade. However, psychiatric conditions are dynamic; a
prior depression diagnosis may no longer be indicative of an individual's
mental health, either due to treatment or other mitigating factors. We ask: to
what extent are self-disclosures of mental health diagnoses actually relevant
over time? We analyze recent activity from individuals who disclosed a
depression diagnosis on social media over five years ago and, in turn, acquire
a new understanding of how presentations of mental health status on social
media manifest longitudinally. We also provide expanded evidence for the
presence of personality-related biases in datasets curated using self-disclosed
diagnoses. Our findings motivate three practical recommendations for improving
mental health datasets curated using self-disclosed diagnoses: 1) Annotate
diagnosis dates and psychiatric comorbidities; 2) Sample control groups using
propensity score matching; 3) Identify and remove spurious correlations
introduced by selection bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Problem of Semantic Shift in Longitudinal Monitoring of Social Media: A Case Study on Mental Health During the COVID-19 Pandemic. (arXiv:2206.11160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11160">
<div class="article-summary-box-inner">
<span><p>Social media allows researchers to track societal and cultural changes over
time based on language analysis tools. Many of these tools rely on statistical
algorithms which need to be tuned to specific types of language. Recent studies
have shown the absence of appropriate tuning, specifically in the presence of
semantic shift, can hinder robustness of the underlying methods. However,
little is known about the practical effect this sensitivity may have on
downstream longitudinal analyses. We explore this gap in the literature through
a timely case study: understanding shifts in depression during the course of
the COVID-19 pandemic. We find that inclusion of only a small number of
semantically-unstable features can promote significant changes in longitudinal
estimates of our target outcome. At the same time, we demonstrate that a
recently-introduced method for measuring semantic shift may be used to
proactively identify failure points of language-based models and, in turn,
improve predictive generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unsupervised Content Disentanglement in Sentence Representations via Syntactic Roles. (arXiv:2206.11184v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11184">
<div class="article-summary-box-inner">
<span><p>Linking neural representations to linguistic factors is crucial in order to
build and analyze NLP models interpretable by humans. Among these factors,
syntactic roles (e.g. subjects, direct objects,$\dots$) and their realizations
are essential markers since they can be understood as a decomposition of
predicative structures and thus the meaning of sentences. Starting from a deep
probabilistic generative model with attention, we measure the interaction
between latent variables and realizations of syntactic roles and show that it
is possible to obtain, without supervision, representations of sentences where
different syntactic roles correspond to clearly identified different latent
variables. The probabilistic model we propose is an Attention-Driven
Variational Autoencoder (ADVAE). Drawing inspiration from Transformer-based
machine translation models, ADVAEs enable the analysis of the interactions
between latent variables and input tokens through attention. We also develop an
evaluation protocol to measure disentanglement with regard to the realizations
of syntactic roles. This protocol is based on attention maxima for the encoder
and on latent variable perturbations for the decoder. Our experiments on raw
English text from the SNLI dataset show that $\textit{i)}$ disentanglement of
syntactic roles can be induced without supervision, $\textit{ii)}$ ADVAE
separates syntactic roles better than classical sequence VAEs and Transformer
VAEs, $\textit{iii)}$ realizations of syntactic roles can be separately
modified in sentences by mere intervention on the associated latent variables.
Our work constitutes a first step towards unsupervised controllable content
generation. The code for our work is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives. (arXiv:2206.11212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11212">
<div class="article-summary-box-inner">
<span><p>Many past works aim to improve visual reasoning in models by supervising
feature importance (estimated by model explanation techniques) with human
annotations such as highlights of important image regions. However, recent work
has shown that performance gains from feature importance (FI) supervision for
Visual Question Answering (VQA) tasks persist even with random supervision,
suggesting that these methods do not meaningfully align model FI with human FI.
In this paper, we show that model FI supervision can meaningfully improve VQA
model accuracy as well as performance on several Right-for-the-Right-Reason
(RRR) metrics by optimizing for four key model objectives: (1) accurate
predictions given limited but sufficient information (Sufficiency); (2)
max-entropy predictions given no important information (Uncertainty); (3)
invariance of predictions to changes in unimportant features (Invariance); and
(4) alignment between model FI explanations and human FI explanations
(Plausibility). Our best performing method, Visual Feature Importance
Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in
terms of both in-distribution and out-of-distribution accuracy. While past work
suggests that the mechanism for improved accuracy is through improved
explanation plausibility, we show that this relationship depends crucially on
explanation faithfulness (whether explanations truly represent the model's
internal reasoning). Predictions are more accurate when explanations are
plausible and faithful, and not when they are plausible but not faithful.
Lastly, we show that, surprisingly, RRR metrics are not predictive of
out-of-distribution model accuracy when controlling for a model's
in-distribution accuracy, which calls into question the value of these metrics
for evaluating model reasoning. All supporting code is available at
https://github.com/zfying/visfis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Context Tagging for Utterance Rewriting. (arXiv:2206.11218v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11218">
<div class="article-summary-box-inner">
<span><p>Utterance rewriting aims to recover coreferences and omitted information from
the latest turn of a multi-turn dialogue. Recently, methods that tag rather
than linearly generate sequences have proven stronger in both in- and
out-of-domain rewriting settings. This is due to a tagger's smaller search
space as it can only copy tokens from the dialogue context. However, these
methods may suffer from low coverage when phrases that must be added to a
source utterance cannot be covered by a single context span. This can occur in
languages like English that introduce tokens such as prepositions into the
rewrite for grammaticality. We propose a hierarchical context tagger (HCT) that
mitigates this issue by predicting slotted rules (e.g., "besides _") whose
slots are later filled with context spans. HCT (i) tags the source string with
token-level edit actions and slotted rules and (ii) fills in the resulting rule
slots with spans from the dialogue context. This rule tagging allows HCT to add
out-of-context tokens and multiple spans at once; we further cluster the rules
to truncate the long tail of the rule distribution. Experiments on several
benchmarks show that HCT can outperform state-of-the-art rewriting systems by
~2 BLEU points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Properties of Generated Corpora. (arXiv:2206.11219v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11219">
<div class="article-summary-box-inner">
<span><p>Models for text generation have become focal for many research tasks and
especially for the generation of sentence corpora. However, understanding the
properties of an automatically generated text corpus remains challenging. We
propose a set of tools that examine the properties of generated text corpora.
Applying these tools on various generated corpora allowed us to gain new
insights into the properties of the generative models. As part of our
characterization process, we found remarkable differences in the corpora
generated by two leading generative technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEMv2: Multilingual NLG Benchmarking in a Single Line of Code. (arXiv:2206.11249v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11249">
<div class="article-summary-box-inner">
<span><p>Evaluation in machine learning is usually informed by past choices, for
example which datasets or metrics to use. This standardization enables the
comparison on equal footing using leaderboards, but the evaluation choices
become sub-optimal as better alternatives arise. This problem is especially
pertinent in natural language generation which requires ever-improving suites
of datasets, metrics, and human evaluation to make definitive claims. To make
following best model evaluation practices easier, we introduce GEMv2. The new
version of the Generation, Evaluation, and Metrics Benchmark introduces a
modular infrastructure for dataset, model, and metric developers to benefit
from each others work. GEMv2 supports 40 documented datasets in 51 languages.
Models for all datasets can be evaluated online and our interactive data card
creation and rendering tools make it easier to add new datasets to the living
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedFilter: Improving Extraction of Task-relevant Utterances from Doctor-Patient Conversations through Integration of Discourse Structure and Ontological Knowledge. (arXiv:2010.02246v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02246">
<div class="article-summary-box-inner">
<span><p>Information extraction from conversational data is particularly challenging
because the task-centric nature of conversation allows for effective
communication of implicit information by humans, but is challenging for
machines. The challenges may differ between utterances depending on the role of
the speaker within the conversation, especially when relevant expertise is
distributed asymmetrically across roles. Further, the challenges may also
increase over the conversation as more shared context is built up through
information communicated implicitly earlier in the dialogue. In this paper, we
propose the novel modeling approach MedFilter, which addresses these insights
in order to increase performance at identifying and categorizing task-relevant
utterances, and in so doing, positively impacts performance at a downstream
information extraction task. We evaluate this approach on a corpus of nearly
7,000 doctor-patient conversations where MedFilter is used to identify
medically relevant contributions to the discussion (achieving a 10% improvement
over SOTA baselines in terms of area under the PR curve). Identifying
task-relevant utterances benefits downstream medical processing, achieving
improvements of 15%, 105%, and 23% respectively for the extraction of symptoms,
medications, and complaints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diff-Explainer: Differentiable Convex Optimization for Explainable Multi-hop Inference. (arXiv:2105.03417v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03417">
<div class="article-summary-box-inner">
<span><p>This paper presents Diff-Explainer, the first hybrid framework for
explainable multi-hop inference that integrates explicit constraints with
neural architectures through differentiable convex optimization. Specifically,
Diff-Explainer allows for the fine-tuning of neural representations within a
constrained optimization framework to answer and explain multi-hop questions in
natural language. To demonstrate the efficacy of the hybrid framework, we
combine existing ILP-based solvers for multi-hop Question Answering (QA) with
Transformer-based representations. An extensive empirical evaluation on
scientific and commonsense QA tasks demonstrates that the integration of
explicit constraints in an end-to-end differentiable framework can
significantly improve the performance of non-differentiable ILP solvers (8.91%
- 13.3%). Moreover, additional analysis reveals that Diff-Explainer is able to
achieve strong performance when compared to standalone Transformers and
previous multi-hop approaches while still providing structured explanations in
support of its predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Dementia from Speech and Transcripts using Transformers. (arXiv:2110.14769v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14769">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious
consequences to peoples' everyday lives, if it is not diagnosed early since
there is no available cure. Alzheimer's is the most common cause of dementia,
which constitutes a general term for loss of memory. Due to the fact that
dementia affects speech, existing research initiatives focus on detecting
dementia from spontaneous speech. However, little work has been done regarding
the conversion of speech data to Log-Mel spectrograms and Mel-frequency
cepstral coefficients (MFCCs) and the usage of pretrained models. Concurrently,
little work has been done in terms of both the usage of transformer networks
and the way the two modalities, i.e., speech and transcripts, are combined in a
single neural network. To address these limitations, first we employ several
pretrained models, with Vision Transformer (ViT) achieving the highest
evaluation results. Secondly, we propose multimodal models. More specifically,
our introduced models include Gated Multimodal Unit in order to control the
influence of each modality towards the final classification and crossmodal
attention so as to capture in an effective way the relationships between the
two modalities. Extensive experiments conducted on the ADReSS Challenge dataset
demonstrate the effectiveness of the proposed models and their superiority over
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07991">
<div class="article-summary-box-inner">
<span><p>This paper presents contrastive-tuning, a simple method employing contrastive
training to align image and text models while still taking advantage of their
pre-training. In our empirical study we find that locked pre-trained image
models with unlocked text models work best. We call this instance of
contrastive-tuning "Locked-image Tuning" (LiT), which just teaches a text model
to read out good representations from a pre-trained image model for new tasks.
A LiT model gains the capability of zero-shot transfer to new vision tasks,
such as image classification or retrieval. The proposed LiT is widely
applicable; it works reliably with multiple pre-training methods (supervised
and unsupervised) and across diverse architectures (ResNet, Vision Transformers
and MLP-Mixer) using three different image-text datasets. With the
transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2%
zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the
challenging out-of-distribution ObjectNet test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surfer100: Generating Surveys From Web Resources, Wikipedia-style. (arXiv:2112.06377v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06377">
<div class="article-summary-box-inner">
<span><p>Fast-developing fields such as Artificial Intelligence (AI) often outpace the
efforts of encyclopedic sources such as Wikipedia, which either do not
completely cover recently-introduced topics or lack such content entirely. As a
result, methods for automatically producing content are valuable tools to
address this information overload. We show that recent advances in pretrained
language modeling can be combined for a two-stage extractive and abstractive
approach for Wikipedia lead paragraph generation. We extend this approach to
generate longer Wikipedia-style summaries with sections and examine how such
methods struggle in this application through detailed studies with 100
reference human-collected surveys. This is the first study on utilizing web
resources for long Wikipedia-style summaries to the best of our knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02394">
<div class="article-summary-box-inner">
<span><p>Large Language Models have been successful in a wide variety of Natural
Language Processing tasks by capturing the compositionality of the text
representations. In spite of their great success, these vector representations
fail to capture meaning of idiomatic multi-word expressions (MWEs). In this
paper, we focus on the detection of idiomatic expressions by using binary
classification. We use a dataset consisting of the literal and idiomatic usage
of MWEs in English and Portuguese. Thereafter, we perform the classification in
two different settings: zero shot and one shot, to determine if a given
sentence contains an idiom or not. N shot classification for this task is
defined by N number of common idioms between the training and testing sets. In
this paper, we train multiple Large Language Models in both the settings and
achieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score
(macro) of 0.85 for the one shot setting. An implementation of our work can be
found at
https://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05482">
<div class="article-summary-box-inner">
<span><p>The conventional recipe for maximizing model accuracy is to (1) train
multiple models with various hyperparameters and (2) pick the individual model
which performs best on a held-out validation set, discarding the remainder. In
this paper, we revisit the second step of this procedure in the context of
fine-tuning large pre-trained models, where fine-tuned models often appear to
lie in a single low error basin. We show that averaging the weights of multiple
models fine-tuned with different hyperparameter configurations often improves
accuracy and robustness. Unlike a conventional ensemble, we may average many
models without incurring any additional inference or memory costs -- we call
the results "model soups." When fine-tuning large pre-trained models such as
CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides
significant improvements over the best model in a hyperparameter sweep on
ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on
ImageNet, achieved a new state of the art. Furthermore, we show that the model
soup approach extends to multiple image classification and natural language
processing tasks, improves out-of-distribution performance, and improves
zero-shot performance on new downstream tasks. Finally, we analytically relate
the performance similarity of weight-averaging and logit-ensembling to flatness
of the loss and confidence of the predictions, and validate this relation
empirically. Code is available at https://github.com/mlfoundations/model-soups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EHRKit: A Python Natural Language Processing Toolkit for Electronic Health Record Texts. (arXiv:2204.06604v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06604">
<div class="article-summary-box-inner">
<span><p>The Electronic Health Record (EHR) is an essential part of the modern medical
system and impacts healthcare delivery, operations, and research. Unstructured
text is attracting much attention despite structured information in the EHRs
and has become an exciting research field. The success of the recent neural
Natural Language Processing (NLP) method has led to a new direction for
processing unstructured clinical notes. In this work, we create a python
library for clinical texts, EHRKit. This library contains two main parts:
MIMIC-III-specific functions and tasks specific functions. The first part
introduces a list of interfaces for accessing MIMIC-III NOTEEVENTS data,
including basic search, information retrieval, and information extraction. The
second part integrates many third-party libraries for up to 12 off-shelf NLP
tasks such as named entity recognition, summarization, machine translation,
etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning. (arXiv:2206.03715v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03715">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning systems should be able to generalize to diverse
reasoning cases. However, most state-of-the-art approaches depend on expensive
data annotations and overfit to a specific benchmark without learning how to
perform general semantic reasoning. To overcome these drawbacks, zero-shot QA
systems have shown promise as a robust learning scheme by transforming a
commonsense knowledge graph (KG) into synthetic QA-form samples for model
training. Considering the increasing type of different commonsense KGs, this
paper aims to extend the zero-shot transfer learning scenario into
multiple-source settings, where different KGs can be utilized synergetically.
Towards this goal, we propose to mitigate the loss of knowledge from the
interference among the different knowledge sources, by developing a modular
variant of the knowledge aggregation as a new zero-shot commonsense reasoning
framework. Results on five commonsense reasoning benchmarks demonstrate the
efficacy of our framework, improving the performance with multiple KGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAAMA 2.0: An Integrated System that Answers Boolean and Extractive Questions. (arXiv:2206.08441v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08441">
<div class="article-summary-box-inner">
<span><p>Recent machine reading comprehension datasets include extractive and boolean
questions but current approaches do not offer integrated support for answering
both question types. We present a multilingual machine reading comprehension
system and front-end demo that handles boolean questions by providing both a
YES/NO answer and highlighting supporting evidence, and handles extractive
questions by highlighting the answer in the passage. Our system, GAAMA 2.0, is
ranked first on the Tydi QA leaderboard at the time of this writing. We
contrast two different implementations of our approach. The first includes
several independent stacks of transformers allowing easy deployment of each
component. The second is a single stack of transformers utilizing adapters to
reduce GPU memory footprint in a resource-constrained environment.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCoPIE XGen: A Full-Stack AI-Oriented Optimizing Framework. (arXiv:2206.10620v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10620">
<div class="article-summary-box-inner">
<span><p>There is a growing demand for shifting the delivery of AI capability from
data centers on the cloud to edge or end devices, exemplified by the fast
emerging real-time AI-based apps running on smartphones, AR/VR devices,
autonomous vehicles, and various IoT devices. The shift has however been
seriously hampered by the large growing gap between DNN computing demands and
the computing power on edge or end devices. This article presents the design of
XGen, an optimizing framework for DNN designed to bridge the gap. XGen takes
cross-cutting co-design as its first-order consideration. Its full-stack
AI-oriented optimizations consist of a number of innovative optimizations at
every layer of the DNN software stack, all designed in a cooperative manner.
The unique technology makes XGen able to optimize various DNNs, including those
with an extreme depth (e.g., BERT, GPT, other transformers), and generate code
that runs several times faster than those from existing DNN frameworks, while
delivering the same level of accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BOSS: A Benchmark for Human Belief Prediction in Object-context Scenarios. (arXiv:2206.10665v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10665">
<div class="article-summary-box-inner">
<span><p>Humans with an average level of social cognition can infer the beliefs of
others based solely on the nonverbal communication signals (e.g. gaze, gesture,
pose and contextual information) exhibited during social interactions. This
social cognitive ability to predict human beliefs and intentions is more
important than ever for ensuring safe human-robot interaction and
collaboration. This paper uses the combined knowledge of Theory of Mind (ToM)
and Object-Context Relations to investigate methods for enhancing collaboration
between humans and autonomous systems in environments where verbal
communication is prohibited. We propose a novel and challenging multimodal
video dataset for assessing the capability of artificial intelligence (AI)
systems in predicting human belief states in an object-context scenario. The
proposed dataset consists of precise labelling of human belief state
ground-truth and multimodal inputs replicating all nonverbal communication
inputs captured by human perception. We further evaluate our dataset with
existing deep learning models and provide new insights into the effects of the
various input modalities and object-context relations on the performance of the
baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCIM: Simultaneous Clustering, Inference, and Mapping for Open-World Semantic Scene Understanding. (arXiv:2206.10670v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10670">
<div class="article-summary-box-inner">
<span><p>In order to operate in human environments, a robot's semantic perception has
to overcome open-world challenges such as novel objects and domain gaps.
Autonomous deployment to such environments therefore requires robots to update
their knowledge and learn without supervision. We investigate how a robot can
autonomously discover novel semantic classes and improve accuracy on known
classes when exploring an unknown environment. To this end, we develop a
general framework for mapping and clustering that we then use to generate a
self-supervised learning signal to update a semantic segmentation model. In
particular, we show how clustering parameters can be optimized during
deployment and that fusion of multiple observation modalities improves novel
object discovery compared to prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Backdoor Datasets. (arXiv:2206.10673v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10673">
<div class="article-summary-box-inner">
<span><p>Extensive literature on backdoor poison attacks has studied attacks and
defenses for backdoors using "digital trigger patterns." In contrast, "physical
backdoors" use physical objects as triggers, have only recently been
identified, and are qualitatively different enough to resist all defenses
targeting digital trigger backdoors. Research on physical backdoors is limited
by access to large datasets containing real images of physical objects
co-located with targets of classification. Building these datasets is time- and
labor-intensive. This works seeks to address the challenge of accessibility for
research on physical backdoor attacks. We hypothesize that there may be
naturally occurring physically co-located objects already present in popular
datasets such as ImageNet. Once identified, a careful relabeling of these data
can transform them into training samples for physical backdoor attacks. We
propose a method to scalably identify these subsets of potential triggers in
existing datasets, along with the specific classes they can poison. We call
these naturally occurring trigger-class subsets natural backdoor datasets. Our
techniques successfully identify natural backdoors in widely-available
datasets, and produce models behaviorally equivalent to those trained on
manually curated datasets. We release our code to allow the research community
to create their own datasets for research on physical backdoor attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Continuous Rotation Canonicalization with Radial Beam Sampling. (arXiv:2206.10690v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10690">
<div class="article-summary-box-inner">
<span><p>Nearly all state of the art vision models are sensitive to image rotations.
Existing methods often compensate for missing inductive biases by using
augmented training data to learn pseudo-invariances. Alongside the resource
demanding data inflation process, predictions often poorly generalize. The
inductive biases inherent to convolutional neural networks allow for
translation equivariance through kernels acting parallely to the horizontal and
vertical axes of the pixel grid. This inductive bias, however, does not allow
for rotation equivariance. We propose a radial beam sampling strategy along
with radial kernels operating on these beams to inherently incorporate
center-rotation covariance. Together with an angle distance loss, we present a
radial beam-based image canonicalization model, short BIC. Our model allows for
maximal continuous angle regression and canonicalizes arbitrary center-rotated
input images. As a pre-processing model, this enables rotation-invariant vision
pipelines with model-agnostic rotation-sensitive downstream predictions. We
show that our end-to-end trained angle regressor is able to predict continuous
rotation angles on several vision datasets, i.e. FashionMNIST, CIFAR10,
COIL100, and LFW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Domain Adaptation for Lane Detection. (arXiv:2206.10692v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10692">
<div class="article-summary-box-inner">
<span><p>We focus on bridging domain discrepancy in lane detection among different
scenarios to greatly reduce extra annotation and re-training costs for
autonomous driving. Critical factors hinder the performance improvement of
cross-domain lane detection that conventional methods only focus on pixel-wise
loss while ignoring shape and position priors of lanes. To address the issue,
we propose the Multi-level Domain Adaptation (MLDA) framework, a new
perspective to handle cross-domain lane detection at three complementary
semantic levels of pixel, instance and category. Specifically, at pixel level,
we propose to apply cross-class confidence constraints in self-training to
tackle the imbalanced confidence distribution of lane and background. At
instance level, we go beyond pixels to treat segmented lanes as instances and
facilitate discriminative features in target domain with triplet learning,
which effectively rebuilds the semantic context of lanes and contributes to
alleviating the feature confusion. At category level, we propose an adaptive
inter-domain embedding module to utilize the position prior of lanes during
adaptation. In two challenging datasets, ie TuSimple and CULane, our approach
improves lane detection performance by a large margin with gains of 8.8% on
accuracy and 7.4% on F1-score respectively, compared with state-of-the-art
domain adaptation algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning. (arXiv:2206.10698v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10698">
<div class="article-summary-box-inner">
<span><p>We present Transformation Invariance and Covariance Contrast (TiCo) for
self-supervised visual representation learning. Similar to other recent
self-supervised learning methods, our method is based on maximizing the
agreement among embeddings of different distorted versions of the same image,
which pushes the encoder to produce transformation invariant representations.
To avoid the trivial solution where the encoder generates constant vectors, we
regularize the covariance matrix of the embeddings from different images by
penalizing low rank solutions. By jointly minimizing the transformation
invariance loss and covariance contrast loss, we get an encoder that is able to
produce useful representations for downstream tasks. We analyze our method and
show that it can be viewed as a variant of MoCo with an implicit memory bank of
unlimited size at no extra memory cost. This makes our method perform better
than alternative methods when using small batch sizes. TiCo can also be seen as
a modification of Barlow Twins. By connecting the contrastive and
redundancy-reduction methods together, TiCo gives us new insights into how
joint embedding methods work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoramic Panoptic Segmentation: Insights Into Surrounding Parsing for Mobile Agents via Unsupervised Contrastive Learning. (arXiv:2206.10711v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10711">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce panoramic panoptic segmentation, as the most
holistic scene understanding, both in terms of Field of View (FoV) and
image-level understanding for standard camera-based input. A complete
surrounding understanding provides a maximum of information to a mobile agent,
which is essential for any intelligent vehicle in order to make informed
decisions in a safety-critical dynamic environment such as real-world traffic.
In order to overcome the lack of annotated panoramic images, we propose a
framework which allows model training on standard pinhole images and transfers
the learned features to a different domain in a cost-minimizing way. Using our
proposed method with dense contrastive learning, we manage to achieve
significant improvements over a non-adapted approach. Depending on the
efficient panoptic segmentation architecture, we can improve 3.5-6.5% measured
in Panoptic Quality (PQ) over non-adapted models on our established Wild
Panoramic Panoptic Segmentation (WildPPS) dataset. Furthermore, our efficient
framework does not need access to the images of the target domain, making it a
feasible domain generalization approach suitable for a limited hardware
setting. As additional contributions, we publish WildPPS: The first panoramic
panoptic image dataset to foster progress in surrounding perception and explore
a novel training procedure combining supervised and contrastive training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Metric Color Embeddings for Splicing Localization in Severely Degraded Images. (arXiv:2206.10737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10737">
<div class="article-summary-box-inner">
<span><p>One common task in image forensics is to detect spliced images, where
multiple source images are composed to one output image. Most of the currently
best performing splicing detectors leverage high-frequency artifacts. However,
after an image underwent strong compression, most of the high frequency
artifacts are not available anymore. In this work, we explore an alternative
approach to splicing detection, which is potentially better suited for images
in-the-wild, subject to strong compression and downsampling. Our proposal is to
model the color formation of an image. The color formation largely depends on
variations at the scale of scene objects, and is hence much less dependent on
high-frequency artifacts. We learn a deep metric space that is on one hand
sensitive to illumination color and camera white-point estimation, but on the
other hand insensitive to variations in object color. Large distances in the
embedding space indicate that two image regions either stem from different
scenes or different cameras. In our evaluation, we show that the proposed
embedding space outperforms the state of the art on images that have been
subject to strong compression and downsampling. We confirm in two further
experiments the dual nature of the metric space, namely to both characterize
the acquisition camera and the scene illuminant color. As such, this work
resides at the intersection of physics-based and statistical forensics with
benefits from both sides.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Floor Map Reconstruction Through Radio Sensing and Learning By a Large Intelligent Surface. (arXiv:2206.10750v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10750">
<div class="article-summary-box-inner">
<span><p>Environmental scene reconstruction is of great interest for autonomous
robotic applications, since an accurate representation of the environment is
necessary to ensure safe interaction with robots. Equally important, it is also
vital to ensure reliable communication between the robot and its controller.
Large Intelligent Surface (LIS) is a technology that has been extensively
studied due to its communication capabilities. Moreover, due to the number of
antenna elements, these surfaces arise as a powerful solution to radio sensing.
This paper presents a novel method to translate radio environmental maps
obtained at the LIS to floor plans of the indoor environment built of
scatterers spread along its area. The usage of a Least Squares (LS) based
method, U-Net (UN) and conditional Generative Adversarial Networks (cGANs) were
leveraged to perform this task. We show that the floor plan can be correctly
reconstructed using both local and global measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Ground Truth for Single Image Deraining. (arXiv:2206.10779v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10779">
<div class="article-summary-box-inner">
<span><p>We propose a large-scale dataset of real-world rainy and clean image pairs
and a method to remove degradations, induced by rain streaks and rain
accumulation, from the image. As there exists no real-world dataset for
deraining, current state-of-the-art methods rely on synthetic data and thus are
limited by the sim2real domain gap; moreover, rigorous evaluation remains a
challenge due to the absence of a real paired dataset. We fill this gap by
collecting the first real paired deraining dataset through meticulous control
of non-rain variations. Our dataset enables paired training and quantitative
evaluation for diverse real-world rain phenomena (e.g. rain streaks and rain
accumulation). To learn a representation invariant to rain phenomena, we
propose a deep neural network that reconstructs the underlying scene by
minimizing a rain-invariant loss between rainy and clean images. Extensive
experiments demonstrate that the proposed dataset benefits existing derainers,
and our model can outperform the state-of-the-art deraining methods on real
rainy images under various conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Autoregressive Models for Content-Rich Text-to-Image Generation. (arXiv:2206.10789v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10789">
<div class="article-summary-box-inner">
<span><p>We present the Pathways Autoregressive Text-to-Image (Parti) model, which
generates high-fidelity photorealistic images and supports content-rich
synthesis involving complex compositions and world knowledge. Parti treats
text-to-image generation as a sequence-to-sequence modeling problem, akin to
machine translation, with sequences of image tokens as the target outputs
rather than text tokens in another language. This strategy can naturally tap
into the rich body of prior work on large language models, which have seen
continued advances in capabilities and performance through scaling data and
model sizes. Our approach is simple: First, Parti uses a Transformer-based
image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens.
Second, we achieve consistent quality improvements by scaling the
encoder-decoder Transformer model up to 20B parameters, with a new
state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on
MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts
(P2), a new holistic benchmark of over 1600 English prompts, demonstrate the
effectiveness of Parti across a wide variety of categories and difficulty
aspects. We also explore and highlight limitations of our models in order to
define and exemplify key areas of focus for further improvements. See
https://parti.research.google/ for high-resolution images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imitation Learning for Generalizable Self-driving Policy with Sim-to-real Transfer. (arXiv:2206.10797v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10797">
<div class="article-summary-box-inner">
<span><p>Imitation Learning uses the demonstrations of an expert to uncover the
optimal policy and it is suitable for real-world robotics tasks as well. In
this case, however, the training of the agent is carried out in a simulation
environment due to safety, economic and time constraints. Later, the agent is
applied in the real-life domain using sim-to-real methods. In this paper, we
apply Imitation Learning methods that solve a robotics task in a simulated
environment and use transfer learning to apply these solutions in the
real-world environment. Our task is set in the Duckietown environment, where
the robotic agent has to follow the right lane based on the input images of a
single forward-facing camera. We present three Imitation Learning and two
sim-to-real methods capable of achieving this task. A detailed comparison is
provided on these techniques to highlight their advantages and disadvantages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI. (arXiv:2206.10802v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10802">
<div class="article-summary-box-inner">
<span><p>Volumetric reconstruction of fetal brains from multiple stacks of MR slices,
acquired in the presence of almost unpredictable and often severe subject
motion, is a challenging task that is highly sensitive to the initialization of
slice-to-volume transformations. We propose a novel slice-to-volume
registration method using Transformers trained on synthetically transformed
data, which model multiple stacks of MR slices as a sequence. With the
attention mechanism, our model automatically detects the relevance between
slices and predicts the transformation of one slice using information from
other slices. We also estimate the underlying 3D volume to assist
slice-to-volume registration and update the volume and transformations
alternately to improve accuracy. Results on synthetic data show that our method
achieves lower registration error and better reconstruction quality compared
with existing state-of-the-art methods. Experiments with real-world MRI data
are also performed to demonstrate the ability of the proposed model to improve
the quality of 3D reconstruction under severe fetal motion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSMI: How to Make Objects of Interest Disappear without Accessing Object Detectors?. (arXiv:2206.10809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10809">
<div class="article-summary-box-inner">
<span><p>Most black-box adversarial attack schemes for object detectors mainly face
two shortcomings: requiring access to the target model and generating
inefficient adversarial examples (failing to make objects disappear in large
numbers). To overcome these shortcomings, we propose a black-box adversarial
attack scheme based on semantic segmentation and model inversion (SSMI). We
first locate the position of the target object using semantic segmentation
techniques. Next, we design a neighborhood background pixel replacement to
replace the target region pixels with background pixels to ensure that the
pixel modifications are not easily detected by human vision. Finally, we
reconstruct a machine-recognizable example and use the mask matrix to select
pixels in the reconstructed example to modify the benign image to generate an
adversarial example. Detailed experimental results show that SSMI can generate
efficient adversarial examples to evade human-eye perception and make objects
of interest disappear. And more importantly, SSMI outperforms existing same
kinds of attacks. The maximum increase in new and disappearing labels is 16%,
and the maximum decrease in mAP metrics for object detection is 36%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Attention is Needed: Grouped Spatial-temporal Shift for Simple and Efficient Video Restorers. (arXiv:2206.10810v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10810">
<div class="article-summary-box-inner">
<span><p>Video restoration, aiming at restoring clear frames from degraded videos, has
been attracting increasing attention. Video restoration is required to
establish the temporal correspondences from multiple misaligned frames. To
achieve that end, existing deep methods generally adopt complicated network
architectures, such as integrating optical flow, deformable convolution,
cross-frame or cross-pixel self-attention layers, resulting in expensive
computational cost. We argue that with proper design, temporal information
utilization in video restoration can be much more efficient and effective. In
this study, we propose a simple, fast yet effective framework for video
restoration. The key of our framework is the grouped spatial-temporal shift,
which is simple and lightweight, but can implicitly establish inter-frame
correspondences and achieve multi-frame aggregation. Coupled with basic 2D
U-Nets for frame-wise encoding and decoding, such an efficient spatial-temporal
shift module can effectively tackle the challenges in video restoration.
Extensive experiments show that our framework surpasses previous
state-of-the-art method with 43% of its computational cost on both video
deblurring and video denoising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fighting Fire with Fire: Avoiding DNN Shortcuts through Priming. (arXiv:2206.10816v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10816">
<div class="article-summary-box-inner">
<span><p>Across applications spanning supervised classification and sequential
control, deep learning has been reported to find "shortcut" solutions that fail
catastrophically under minor changes in the data distribution. In this paper,
we show empirically that DNNs can be coaxed to avoid poor shortcuts by
providing an additional "priming" feature computed from key input features,
usually a coarse output estimate. Priming relies on approximate domain
knowledge of these task-relevant key input features, which is often easy to
obtain in practical settings. For example, one might prioritize recent frames
over past frames in a video input for visual imitation learning, or salient
foreground over background pixels for image classification. On NICO image
classification, MuJoCo continuous control, and CARLA autonomous driving, our
priming strategy works significantly better than several popular
state-of-the-art approaches for feature selection and data augmentation. We
connect these empirical findings to recent theoretical results on DNN
optimization, and argue theoretically that priming distracts the optimizer away
from poor shortcuts by creating better, simpler shortcuts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coupling Visual Semantics of Artificial Neural Networks and Human Brain Function via Synchronized Activations. (arXiv:2206.10821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10821">
<div class="article-summary-box-inner">
<span><p>Artificial neural networks (ANNs), originally inspired by biological neural
networks (BNNs), have achieved remarkable successes in many tasks such as
visual representation learning. However, whether there exists semantic
correlations/connections between the visual representations in ANNs and those
in BNNs remains largely unexplored due to both the lack of an effective tool to
link and couple two different domains, and the lack of a general and effective
framework of representing the visual semantics in BNNs such as human functional
brain networks (FBNs). To answer this question, we propose a novel
computational framework, Synchronized Activations (Sync-ACT), to couple the
visual representation spaces and semantics between ANNs and BNNs in human brain
based on naturalistic functional magnetic resonance imaging (nfMRI) data. With
this approach, we are able to semantically annotate the neurons in ANNs with
biologically meaningful description derived from human brain imaging for the
first time. We evaluated the Sync-ACT framework on two publicly available
movie-watching nfMRI datasets. The experiments demonstrate a) the significant
correlation and similarity of the semantics between the visual representations
in FBNs and those in a variety of convolutional neural networks (CNNs) models;
b) the close relationship between CNN's visual representation similarity to
BNNs and its performance in image classification tasks. Overall, our study
introduces a general and effective paradigm to couple the ANNs and BNNs and
provides novel insights for future studies such as brain-inspired artificial
intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Feature Memory Rearrangement Network for Visual Inspection of Textured Surface Defects Toward Edge Intelligent Manufacturing. (arXiv:2206.10830v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10830">
<div class="article-summary-box-inner">
<span><p>Recent advances in the industrial inspection of textured surfaces-in the form
of visual inspection-have made such inspections possible for efficient,
flexible manufacturing systems. We propose an unsupervised feature memory
rearrangement network (FMR-Net) to accurately detect various textural defects
simultaneously. Consistent with mainstream methods, we adopt the idea of
background reconstruction; however, we innovatively utilize artificial
synthetic defects to enable the model to recognize anomalies, while traditional
wisdom relies only on defect-free samples. First, we employ an encoding module
to obtain multiscale features of the textured surface. Subsequently, a
contrastive-learning-based memory feature module (CMFM) is proposed to obtain
discriminative representations and construct a normal feature memory bank in
the latent space, which can be employed as a substitute for defects and fast
anomaly scores at the patch level. Next, a novel global feature rearrangement
module (GFRM) is proposed to further suppress the reconstruction of residual
defects. Finally, a decoding module utilizes the restored features to
reconstruct the normal texture background. In addition, to improve inspection
performance, a two-phase training strategy is utilized for accurate defect
restoration refinement, and we exploit a multimodal inspection method to
achieve noise-robust defect localization. We verify our method through
extensive experiments and test its practical deployment in collaborative
edge--cloud intelligent manufacturing scenarios by means of a multilevel
detection method, demonstrating that FMR-Net exhibits state-of-the-art
inspection accuracy and shows great potential for use in edge-computing-enabled
smart industries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiEarth 2022 Deforestation Challenge -- ForestGump. (arXiv:2206.10831v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10831">
<div class="article-summary-box-inner">
<span><p>The estimation of deforestation in the Amazon Forest is challenge task
because of the vast size of the area and the difficulty of direct human access.
However, it is a crucial problem in that deforestation results in serious
environmental problems such as global climate change, reduced biodiversity,
etc. In order to effectively solve the problems, satellite imagery would be a
good alternative to estimate the deforestation of the Amazon. With a
combination of optical images and Synthetic aperture radar (SAR) images,
observation of such a massive area regardless of weather conditions become
possible. In this paper, we present an accurate deforestation estimation method
with conventional UNet and comprehensive data processing. The diverse channels
of Sentinel-1, Sentinel-2 and Landsat 8 are carefully selected and utilized to
train deep neural networks. With the proposed method, deforestation status for
novel queries are successfully estimated with high accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Debiased Classifier with Biased Committee. (arXiv:2206.10843v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10843">
<div class="article-summary-box-inner">
<span><p>Neural networks are prone to be biased towards spurious correlations between
classes and latent attributes exhibited in a major portion of training data,
which ruins their generalization capability. This paper proposes a new method
for training debiased classifiers with no spurious attribute label. The key
idea of the method is to employ a committee of classifiers as an auxiliary
module that identifies bias-conflicting data, i.e., data without spurious
correlations, and assigns large weights to them when training the main
classifier. The committee is learned as a bootstrapped ensemble so that a
majority of its classifiers are biased as well as being diverse, and
intentionally fail to predict classes of bias-conflicting data accordingly. The
consensus within the committee on prediction difficulty thus provides a
reliable cue for identifying and weighting bias-conflicting data. Moreover, the
committee is also trained with knowledge transferred from the main classifier
so that it gradually becomes debiased along with the main classifier and
emphasizes more difficult data as training progresses. On five real-world
datasets, our method outperforms existing methods using no spurious attribute
label like ours and even surpasses those relying on bias labels occasionally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation. (arXiv:2206.10845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10845">
<div class="article-summary-box-inner">
<span><p>Recently, Synthetic data-based Instance Segmentation has become an
exceedingly favorable optimization paradigm since it leverages simulation
rendering and physics to generate high-quality image-annotation pairs. In this
paper, we propose a Parallel Pre-trained Transformers (PPT) framework to
accomplish the synthetic data-based Instance Segmentation task. Specifically,
we leverage the off-the-shelf pre-trained vision Transformers to alleviate the
gap between natural and synthetic data, which helps to provide good
generalization in the downstream synthetic data scene with few samples.
Swin-B-based CBNet V2, SwinL-based CBNet V2 and Swin-L-based Uniformer are
employed for parallel feature learning, and the results of these three models
are fused by pixel-level Non-maximum Suppression (NMS) algorithm to obtain more
robust results. The experimental results reveal that PPT ranks first in the
CVPR2022 AVA Accessibility Vision and Autonomy Challenge, with a 65.155% mAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniCon+: ICTCAS-UCAS Submission to the AVA-ActiveSpeaker Task at ActivityNet Challenge 2022. (arXiv:2206.10861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10861">
<div class="article-summary-box-inner">
<span><p>This report presents a brief description of our winning solution to the AVA
Active Speaker Detection (ASD) task at ActivityNet Challenge 2022. Our
underlying model UniCon+ continues to build on our previous work, the Unified
Context Network (UniCon) and Extended UniCon which are designed for robust
scene-level ASD. We augment the architecture with a simple GRU-based module
that allows information of recurring identities to flow across scenes through
read and update operations. We report a best result of 94.47% mAP on the
AVA-ActiveSpeaker test set, which continues to rank first on this year's
challenge leaderboard and significantly pushes the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NVIDIA-UNIBZ Submission for EPIC-KITCHENS-100 Action Anticipation Challenge 2022. (arXiv:2206.10869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10869">
<div class="article-summary-box-inner">
<span><p>In this report, we describe the technical details of our submission for the
EPIC-Kitchen-100 action anticipation challenge. Our modelings, the higher-order
recurrent space-time transformer and the message-passing neural network with
edge learning, are both recurrent-based architectures which observe only 2.5
seconds inference context to form the action anticipation prediction. By
averaging the prediction scores from a set of models compiled with our proposed
training pipeline, we achieved strong performance on the test set, which is
19.61% overall mean top-5 recall, recorded as second place on the public
leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Re-calibration based MIL for Whole Slide Image Classification. (arXiv:2206.10878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10878">
<div class="article-summary-box-inner">
<span><p>Whole slide image (WSI) classification is a fundamental task for the
diagnosis and treatment of diseases; but, curation of accurate labels is
time-consuming and limits the application of fully-supervised methods. To
address this, multiple instance learning (MIL) is a popular method that poses
classification as a weakly supervised learning task with slide-level labels
only. While current MIL methods apply variants of the attention mechanism to
re-weight instance features with stronger models, scant attention is paid to
the properties of the data distribution. In this work, we propose to
re-calibrate the distribution of a WSI bag (instances) by using the statistics
of the max-instance (critical) feature. We assume that in binary MIL, positive
bags have larger feature magnitudes than negatives, thus we can enforce the
model to maximize the discrepancy between bags with a metric feature loss that
models positive bags as out-of-distribution. To achieve this, unlike existing
MIL methods that use single-batch training modes, we propose balanced-batch
sampling to effectively use the feature loss i.e., (+/-) bags simultaneously.
Further, we employ a position encoding module (PEM) to model
spatial/morphological information, and perform pooling by multi-head
self-attention (PSMA) with a Transformer encoder. Experimental results on
existing benchmark datasets show our approach is effective and improves over
state-of-the-art MIL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symmetric Network with Spatial Relationship Modeling for Natural Language-based Vehicle Retrieval. (arXiv:2206.10879v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10879">
<div class="article-summary-box-inner">
<span><p>Natural language (NL) based vehicle retrieval aims to search specific vehicle
given text description. Different from the image-based vehicle retrieval,
NL-based vehicle retrieval requires considering not only vehicle appearance,
but also surrounding environment and temporal relations. In this paper, we
propose a Symmetric Network with Spatial Relationship Modeling (SSM) method for
NL-based vehicle retrieval. Specifically, we design a symmetric network to
learn the unified cross-modal representations between text descriptions and
vehicle images, where vehicle appearance details and vehicle trajectory global
information are preserved. Besides, to make better use of location information,
we propose a spatial relationship modeling methods to take surrounding
environment and mutual relationship between vehicles into consideration. The
qualitative and quantitative experiments verify the effectiveness of the
proposed method. We achieve 43.92% MRR accuracy on the test set of the 6th AI
City Challenge on natural language-based vehicle retrieval track, yielding the
1st place among all valid submissions on the public leaderboard. The code is
available at https://github.com/hbchen121/AICITY2022_Track2_SSM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KiloNeuS: Implicit Neural Representations with Real-Time Global Illumination. (arXiv:2206.10885v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10885">
<div class="article-summary-box-inner">
<span><p>The latest trends in inverse rendering techniques for reconstruction use
neural networks to learn 3D representations as neural fields. NeRF-based
techniques fit multi-layer perceptrons (MLPs) to a set of training images to
estimate a radiance field which can then be rendered from any virtual camera by
means of volume rendering algorithms. Major drawbacks of these representations
are the lack of well-defined surfaces and non-interactive rendering times, as
wide and deep MLPs must be queried millions of times per single frame. These
limitations have recently been singularly overcome, but managing to accomplish
this simultaneously opens up new use cases. We present KiloNeuS, a new neural
object representation that can be rendered in path-traced scenes at interactive
frame rates. KiloNeuS enables the simulation of realistic light interactions
between neural and classic primitives in shared scenes, and it demonstrably
performs in real-time with plenty of room for future optimizations and
extensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Flow Regularization of Implicit Neural Representations for Video Frame Interpolation. (arXiv:2206.10886v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10886">
<div class="article-summary-box-inner">
<span><p>Recent works have shown the ability of Implicit Neural Representations (INR)
to carry meaningful representations of signal derivatives. In this work, we
leverage this property to perform Video Frame Interpolation (VFI) by explicitly
constraining the derivatives of the INR to satisfy the optical flow constraint
equation. We achieve state of the art VFI on limited motion ranges using only a
target video and its optical flow, without learning the interpolation operator
from additional training data. We further show that constraining the INR
derivatives not only allows to better interpolate intermediate frames but also
improves the ability of narrow networks to fit the observed frames, which
suggests potential applications to video compression and INR optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose Estimation. (arXiv:2206.10892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10892">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the Intra- and Inter-Human Relation Networks
(I^2R-Net) for Multi-Person Pose Estimation. It involves two basic modules.
First, the Intra-Human Relation Module operates on a single person and aims to
capture Intra-Human dependencies. Second, the Inter-Human Relation Module
considers the relation between multiple instances and focuses on capturing
Inter-Human interactions. The Inter-Human Relation Module can be designed very
lightweight by reducing the resolution of feature map, yet learn useful
relation information to significantly boost the performance of the Intra-Human
Relation Module. Even without bells and whistles, our method can compete or
outperform current competition winners. We conduct extensive experiments on
COCO, CrowdPose, and OCHuman datasets. The results demonstrate that the
proposed model surpasses all the state-of-the-art methods. Concretely, the
proposed method achieves 77.4% AP on CrowPose dataset and 67.8% AP on OCHuman
dataset respectively, outperforming existing methods by a large margin.
Additionally, the ablation study and visualization analysis also prove the
effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S2TNet: Spatio-Temporal Transformer Networks for Trajectory Prediction in Autonomous Driving. (arXiv:2206.10902v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10902">
<div class="article-summary-box-inner">
<span><p>To safely and rationally participate in dense and heterogeneous traffic,
autonomous vehicles require to sufficiently analyze the motion patterns of
surrounding traffic-agents and accurately predict their future trajectories.
This is challenging because the trajectories of traffic-agents are not only
influenced by the traffic-agents themselves but also by spatial interaction
with each other. Previous methods usually rely on the sequential step-by-step
processing of Long Short-Term Memory networks (LSTMs) and merely extract the
interactions between spatial neighbors for single type traffic-agents. We
propose the Spatio-Temporal Transformer Networks (S2TNet), which models the
spatio-temporal interactions by spatio-temporal Transformer and deals with the
temporel sequences by temporal Transformer. We input additional category, shape
and heading information into our networks to handle the heterogeneity of
traffic-agents. The proposed methods outperforms state-of-the-art methods on
ApolloScape Trajectory dataset by more than 7\% on both the weighted sum of
Average and Final Displacement Error. Our code is available at
https://github.com/chenghuang66/s2tnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniUD-FBK-UB-UniBZ Submission to the EPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2022. (arXiv:2206.10903v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10903">
<div class="article-summary-box-inner">
<span><p>This report presents the technical details of our submission to the
EPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2022. To participate in
the challenge, we designed an ensemble consisting of different models trained
with two recently developed relevance-augmented versions of the widely used
triplet loss. Our submission, visible on the public leaderboard, obtains an
average score of 61.02% nDCG and 49.77% mAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpA-Former: Transformer image shadow detection and removal via spatial attention. (arXiv:2206.10910v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10910">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an end-to-end SpA-Former to recover a shadow-free
image from a single shaded image. Unlike traditional methods that require two
steps for shadow detection and then shadow removal, the SpA-Former unifies
these steps into one, which is a one-stage network capable of directly learning
the mapping function between shadows and no shadows, it does not require a
separate shadow detection. Thus, SpA-former is adaptable to real image
de-shadowing for shadows projected on different semantic regions. SpA-Former
consists of transformer layer and a series of joint Fourier transform residual
blocks and two-wheel joint spatial attention. The network in this paper is able
to handle the task while achieving a very fast processing efficiency.
</p>
<p>Our code is relased on https://github.com/
zhangbaijin/Spatial-Transformer-shadow-removal
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Influence of uncertainty estimation techniques on false-positive reduction in liver lesion detection. (arXiv:2206.10911v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10911">
<div class="article-summary-box-inner">
<span><p>Deep learning techniques show success in detecting objects in medical images,
but still suffer from false-positive predictions that may hinder accurate
diagnosis. The estimated uncertainty of the neural network output has been used
to flag incorrect predictions. We study the role played by features computed
from neural network uncertainty estimates and shape-based features computed
from binary predictions in reducing false positives in liver lesion detection
by developing a classification-based post-processing step for different
uncertainty estimation methods. We demonstrate an improvement in the lesion
detection performance of the neural network (with respect to F1-score) for all
uncertainty estimation methods on two datasets, comprising abdominal MR and CT
images respectively. We show that features computed from neural network
uncertainty estimates tend not to contribute much toward reducing false
positives. Our results show that factors like class imbalance (true over false
positive ratio) and shape-based features extracted from uncertainty maps play
an important role in distinguishing false positive from true positive
predictions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-based software for lung nodule detection in chest X-rays -- Time for a second reader approach?. (arXiv:2206.10912v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10912">
<div class="article-summary-box-inner">
<span><p>Objectives: To compare artificial intelligence (AI) as a second reader in
detecting lung nodules on chest X-rays (CXR) versus radiologists of two
binational institutions, and to evaluate AI performance when using two
different modes: automated versus assisted (additional remote radiologist
review).
</p>
<p>Methods: The CXR public database (n = 247) of the Japanese Society of
Radiological Technology with various types and sizes of lung nodules was
analyzed. Eight radiologists evaluated the CXR images with regard to the
presence of lung nodules and nodule conspicuity. After radiologist review, the
AI software processed and flagged the CXR with the highest probability of
missed nodules. The calculated accuracy metrics were the area under the curve
(AUC), sensitivity, specificity, F1 score, false negative case number (FN), and
the effect of different AI modes (automated/assisted) on the accuracy of nodule
detection.
</p>
<p>Results: For radiologists, the average AUC value was 0.77 $\pm$ 0.07, while
the average FN was 52.63 $\pm$ 17.53 (all studies) and 32 $\pm$ 11.59 (studies
containing a nodule of malignant etiology = 32% rate of missed malignant
nodules). Both AI modes -- automated and assisted -- produced an average
increase in sensitivity (by 14% and 12%) and of F1-score (5% and 6%) and a
decrease in specificity (by 10% and 3%, respectively).
</p>
<p>Conclusions: Both AI modes flagged the pulmonary nodules missed by
radiologists in a significant number of cases. AI as a second reader has a high
potential to improve diagnostic accuracy and radiology workflow. AI might
detect certain pulmonary nodules earlier than radiologists, with a potentially
significant impact on patient outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the effect of sparsity on neural networks robustness. (arXiv:2206.10915v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10915">
<div class="article-summary-box-inner">
<span><p>This paper examines the impact of static sparsity on the robustness of a
trained network to weight perturbations, data corruption, and adversarial
examples. We show that, up to a certain sparsity achieved by increasing network
width and depth while keeping the network capacity fixed, sparsified networks
consistently match and often outperform their initially dense versions.
Robustness and accuracy decline simultaneously for very high sparsity due to
loose connectivity between network layers. Our findings show that a rapid
robustness drop caused by network compression observed in the literature is due
to a reduced network capacity rather than sparsity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study on the Evaluation of Generative Models. (arXiv:2206.10935v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10935">
<div class="article-summary-box-inner">
<span><p>Implicit generative models, which do not return likelihood values, such as
generative adversarial networks and diffusion models, have become prevalent in
recent years. While it is true that these models have shown remarkable results,
evaluating their performance is challenging. This issue is of vital importance
to push research forward and identify meaningful gains from random noise.
Currently, heuristic metrics such as the Inception score (IS) and Frechet
Inception Distance (FID) are the most common evaluation metrics, but what they
measure is not entirely clear. Additionally, there are questions regarding how
meaningful their score actually is. In this work, we study the evaluation
metrics of generative models by generating a high-quality synthetic dataset on
which we can estimate classical metrics for comparison. Our study shows that
while FID and IS do correlate to several f-divergences, their ranking of close
models can vary considerably making them problematic when used for fain-grained
comparison. We further used this experimental setting to study which evaluation
metric best correlates with our probabilistic metrics. Lastly, we look into the
base features used for metrics such as FID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polar Parametrization for Vision-based Surround-View 3D Detection. (arXiv:2206.10965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10965">
<div class="article-summary-box-inner">
<span><p>3D detection based on surround-view camera system is a critical technique in
autopilot. In this work, we present Polar Parametrization for 3D detection,
which reformulates position parametrization, velocity decomposition, perception
range, label assignment and loss function in polar coordinate system. Polar
Parametrization establishes explicit associations between image patterns and
prediction targets, exploiting the view symmetry of surround-view cameras as
inductive bias to ease optimization and boost performance. Based on Polar
Parametrization, we propose a surround-view 3D DEtection TRansformer, named
PolarDETR. PolarDETR achieves promising performance-speed trade-off on
different backbone configurations. Besides, PolarDETR ranks 1st on the
leaderboard of nuScenes benchmark in terms of both 3D detection and 3D tracking
at the submission time (Mar. 4th, 2022). Code will be released at
\url{https://github.com/hustvl/PolarDETR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Morphing Attack Detection using Siamese Network and Few-shot Learning. (arXiv:2206.10969v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10969">
<div class="article-summary-box-inner">
<span><p>Face morphing attack detection is challenging and presents a concrete and
severe threat for face verification systems. Reliable detection mechanisms for
such attacks, which have been tested with a robust cross-database protocol and
unknown morphing tools still is a research challenge. This paper proposes a
framework following the Few-Shot-Learning approach that shares image
information based on the siamese network using triplet-semi-hard-loss to tackle
the morphing attack detection and boost the clustering classification process.
This network compares a bona fide or potentially morphed image with triplets of
morphing and bona fide face images. Our results show that this new network
cluster the data points, and assigns them to classes in order to obtain a lower
equal error rate in a cross-database scenario sharing only small image numbers
from an unknown database. Few-shot learning helps to boost the learning
process. Experimental results using a cross-datasets trained with FRGCv2 and
tested with FERET and the AMSL open-access databases reduced the BPCER10 from
43% to 4.91% using ResNet50 and 5.50% for MobileNetV2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdvSmo: Black-box Adversarial Attack by Smoothing Linear Structure of Texture. (arXiv:2206.10988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10988">
<div class="article-summary-box-inner">
<span><p>Black-box attacks usually face two problems: poor transferability and the
inability to evade the adversarial defense. To overcome these shortcomings, we
create an original approach to generate adversarial examples by smoothing the
linear structure of the texture in the benign image, called AdvSmo. We
construct the adversarial examples without relying on any internal information
to the target model and design the imperceptible-high attack success rate
constraint to guide the Gabor filter to select appropriate angles and scales to
smooth the linear texture from the input images to generate adversarial
examples. Benefiting from the above design concept, AdvSmo will generate
adversarial examples with strong transferability and solid evasiveness.
Finally, compared to the four advanced black-box adversarial attack methods,
for the eight target models, the results show that AdvSmo improves the average
attack success rate by 9% on the CIFAR-10 and 16% on the Tiny-ImageNet dataset
compared to the best of these attack methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identity Documents Authentication based on Forgery Detection of Guilloche Pattern. (arXiv:2206.10989v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10989">
<div class="article-summary-box-inner">
<span><p>In cases such as digital enrolment via mobile and online services, identity
document verification is critical in order to efficiently detect forgery and
therefore build user trust in the digital world. In this paper, an
authentication model for identity documents based on forgery detection of
guilloche patterns is proposed. The proposed approach is made up of two steps:
feature extraction and similarity measure between a pair of feature vectors of
identity documents. The feature extraction step involves learning the
similarity between a pair of identity documents via a convolutional neural
network (CNN) architecture and ends by extracting highly discriminative
features between them. While, the similarity measure step is applied to decide
if a given identity document is authentic or forged. In this work, these two
steps are combined together to achieve two objectives: (i) extracted features
should have good anticollision (discriminative) capabilities to distinguish
between a pair of identity documents belonging to different classes, (ii)
checking out the conformity of the guilloche pattern of a given identity
document and its similarity to the guilloche pattern of an authentic version of
the same country. Experiments are conducted in order to analyze and identify
the most proper parameters to achieve higher authentication performance. The
experimental results are performed on the MIDV-2020 dataset. The results show
the ability of the proposed approach to extract the relevant characteristics of
the processed pair of identity documents in order to model the guilloche
patterns, and thus distinguish them correctly. The implementation code and the
forged dataset are provided here (https://drive.google.com/id-FDGP-1)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototypical Contrastive Language Image Pretraining. (arXiv:2206.10996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10996">
<div class="article-summary-box-inner">
<span><p>Contrastive Language Image Pretraining (CLIP) received widespread attention
since its learned representations can be transferred well to various downstream
tasks. During CLIP training, the InfoNCE objective aims to align positive
image-text pairs and separate negative ones. In this paper, we show a
representation grouping effect during this process: the InfoNCE objective
indirectly groups semantically similar representations together via randomly
emerged within-modal anchors. We introduce Prototypical Contrastive Language
Image Pretraining (ProtoCLIP) to enhance such grouping by boosting its
efficiency and increasing its robustness against modality gap. Specifically,
ProtoCLIP sets up prototype-level discrimination between image and text spaces,
which efficiently transfers higher-level structural knowledge. We further
propose Prototypical Back Translation (PBT) to decouple representation grouping
from representation alignment, resulting in effective learning of meaningful
representations under large modality gap. PBT also enables us to introduce
additional external teachers with richer prior knowledge. ProtoCLIP is trained
with an online episodic training strategy, which makes it can be scaled up to
unlimited amounts of data. Combining the above novel designs, we train our
ProtoCLIP on Conceptual Captions and achieved an +5.81% ImageNet linear probing
improvement and an +2.01% ImageNet zero-shot classification improvement. Codes
are available at https://github.com/megvii-research/protoclip.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised Action Localization via Hierarchical Mining. (arXiv:2206.11011v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11011">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised action localization aims to localize and classify action
instances in the given videos temporally with only video-level categorical
labels. Thus, the crucial issue of existing weakly-supervised action
localization methods is the limited supervision from the weak annotations for
precise predictions. In this work, we propose a hierarchical mining strategy
under video-level and snippet-level manners, i.e., hierarchical supervision and
hierarchical consistency mining, to maximize the usage of the given annotations
and prediction-wise consistency. To this end, a Hierarchical Mining Network
(HiM-Net) is proposed. Concretely, it mines hierarchical supervision for
classification in two grains: one is the video-level existence for ground truth
categories captured by multiple instance learning; the other is the
snippet-level inexistence for each negative-labeled category from the
perspective of complementary labels, which is optimized by our proposed
complementary label learning. As for hierarchical consistency, HiM-Net explores
video-level co-action feature similarity and snippet-level
foreground-background opposition, for discriminative representation learning
and consistent foreground-background separation. Specifically, prediction
variance is viewed as uncertainty to select the pairs with high consensus for
proposed foreground-background collaborative learning. Comprehensive
experimental results show that HiM-Net outperforms existing methods on THUMOS14
and ActivityNet1.3 datasets with large margins by hierarchically mining the
supervision and consistency. Code will be available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated GI tract segmentation using deep learning. (arXiv:2206.11048v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11048">
<div class="article-summary-box-inner">
<span><p>The job of Radiation oncologists is to deliver x-ray beams pointed toward the
tumor and at the same time avoid the stomach and intestines. With MR-Linacs
(magnetic resonance imaging and linear accelerator systems), oncologists can
visualize the position of the tumor and allow for precise dose according to
tumor cell presence which can vary from day to day. The current job of
outlining the position of the stomach and intestines to adjust the X-ray beams
direction for the dose delivery to the tumor while avoiding the organs. This is
a time-consuming and labor-intensive process that can easily prolong treatments
from 15 minutes to an hour a day unless deep learning methods can automate the
segmentation process. This paper discusses an automated segmentation process
using deep learning to make this process faster and allow more patients to get
effective treatment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer. (arXiv:2206.11053v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11053">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) in surgery is largely unexplored. Expert
surgeons are scarce and are often overloaded with clinical and academic
workloads. This overload often limits their time answering questionnaires from
patients, medical students or junior residents related to surgical procedures.
At times, students and junior residents also refrain from asking too many
questions during classes to reduce disruption. While computer-aided simulators
and recording of past surgical procedures have been made available for them to
observe and improve their skills, they still hugely rely on medical experts to
answer their questions. Having a Surgical-VQA system as a reliable 'second
opinion' could act as a backup and ease the load on the medical experts in
answering these questions. The lack of annotated medical data and the presence
of domain-specific terms has limited the exploration of VQA for surgical
procedures. In this work, we design a Surgical-VQA task that answers
questionnaires on surgical procedures based on the surgical scene. Extending
the MICCAI endoscopic vision challenge 2018 dataset and workflow recognition
dataset further, we introduce two Surgical-VQA datasets with classification and
sentence-based answers. To perform Surgical-VQA, we employ vision-text
transformers models. We further introduce a residual MLP-based VisualBert
encoder model that enforces interaction between visual and text tokens,
improving performance in classification-based answering. Furthermore, we study
the influence of the number of input image patches and temporal visual features
on the model performance in both classification and sentence-based answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified and Biologically-Plausible Relational Graph Representation of Vision Transformers. (arXiv:2206.11073v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11073">
<div class="article-summary-box-inner">
<span><p>Vision transformer (ViT) and its variants have achieved remarkable successes
in various visual tasks. The key characteristic of these ViT models is to adopt
different aggregation strategies of spatial patch information within the
artificial neural networks (ANNs). However, there is still a key lack of
unified representation of different ViT architectures for systematic
understanding and assessment of model representation performance. Moreover, how
those well-performing ViT ANNs are similar to real biological neural networks
(BNNs) is largely unexplored. To answer these fundamental questions, we, for
the first time, propose a unified and biologically-plausible relational graph
representation of ViT models. Specifically, the proposed relational graph
representation consists of two key sub-graphs: aggregation graph and affine
graph. The former one considers ViT tokens as nodes and describes their spatial
interaction, while the latter one regards network channels as nodes and
reflects the information communication between channels. Using this unified
relational graph representation, we found that: a) a sweet spot of the
aggregation graph leads to ViTs with significantly improved predictive
performance; b) the graph measures of clustering coefficient and average path
length are two effective indicators of model prediction performance, especially
when applying on the datasets with small samples; c) our findings are
consistent across various ViT architectures and multiple datasets; d) the
proposed relational graph representation of ViT has high similarity with real
BNNs derived from brain science data. Overall, our work provides a novel
unified and biologically-plausible paradigm for more interpretable and
effective representation of ViT ANNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Gait: Gait Recognition via Motion Excitation. (arXiv:2206.11080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11080">
<div class="article-summary-box-inner">
<span><p>Gait recognition, which can realize long-distance and contactless
identification, is an important biometric technology. Recent gait recognition
methods focus on learning the pattern of human movement or appearance during
walking, and construct the corresponding spatio-temporal representations.
However, different individuals have their own laws of movement patterns, simple
spatial-temporal features are difficult to describe changes in motion of human
parts, especially when confounding variables such as clothing and carrying are
included, thus distinguishability of features is reduced. In this paper, we
propose the Motion Excitation Module (MEM) to guide spatio-temporal features to
focus on human parts with large dynamic changes, MEM learns the difference
information between frames and intervals, so as to obtain the representation of
temporal motion changes, it is worth mentioning that MEM can adapt to frame
sequences with uncertain length, and it does not add any additional parameters.
Furthermore, we present the Fine Feature Extractor (FFE), which independently
learns the spatio-temporal representations of human body according to different
horizontal parts of individuals. Benefiting from MEM and FFE, our method
innovatively combines motion change information, significantly improving the
performance of the model under cross appearance conditions. On the popular
dataset CASIA-B, our proposed Motion Gait is better than the existing gait
recognition methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes. (arXiv:2206.11095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11095">
<div class="article-summary-box-inner">
<span><p>Immersive displays such as VR headsets, AR glasses, Multiview displays, Free
point televisions have emerged as a new class of display technologies in recent
years, offering a better visual experience and viewer engagement as compared to
conventional displays. With the evolution of 3D video and display technologies,
the consumer market for High Dynamic Range (HDR) cameras and displays is
quickly growing. The lack of appropriate experimental data is a critical
hindrance for the development of primary research efforts in the field of 3D
HDR video technology. Also, the unavailability of sufficient real world
multi-exposure experimental dataset is a major bottleneck for HDR imaging
research, thereby limiting the quality of experience (QoE) for the viewers. In
this paper, we introduce a diversified stereoscopic multi-exposure dataset
captured within the campus of Indian Institute of Technology Madras, which is
home to a diverse flora and fauna. The dataset is captured using ZED
stereoscopic camera and provides intricate scenes of outdoor locations such as
gardens, roadside views, festival venues, buildings and indoor locations such
as academic and residential areas. The proposed dataset accommodates wide depth
range, complex depth structure, complicate object movement, illumination
variations, rich color dynamics, texture discrepancy in addition to significant
randomness introduced by moving camera and background motion. The proposed
dataset is made publicly available to the research community. Furthermore, the
procedure for capturing, aligning and calibrating multi-exposure stereo videos
and images is described in detail. Finally, we have discussed the progress,
challenges, potential use cases and future research opportunities with respect
to HDR imaging, depth estimation, consistent tone mapping and 3D HDR coding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICC++: Explainable Image Retrieval for Art Historical Corpora using Image Composition Canvas. (arXiv:2206.11115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11115">
<div class="article-summary-box-inner">
<span><p>Image compositions are helpful in the study of image structures and assist in
discovering the semantics of the underlying scene portrayed across art forms
and styles. With the digitization of artworks in recent years, thousands of
images of a particular scene or narrative could potentially be linked together.
However, manually linking this data with consistent objectiveness can be a
highly challenging and time-consuming task. In this work, we present a novel
approach called Image Composition Canvas (ICC++) to compare and retrieve images
having similar compositional elements. ICC++ is an improvement over ICC
specializing in generating low and high-level features (compositional elements)
motivated by Max Imdahl's work. To this end, we present a rigorous quantitative
and qualitative comparison of our approach with traditional and
state-of-the-art (SOTA) methods showing that our proposed method outperforms
all of them. In combination with deep features, our method outperforms the best
deep learning-based method, opening the research direction for explainable
machine learning for digital humanities. We will release the code and the data
post-publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN-based fully automatic wrist cartilage volume quantification in MR Image. (arXiv:2206.11127v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11127">
<div class="article-summary-box-inner">
<span><p>Detection of cartilage loss is crucial for the diagnosis of osteo- and
rheumatoid arthritis. A large number of automatic segmentation tools have been
reported so far for cartilage assessment in magnetic resonance images of large
joints. As compared to knee or hip, wrist cartilage has a more complex
structure so that automatic tools developed for large joints are not expected
to be operational for wrist cartilage segmentation. In that respect, a fully
automatic wrist cartilage segmentation method would be of high clinical
interest. We assessed the performance of four optimized variants of the U-Net
architecture with truncation of its depth and addition of attention layers
(U-Net_AL). The corresponding results were compared to those from a patch-based
convolutional neural network (CNN) we previously designed. The segmentation
quality was assessed on the basis of a comparative analysis with manual
segmentation using several morphological (2D DSC, 3D DSC, precision) and a
volumetric metrics. The four networks outperformed the patch-based CNN in terms
of segmentation homogeneity and quality. The median 3D DSC value computed with
the U-Net_AL (0.817) was significantly larger than the corresponding DSC values
computed with the other networks. In addition, the U-Net_AL CNN provided the
lowest mean volume error (17%) and the highest Pearson correlation coefficient
(0.765) with respect to the ground truth. Of interest, the reproducibility
computed from using U-Net_AL was larger than the reproducibility of the manual
segmentation. U-net convolutional neural network with additional attention
layers provides the best wrist cartilage segmentation performance. In order to
be used in clinical conditions, the trained network can be fine-tuned on a
dataset representing a group of specific patients. The error of cartilage
volume measurement should be assessed independently using a non-MRI method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization. (arXiv:2206.11134v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11134">
<div class="article-summary-box-inner">
<span><p>Open-vocabulary object detection (OVD) aims to scale up vocabulary size to
detect objects of novel categories beyond the training vocabulary. Recent work
resorts to the rich knowledge in pre-trained vision-language models. However,
existing methods are ineffective in proposal-level vision-language alignment.
Meanwhile, the models usually suffer from confidence bias toward base
categories and perform worse on novel ones. To overcome the challenges, we
present MEDet, a novel and effective OVD framework with proposal mining and
prediction equalization. First, we design an online proposal mining to refine
the inherited vision-semantic knowledge from coarse to fine, allowing for
proposal-level detection-oriented feature alignment. Second, based on causal
inference theory, we introduce a class-wise backdoor adjustment to reinforce
the predictions on novel categories to improve the overall OVD performance.
Extensive experiments on COCO and LVIS benchmarks verify the superiority of
MEDet over the competing approaches in detecting objects of novel categories,
e.g., 32.6% AP50 on COCO and 22.4% mask mAP on LVIS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Physical Metric For 6-DoF Grasp Pose Detection. (arXiv:2206.11141v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11141">
<div class="article-summary-box-inner">
<span><p>6-DoF grasp pose detection of multi-grasp and multi-object is a challenge
task in the field of intelligent robot. To imitate human reasoning ability for
grasping objects, data driven methods are widely studied. With the introduction
of large-scale datasets, we discover that a single physical metric usually
generates several discrete levels of grasp confidence scores, which cannot
finely distinguish millions of grasp poses and leads to inaccurate prediction
results. In this paper, we propose a hybrid physical metric to solve this
evaluation insufficiency. First, we define a novel metric is based on the
force-closure metric, supplemented by the measurement of the object flatness,
gravity and collision. Second, we leverage this hybrid physical metric to
generate elaborate confidence scores. Third, to learn the new confidence scores
effectively, we design a multi-resolution network called Flatness Gravity
Collision GraspNet (FGC-GraspNet). FGC-GraspNet proposes a multi-resolution
features learning architecture for multiple tasks and introduces a new joint
loss function that enhances the average precision of the grasp detection. The
network evaluation and adequate real robot experiments demonstrate the
effectiveness of our hybrid physical metric and FGC-GraspNet. Our method
achieves 90.5\% success rate in real-world cluttered scenes. Our code is
available at https://github.com/luyh20/FGC-GraspNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal transport meets noisy label robust loss and MixUp regularization for domain adaptation. (arXiv:2206.11180v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11180">
<div class="article-summary-box-inner">
<span><p>It is common in computer vision to be confronted with domain shift: images
which have the same class but different acquisition conditions. In domain
adaptation (DA), one wants to classify unlabeled target images using source
labeled images. Unfortunately, deep neural networks trained on a source
training set perform poorly on target images which do not belong to the
training domain. One strategy to improve these performances is to align the
source and target image distributions in an embedded space using optimal
transport (OT). However OT can cause negative transfer, i.e. aligning samples
with different labels, which leads to overfitting especially in the presence of
label shift between domains. In this work, we mitigate negative alignment by
explaining it as a noisy label assignment to target images. We then mitigate
its effect by appropriate regularization. We propose to couple the MixUp
regularization \citep{zhang2018mixup} with a loss that is robust to noisy
labels in order to improve domain adaptation performance. We show in an
extensive ablation study that a combination of the two techniques is critical
to achieve improved performance. Finally, we evaluate our method, called
\textsc{mixunbot}, on several benchmarks and real-world DA problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facke: a Survey on Generative Models for Face Swapping. (arXiv:2206.11203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11203">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate into the performance of mainstream neural
generative models on the very task of swapping faces. We have experimented on
CVAE, CGAN, CVAE-GAN, and conditioned diffusion models. Existing finely trained
models have already managed to produce fake faces (Facke) indistinguishable to
the naked eye as well as achieve high objective metrics. We perform a
comparison among them and analyze their pros and cons. Furthermore, we proposed
some promising tricks though they do not apply to this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives. (arXiv:2206.11212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11212">
<div class="article-summary-box-inner">
<span><p>Many past works aim to improve visual reasoning in models by supervising
feature importance (estimated by model explanation techniques) with human
annotations such as highlights of important image regions. However, recent work
has shown that performance gains from feature importance (FI) supervision for
Visual Question Answering (VQA) tasks persist even with random supervision,
suggesting that these methods do not meaningfully align model FI with human FI.
In this paper, we show that model FI supervision can meaningfully improve VQA
model accuracy as well as performance on several Right-for-the-Right-Reason
(RRR) metrics by optimizing for four key model objectives: (1) accurate
predictions given limited but sufficient information (Sufficiency); (2)
max-entropy predictions given no important information (Uncertainty); (3)
invariance of predictions to changes in unimportant features (Invariance); and
(4) alignment between model FI explanations and human FI explanations
(Plausibility). Our best performing method, Visual Feature Importance
Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in
terms of both in-distribution and out-of-distribution accuracy. While past work
suggests that the mechanism for improved accuracy is through improved
explanation plausibility, we show that this relationship depends crucially on
explanation faithfulness (whether explanations truly represent the model's
internal reasoning). Predictions are more accurate when explanations are
plausible and faithful, and not when they are plausible but not faithful.
Lastly, we show that, surprisingly, RRR metrics are not predictive of
out-of-distribution model accuracy when controlling for a model's
in-distribution accuracy, which calls into question the value of these metrics
for evaluating model reasoning. All supporting code is available at
https://github.com/zfying/visfis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correct and Certify: A New Approach to Self-Supervised 3D-Object Perception. (arXiv:2206.11215v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11215">
<div class="article-summary-box-inner">
<span><p>We consider an object pose estimation and model fitting problem, where -
given a partial point cloud of an object - the goal is to estimate the object
pose by fitting a CAD model to the sensor data. We solve this problem by
combining (i) a semantic keypoint-based pose estimation model, (ii) a novel
self-supervised training approach, and (iii) a certification procedure, that
not only verifies whether the output produced by the model is correct or not,
but also flags uniqueness of the produced solution. The semantic keypoint
detector model is initially trained in simulation and does not perform well on
real-data due to the domain gap. Our self-supervised training procedure uses a
corrector and a certification module to improve the detector. The corrector
module corrects the detected keypoints to compensate for the domain gap, and is
implemented as a declarative layer, for which we develop a simple
differentiation rule. The certification module declares whether the corrected
output produced by the model is certifiable (i.e. correct) or not. At each
iteration, the approach optimizes over the loss induced only by the certifiable
input-output pairs. As training progresses, we see that the fraction of outputs
that are certifiable increases, eventually reaching near $100\%$ in many cases.
We also introduce the notion of strong certifiability wherein the model can
determine if the predicted object model fit is unique or not. The detected
semantic keypoints help us implement this in the forward pass. We conduct
extensive experiments to evaluate the performance of the corrector, the
certification, and the proposed self-supervised training using the ShapeNet and
YCB datasets, and show the proposed approach achieves performance comparable to
fully supervised baselines while not requiring pose or keypoint supervision on
real data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Business Document Information Extraction: Towards Practical Benchmarks. (arXiv:2206.11229v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11229">
<div class="article-summary-box-inner">
<span><p>Information extraction from semi-structured documents is crucial for
frictionless business-to-business (B2B) communication. While machine learning
problems related to Document Information Extraction (IE) have been studied for
decades, many common problem definitions and benchmarks do not reflect
domain-specific aspects and practical needs for automating B2B document
communication. We review the landscape of Document IE problems, datasets and
benchmarks. We highlight the practical aspects missing in the common
definitions and define the Key Information Localization and Extraction (KILE)
and Line Item Recognition (LIR) problems. There is a lack of relevant datasets
and benchmarks for Document IE on semi-structured business documents as their
content is typically legally protected or sensitive. We discuss potential
sources of available documents including synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-aware Glass Surface Detection with Cross-modal Context Mining. (arXiv:2206.11250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11250">
<div class="article-summary-box-inner">
<span><p>Glass surfaces are becoming increasingly ubiquitous as modern buildings tend
to use a lot of glass panels. This however poses substantial challenges on the
operations of autonomous systems such as robots, self-driving cars and drones,
as the glass panels can become transparent obstacles to the navigation.Existing
works attempt to exploit various cues, including glass boundary context or
reflections, as a prior. However, they are all based on input RGB images.We
observe that the transmission of 3D depth sensor light through glass surfaces
often produces blank regions in the depth maps, which can offer additional
insights to complement the RGB image features for glass surface detection. In
this paper, we propose a novel framework for glass surface detection by
incorporating RGB-D information, with two novel modules: (1) a cross-modal
context mining (CCM) module to adaptively learn individual and mutual context
features from RGB and depth information, and (2) a depth-missing aware
attention (DAA) module to explicitly exploit spatial locations where missing
depths occur to help detect the presence of glass surfaces. In addition, we
propose a large-scale RGB-D glass surface detection dataset, called
\textit{RGB-D GSD}, for RGB-D glass surface detection. Our dataset comprises
3,009 real-world RGB-D glass surface images with precise annotations. Extensive
experimental results show that our proposed model outperforms state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Behavior Transformers: Cloning $k$ modes with one stone. (arXiv:2206.11251v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11251">
<div class="article-summary-box-inner">
<span><p>While behavior learning has made impressive progress in recent times, it lags
behind computer vision and natural language processing due to its inability to
leverage large, human-generated datasets. Human behaviors have wide variance,
multiple modes, and human demonstrations typically do not come with reward
labels. These properties limit the applicability of current methods in Offline
RL and Behavioral Cloning to learn from large, pre-collected datasets. In this
work, we present Behavior Transformer (BeT), a new technique to model unlabeled
demonstration data with multiple modes. BeT retrofits standard transformer
architectures with action discretization coupled with a multi-task action
correction inspired by offset prediction in object detection. This allows us to
leverage the multi-modal modeling ability of modern transformers to predict
multi-modal continuous actions. We experimentally evaluate BeT on a variety of
robotic manipulation and self-driving behavior datasets. We show that BeT
significantly improves over prior state-of-the-art work on solving demonstrated
tasks while capturing the major modes present in the pre-collected datasets.
Finally, through an extensive ablation study, we analyze the importance of
every crucial component in BeT. Videos of behavior generated by BeT are
available at https://notmahi.github.io/bet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Blind Face Restoration with Codebook Lookup Transformer. (arXiv:2206.11253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11253">
<div class="article-summary-box-inner">
<span><p>Blind face restoration is a highly ill-posed problem that often requires
auxiliary guidance to 1) improve the mapping from degraded inputs to desired
outputs, or 2) complement high-quality details lost in the inputs. In this
paper, we demonstrate that a learned discrete codebook prior in a small proxy
space largely reduces the uncertainty and ambiguity of restoration mapping by
casting blind face restoration as a code prediction task, while providing rich
visual atoms for generating high-quality faces. Under this paradigm, we propose
a Transformer-based prediction network, named CodeFormer, to model global
composition and context of the low-quality faces for code prediction, enabling
the discovery of natural faces that closely approximate the target faces even
when the inputs are severely degraded. To enhance the adaptiveness for
different degradation, we also propose a controllable feature transformation
module that allows a flexible trade-off between fidelity and quality. Thanks to
the expressive codebook prior and global modeling, CodeFormer outperforms state
of the arts in both quality and fidelity, showing superior robustness to
degradation. Extensive experimental results on synthetic and real-world
datasets verify the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegGroup: Seg-Level Supervision for 3D Instance and Semantic Segmentation. (arXiv:2012.10217v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10217">
<div class="article-summary-box-inner">
<span><p>Most existing point cloud instance and semantic segmentation methods rely
heavily on strong supervision signals, which require point-level labels for
every point in the scene. However, such strong supervision suffers from large
annotation costs, arousing the need to study efficient annotating. In this
paper, we discover that the locations of instances matter for both instance and
semantic 3D scene segmentation. By fully taking advantage of locations, we
design a weakly supervised point cloud segmentation algorithm that only
requires clicking on one point per instance to indicate its location for
annotation. With over-segmentation for pre-processing, we extend these location
annotations into segments as seg-level labels. We further design a segment
grouping network (SegGroup) to generate point-level pseudo labels under
seg-level labels by hierarchically grouping the unlabeled segments into the
relevant nearby labeled segments, so that existing point-level supervised
segmentation models can directly consume these pseudo labels for training.
Experimental results show that our seg-level supervised method (SegGroup)
achieves comparable results with the fully annotated point-level supervised
methods. Moreover, it outperforms the recent weakly supervised methods given a
fixed annotation budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation. (arXiv:2101.07253v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07253">
<div class="article-summary-box-inner">
<span><p>Domain adaptation is an important task to enable learning when labels are
scarce. While most works focus only on the image modality, there are many
important multi-modal datasets. In order to leverage multi-modality for domain
adaptation, we propose cross-modal learning, where we enforce consistency
between the predictions of two modalities via mutual mimicking. We constrain
our network to make correct predictions on labeled data and consistent
predictions across modalities on unlabeled target-domain data. Experiments in
unsupervised and semi-supervised domain adaptation settings prove the
effectiveness of this novel domain adaptation strategy. Specifically, we
evaluate on the task of 3D semantic segmentation from either the 2D image, the
3D point cloud or from both. We leverage recent driving datasets to produce a
wide variety of domain adaptation scenarios including changes in scene layout,
lighting, sensor setup and weather, as well as the synthetic-to-real setup. Our
method significantly improves over previous uni-modal adaptation baselines on
all adaption scenarios. Our code is publicly available at
https://github.com/valeoai/xmuda_journal
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Generated Images Hard To Spot: A Transferable Attack On Synthetic Image Detectors. (arXiv:2104.12069v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12069">
<div class="article-summary-box-inner">
<span><p>Visually realistic GAN-generated images have recently emerged as an important
misinformation threat. Research has shown that these synthetic images contain
forensic traces that are readily identifiable by forensic detectors.
Unfortunately, these detectors are built upon neural networks, which are
vulnerable to recently developed adversarial attacks. In this paper, we propose
a new anti-forensic attack capable of fooling GAN-generated image detectors.
Our attack uses an adversarially trained generator to synthesize traces that
these detectors associate with real images. Furthermore, we propose a technique
to train our attack so that it can achieve transferability, i.e. it can fool
unknown CNNs that it was not explicitly trained against. We evaluate our attack
through an extensive set of experiments, where we show that our attack can fool
eight state-of-the-art detection CNNs with synthetic images created using seven
different GANs, and outperform other alternative attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11541">
<div class="article-summary-box-inner">
<span><p>Kernel segmentation aims at partitioning a data sequence into several
non-overlapping segments that may have nonlinear and complex structures. In
general, it is formulated as a discrete optimization problem with combinatorial
constraints. A popular algorithm for optimally solving this problem is dynamic
programming (DP), which has quadratic computation and memory requirements.
Given that sequences in practice are too long, this algorithm is not a
practical approach. Although many heuristic algorithms have been proposed to
approximate the optimal segmentation, they have no guarantee on the quality of
their solutions. In this paper, we take a differentiable approach to alleviate
the aforementioned issues. First, we introduce a novel sigmoid-based
regularization to smoothly approximate the combinatorial constraints. Combining
it with objective of the balanced kernel clustering, we formulate a
differentiable model termed Kernel clustering with sigmoid-based regularization
(KCSR), where the gradient-based algorithm can be exploited to obtain the
optimal segmentation. Second, we develop a stochastic variant of the proposed
model. By using the stochastic gradient descent algorithm, which has much lower
time and space complexities, for optimization, the second model can perform
segmentation on overlong data sequences. Finally, for simultaneously segmenting
multiple data sequences, we slightly modify the sigmoid-based regularization to
further introduce an extended variant of the proposed model. Through extensive
experiments on various types of data sequences performances of our models are
evaluated and compared with those of the existing methods. The experimental
results validate advantages of the proposed models. Our Matlab source code is
available on github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11214">
<div class="article-summary-box-inner">
<span><p>Conventional methods for human pose estimation either require a high degree
of instrumentation, by relying on many inertial measurement units (IMUs), or
constraint the recording space, by relying on extrinsic cameras. These deficits
are tackled through the approach of human pose estimation from sparse IMU data.
We define adjacency adaptive graph convolutional long-short term memory
networks (AAGC-LSTM), to tackle human pose estimation based on six IMUs, while
incorporating the human body graph structure directly into the network. The
AAGC-LSTM combines both spatial and temporal dependency in a single network
operation, more memory efficiently than previous approaches. This is made
possible by equipping graph convolutions with adjacency adaptivity, which
eliminates the problem of information loss in deep or recurrent graph networks,
while it also allows for learning unknown dependencies between the human body
joints. To further boost accuracy, we propose longitudinal loss weighting to
consider natural movement patterns. With our presented approach, we are able to
utilize the inherent graph nature of the human body, and thus can outperform
the state of the art (SOTA) for human pose estimation from sparse IMU data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual SLAM with Graph-Cut Optimized Multi-Plane Reconstruction. (arXiv:2108.04281v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04281">
<div class="article-summary-box-inner">
<span><p>This paper presents a semantic planar SLAM system that improves pose
estimation and mapping using cues from an instance planar segmentation network.
While the mainstream approaches are using RGB-D sensors, employing a monocular
camera with such a system still faces challenges such as robust data
association and precise geometric model fitting. In the majority of existing
work, geometric model estimation problems such as homography estimation and
piece-wise planar reconstruction (PPR) are usually solved by standard (greedy)
RANSAC separately and sequentially. However, setting the inlier-outlier
threshold is difficult in absence of information about the scene (i.e. the
scale). In this work, we revisit these problems and argue that two mentioned
geometric models (homographies/3D planes) can be solved by minimizing an energy
function that exploits the spatial coherence, i.e. with graph-cut optimization,
which also tackles the practical issue when the output of a trained CNN is
inaccurate. Moreover, we propose an adaptive parameter setting strategy based
on our experiments, and report a comprehensive evaluation on various
open-source datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust fine-tuning of zero-shot models. (arXiv:2109.01903v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01903">
<div class="article-summary-box-inner">
<span><p>Large pre-trained models such as CLIP or ALIGN offer consistent accuracy
across a range of data distributions when performing zero-shot inference (i.e.,
without fine-tuning on a specific dataset). Although existing fine-tuning
methods substantially improve accuracy on a given target distribution, they
often reduce robustness to distribution shifts. We address this tension by
introducing a simple and effective method for improving robustness while
fine-tuning: ensembling the weights of the zero-shot and fine-tuned models
(WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy
improvements under distribution shift, while preserving high accuracy on the
target distribution. On ImageNet and five derived distribution shifts, WiSE-FT
improves accuracy under distribution shift by 4 to 6 percentage points (pp)
over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves
similarly large robustness gains (2 to 23 pp) on a diverse set of six further
distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard
fine-tuning on seven commonly used transfer learning datasets. These
improvements come at no additional computational cost during fine-tuning or
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining for Strong Gravitational Lenses with Self-supervised Learning. (arXiv:2110.00023v2 [astro-ph.IM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00023">
<div class="article-summary-box-inner">
<span><p>We employ self-supervised representation learning to distill information from
76 million galaxy images from the Dark Energy Spectroscopic Instrument Legacy
Imaging Surveys' Data Release 9. Targeting the identification of new strong
gravitational lens candidates, we first create a rapid similarity search tool
to discover new strong lenses given only a single labelled example. We then
show how training a simple linear classifier on the self-supervised
representations, requiring only a few minutes on a CPU, can automatically
classify strong lenses with great efficiency. We present 1192 new strong lens
candidates that we identified through a brief visual identification campaign,
and release an interactive web-based similarity search tool and the top network
predictions to facilitate crowd-sourcing rapid discovery of additional strong
gravitational lenses and other rare objects:
https://github.com/georgestein/ssl-legacysurvey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Dementia from Speech and Transcripts using Transformers. (arXiv:2110.14769v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14769">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious
consequences to peoples' everyday lives, if it is not diagnosed early since
there is no available cure. Alzheimer's is the most common cause of dementia,
which constitutes a general term for loss of memory. Due to the fact that
dementia affects speech, existing research initiatives focus on detecting
dementia from spontaneous speech. However, little work has been done regarding
the conversion of speech data to Log-Mel spectrograms and Mel-frequency
cepstral coefficients (MFCCs) and the usage of pretrained models. Concurrently,
little work has been done in terms of both the usage of transformer networks
and the way the two modalities, i.e., speech and transcripts, are combined in a
single neural network. To address these limitations, first we employ several
pretrained models, with Vision Transformer (ViT) achieving the highest
evaluation results. Secondly, we propose multimodal models. More specifically,
our introduced models include Gated Multimodal Unit in order to control the
influence of each modality towards the final classification and crossmodal
attention so as to capture in an effective way the relationships between the
two modalities. Extensive experiments conducted on the ADReSS Challenge dataset
demonstrate the effectiveness of the proposed models and their superiority over
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07991">
<div class="article-summary-box-inner">
<span><p>This paper presents contrastive-tuning, a simple method employing contrastive
training to align image and text models while still taking advantage of their
pre-training. In our empirical study we find that locked pre-trained image
models with unlocked text models work best. We call this instance of
contrastive-tuning "Locked-image Tuning" (LiT), which just teaches a text model
to read out good representations from a pre-trained image model for new tasks.
A LiT model gains the capability of zero-shot transfer to new vision tasks,
such as image classification or retrieval. The proposed LiT is widely
applicable; it works reliably with multiple pre-training methods (supervised
and unsupervised) and across diverse architectures (ResNet, Vision Transformers
and MLP-Mixer) using three different image-text datasets. With the
transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2%
zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the
challenging out-of-distribution ObjectNet test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition. (arXiv:2111.11011v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11011">
<div class="article-summary-box-inner">
<span><p>The Transformer-based encoder-decoder framework is becoming popular in scene
text recognition, largely because it naturally integrates recognition clues
from both visual and semantic domains. However, recent studies show that the
two kinds of clues are not always well registered and therefore, feature and
character might be misaligned in the difficult text (e.g., with rare shapes).
As a result, constraints such as character position are introduced to alleviate
this problem. Despite certain success, a content-free positional embedding
hardly associates stably with meaningful local image regions. In this paper, we
propose a novel module called Multi-Domain Character Distance Perception
(MDCDP) to establish a visual and semantic related positional encoding. MDCDP
uses positional embedding to query both visual and semantic features following
the attention mechanism. The two kinds of constrained features are then fused
to produce a reinforced feature, generating a content-aware embedding that well
perceives spacing variations and semantic affinities among characters, i.e.,
multi-domain character distance. We develop a novel network named CDistNet that
stacks multiple MDCDPs to guide a gradually precise distance modeling. Thus,
the feature-character alignment is well built even various recognition
difficulties presented. We create two series of augmented datasets with
increasing recognition difficulties and apply CDistNet to both them and six
public benchmarks. The experiments demonstrate that CDistNet outperforms recent
popular methods by large margins in challenging recognition scenarios. It also
achieves state-of-the-art accuracy on standard benchmarks. In addition, the
visualization shows that CDistNet achieves proper information utilization in
both visual and semantic domains. Our code is given in
https://github.com/simplify23/CDistNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Talk-to-Resolve: Combining scene understanding and spatial dialogue to resolve granular task ambiguity for a collocated robot. (arXiv:2111.11099v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11099">
<div class="article-summary-box-inner">
<span><p>The utility of collocating robots largely depends on the easy and intuitive
interaction mechanism with the human. If a robot accepts task instruction in
natural language, first, it has to understand the user's intention by decoding
the instruction. However, while executing the task, the robot may face
unforeseeable circumstances due to the variations in the observed scene and
therefore requires further user intervention. In this article, we present a
system called Talk-to-Resolve (TTR) that enables a robot to initiate a coherent
dialogue exchange with the instructor by observing the scene visually to
resolve the impasse. Through dialogue, it either finds a cue to move forward in
the original plan, an acceptable alternative to the original plan, or
affirmation to abort the task altogether. To realize the possible stalemate, we
utilize the dense captions of the observed scene and the given instruction
jointly to compute the robot's next action. We evaluate our system based on a
data set of initial instruction and situational scene pairs. Our system can
identify the stalemate and resolve them with appropriate dialogue exchange with
82% accuracy. Additionally, a user study reveals that the questions from our
systems are more natural (4.02 on average on a scale of 1 to 5) as compared to
a state-of-the-art (3.08 on average).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intuitive Shape Editing in Latent Space. (arXiv:2111.12488v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12488">
<div class="article-summary-box-inner">
<span><p>The use of autoencoders for shape editing or generation through latent space
manipulation suffers from unpredictable changes in the output shape. Our
autoencoder-based method enables intuitive shape editing in latent space by
disentangling latent sub-spaces into style variables and control points on the
surface that can be manipulated independently. The key idea is adding a
Lipschitz-type constraint to the loss function, i.e. bounding the change of the
output shape proportionally to the change in latent space, leading to
interpretable latent space representations. The control points on the surface
that are part of the latent code of an object can then be freely moved,
allowing for intuitive shape editing directly in latent space. We evaluate our
method by comparing to state-of-the-art data-driven shape editing methods. We
further demonstrate the expressiveness of our learned latent space by
leveraging it for unsupervised part segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07225">
<div class="article-summary-box-inner">
<span><p>The long-tailed class distribution in visual recognition tasks poses great
challenges for neural networks on how to handle the biased predictions between
head and tail classes, i.e., the model tends to classify tail classes as head
classes. While existing research focused on data resampling and loss function
engineering, in this paper, we take a different perspective: the classification
margins. We study the relationship between the margins and logits
(classification scores) and empirically observe the biased margins and the
biased logits are positively correlated. We propose MARC, a simple yet
effective MARgin Calibration function to dynamically calibrate the biased
margins for unbiased logits. We validate MARC through extensive experiments on
common long-tailed benchmarks including CIFAR-LT, ImageNet-LT, Places-LT, and
iNaturalist-LT. Experimental results demonstrate that our MARC achieves
favorable results on these benchmarks. In addition, MARC is extremely easy to
implement with just three lines of code. We hope this simple method will
motivate people to rethink the biased margins and biased logits in long-tailed
visual recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Instance Segmentation of MVS Buildings. (arXiv:2112.09902v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09902">
<div class="article-summary-box-inner">
<span><p>We present a novel 3D instance segmentation framework for Multi-View Stereo
(MVS) buildings in urban scenes. Unlike existing works focusing on semantic
segmentation of urban scenes, the emphasis of this work lies in detecting and
segmenting 3D building instances even if they are attached and embedded in a
large and imprecise 3D surface model. Multi-view RGB images are first enhanced
to RGBH images by adding a heightmap and are segmented to obtain all roof
instances using a fine-tuned 2D instance segmentation neural network. Instance
masks from different multi-view images are then clustered into global masks.
Our mask clustering accounts for spatial occlusion and overlapping, which can
eliminate segmentation ambiguities among multi-view images. Based on these
global masks, 3D roof instances are segmented out by mask back-projections and
extended to the entire building instances through a Markov random field
optimization. A new dataset that contains instance-level annotation for both 3D
urban scenes (roofs and buildings) and drone images (roofs) is provided. To the
best of our knowledge, it is the first outdoor dataset dedicated to 3D instance
segmentation with much more annotations of attached 3D buildings than existing
datasets. Quantitative evaluations and ablation studies have shown the
effectiveness of all major steps and the advantages of our multi-view framework
over the orthophoto-based method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Graph Generation: A Comprehensive Survey. (arXiv:2201.00443v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00443">
<div class="article-summary-box-inner">
<span><p>Deep learning techniques have led to remarkable breakthroughs in the field of
generic object detection and have spawned a lot of scene-understanding tasks in
recent years. Scene graph has been the focus of research because of its
powerful semantic representation and applications to scene understanding. Scene
Graph Generation (SGG) refers to the task of automatically mapping an image
into a semantic structural scene graph, which requires the correct labeling of
detected objects and their relationships. Although this is a challenging task,
the community has proposed a lot of SGG approaches and achieved good results.
In this paper, we provide a comprehensive survey of recent achievements in this
field brought about by deep learning techniques. We review 138 representative
works that cover different input modalities, and systematically summarize
existing methods of image-based SGG from the perspective of feature extraction
and fusion. We attempt to connect and systematize the existing visual
relationship detection methods, to summarize, and interpret the mechanisms and
the strategies of SGG in a comprehensive way. Finally, we finish this survey
with deep discussions about current existing problems and future research
directions. This survey will help readers to develop a better understanding of
the current research status and ideas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Face Morphing Attacks: Generation, Vulnerability and Detection. (arXiv:2201.03454v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03454">
<div class="article-summary-box-inner">
<span><p>Face Recognition systems (FRS) have been found vulnerable to morphing
attacks, where the morphed face image is generated by blending the face images
from contributory data subjects. This work presents a novel direction towards
generating face morphing attacks in 3D. To this extent, we have introduced a
novel approach based on blending the 3D face point clouds corresponding to the
contributory data subjects. The proposed method will generate the 3D face
morphing by projecting the input 3D face point clouds to depth-maps \&amp; 2D color
images followed by the image blending and wrapping operations performed
independently on the color images and depth maps. We then back-project the 2D
morphing color-map and the depth-map to the point cloud using the canonical
(fixed) view. Given that the generated 3D face morphing models will result in
the holes due to a single canonical view, we have proposed a new algorithm for
hole filling that will result in a high-quality 3D face morphing model.
Extensive experiments are carried out on the newly generated 3D face dataset
comprised of 675 3D scans corresponding to 41 unique data subjects. Experiments
are performed to benchmark the vulnerability of automatic 2D and 3D FRS and
human observer analysis. We also present the quantitative assessment of the
quality of the generated 3D face morphing models using eight different quality
metrics. Finally, we have proposed three different 3D face Morphing Attack
Detection (3D-MAD) algorithms to benchmark the performance of the 3D MAD
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Masking for Self-Supervised Learning. (arXiv:2201.13100v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13100">
<div class="article-summary-box-inner">
<span><p>We propose ADIOS, a masked image model (MIM) framework for self-supervised
learning, which simultaneously learns a masking function and an image encoder
using an adversarial objective. The image encoder is trained to minimise the
distance between representations of the original and that of a masked image.
The masking function, conversely, aims at maximising this distance. ADIOS
consistently improves on state-of-the-art self-supervised learning (SSL)
methods on a variety of tasks and datasets -- including classification on
ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and
iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao
et al., 2021) -- while generating semantically meaningful masks. Unlike modern
MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch
tokenisation construction of Vision Transformers, and can be implemented with
convolutional backbones. We further demonstrate that the masks learned by ADIOS
are more effective in improving representation learning of SSL methods than
masking schemes used in popular MIM models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metrics for saliency map evaluation of deep learning explanation methods. (arXiv:2201.13291v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13291">
<div class="article-summary-box-inner">
<span><p>Due to the black-box nature of deep learning models, there is a recent
development of solutions for visual explanations of CNNs. Given the high cost
of user studies, metrics are necessary to compare and evaluate these different
methods. In this paper, we critically analyze the Deletion Area Under Curve
(DAUC) and Insertion Area Under Curve (IAUC) metrics proposed by Petsiuk et al.
(2018). These metrics were designed to evaluate the faithfulness of saliency
maps generated by generic methods such as Grad-CAM or RISE. First, we show that
the actual saliency score values given by the saliency map are ignored as only
the ranking of the scores is taken into account. This shows that these metrics
are insufficient by themselves, as the visual appearance of a saliency map can
change significantly without the ranking of the scores being modified.
Secondly, we argue that during the computation of DAUC and IAUC, the model is
presented with images that are out of the training distribution which might
lead to an unreliable behavior of the model being explained. To complement
DAUC/IAUC, we propose new metrics that quantify the sparsity and the
calibration of explanation methods, two previously unstudied properties.
Finally, we give general remarks about the metrics studied in this paper and
discuss how to evaluate them in a user study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GiraffeDet: A Heavy-Neck Paradigm for Object Detection. (arXiv:2202.04256v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04256">
<div class="article-summary-box-inner">
<span><p>In conventional object detection frameworks, a backbone body inherited from
image recognition models extracts deep latent features and then a neck module
fuses these latent features to capture information at different scales. As the
resolution in object detection is much larger than in image recognition, the
computational cost of the backbone often dominates the total inference cost.
This heavy-backbone design paradigm is mostly due to the historical legacy when
transferring image recognition models to object detection rather than an
end-to-end optimized design for object detection. In this work, we show that
such paradigm indeed leads to sub-optimal object detection models. To this end,
we propose a novel heavy-neck paradigm, GiraffeDet, a giraffe-like network for
efficient object detection. The GiraffeDet uses an extremely lightweight
backbone and a very deep and large neck module which encourages dense
information exchange among different spatial scales as well as different levels
of latent semantics simultaneously. This design paradigm allows detectors to
process the high-level semantic information and low-level spatial information
at the same priority even in the early stage of the network, making it more
effective in detection tasks. Numerical evaluations on multiple popular object
detection benchmarks show that GiraffeDet consistently outperforms previous
SOTA models across a wide spectrum of resource constraints. The source code is
available at https://github.com/jyqi/GiraffeDet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation for Underwater Image Enhancement via Content and Style Separation. (arXiv:2202.08537v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08537">
<div class="article-summary-box-inner">
<span><p>Underwater image suffer from color cast, low contrast and hazy effect due to
light absorption, refraction and scattering, which degraded the high-level
application, e.g, object detection and object tracking. Recent learning-based
methods demonstrate astonishing performance on underwater image enhancement,
however, most of these works use synthetic pair data for supervised learning
and ignore the domain gap to real-world data. To solve this problem, we propose
a domain adaptation framework for underwater image enhancement via content and
style separation, different from prior works of domain adaptation for
underwater image enhancement, which target to minimize the latent discrepancy
of synthesis and real-world data, we aim to separate encoded feature into
content and style latent and distinguish style latent from different domains,
i.e. synthesis, real-world underwater and clean domain, and process domain
adaptation and image enhancement in latent space. By latent manipulation, our
model provide a user interact interface to adjust different enhanced level for
continuous change. Experiment on various public real-world underwater
benchmarks demonstrate that the proposed framework is capable to perform domain
adaptation for underwater image enhancement and outperform various
state-of-the-art underwater image enhancement algorithms in quantity and
quality. The model and source code will be available at
https://github.com/fordevoted/UIESS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpolation-based Contrastive Learning for Few-Label Semi-Supervised Learning. (arXiv:2202.11915v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11915">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) has long been proved to be an effective
technique to construct powerful models with limited labels. In the existing
literature, consistency regularization-based methods, which force the perturbed
samples to have similar predictions with the original ones have attracted much
attention for their promising accuracy. However, we observe that, the
performance of such methods decreases drastically when the labels get extremely
limited, e.g., 2 or 3 labels for each category. Our empirical study finds that
the main problem lies with the drifting of semantic information in the
procedure of data augmentation. The problem can be alleviated when enough
supervision is provided. However, when little guidance is available, the
incorrect regularization would mislead the network and undermine the
performance of the algorithm. To tackle the problem, we (1) propose an
interpolation-based method to construct more reliable positive sample pairs;
(2) design a novel contrastive loss to guide the embedding of the learned
network to change linearly between samples so as to improve the discriminative
capability of the network by enlarging the margin decision boundaries. Since no
destructive regularization is introduced, the performance of our proposed
algorithm is largely improved. Specifically, the proposed algorithm outperforms
the second best algorithm (Comatch) with 5.3% by achieving 88.73%
classification accuracy when only two labels are available for each class on
the CIFAR-10 dataset. Moreover, we further prove the generality of the proposed
method by improving the performance of the existing state-of-the-art algorithms
considerably with our proposed strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding person identification via gait. (arXiv:2203.04179v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04179">
<div class="article-summary-box-inner">
<span><p>Gait recognition is the process of identifying humans from their bipedal
locomotion such as walking or running. As such, gait data is privacy sensitive
information and should be anonymized where possible. With the rise of higher
quality gait recording techniques, such as depth cameras or motion capture
suits, an increasing amount of detailed gait data is captured and processed.
Introduction and rise of the Metaverse is but one popular application scenario
in which the gait of users is transferred onto digital avatars. As a first step
towards developing effective anonymization techniques for high-quality gait
data, we study different aspects of movement data to quantify their
contribution to gait recognition. We first extract categories of features from
the literature on human gait perception and then design experiments for each
category to assess how much the information they contain contributes to
recognition success. Our results show that gait anonymization will be
challenging, as the data is highly redundant and interdependent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05482">
<div class="article-summary-box-inner">
<span><p>The conventional recipe for maximizing model accuracy is to (1) train
multiple models with various hyperparameters and (2) pick the individual model
which performs best on a held-out validation set, discarding the remainder. In
this paper, we revisit the second step of this procedure in the context of
fine-tuning large pre-trained models, where fine-tuned models often appear to
lie in a single low error basin. We show that averaging the weights of multiple
models fine-tuned with different hyperparameter configurations often improves
accuracy and robustness. Unlike a conventional ensemble, we may average many
models without incurring any additional inference or memory costs -- we call
the results "model soups." When fine-tuning large pre-trained models such as
CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides
significant improvements over the best model in a hyperparameter sweep on
ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on
ImageNet, achieved a new state of the art. Furthermore, we show that the model
soup approach extends to multiple image classification and natural language
processing tasks, improves out-of-distribution performance, and improves
zero-shot performance on new downstream tasks. Finally, we analytically relate
the performance similarity of weight-averaging and logit-ensembling to flatness
of the loss and confidence of the predictions, and validate this relation
empirically. Code is available at https://github.com/mlfoundations/model-soups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09215">
<div class="article-summary-box-inner">
<span><p>We propose Human-centered 4D Scene Capture (HSC4D) to accurately and
efficiently create a dynamic digital world, containing large-scale
indoor-outdoor scenes, diverse human motions, and rich interactions between
humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is
space-free without any external devices' constraints and map-free without
pre-built maps. Considering that IMUs can capture human poses but always drift
for long-period use, while LiDAR is stable for global localization but rough
for local positions and orientations, HSC4D makes both sensors complement each
other by a joint optimization and achieves promising results for long-term
capture. Relationships between humans and environments are also explored to
make their interaction more realistic. To facilitate many down-stream tasks,
like AR, VR, robots, autonomous driving, etc., we propose a dataset containing
three large scenes (1k-5k $m^2$) with accurate dynamic human motions and
locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)
and challenging human activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization ability of HSC4D.
The dataset and code are available at <a href="http://www.lidarhumanmotion.net/hsc4d/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09550">
<div class="article-summary-box-inner">
<span><p>Few-shot semantic segmentation aims at recognizing the object regions of
unseen categories with only a few annotated examples as supervision. The key to
few-shot segmentation is to establish a robust semantic relationship between
the support and query images and to prevent overfitting. In this paper, we
propose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle
the few-shot semantic segmentation problem. In MSHNet, we propose a new
Generative Prototype Similarity (GPS), which together with cosine similarity
can establish a strong semantic relation between the support and query images.
The locally generated prototype similarity based on global feature is logically
complementary to the global cosine similarity based on local feature, and the
relationship between the query image and the supported image can be expressed
more comprehensively by using the two similarities simultaneously. In addition,
we propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge
multi-layer, multi-shot and multi-similarity hyperrelational features. MSHNet
is built on the basis of similarity rather than specific category features,
which can achieve more general unity and effectively reduce overfitting. On two
benchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet
achieves new state-of-the-art performances on 1-shot and 5-shot semantic
segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Free Quantization with Accurate Activation Clipping and Adaptive Batch Normalization. (arXiv:2204.04215v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04215">
<div class="article-summary-box-inner">
<span><p>Data-free quantization is a task that compresses the neural network to low
bit-width without access to original training data. Most existing data-free
quantization methods cause severe performance degradation due to inaccurate
activation clipping range and quantization error, especially for low bit-width.
In this paper, we present a simple yet effective data-free quantization method
with accurate activation clipping and adaptive batch normalization. Accurate
activation clipping (AAC) improves the model accuracy by exploiting accurate
activation information from the full-precision model. Adaptive batch
normalization firstly proposes to address the quantization error from
distribution changes by updating the batch normalization layer adaptively.
Extensive experiments demonstrate that the proposed data-free quantization
method can yield surprisingly performance, achieving 64.33% top-1 accuracy of
ResNet18 on ImageNet dataset, with 3.7% absolute improvement outperforming the
existing state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Autonomous Driving with Semantic Depth Cloud Mapping and Multi-agent. (arXiv:2204.05513v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05513">
<div class="article-summary-box-inner">
<span><p>Focusing on the task of point-to-point navigation for an autonomous driving
vehicle, we propose a novel deep learning model trained with end-to-end and
multi-task learning manners to perform both perception and control tasks
simultaneously. The model is used to drive the ego vehicle safely by following
a sequence of routes defined by the global planner. The perception part of the
model is used to encode high-dimensional observation data provided by an RGBD
camera while performing semantic segmentation, semantic depth cloud (SDC)
mapping, and traffic light state and stop sign prediction. Then, the control
part decodes the encoded features along with additional information provided by
GPS and speedometer to predict waypoints that come with a latent feature space.
Furthermore, two agents are employed to process these outputs and make a
control policy that determines the level of steering, throttle, and brake as
the final action. The model is evaluated on CARLA simulator with various
scenarios made of normal-adversarial situations and different weathers to mimic
real-world conditions. In addition, we do a comparative study with some recent
models to justify the performance in multiple aspects of driving. Moreover, we
also conduct an ablation study on SDC mapping and multi-agent to understand
their roles and behavior. As a result, our model achieves the highest driving
score even with fewer parameters and computation load. To support future
studies, we share our codes at
https://github.com/oskarnatan/end-to-end-driving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge-enhanced Feature Distillation Network for Efficient Super-Resolution. (arXiv:2204.08759v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08759">
<div class="article-summary-box-inner">
<span><p>With the recently massive development in convolution neural networks,
numerous lightweight CNN-based image super-resolution methods have been
proposed for practical deployments on edge devices. However, most existing
methods focus on one specific aspect: network or loss design, which leads to
the difficulty of minimizing the model size. To address the issue, we conclude
block devising, architecture searching, and loss design to obtain a more
efficient SR structure. In this paper, we proposed an edge-enhanced feature
distillation network, named EFDN, to preserve the high-frequency information
under constrained resources. In detail, we build an edge-enhanced convolution
block based on the existing reparameterization methods. Meanwhile, we propose
edge-enhanced gradient loss to calibrate the reparameterized path training.
Experimental results show that our edge-enhanced strategies preserve the edge
and significantly improve the final restoration quality. Code is available at
https://github.com/icandle/EFDN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Purification for Unsupervised Person Re-identification. (arXiv:2204.09931v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09931">
<div class="article-summary-box-inner">
<span><p>Unsupervised person re-identification is a challenging and promising task in
computer vision. Nowadays unsupervised person re-identification methods have
achieved great progress by training with pseudo labels. However, how to purify
feature and label noise is less explicitly studied in the unsupervised manner.
To purify the feature, we take into account two types of additional features
from different local views to enrich the feature representation. The proposed
multi-view features are carefully integrated into our cluster contrast learning
to leverage more discriminative cues that the global feature easily ignored and
biased. To purify the label noise, we propose to take advantage of the
knowledge of teacher model in an offline scheme. Specifically, we first train a
teacher model from noisy pseudo labels, and then use the teacher model to guide
the learning of our student model. In our setting, the student model could
converge fast with the supervision of the teacher model thus reduce the
interference of noisy labels as the teacher model greatly suffered. After
carefully handling the noise and bias in the feature learning, our purification
modules are proven to be very effective for unsupervised person
re-identification. Extensive experiments on three popular person
re-identification datasets demonstrate the superiority of our method.
Especially, our approach achieves a state-of-the-art accuracy 85.8\% @mAP and
94.5\% @Rank-1 on the challenging Market-1501 benchmark with ResNet-50 under
the fully unsupervised setting. The code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Isometric Shape Matching via Functional Maps on Landmark-Adapted Bases. (arXiv:2205.04800v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04800">
<div class="article-summary-box-inner">
<span><p>We propose a principled approach for non-isometric landmark-preserving
non-rigid shape matching. Our method is based on the functional maps framework,
but rather than promoting isometries we focus instead on near-conformal maps
that preserve landmarks exactly. We achieve this, first, by introducing a novel
landmark-adapted basis using an intrinsic Dirichlet-Steklov eigenproblem.
Second, we establish the functional decomposition of conformal maps expressed
in this basis. Finally, we formulate a conformally-invariant energy that
promotes high-quality landmark-preserving maps, and show how it can be solved
via a variant of the recently proposed ZoomOut method that we extend to our
setting. Our method is descriptor-free, efficient and robust to significant
mesh variability. We evaluate our approach on a range of benchmark datasets and
demonstrate state-of-the-art performance on non-isometric benchmarks and near
state-of-the-art performance on isometric ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EXACT: How to Train Your Accuracy. (arXiv:2205.09615v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09615">
<div class="article-summary-box-inner">
<span><p>Classification tasks are usually evaluated in terms of accuracy. However,
accuracy is discontinuous and cannot be directly optimized using gradient
ascent. Popular methods minimize cross-entropy, Hinge loss, or other surrogate
losses, which can lead to suboptimal results. In this paper, we propose a new
optimization framework by introducing stochasticity to a model's output and
optimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive
experiments on image classification show that the proposed optimization method
is a powerful alternative to widely used classification losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Greedy Search: Tracking by Multi-Agent Reinforcement Learning-based Beam Search. (arXiv:2205.09676v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09676">
<div class="article-summary-box-inner">
<span><p>To track the target in a video, current visual trackers usually adopt greedy
search for target object localization in each frame, that is, the candidate
region with the maximum response score will be selected as the tracking result
of each frame. However, we found that this may be not an optimal choice,
especially when encountering challenging tracking scenarios such as heavy
occlusion and fast motion. To address this issue, we propose to maintain
multiple tracking trajectories and apply beam search strategy for visual
tracking, so that the trajectory with fewer accumulated errors can be
identified. Accordingly, this paper introduces a novel multi-agent
reinforcement learning based beam search tracking strategy, termed
BeamTracking. It is mainly inspired by the image captioning task, which takes
an image as input and generates diverse descriptions using beam search
algorithm. Accordingly, we formulate the tracking as a sample selection problem
fulfilled by multiple parallel decision-making processes, each of which aims at
picking out one sample as their tracking result in each frame. Each maintained
trajectory is associated with an agent to perform the decision-making and
determine what actions should be taken to update related information. When all
the frames are processed, we select the trajectory with the maximum accumulated
score as the tracking result. Extensive experiments on seven popular tracking
benchmark datasets validated the effectiveness of the proposed algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient visual object representation using a biologically plausible spike-latency code and winner-take-all inhibition. (arXiv:2205.10338v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10338">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have surpassed human performance in key visual
challenges such as object recognition, but require a large amount of energy,
computation, and memory. In contrast, spiking neural networks (SNNs) have the
potential to improve both the efficiency and biological plausibility of object
recognition systems. Here we present a SNN model that uses spike-latency coding
and winner-take-all inhibition (WTA-I) to efficiently represent visual stimuli
from the Fashion MNIST dataset. Stimuli were preprocessed with center-surround
receptive fields and then fed to a layer of spiking neurons whose synaptic
weights were updated using spike-timing-dependent-plasticity (STDP). We
investigate how the quality of the represented objects changes under different
WTA-I schemes and demonstrate that a network of 150 spiking neurons can
efficiently represent objects with as little as 40 spikes. Studying how core
object recognition may be implemented using biologically plausible learning
rules in SNNs may not only further our understanding of the brain, but also
lead to novel and efficient artificial vision systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy. (arXiv:2205.10683v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10683">
<div class="article-summary-box-inner">
<span><p>Large convolutional neural networks (CNN) can be difficult to train in the
differentially private (DP) regime, since the optimization algorithms require a
computationally expensive operation, known as the per-sample gradient clipping.
We propose an efficient and scalable implementation of this clipping on
convolutional layers, termed as the mixed ghost clipping, that significantly
eases the private training in terms of both time and space complexities,
without affecting the accuracy. The improvement in efficiency is rigorously
studied through the first complexity analysis for the mixed ghost clipping and
existing DP training algorithms.
</p>
<p>Extensive experiments on vision classification tasks, with large ResNet, VGG,
and Vision Transformers, demonstrate that DP training with mixed ghost clipping
adds $1\sim 10\%$ memory overhead and $&lt;2\times$ slowdown to the standard
non-private training. Specifically, when training VGG19 on CIFAR10, the mixed
ghost clipping is $3\times$ faster than state-of-the-art Opacus library with
$18\times$ larger maximum batch size. To emphasize the significance of
efficient DP training on convolutional layers, we achieve 96.7\% accuracy on
CIFAR10 and 83.0\% on CIFAR100 at $\epsilon=1$ using BEiT, while the previous
best results are 94.8\% and 67.4\%, respectively. We open-source a privacy
engine (\url{https://github.com/JialinMao/private_CNN}) that implements DP
training of CNN with a few lines of code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Effective Fusion Method to Enhance the Robustness of CNN. (arXiv:2205.15582v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15582">
<div class="article-summary-box-inner">
<span><p>With the development of technology rapidly, applications of convolutional
neural networks have improved the convenience of our life. However, in image
classification field, it has been found that when some perturbations are added
to images, the CNN would misclassify it. Thus various defense methods have been
proposed. The previous approach only considered how to incorporate modules in
the network to improve robustness, but did not focus on the way the modules
were incorporated. In this paper, we design a new fusion method to enhance the
robustness of CNN. We use a dot product-based approach to add the denoising
module to ResNet18 and the attention mechanism to further improve the
robustness of the model. The experimental results on CIFAR10 have shown that
our method is effective and better than the state-of-the-art methods under the
attack of FGSM and PGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection. (arXiv:2206.01736v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01736">
<div class="article-summary-box-inner">
<span><p>It is known that Deep Neural networks (DNNs) are vulnerable to adversarial
attacks, and the adversarial robustness of DNNs could be improved by adding
adversarial noises to training data (e.g., the standard adversarial training
(SAT)). However, inappropriate noises added to training data may reduce a
model's performance, which is termed the trade-off between accuracy and
robustness. This problem has been sufficiently studied for the classification
of whole images but has rarely been explored for image analysis tasks in the
medical application domain, including image segmentation, landmark detection,
and object detection tasks. In this study, we show that, for those medical
image analysis tasks, the SAT method has a severe issue that limits its
practical use: it generates a fixed and unified level of noise for all training
samples for robust DNN training. A high noise level may lead to a large
reduction in model performance and a low noise level may not be effective in
improving robustness. To resolve this issue, we design an adaptive-margin
adversarial training (AMAT) method that generates sample-wise adaptive
adversarial noises for robust DNN training. In contrast to the existing,
classification-oriented adversarial training methods, our AMAT method uses a
loss-defined-margin strategy so that it can be applied to different tasks as
long as the loss functions are well-defined. We successfully apply our AMAT
method to state-of-the-art DNNs, using five publicly available datasets. The
experimental results demonstrate that: (1) our AMAT method can be applied to
the three seemingly different tasks in the medical image application domain;
(2) AMAT outperforms the SAT method in adversarial robustness; (3) AMAT has a
minimal reduction in prediction accuracy on clean data, compared with the SAT
method; and (4) AMAT has almost the same training time cost as SAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MS-RNN: A Flexible Multi-Scale Framework for Spatiotemporal Predictive Learning. (arXiv:2206.03010v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03010">
<div class="article-summary-box-inner">
<span><p>Spatiotemporal predictive learning, which predicts future frames through
historical prior knowledge with the aid of deep learning, is widely used in
many fields. Previous work essentially improves the model performance by
widening or deepening the network, but it also brings surging memory overhead,
which seriously hinders the development and application of this technology. In
order to improve the performance without increasing memory consumption, we
focus on scale, which is another dimension to improve model performance but
with low memory requirement. The effectiveness has been widely proved in many
CNN-based tasks such as image classification and semantic segmentation, but it
has not been fully explored in recent RNN models. In this paper, learning from
the benefit of multi-scale, we propose a general framework named Multi-Scale
RNN (MS-RNN) to boost recent RNN models for spatiotemporal predictive learning.
By integrating different scales, we enhance the existing models with both
improved performance and greatly reduced overhead. We verify our MS-RNN
framework by exhaustive experiments with 6 popular RNN models (ConvLSTM,
TrajGRU, PredRNN, PredRNN++, MIM, and MotionRNN) on 4 different datasets
(Moving MNIST, KTH, TaxiBJ, and HKO-7). The results show the efficiency that
the RNN models incorporating our framework have much lower memory cost but
better performance than before. Our code is released at
\url{https://github.com/mazhf/MS-RNN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer. (arXiv:2206.05763v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05763">
<div class="article-summary-box-inner">
<span><p>Clinically, the accurate annotation of lesions/tissues can significantly
facilitate the disease diagnosis. For example, the segmentation of optic
disc/cup (OD/OC) on fundus image would facilitate the glaucoma diagnosis, the
segmentation of skin lesions on dermoscopic images is helpful to the melanoma
diagnosis, etc. With the advancement of deep learning techniques, a wide range
of methods proved the lesions/tissues segmentation can also facilitate the
automated disease diagnosis models. However, existing methods are limited in
the sense that they can only capture static regional correlations in the
images. Inspired by the global and dynamic nature of Vision Transformer, in
this paper, we propose Segmentation-Assisted diagnosis Transformer (SeATrans)
to transfer the segmentation knowledge to the disease diagnosis network.
Specifically, we first propose an asymmetric multi-scale interaction strategy
to correlate each single low-level diagnosis feature with multi-scale
segmentation features. Then, an effective strategy called SeA-block is adopted
to vitalize diagnosis feature via correlated segmentation features. To model
the segmentation-diagnosis interaction, SeA-block first embeds the diagnosis
feature based on the segmentation information via the encoder, and then
transfers the embedding back to the diagnosis feature space by a decoder.
Experimental results demonstrate that SeATrans surpasses a wide range of
state-of-the-art (SOTA) segmentation-assisted diagnosis methods on several
disease diagnosis tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Stream Transformer with Cross-Attention on Whole-Slide Image Pyramids for Cancer Prognosis. (arXiv:2206.05782v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05782">
<div class="article-summary-box-inner">
<span><p>The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a
challenging task. Most existing approaches focus solely on single-resolution
images. The multi-resolution schemes, utilizing image pyramids to enhance WSI
visual representations, have not yet been paid enough attention to. In order to
explore a multi-resolution solution for improving cancer prognosis accuracy,
this paper proposes a dual-stream architecture to model WSIs by an image
pyramid strategy. This architecture consists of two sub-streams: one for
low-resolution WSIs, and the other especially for high-resolution ones.
Compared to other approaches, our scheme has three highlights: (i) there exists
a one-to-one relation between stream and resolution; (ii) a square pooling
layer is added to align the patches from two resolution streams, largely
reducing computation cost and enabling a natural stream feature fusion; (iii) a
cross-attention-based method is proposed to pool high-resolution patches
spatially under the guidance of low-resolution ones. We validate our scheme on
three publicly-available datasets with a total number of 3,101 WSIs from 1,911
patients. Experimental results verify that (i) hierarchical dual-stream
representation is more effective than single-stream ones for cancer prognosis,
gaining an average C-Index rise of 5.0% and 1.8% on a single low-resolution and
high-resolution stream, respectively; (ii) our dual-stream scheme could
outperform current state-of-the-art ones, by an average C-Index improvement of
5.1%; (iii) the cancer diseases with observable survival differences could have
different preferences for model complexity. Our scheme could serve as an
alternative tool for further facilitating WSI prognosis research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Generative Model of Neonatal Cortical Surface Development. (arXiv:2206.07542v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07542">
<div class="article-summary-box-inner">
<span><p>The neonatal cortical surface is known to be affected by preterm birth, and
the subsequent changes to cortical organisation have been associated with
poorer neurodevelopmental outcomes. Deep Generative models have the potential
to lead to clinically interpretable models of disease, but developing these on
the cortical surface is challenging since established techniques for learning
convolutional filters are inappropriate on non-flat topologies. To close this
gap, we implement a surface-based CycleGAN using mixture model CNNs (MoNet) to
translate sphericalised neonatal cortical surface features (curvature and
T1w/T2w cortical myelin) between different stages of cortical maturity. Results
show our method is able to reliably predict changes in individual patterns of
cortical organisation at later stages of gestation, validated by comparison to
longitudinal data; and translate appearance between preterm and term gestation
(&gt; 37 weeks gestation), validated through comparison with a trained
term/preterm classifier. Simulated differences in cortical maturation are
consistent with observations in the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Using Privileged Information for Zero-Shot Action Recognition. (arXiv:2206.08632v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08632">
<div class="article-summary-box-inner">
<span><p>Zero-Shot Action Recognition (ZSAR) aims to recognize video actions that have
never been seen during training. Most existing methods assume a shared semantic
space between seen and unseen actions and intend to directly learn a mapping
from a visual space to the semantic space. This approach has been challenged by
the semantic gap between the visual space and semantic space. This paper
presents a novel method that uses object semantics as privileged information to
narrow the semantic gap and, hence, effectively, assist the learning. In
particular, a simple hallucination network is proposed to implicitly extract
object semantics during testing without explicitly extracting objects and a
cross-attention module is developed to augment visual feature with the object
semantics. Experiments on the Olympic Sports, HMDB51 and UCF101 datasets have
shown that the proposed method outperforms the state-of-the-art methods by a
large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Slot Attention for Vision-and-Language Navigation. (arXiv:2206.08645v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08645">
<div class="article-summary-box-inner">
<span><p>Vision-and-language navigation (VLN), a frontier study aiming to pave the way
for general-purpose robots, has been a hot topic in the computer vision and
natural language processing community. The VLN task requires an agent to
navigate to a goal location following natural language instructions in
unfamiliar environments.
</p>
<p>Recently, transformer-based models have gained significant improvements on
the VLN task. Since the attention mechanism in the transformer architecture can
better integrate inter- and intra-modal information of vision and language.
</p>
<p>However, there exist two problems in current transformer-based models.
</p>
<p>1) The models process each view independently without taking the integrity of
the objects into account.
</p>
<p>2) During the self-attention operation in the visual modality, the views that
are spatially distant can be inter-weaved with each other without explicit
restriction. This kind of mixing may introduce extra noise instead of useful
information.
</p>
<p>To address these issues, we propose 1) A slot-attention based module to
incorporate information from segmentation of the same object. 2) A local
attention mask mechanism to limit the visual attention span. The proposed
modules can be easily plugged into any VLN architecture and we use the
Recurrent VLN-Bert as our base model. Experiments on the R2R dataset show that
our model has achieved the state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bear the Query in Mind: Visual Grounding with Query-conditioned Convolution. (arXiv:2206.09114v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09114">
<div class="article-summary-box-inner">
<span><p>Visual grounding is a task that aims to locate a target object according to a
natural language expression. As a multi-modal task, feature interaction between
textual and visual inputs is vital. However, previous solutions mainly handle
each modality independently before fusing them together, which does not take
full advantage of relevant textual information while extracting visual
features. To better leverage the textual-visual relationship in visual
grounding, we propose a Query-conditioned Convolution Module (QCM) that
extracts query-aware visual features by incorporating query information into
the generation of convolutional kernels. With our proposed QCM, the downstream
fusion module receives visual features that are more discriminative and focused
on the desired object described in the expression, leading to more accurate
predictions. Extensive experiments on three popular visual grounding datasets
demonstrate that our method achieves state-of-the-art performance. In addition,
the query-aware visual features are informative enough to achieve comparable
performance to the latest methods when directly used for prediction without
further multi-modal fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modality Image Super-Resolution using Generative Adversarial Networks. (arXiv:2206.09193v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09193">
<div class="article-summary-box-inner">
<span><p>Over the past few years deep learning-based techniques such as Generative
Adversarial Networks (GANs) have significantly improved solutions to image
super-resolution and image-to-image translation problems. In this paper, we
propose a solution to the joint problem of image super-resolution and
multi-modality image-to-image translation. The problem can be stated as the
recovery of a high-resolution image in a modality, given a low-resolution
observation of the same image in an alternative modality. Our paper offers two
models to address this problem and will be evaluated on the recovery of
high-resolution day images given low-resolution night images of the same scene.
Promising qualitative and quantitative results will be presented for each
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modality Image Inpainting using Generative Adversarial Networks. (arXiv:2206.09210v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09210">
<div class="article-summary-box-inner">
<span><p>Deep learning techniques, especially Generative Adversarial Networks (GANs)
have significantly improved image inpainting and image-to-image translation
tasks over the past few years. To the best of our knowledge, the problem of
combining the image inpainting task with the multi-modality image-to-image
translation remains intact. In this paper, we propose a model to address this
problem. The model will be evaluated on combined night-to-day image translation
and inpainting, along with promising qualitative and quantitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAViR-T: Spatially Attentive Visual Reasoning with Transformers. (arXiv:2206.09265v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09265">
<div class="article-summary-box-inner">
<span><p>We present a novel computational model, "SAViR-T", for the family of visual
reasoning problems embodied in the Raven's Progressive Matrices (RPM). Our
model considers explicit spatial semantics of visual elements within each image
in the puzzle, encoded as spatio-visual tokens, and learns the intra-image as
well as the inter-image token dependencies, highly relevant for the visual
reasoning task. Token-wise relationship, modeled through a transformer-based
SAViR-T architecture, extract group (row or column) driven representations by
leveraging the group-rule coherence and use this as the inductive bias to
extract the underlying rule representations in the top two row (or column) per
token in the RPM. We use this relation representations to locate the correct
choice image that completes the last row or column for the RPM. Extensive
experiments across both synthetic RPM benchmarks, including RAVEN, I-RAVEN,
RAVEN-FAIR, and PGM, and the natural image-based "V-PROM" demonstrate that
SAViR-T sets a new state-of-the-art for visual reasoning, exceeding prior
models' performance by a considerable margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Winning the CVPR'2022 AQTC Challenge: A Two-stage Function-centric Approach. (arXiv:2206.09597v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09597">
<div class="article-summary-box-inner">
<span><p>Affordance-centric Question-driven Task Completion for Egocentric
Assistant(AQTC) is a novel task which helps AI assistant learn from
instructional videos and scripts and guide the user step-by-step. In this
paper, we deal with the AQTC via a two-stage Function-centric approach, which
consists of Question2Function Module to ground the question with the related
function and Function2Answer Module to predict the action based on the
historical steps. We evaluated several possible solutions in each module and
obtained significant gains compared to the given baselines. Our code is
available at \url{https://github.com/starsholic/LOVEU-CVPR22-AQTC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Labeling of High Resolution Images Using EfficientUNets and Transformers. (arXiv:2206.09731v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09731">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation necessitates approaches that learn high-level
characteristics while dealing with enormous amounts of data. Convolutional
neural networks (CNNs) can learn unique and adaptive features to achieve this
aim. However, due to the large size and high spatial resolution of remote
sensing images, these networks cannot analyze an entire scene efficiently.
Recently, deep transformers have proven their capability to record global
interactions between different objects in the image. In this paper, we propose
a new segmentation model that combines convolutional neural networks with
transformers, and show that this mixture of local and global feature extraction
techniques provides significant advantages in remote sensing segmentation. In
addition, the proposed model includes two fusion layers that are designed to
represent multi-modal inputs and output of the network efficiently. The input
fusion layer extracts feature maps summarizing the relationship between image
content and elevation maps (DSM). The output fusion layer uses a novel
multi-task segmentation strategy where class labels are identified using
class-specific feature extraction layers and loss functions. Finally, a
fast-marching method is used to convert all unidentified class labels to their
closest known neighbors. Our results demonstrate that the proposed methodology
improves segmentation accuracy compared to state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Max: Few-Shot Domain Adaptation for Unsupervised Contrastive Representation Learning. (arXiv:2206.10137v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10137">
<div class="article-summary-box-inner">
<span><p>Contrastive self-supervised learning methods learn to map data points such as
images into non-parametric representation space without requiring labels. While
highly successful, current methods require a large amount of data in the
training phase. In situations where the target training set is limited in size,
generalization is known to be poor. Pretraining on a large source data set and
fine-tuning on the target samples is prone to overfitting in the few-shot
regime, where only a small number of target samples are available. Motivated by
this, we propose a domain adaption method for self-supervised contrastive
learning, termed Few-Max, to address the issue of adaptation to a target
distribution under few-shot learning. To quantify the representation quality,
we evaluate Few-Max on a range of source and target datasets, including
ImageNet, VisDA, and fastMRI, on which Few-Max consistently outperforms other
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Estimate and Refine Fluid Motion with Physical Dynamics. (arXiv:2206.10480v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10480">
<div class="article-summary-box-inner">
<span><p>Extracting information on fluid motion directly from images is challenging.
Fluid flow represents a complex dynamic system governed by the Navier-Stokes
equations. General optical flow methods are typically designed for rigid body
motion, and thus struggle if applied to fluid motion estimation directly.
Further, optical flow methods only focus on two consecutive frames without
utilising historical temporal information, while the fluid motion (velocity
field) can be considered a continuous trajectory constrained by time-dependent
partial differential equations (PDEs). This discrepancy has the potential to
induce physically inconsistent estimations. Here we propose an unsupervised
learning based prediction-correction scheme for fluid flow estimation. An
estimate is first given by a PDE-constrained optical flow predictor, which is
then refined by a physical based corrector. The proposed approach outperforms
optical flow methods and shows competitive results compared to existing
supervised learning based methods on a benchmark dataset. Furthermore, the
proposed approach can generalize to complex real-world fluid scenarios where
ground truth information is effectively unknowable. Finally, experiments
demonstrate that the physical corrector can refine flow estimates by mimicking
the operator splitting method commonly utilised in fluid dynamical simulation.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-23 23:08:16.779020282 UTC">2022-06-23 23:08:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>