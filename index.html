<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-25T01:30:00Z">07-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Data Pattern Extraction Attacks on Generative Language Models. (arXiv:2207.10802v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10802">
<div class="article-summary-box-inner">
<span><p>With the wide availability of large pre-trained language model checkpoints,
such as GPT-2 and BERT, the recent trend has been to fine-tune them on a
downstream task to achieve the state-of-the-art performance with a small
computation overhead. One natural example is the Smart Reply application where
a pre-trained model is fine-tuned for suggesting a number of responses given a
query message. In this work, we set out to investigate potential information
leakage vulnerabilities in a typical Smart Reply pipeline and show that it is
possible for an adversary, having black-box or gray-box access to a Smart Reply
model, to extract sensitive user information present in the training data. We
further analyse the privacy impact of specific components, e.g. the decoding
strategy, pertained to this application through our attack settings. We explore
potential mitigation strategies and demonstrate how differential privacy can be
a strong defense mechanism to such data extraction attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASR Error Detection via Audio-Transcript entailment. (arXiv:2207.10849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10849">
<div class="article-summary-box-inner">
<span><p>Despite improved performances of the latest Automatic Speech Recognition
(ASR) systems, transcription errors are still unavoidable. These errors can
have a considerable impact in critical domains such as healthcare, when used to
help with clinical documentation. Therefore, detecting ASR errors is a critical
first step in preventing further error propagation to downstream applications.
To this end, we propose a novel end-to-end approach for ASR error detection
using audio-transcript entailment. To the best of our knowledge, we are the
first to frame this problem as an end-to-end entailment task between the audio
segment and its corresponding transcript segment. Our intuition is that there
should be a bidirectional entailment between audio and transcript when there is
no recognition error and vice versa. The proposed model utilizes an acoustic
encoder and a linguistic encoder to model the speech and transcript
respectively. The encoded representations of both modalities are fused to
predict the entailment. Since doctor-patient conversations are used in our
experiments, a particular emphasis is placed on medical terms. Our proposed
model achieves classification error rates (CER) of 26.2% on all transcription
errors and 23% on medical errors specifically, leading to improvements upon a
strong baseline by 12% and 15.4%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stage Fine-Tuning: A Novel Strategy for Learning Class-Imbalanced Data. (arXiv:2207.10858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10858">
<div class="article-summary-box-inner">
<span><p>Classification on long-tailed distributed data is a challenging problem,
which suffers from serious class-imbalance and hence poor performance on tail
classes with only a few samples. Owing to this paucity of samples, learning on
the tail classes is especially challenging for the fine-tuning when
transferring a pretrained model to a downstream task. In this work, we present
a simple modification of standard fine-tuning to cope with these challenges.
Specifically, we propose a two-stage fine-tuning: we first fine-tune the final
layer of the pretrained model with class-balanced reweighting loss, and then we
perform the standard fine-tuning. Our modification has several benefits: (1) it
leverages pretrained representations by only fine-tuning a small portion of the
model parameters while keeping the rest untouched; (2) it allows the model to
learn an initial representation of the specific task; and importantly (3) it
protects the learning of tail classes from being at a disadvantage during the
model updating. We conduct extensive experiments on synthetic datasets of both
two-class and multi-class tasks of text classification as well as a real-world
application to ADME (i.e., absorption, distribution, metabolism, and excretion)
semantic labeling. The experimental results show that the proposed two-stage
fine-tuning outperforms both fine-tuning with conventional loss and fine-tuning
with a reweighting loss on the above datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing mortality prediction through different representation models based on concepts extracted from clinical notes. (arXiv:2207.10872v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10872">
<div class="article-summary-box-inner">
<span><p>Recent years have seen particular interest in using electronic medical
records (EMRs) for secondary purposes to enhance the quality and safety of
healthcare delivery. EMRs tend to contain large amounts of valuable clinical
notes. Learning of embedding is a method for converting notes into a format
that makes them comparable. Transformer-based representation models have
recently made a great leap forward. These models are pre-trained on large
online datasets to understand natural language texts effectively. The quality
of a learning embedding is influenced by how clinical notes are used as input
to representation models. A clinical note has several sections with different
levels of information value. It is also common for healthcare providers to use
different expressions for the same concept. Existing methods use clinical notes
directly or with an initial preprocessing as input to representation models.
However, to learn a good embedding, we identified the most essential clinical
notes section. We then mapped the extracted concepts from selected sections to
the standard names in the Unified Medical Language System (UMLS). We used the
standard phrases corresponding to the unique concepts as input for clinical
models. We performed experiments to measure the usefulness of the learned
embedding vectors in the task of hospital mortality prediction on a subset of
the publicly available Medical Information Mart for Intensive Care (MIMIC-III)
dataset. According to the experiments, clinical transformer-based
representation models produced better results with getting input generated by
standard names of extracted unique concepts compared to other input formats.
The best-performing models were BioBERT, PubMedBERT, and UmlsBERT,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Level Fine-Tuning, Data Augmentation, and Few-Shot Learning for Specialized Cyber Threat Intelligence. (arXiv:2207.11076v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11076">
<div class="article-summary-box-inner">
<span><p>Gathering cyber threat intelligence from open sources is becoming
increasingly important for maintaining and achieving a high level of security
as systems become larger and more complex. However, these open sources are
often subject to information overload. It is therefore useful to apply machine
learning models that condense the amount of information to what is necessary.
Yet, previous studies and applications have shown that existing classifiers are
not able to extract specific information about emerging cybersecurity events
due to their low generalization ability. Therefore, we propose a system to
overcome this problem by training a new classifier for each new incident. Since
this requires a lot of labelled data using standard training methods, we
combine three different low-data regime techniques - transfer learning, data
augmentation, and few-shot learning - to train a high-quality classifier from
very few labelled instances. We evaluated our approach using a novel dataset
derived from the Microsoft Exchange Server data breach of 2021 which was
labelled by three experts. Our findings reveal an increase in F1 score of more
than 21 points compared to standard training methods and more than 18 points
compared to a state-of-the-art method in few-shot learning. Furthermore, the
classifier trained with this method and 32 instances is only less than 5 F1
score points worse than a classifier trained with 1800 instances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lagrangian Method for Q-Function Learning (with Applications to Machine Translation). (arXiv:2207.11161v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11161">
<div class="article-summary-box-inner">
<span><p>This paper discusses a new approach to the fundamental problem of learning
optimal Q-functions. In this approach, optimal Q-functions are formulated as
saddle points of a nonlinear Lagrangian function derived from the classic
Bellman optimality equation. The paper shows that the Lagrangian enjoys strong
duality, in spite of its nonlinearity, which paves the way to a general
Lagrangian method to Q-function learning. As a demonstration, the paper
develops an imitation learning algorithm based on the duality theory, and
applies the algorithm to a state-of-the-art machine translation benchmark. The
paper then turns to demonstrate a symmetry breaking phenomenon regarding the
optimality of the Lagrangian saddle points, which justifies a largely
overlooked direction in developing the Lagrangian method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target-Driven Structured Transformer Planner for Vision-Language Navigation. (arXiv:2207.11201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11201">
<div class="article-summary-box-inner">
<span><p>Vision-language navigation is the task of directing an embodied agent to
navigate in 3D scenes with natural language instructions. For the agent,
inferring the long-term navigation target from visual-linguistic clues is
crucial for reliable path planning, which, however, has rarely been studied
before in literature. In this article, we propose a Target-Driven Structured
Transformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware
navigation. Specifically, we devise an Imaginary Scene Tokenization mechanism
for explicit estimation of the long-term target (even located in unexplored
environments). In addition, we design a Structured Transformer Planner which
elegantly incorporates the explored room layout into a neural attention
architecture for structured and global planning. Experimental results
demonstrate that our TD-STP substantially improves previous best methods'
success rate by 2% and 5% on the test set of R2R and REVERIE benchmarks,
respectively. Our code is available at https://github.com/YushengZhao/TD-STP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitmo: A Twitter Data Topic Modeling and Visualization Package for R. (arXiv:2207.11236v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11236">
<div class="article-summary-box-inner">
<span><p>We present Twitmo, a package that provides a broad range of methods to
collect, pre-process, analyze and visualize geo-tagged Twitter data. Twitmo
enables the user to collect geo-tagged Tweets from Twitter and and provides a
comprehensive and user-friendly toolbox to generate topic distributions from
Latent Dirichlet Allocations (LDA), correlated topic models (CTM) and
structural topic models (STM). Functions are included for pre-processing of
text, model building and prediction. In addition, one of the innovations of the
package is the automatic pooling of Tweets into longer pseudo-documents using
hashtags and cosine similarities for better topic coherence. The package
additionally comes with functionality to visualize collected data sets and
fitted models in static as well as interactive ways and offers built-in support
for model visualizations via LDAvis providing great convenience for researchers
in this area. The Twitmo package is an innovative toolbox that can be used to
analyze public discourse of various topics, political parties or persons of
interest in space and time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic Scene Graph Generation. (arXiv:2207.11247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11247">
<div class="article-summary-box-inner">
<span><p>Existing research addresses scene graph generation (SGG) -- a critical
technology for scene understanding in images -- from a detection perspective,
i.e., objects are detected using bounding boxes followed by prediction of their
pairwise relationships. We argue that such a paradigm causes several problems
that impede the progress of the field. For instance, bounding box-based labels
in current datasets usually contain redundant classes like hairs, and leave out
background information that is crucial to the understanding of context. In this
work, we introduce panoptic scene graph generation (PSG), a new problem task
that requires the model to generate a more comprehensive scene graph
representation based on panoptic segmentations rather than rigid bounding
boxes. A high-quality PSG dataset, which contains 49k well-annotated
overlapping images from COCO and Visual Genome, is created for the community to
keep track of its progress. For benchmarking, we build four two-stage
baselines, which are modified from classic methods in SGG, and two one-stage
baselines called PSGTR and PSGFormer, which are based on the efficient
Transformer-based detector, i.e., DETR. While PSGTR uses a set of queries to
directly learn triplets, PSGFormer separately models the objects and relations
in the form of queries from two Transformer decoders, followed by a
prompting-like relation-object matching mechanism. In the end, we share
insights on open challenges and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming Wikipedia into Augmented Data for Query-Focused Summarization. (arXiv:1911.03324v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.03324">
<div class="article-summary-box-inner">
<span><p>The limited size of existing query-focused summarization datasets renders
training data-driven summarization models challenging. Meanwhile, the manual
construction of a query-focused summarization corpus is costly and
time-consuming. In this paper, we use Wikipedia to automatically collect a
large query-focused summarization dataset (named WIKIREF) of more than 280, 000
examples, which can serve as a means of data augmentation. We also develop a
BERT-based query-focused summarization model (Q-BERT) to extract sentences from
the documents as summaries. To better adapt a huge model containing millions of
parameters to tiny benchmarks, we identify and fine-tune only a sparse
subnetwork, which corresponds to a small fraction of the whole model
parameters. Experimental results on three DUC benchmarks show that the model
pre-trained on WIKIREF has already achieved reasonable performance. After
fine-tuning on the specific benchmark datasets, the model with data
augmentation outperforms strong comparison systems. Moreover, both our proposed
Q-BERT model and subnetwork fine-tuning further improve the model performance.
The dataset is publicly available at https://aka.ms/wikiref.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Socially Intelligent Agents with Mental State Transition and Human Utility. (arXiv:2103.07011v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07011">
<div class="article-summary-box-inner">
<span><p>Building a socially intelligent agent involves many challenges. One of which
is to track the agent's mental state transition and teach the agent to make
decisions guided by its value like a human. Towards this end, we propose to
incorporate mental state simulation and value modeling into dialogue agents.
First, we build a hybrid mental state parser that extracts information from
both the dialogue and event observations and maintains a graphical
representation of the agent's mind; Meanwhile, the transformer-based value
model learns human preferences from the human value dataset, ValueNet.
Empirical results show that the proposed model attains state-of-the-art
performance on the dialogue/action/emotion prediction task in the fantasy
text-adventure game dataset, LIGHT. We also show example cases to demonstrate:
(i) how the proposed mental state parser can assist the agent's decision by
grounding on the context like locations and objects, and (ii) how the value
model can help the agent make decisions based on its personal priorities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation in Natural Language Processing: A Novel Text Generation Approach for Long and Short Text Classifiers. (arXiv:2103.14453v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14453">
<div class="article-summary-box-inner">
<span><p>In many cases of machine learning, research suggests that the development of
training data might have a higher relevance than the choice and modelling of
classifiers themselves. Thus, data augmentation methods have been developed to
improve classifiers by artificially created training data. In NLP, there is the
challenge of establishing universal rules for text transformations which
provide new linguistic patterns. In this paper, we present and evaluate a text
generation method suitable to increase the performance of classifiers for long
and short texts. We achieved promising improvements when evaluating short as
well as long text tasks with the enhancement by our text generation method.
Especially with regard to small data analytics, additive accuracy gains of up
to 15.53% and 3.56% are achieved within a constructed low data regime, compared
to the no augmentation baseline and another data augmentation technique. As the
current track of these constructed regimes is not universally applicable, we
also show major improvements in several real world low data tasks (up to +4.84
F1-score). Since we are evaluating the method from many perspectives (in total
11 datasets), we also observe situations where the method might not be
suitable. We discuss implications and patterns for the successful application
of our approach on different types of datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Structure Guided Transformer for Source Code Summarization. (arXiv:2104.09340v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09340">
<div class="article-summary-box-inner">
<span><p>Code summaries help developers comprehend programs and reduce their time to
infer the program functionalities during software maintenance. Recent efforts
resort to deep learning techniques such as sequence-to-sequence models for
generating accurate code summaries, among which Transformer-based approaches
have achieved promising performance. However, effectively integrating the code
structure information into the Transformer is under-explored in this task
domain. In this paper, we propose a novel approach named SG-Trans to
incorporate code structural properties into Transformer. Specifically, we
inject the local symbolic information (e.g., code tokens and statements) and
global syntactic structure (e.g., data flow graph) into the self-attention
module of Transformer as inductive bias. To further capture the hierarchical
characteristics of code, the local information and global structure are
designed to distribute in the attention heads of lower layers and high layers
of Transformer. Extensive evaluation shows the superior performance of SG-Trans
over the state-of-the-art approaches. Compared with the best-performing
baseline, SG-Trans still improves 1.4% and 2.0% in terms of METEOR score, a
metric widely used for measuring generation quality, respectively on two
benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03158">
<div class="article-summary-box-inner">
<span><p>Data augmentation, the artificial creation of training data for machine
learning by transformations, is a widely studied research field across machine
learning disciplines. While it is useful for increasing a model's
generalization capabilities, it can also address many other challenges and
problems, from overcoming a limited amount of training data, to regularizing
the objective, to limiting the amount data used to protect privacy. Based on a
precise description of the goals and applications of data augmentation and a
taxonomy for existing works, this survey is concerned with data augmentation
methods for textual classification and aims to provide a concise and
comprehensive overview for researchers and practitioners. Derived from the
taxonomy, we divide more than 100 methods into 12 different groupings and give
state-of-the-art references expounding which methods are highly promising by
relating them to each other. Finally, research perspectives that may constitute
a building block for future work are provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-based Aspect Reasoning for Knowledge Base Question Answering on Clinical Notes. (arXiv:2108.00513v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00513">
<div class="article-summary-box-inner">
<span><p>Question Answering (QA) in clinical notes has gained a lot of attention in
the past few years. Existing machine reading comprehension approaches in
clinical domain can only handle questions about a single block of clinical
texts and fail to retrieve information about multiple patients and their
clinical notes. To handle more complex questions, we aim at creating knowledge
base from clinical notes to link different patients and clinical notes, and
performing knowledge base question answering (KBQA). Based on the expert
annotations available in the n2c2 dataset, we first created the ClinicalKBQA
dataset that includes around 9K QA pairs and covers questions about seven
medical topics using more than 300 question templates. Then, we investigated an
attention-based aspect reasoning (AAR) method for KBQA and analyzed the impact
of different aspects of answers (e.g., entity, type, path, and context) for
prediction. The AAR method achieves better performance due to the well-designed
encoder and attention mechanism. From our experiments, we find that both
aspects, type and path, enable the model to identify answers satisfying the
general conditions and produce lower precision and higher recall. On the other
hand, the aspects, entity and context, limit the answers by node-specific
information and lead to higher precision and lower recall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing and Mitigating Interference in Neural Architecture Search. (arXiv:2108.12821v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12821">
<div class="article-summary-box-inner">
<span><p>Weight sharing is a popular approach to reduce the cost of neural
architecture search (NAS) by reusing the weights of shared operators from
previously trained child models. However, the rank correlation between the
estimated accuracy and ground truth accuracy of those child models is low due
to the interference among different child models caused by weight sharing. In
this paper, we investigate the interference issue by sampling different child
models and calculating the gradient similarity of shared operators, and
observe: 1) the interference on a shared operator between two child models is
positively correlated with the number of different operators; 2) the
interference is smaller when the inputs and outputs of the shared operator are
more similar. Inspired by these two observations, we propose two approaches to
mitigate the interference: 1) MAGIC-T: rather than randomly sampling child
models for optimization, we propose a gradual modification scheme by modifying
one operator between adjacent optimization steps to minimize the interference
on the shared operators; 2) MAGIC-A: forcing the inputs and outputs of the
operator across all child models to be similar to reduce the interference.
Experiments on a BERT search space verify that mitigating interference via each
of our proposed methods improves the rank correlation of super-pet and
combining both methods can achieve better results. Our discovered architecture
outperforms RoBERTa$_{\rm base}$ by 1.1 and 0.6 points and ELECTRA$_{\rm base}$
by 1.6 and 1.1 points on the dev and test set of GLUE benchmark. Extensive
results on the BERT compression, reading comprehension and ImageNet task
demonstrate the effectiveness and generality of our proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition. (arXiv:2109.13226v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13226">
<div class="article-summary-box-inner">
<span><p>We summarize the results of a host of efforts using giant automatic speech
recognition (ASR) models pre-trained using large, diverse unlabeled datasets
containing approximately a million hours of audio. We find that the combination
of pre-training, self-training and scaling up model size greatly increases data
efficiency, even for extremely large tasks with tens of thousands of hours of
labeled data. In particular, on an ASR task with 34k hours of labeled data, by
fine-tuning an 8 billion parameter pre-trained Conformer model we can match
state-of-the-art (SoTA) performance with only 3% of the training data and
significantly improve SoTA with the full training set. We also report on the
universal benefits gained from using big pre-trained and self-trained models
for a large set of downstream tasks that cover a wide range of speech domains
and span multiple orders of magnitudes of dataset sizes, including obtaining
SoTA performance on many public benchmarks. In addition, we utilize the learned
representation of pre-trained networks to achieve SoTA results on non-ASR
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework. (arXiv:2111.04130v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04130">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have become the standard approach for many NLP
tasks due to strong performance, but they are very expensive to train. We
propose a simple and efficient learning framework, TLM, that does not rely on
large-scale pretraining. Given some labeled task data and a large general
corpus, TLM uses task data as queries to retrieve a tiny subset of the general
corpus and jointly optimizes the task objective and the language modeling
objective from scratch. On eight classification datasets in four domains, TLM
achieves results better than or similar to pretrained language models (e.g.,
RoBERTa-Large) while reducing the training FLOPs by two orders of magnitude.
With high accuracy and efficiency, we hope TLM will contribute to democratizing
NLP and expediting its development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Tuning GPT-2 language model for parameter-efficient domain adaptation of ASR systems. (arXiv:2112.08718v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08718">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems have found their use in numerous
industrial applications in very diverse domains creating a need to adapt to new
domains with small memory and deployment overhead. In this work, we introduce
domain-prompts, a methodology that involves training a small number of domain
embedding parameters to prime a Transformer-based Language Model (LM) to a
particular domain. Using this domain-adapted LM for rescoring ASR hypotheses
can achieve 7-13% WER reduction for a new domain with just 1000 unlabeled
textual domain-specific sentences. This improvement is comparable or even
better than fully fine-tuned models even though just 0.02% of the parameters of
the base LM are updated. Additionally, our method is deployment-friendly as the
learnt domain embeddings are prefixed to the input to the model rather than
changing the base model architecture. Therefore, our method is an ideal choice
for on-the-fly adaptation of LMs used in ASR systems to progressively scale it
to new domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimization of a Real-Time Wavelet-Based Algorithm for Improving Speech Intelligibility. (arXiv:2202.02545v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02545">
<div class="article-summary-box-inner">
<span><p>The optimization of a wavelet-based algorithm to improve speech
intelligibility along with the full data set and results are reported. The
discrete-time speech signal is split into frequency sub-bands via a multi-level
discrete wavelet transform. Various gains are applied to the sub-band signals
before they are recombined to form a modified version of the speech. The
sub-band gains are adjusted while keeping the overall signal energy unchanged,
and the speech intelligibility under various background interference and
simulated hearing loss conditions is enhanced and evaluated objectively and
quantitatively using Google Speech-to-Text transcription. A universal set of
sub-band gains can work over a range of noise-to-signal ratios up to 4.8 dB.
For noise-free speech, overall intelligibility is improved, and the Google
transcription accuracy is increased by 16.9 percentage points on average and
86.7 maximum by reallocating the spectral energy toward the mid-frequency
sub-bands. For speech already corrupted by noise, improving intelligibility is
challenging but still realizable with an increased transcription accuracy of
9.5 percentage points on average and 71.4 maximum. The proposed algorithm is
implementable for real-time speech processing and comparatively simpler than
previous algorithms. Potential applications include speech enhancement, hearing
aids, machine listening, and a better understanding of speech intelligibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. (arXiv:2204.11424v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11424">
<div class="article-summary-box-inner">
<span><p>We propose an explainable approach for relation extraction that mitigates the
tension between generalization and explainability by jointly training for the
two goals. Our approach uses a multi-task learning architecture, which jointly
trains a classifier for relation extraction, and a sequence model that labels
words in the context of the relation that explain the decisions of the relation
classifier. We also convert the model outputs to rules to bring global
explanations to this approach. This sequence model is trained using a hybrid
strategy: supervised, when supervision from pre-existing patterns is available,
and semi-supervised otherwise. In the latter situation, we treat the sequence
model's labels as latent variables, and learn the best assignment that
maximizes the performance of the relation classifier. We evaluate the proposed
approach on the two datasets and show that the sequence model provides labels
that serve as accurate explanations for the relation classifier's decisions,
and, importantly, that the joint training generally improves the performance of
the relation classifier. We also evaluate the performance of the generated
rules and show that the new rules are great add-on to the manual rules and
bring the rule-based system much closer to the neural models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEMOS: Generating diverse human motions from textual descriptions. (arXiv:2204.14109v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14109">
<div class="article-summary-box-inner">
<span><p>We address the problem of generating diverse 3D human motions from textual
descriptions. This challenging task requires joint modeling of both modalities:
understanding and extracting useful human-centric information from the text,
and then generating plausible and realistic sequences of human poses. In
contrast to most previous work which focuses on generating a single,
deterministic, motion from a textual description, we design a variational
approach that can produce multiple diverse human motions. We propose TEMOS, a
text-conditioned generative model leveraging variational autoencoder (VAE)
training with human motion data, in combination with a text encoder that
produces distribution parameters compatible with the VAE latent space. We show
the TEMOS framework can produce both skeleton-based animations as in prior
work, as well more expressive SMPL body motions. We evaluate our approach on
the KIT Motion-Language benchmark and, despite being relatively
straightforward, demonstrate significant improvements over the state of the
art. Code and models are available on our webpage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dialogue Representations from Consecutive Utterances. (arXiv:2205.13568v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13568">
<div class="article-summary-box-inner">
<span><p>Learning high-quality dialogue representations is essential for solving a
variety of dialogue-oriented tasks, especially considering that dialogue
systems often suffer from data scarcity. In this paper, we introduce Dialogue
Sentence Embedding (DSE), a self-supervised contrastive learning method that
learns effective dialogue representations suitable for a wide range of dialogue
tasks. DSE learns from dialogues by taking consecutive utterances of the same
dialogue as positive pairs for contrastive learning. Despite its simplicity,
DSE achieves significantly better representation capability than other dialogue
representation and universal sentence representation models. We evaluate DSE on
five downstream dialogue tasks that examine dialogue representation at
different semantic granularities. Experiments in few-shot and zero-shot
settings show that DSE outperforms baselines by a large margin. For example, it
achieves 13% average performance improvement over the strongest unsupervised
baseline in 1-shot intent classification on 6 datasets. We also provide
analyses on the benefits and limitations of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities. (arXiv:2206.10883v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10883">
<div class="article-summary-box-inner">
<span><p>With the advent of large language models, methods for abstractive
summarization have made great strides, creating potential for use in
applications to aid knowledge workers processing unwieldy document collections.
One such setting is the Civil Rights Litigation Clearinghouse (CRLC)
(https://clearinghouse.net),which posts information about large-scale civil
rights lawsuits, serving lawyers, scholars, and the general public. Today,
summarization in the CRLC requires extensive training of lawyers and law
students who spend hours per case understanding multiple relevant documents in
order to produce high-quality summaries of key events and outcomes. Motivated
by this ongoing real-world summarization effort, we introduce Multi-LexSum, a
collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.
Multi-LexSum presents a challenging multi-document summarization task given the
length of the source documents, often exceeding two hundred pages per case.
Furthermore, Multi-LexSum is distinct from other datasets in its multiple
target summaries, each at a different granularity (ranging from one-sentence
"extreme" summaries to multi-paragraph narrations of over five hundred words).
We present extensive analysis demonstrating that despite the high-quality
summaries in the training data (adhering to strict content and style
guidelines), state-of-the-art summarization models perform poorly on this task.
We release Multi-LexSum for further research in summarization methods as well
as to facilitate development of applications to assist in the CRLC's mission at
https://multilexsum.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Democratizing Ethical Assessment of Natural Language Generation Models. (arXiv:2207.10576v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10576">
<div class="article-summary-box-inner">
<span><p>Natural language generation models are computer systems that generate
coherent language when prompted with a sequence of words as context. Despite
their ubiquity and many beneficial applications, language generation models
also have the potential to inflict social harms by generating discriminatory
language, hateful speech, profane content, and other harmful material. Ethical
assessment of these models is therefore critical. But it is also a challenging
task, requiring an expertise in several specialized domains, such as
computational linguistics and social justice. While significant strides have
been made by the research community in this domain, accessibility of such
ethical assessments to the wider population is limited due to the high entry
barriers. This article introduces a new tool to democratize and standardize
ethical assessment of natural language generation models: Tool for Ethical
Assessment of Language generation models (TEAL), a component of Credo AI Lens,
an open-source assessment framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STOP: A dataset for Spoken Task Oriented Semantic Parsing. (arXiv:2207.10643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10643">
<div class="article-summary-box-inner">
<span><p>End-to-end spoken language understanding (SLU) predicts intent directly from
audio using a single model. It promises to improve the performance of assistant
systems by leveraging acoustic information lost in the intermediate textual
representation and preventing cascading errors from Automatic Speech
Recognition (ASR). Further, having one unified model has efficiency advantages
when deploying assistant systems on-device. However, the limited number of
public audio datasets with semantic parse labels hinders the research progress
in this area. In this paper, we release the Spoken Task-Oriented semantic
Parsing (STOP) dataset, the largest and most complex SLU dataset to be publicly
available. Additionally, we define low-resource splits to establish a benchmark
for improving SLU when limited labeled data is available. Furthermore, in
addition to the human-recorded audio, we are releasing a TTS-generated version
to benchmark the performance for low-resource domain adaptation of end-to-end
SLU systems. Initial experimentation show end-to-end SLU models performing
slightly worse than their cascaded counterparts, which we hope encourages
future work in this direction.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">R2P: A Deep Learning Model from mmWave Radar to Point Cloud. (arXiv:2207.10690v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10690">
<div class="article-summary-box-inner">
<span><p>Recent research has shown the effectiveness of mmWave radar sensing for
object detection in low visibility environments, which makes it an ideal
technique in autonomous navigation systems. In this paper, we introduce Radar
to Point Cloud (R2P), a deep learning model that generates smooth, dense, and
highly accurate point cloud representation of a 3D object with fine geometry
details, based on rough and sparse point clouds with incorrect points obtained
from mmWave radar. These input point clouds are converted from the 2D depth
images that are generated from raw mmWave radar sensor data, characterized by
inconsistency, and orientation and shape errors. R2P utilizes an architecture
of two sequential deep learning encoder-decoder blocks to extract the essential
features of those radar-based input point clouds of an object when observed
from multiple viewpoints, and to ensure the internal consistency of a generated
output point cloud and its accurate and detailed shape reconstruction of the
original object. We implement R2P to replace Stage 2 of our recently proposed
3DRIMR (3D Reconstruction and Imaging via mmWave Radar) system. Our experiments
demonstrate the significant performance improvement of R2P over the popular
existing methods such as PointNet, PCN, and the original 3DRIMR design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Dataset Generation for Adversarial Machine Learning Research. (arXiv:2207.10719v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10719">
<div class="article-summary-box-inner">
<span><p>Existing adversarial example research focuses on digitally inserted
perturbations on top of existing natural image datasets. This construction of
adversarial examples is not realistic because it may be difficult, or even
impossible, for an attacker to deploy such an attack in the real-world due to
sensing and environmental effects. To better understand adversarial examples
against cyber-physical systems, we propose approximating the real-world through
simulation. In this paper we describe our synthetic dataset generation tool
that enables scalable collection of such a synthetic dataset with realistic
adversarial examples. We use the CARLA simulator to collect such a dataset and
demonstrate simulated attacks that undergo the same environmental transforms
and processing as real-world images. Our tools have been used to collect
datasets to help evaluate the efficacy of adversarial examples, and can be
found at https://github.com/carla-simulator/carla/pull/4992.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing Frame and Event Vision for High-speed Optical Flow for Edge Application. (arXiv:2207.10720v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10720">
<div class="article-summary-box-inner">
<span><p>Optical flow computation with frame-based cameras provides high accuracy but
the speed is limited either by the model size of the algorithm or by the frame
rate of the camera. This makes it inadequate for high-speed applications. Event
cameras provide continuous asynchronous event streams overcoming the frame-rate
limitation. However, the algorithms for processing the data either borrow frame
like setup limiting the speed or suffer from lower accuracy. We fuse the
complementary accuracy and speed advantages of the frame and event-based
pipelines to provide high-speed optical flow while maintaining a low error
rate. Our bio-mimetic network is validated with the MVSEC dataset showing 19%
error degradation at 4x speed up. We then demonstrate the system with a
high-speed drone flight scenario where a high-speed event camera computes the
flow even before the optical camera sees the drone making it suited for
applications like tracking and segmentation. This work shows the fundamental
trade-offs in frame-based processing may be overcome by fusing data from other
modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Irrelevant Pixels are Everywhere: Find and Exclude Them for More Efficient Computer Vision. (arXiv:2207.10741v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10741">
<div class="article-summary-box-inner">
<span><p>Computer vision is often performed using Convolutional Neural Networks
(CNNs). CNNs are compute-intensive and challenging to deploy on
power-contrained systems such as mobile and Internet-of-Things (IoT) devices.
CNNs are compute-intensive because they indiscriminately compute many features
on all pixels of the input image. We observe that, given a computer vision
task, images often contain pixels that are irrelevant to the task. For example,
if the task is looking for cars, pixels in the sky are not very useful.
Therefore, we propose that a CNN be modified to only operate on relevant pixels
to save computation and energy. We propose a method to study three popular
computer vision datasets, finding that 48% of pixels are irrelevant. We also
propose the focused convolution to modify a CNN's convolutional layers to
reject the pixels that are marked irrelevant. On an embedded device, we observe
no loss in accuracy, while inference latency, energy consumption, and
multiply-add count are all reduced by about 45%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection. (arXiv:2207.10758v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10758">
<div class="article-summary-box-inner">
<span><p>Modern neural networks use building blocks such as convolutions that are
equivariant to arbitrary 2D translations. However, these vanilla blocks are not
equivariant to arbitrary 3D translations in the projective manifold. Even then,
all monocular 3D detectors use vanilla blocks to obtain the 3D coordinates, a
task for which the vanilla blocks are not designed for. This paper takes the
first step towards convolutions equivariant to arbitrary 3D translations in the
projective manifold. Since the depth is the hardest to estimate for monocular
detection, this paper proposes Depth EquiVarIAnt NeTwork (DEVIANT) built with
existing scale equivariant steerable blocks. As a result, DEVIANT is
equivariant to the depth translations in the projective manifold whereas
vanilla networks are not. The additional depth equivariance forces the DEVIANT
to learn consistent depth estimates, and therefore, DEVIANT achieves
state-of-the-art monocular 3D detection results on KITTI and Waymo datasets in
the image-only category and performs competitively to methods using extra
information. Moreover, DEVIANT works better than vanilla networks in
cross-dataset evaluation. Code and models at
https://github.com/abhi1kumar/DEVIANT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIDEE: Tidying Up Novel Rooms using Visuo-Semantic Commonsense Priors. (arXiv:2207.10761v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10761">
<div class="article-summary-box-inner">
<span><p>We introduce TIDEE, an embodied agent that tidies up a disordered scene based
on learned commonsense object placement and room arrangement priors. TIDEE
explores a home environment, detects objects that are out of their natural
place, infers plausible object contexts for them, localizes such contexts in
the current scene, and repositions the objects. Commonsense priors are encoded
in three modules: i) visuo-semantic detectors that detect out-of-place objects,
ii) an associative neural graph memory of objects and spatial relations that
proposes plausible semantic receptacles and surfaces for object repositions,
and iii) a visual search network that guides the agent's exploration for
efficiently localizing the receptacle-of-interest in the current scene to
reposition the object. We test TIDEE on tidying up disorganized scenes in the
AI2THOR simulation environment. TIDEE carries out the task directly from pixel
and raw depth input without ever having observed the same room beforehand,
relying only on priors learned from a separate set of training houses. Human
evaluations on the resulting room reorganizations show TIDEE outperforms
ablative versions of the model that do not use one or more of the commonsense
priors. On a related room rearrangement benchmark that allows the agent to view
the goal state prior to rearrangement, a simplified version of our model
significantly outperforms a top-performing method by a large margin. Code and
data are available at the project website: https://tidee-agent.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeshLoc: Mesh-Based Visual Localization. (arXiv:2207.10762v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10762">
<div class="article-summary-box-inner">
<span><p>Visual localization, i.e., the problem of camera pose estimation, is a
central component of applications such as autonomous robots and augmented
reality systems. A dominant approach in the literature, shown to scale to large
scenes and to handle complex illumination and seasonal changes, is based on
local features extracted from images. The scene representation is a sparse
Structure-from-Motion point cloud that is tied to a specific local feature.
Switching to another feature type requires an expensive feature matching step
between the database images used to construct the point cloud. In this work, we
thus explore a more flexible alternative based on dense 3D meshes that does not
require features matching between database images to build the scene
representation. We show that this approach can achieve state-of-the-art
results. We further show that surprisingly competitive results can be obtained
when extracting features on renderings of these meshes, without any neural
rendering stage, and even when rendering raw scene geometry without color or
texture. Our results show that dense 3D model-based representations are a
promising alternative to existing representations and point to interesting and
challenging directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Interpretable Video Super-Resolution via Alternating Optimization. (arXiv:2207.10765v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10765">
<div class="article-summary-box-inner">
<span><p>In this paper, we study a practical space-time video super-resolution (STVSR)
problem which aims at generating a high-framerate high-resolution sharp video
from a low-framerate low-resolution blurry video. Such problem often occurs
when recording a fast dynamic event with a low-framerate and low-resolution
camera, and the captured video would suffer from three typical issues: i)
motion blur occurs due to object/camera motions during exposure time; ii)
motion aliasing is unavoidable when the event temporal frequency exceeds the
Nyquist limit of temporal sampling; iii) high-frequency details are lost
because of the low spatial sampling rate. These issues can be alleviated by a
cascade of three separate sub-tasks, including video deblurring, frame
interpolation, and super-resolution, which, however, would fail to capture the
spatial and temporal correlations among video sequences. To address this, we
propose an interpretable STVSR framework by leveraging both model-based and
learning-based methods. Specifically, we formulate STVSR as a joint video
deblurring, frame interpolation, and super-resolution problem, and solve it as
two sub-problems in an alternate way. For the first sub-problem, we derive an
interpretable analytical solution and use it as a Fourier data transform layer.
Then, we propose a recurrent video enhancement layer for the second sub-problem
to further recover high-frequency details. Extensive experiments demonstrate
the superiority of our method in terms of quantitative metrics and visual
quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focused Decoding Enables 3D Anatomical Detection by Transformers. (arXiv:2207.10774v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10774">
<div class="article-summary-box-inner">
<span><p>Detection Transformers represent end-to-end object detection approaches based
on a Transformer encoder-decoder architecture, exploiting the attention
mechanism for global relation modeling. Although Detection Transformers deliver
results on par with or even superior to their highly optimized CNN-based
counterparts operating on 2D natural images, their success is closely coupled
to access to a vast amount of training data. This, however, restricts the
feasibility of employing Detection Transformers in the medical domain, as
access to annotated data is typically limited. To tackle this issue and
facilitate the advent of medical Detection Transformers, we propose a novel
Detection Transformer for 3D anatomical structure detection, dubbed Focused
Decoder. Focused Decoder leverages information from an anatomical region atlas
to simultaneously deploy query anchors and restrict the cross-attention's field
of view to regions of interest, which allows for a precise focus on relevant
anatomical structures. We evaluate our proposed approach on two publicly
available CT datasets and demonstrate that Focused Decoder not only provides
strong detection results and thus alleviates the need for a vast amount of
annotated data but also exhibits exceptional and highly intuitive
explainability of results via attention weights. Code for Focused Decoder is
available in our medical Vision Transformer library
github.com/bwittmann/transoar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-regressive Image Synthesis with Integrated Quantization. (arXiv:2207.10776v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10776">
<div class="article-summary-box-inner">
<span><p>Deep generative models have achieved conspicuous progress in realistic image
synthesis with multifarious conditional inputs, while generating diverse yet
high-fidelity images remains a grand challenge in conditional image generation.
This paper presents a versatile framework for conditional image generation
which incorporates the inductive bias of CNNs and powerful sequence modeling of
auto-regression that naturally leads to diverse image generation. Instead of
independently quantizing the features of multiple domains as in prior research,
we design an integrated quantization scheme with a variational regularizer that
mingles the feature discretization in multiple domains, and markedly boosts the
auto-regressive modeling performance. Notably, the variational regularizer
enables to regularize feature distributions in incomparable latent spaces by
penalizing the intra-domain variations of distributions. In addition, we design
a Gumbel sampling strategy that allows to incorporate distribution uncertainty
into the auto-regressive training procedure. The Gumbel sampling substantially
mitigates the exposure bias that often incurs misalignment between the training
and inference stages and severely impairs the inference performance. Extensive
experiments over multiple conditional image generation tasks show that our
method achieves superior diverse image generation performance qualitatively and
quantitatively as compared with the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An advanced combination of semi-supervised Normalizing Flow & Yolo (YoloNF) to detect and recognize vehicle license plates. (arXiv:2207.10777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10777">
<div class="article-summary-box-inner">
<span><p>Fully Automatic License Plate Recognition (ALPR) has been a frequent research
topic due to several practical applications. However, many of the current
solutions are still not robust enough in real situations, commonly depending on
many constraints. This paper presents a robust and efficient ALPR system based
on the state-of-the-art YOLO object detector and Normalizing flows. The model
uses two new strategies. Firstly, a two-stage network using YOLO and a
normalization flow-based model for normalization to detect Licenses Plates (LP)
and recognize the LP with numbers and Arabic characters. Secondly, Multi-scale
image transformations are implemented to provide a solution to the problem of
the YOLO cropped LP detection including significant background noise.
Furthermore, extensive experiments are led on a new dataset with realistic
scenarios, we introduce a larger public annotated dataset collected from
Moroccan plates. We demonstrate that our proposed model can learn on a small
number of samples free of single or multiple characters. The dataset will also
be made publicly available to encourage further studies and research on plate
detection and recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strategising template-guided needle placement for MR-targeted prostate biopsy. (arXiv:2207.10784v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10784">
<div class="article-summary-box-inner">
<span><p>Clinically significant prostate cancer has a better chance to be sampled
during ultrasound-guided biopsy procedures, if suspected lesions found in
pre-operative magnetic resonance (MR) images are used as targets. However, the
diagnostic accuracy of the biopsy procedure is limited by the
operator-dependent skills and experience in sampling the targets, a sequential
decision making process that involves navigating an ultrasound probe and
placing a series of sampling needles for potentially multiple targets. This
work aims to learn a reinforcement learning (RL) policy that optimises the
actions of continuous positioning of 2D ultrasound views and biopsy needles
with respect to a guiding template, such that the MR targets can be sampled
efficiently and sufficiently. We first formulate the task as a Markov decision
process (MDP) and construct an environment that allows the targeting actions to
be performed virtually for individual patients, based on their anatomy and
lesions derived from MR images. A patient-specific policy can thus be
optimised, before each biopsy procedure, by rewarding positive sampling in the
MDP environment. Experiment results from fifty four prostate cancer patients
show that the proposed RL-learned policies obtained a mean hit rate of 93% and
an average cancer core length of 11 mm, which compared favourably to two
alternative baseline strategies designed by humans, without hand-engineered
rewards that directly maximise these clinically relevant metrics. Perhaps more
interestingly, it is found that the RL agents learned strategies that were
adaptive to the lesion size, where spread of the needles was prioritised for
smaller lesions. Such a strategy has not been previously reported or commonly
adopted in clinical practice, but led to an overall superior targeting
performance when compared with intuitively designed strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inductive and Transductive Few-Shot Video Classification via Appearance and Temporal Alignments. (arXiv:2207.10785v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10785">
<div class="article-summary-box-inner">
<span><p>We present a novel method for few-shot video classification, which performs
appearance and temporal alignments. In particular, given a pair of query and
support videos, we conduct appearance alignment via frame-level feature
matching to achieve the appearance similarity score between the videos, while
utilizing temporal order-preserving priors for obtaining the temporal
similarity score between the videos. Moreover, we introduce a few-shot video
classification framework that leverages the above appearance and temporal
similarity scores across multiple steps, namely prototype-based training and
testing as well as inductive and transductive prototype refinement. To the best
of our knowledge, our work is the first to explore transductive few-shot video
classification. Extensive experiments on both Kinetics and Something-Something
V2 datasets show that both appearance and temporal alignments are crucial for
datasets with temporal order sensitivity such as Something-Something V2. Our
approach achieves similar or better results than previous methods on both
datasets. Our code is available at https://github.com/VinAIResearch/fsvc-ata.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-Time Adaptation via Self-Training with Nearest Neighbor Information. (arXiv:2207.10792v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10792">
<div class="article-summary-box-inner">
<span><p>Adapting trained classifiers using only online test data is important since
it is difficult to access training data or future test data during test time.
One of the popular approaches for test-time adaptation is self-training, which
fine-tunes the trained classifiers using the classifier predictions of the test
data as pseudo labels. However, under the test-time domain shift, self-training
methods have a limitation that learning with inaccurate pseudo labels greatly
degrades the performance of the adapted classifiers. To overcome this
limitation, we propose a novel test-time adaptation method Test-time Adaptation
via Self-Training with nearest neighbor information (TAST). Based on the idea
that a test data and its nearest neighbors in the embedding space of the
trained classifier are more likely to have the same label, we adapt the trained
classifier with the following two steps: (1) generate the pseudo label for the
test data using its nearest neighbors from a set composed of previous test
data, and (2) fine-tune the trained classifier with the pseudo label. Our
experiments on two standard benchmarks, i.e., domain generalization and image
corruption benchmarks, show that TAST outperforms the current state-of-the-art
test-time adaptation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuroimaging Feature Extraction using a Neural Network Classifier for Imaging Genetics. (arXiv:2207.10794v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10794">
<div class="article-summary-box-inner">
<span><p>A major issue in the association of genes to neuroimaging phenotypes is the
high dimension of both genetic data and neuroimaging data. In this article, we
tackle the latter problem with an eye toward developing solutions that are
relevant for disease prediction. Supported by a vast literature on the
predictive power of neural networks, our proposed solution uses neural networks
to extract from neuroimaging data features that are relevant for predicting
Alzheimer's Disease (AD) for subsequent relation to genetics. Our
neuroimaging-genetic pipeline is comprised of image processing, neuroimaging
feature extraction and genetic association steps. We propose a neural network
classifier for extracting neuroimaging features that are related with disease
and a multivariate Bayesian group sparse regression model for genetic
association. We compare the predictive power of these features to expert
selected features and take a closer look at the SNPs identified with the new
neuroimaging features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation. (arXiv:2207.10825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10825">
<div class="article-summary-box-inner">
<span><p>Recent works have demonstrated that deep learning models are vulnerable to
backdoor poisoning attacks, where these attacks instill spurious correlations
to external trigger patterns or objects (e.g., stickers, sunglasses, etc.). We
find that such external trigger signals are unnecessary, as highly effective
backdoors can be easily inserted using rotation-based image transformation. Our
method constructs the poisoned dataset by rotating a limited amount of objects
and labeling them incorrectly; once trained with it, the victim's model will
make undesirable predictions during run-time inference. It exhibits a
significantly high attack success rate while maintaining clean performance
through comprehensive empirical studies on image classification and object
detection tasks. Furthermore, we evaluate standard data augmentation techniques
and four different backdoor defenses against our attack and find that none of
them can serve as a consistent mitigation approach. Our attack can be easily
deployed in the real world since it only requires rotating the object, as we
show in both image classification and object detection applications. Overall,
our work highlights a new, simple, physically realizable, and highly effective
vector for backdoor attacks. Our video demo is available at
https://youtu.be/6JIF8wnX34M.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Image Generation Using Discrete Content Representation. (arXiv:2207.10833v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10833">
<div class="article-summary-box-inner">
<span><p>Few-shot image generation and few-shot image translation are two related
tasks, both of which aim to generate new images for an unseen category with
only a few images. In this work, we make the first attempt to adapt few-shot
image translation method to few-shot image generation task. Few-shot image
translation disentangles an image into style vector and content map. An unseen
style vector can be combined with different seen content maps to produce
different images. However, it needs to store seen images to provide content
maps and the unseen style vector may be incompatible with seen content maps. To
adapt it to few-shot image generation task, we learn a compact dictionary of
local content vectors via quantizing continuous content maps into discrete
content maps instead of storing seen images. Furthermore, we model the
autoregressive distribution of discrete content map conditioned on style
vector, which can alleviate the incompatibility between content map and style
vector. Qualitative and quantitative results on three real datasets demonstrate
that our model can produce images of higher diversity and fidelity for unseen
categories than previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-aware Multi-modal Learning via Cross-modal Random Network Prediction. (arXiv:2207.10851v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10851">
<div class="article-summary-box-inner">
<span><p>Multi-modal learning focuses on training models by equally combining multiple
input data modalities during the prediction process. However, this equal
combination can be detrimental to the prediction accuracy because different
modalities are usually accompanied by varying levels of uncertainty. Using such
uncertainty to combine modalities has been studied by a couple of approaches,
but with limited success because these approaches are either designed to deal
with specific classification or segmentation problems and cannot be easily
translated into other tasks, or suffer from numerical instabilities. In this
paper, we propose a new Uncertainty-aware Multi-modal Learner that estimates
uncertainty by measuring feature density via Cross-modal Random Network
Prediction (CRNP). CRNP is designed to require little adaptation to translate
between different prediction tasks, while having a stable training process.
From a technical point of view, CRNP is the first approach to explore random
network prediction to estimate uncertainty and to combine multi-modal data.
Experiments on two 3D multi-modal medical image segmentation tasks and three 2D
multi-modal computer vision classification tasks show the effectiveness,
adaptability and robustness of CRNP. Also, we provide an extensive discussion
on different fusion functions and visualization to validate the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Deformable Attention Network for Video Deblurring. (arXiv:2207.10852v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10852">
<div class="article-summary-box-inner">
<span><p>The key success factor of the video deblurring methods is to compensate for
the blurry pixels of the mid-frame with the sharp pixels of the adjacent video
frames. Therefore, mainstream methods align the adjacent frames based on the
estimated optical flows and fuse the alignment frames for restoration. However,
these methods sometimes generate unsatisfactory results because they rarely
consider the blur levels of pixels, which may introduce blurry pixels from
video frames. Actually, not all the pixels in the video frames are sharp and
beneficial for deblurring. To address this problem, we propose the
spatio-temporal deformable attention network (STDANet) for video delurring,
which extracts the information of sharp pixels by considering the pixel-wise
blur levels of the video frames. Specifically, STDANet is an encoder-decoder
network combined with the motion estimator and spatio-temporal deformable
attention (STDA) module, where motion estimator predicts coarse optical flows
that are used as base offsets to find the corresponding sharp pixels in STDA
module. Experimental results indicate that the proposed STDANet performs
favorably against state-of-the-art methods on the GoPro, DVD, and BSD datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototype-Guided Continual Adaptation for Class-Incremental Unsupervised Domain Adaptation. (arXiv:2207.10856v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10856">
<div class="article-summary-box-inner">
<span><p>This paper studies a new, practical but challenging problem, called
Class-Incremental Unsupervised Domain Adaptation (CI-UDA), where the labeled
source domain contains all classes, but the classes in the unlabeled target
domain increase sequentially. This problem is challenging due to two
difficulties. First, source and target label sets are inconsistent at each time
step, which makes it difficult to conduct accurate domain alignment. Second,
previous target classes are unavailable in the current step, resulting in the
forgetting of previous knowledge. To address this problem, we propose a novel
Prototype-guided Continual Adaptation (ProCA) method, consisting of two
solution strategies. 1) Label prototype identification: we identify target
label prototypes by detecting shared classes with cumulative prediction
probabilities of target samples. 2) Prototype-based alignment and replay: based
on the identified label prototypes, we align both domains and enforce the model
to retain previous knowledge. With these two strategies, ProCA is able to adapt
the source model to a class-incremental unlabeled target domain effectively.
Extensive experiments demonstrate the effectiveness and superiority of ProCA in
resolving CI-UDA. The source code is available at
https://github.com/Hongbin98/ProCA.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geodesic-Former: a Geodesic-Guided Few-shot 3D Point Cloud Instance Segmenter. (arXiv:2207.10859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10859">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new problem in 3D point cloud: few-shot instance
segmentation. Given a few annotated point clouds exemplified a target class,
our goal is to segment all instances of this target class in a query point
cloud. This problem has a wide range of practical applications where point-wise
instance segmentation annotation is prohibitively expensive to collect. To
address this problem, we present Geodesic-Former -- the first geodesic-guided
transformer for 3D point cloud instance segmentation. The key idea is to
leverage the geodesic distance to tackle the density imbalance of LiDAR 3D
point clouds. The LiDAR 3D point clouds are dense near the object surface and
sparse or empty elsewhere making the Euclidean distance less effective to
distinguish different objects. The geodesic distance, on the other hand, is
more suitable since it encodes the scene's geometry which can be used as a
guiding signal for the attention mechanism in a transformer decoder to generate
kernels representing distinct features of instances. These kernels are then
used in a dynamic convolution to obtain the final instance masks. To evaluate
Geodesic-Former on the new task, we propose new splits of the two common 3D
point cloud instance segmentation datasets: ScannetV2 and S3DIS.
Geodesic-Former consistently outperforms strong baselines adapted from
state-of-the-art 3D point cloud instance segmentation approaches with a
significant margin. Code is available at
https://github.com/VinAIResearch/GeoFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Higher Adversarial Susceptibility of Contrastive Self-Supervised Learning. (arXiv:2207.10862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10862">
<div class="article-summary-box-inner">
<span><p>Contrastive self-supervised learning (CSL) has managed to match or surpass
the performance of supervised learning in image and video classification.
However, it is still largely unknown if the nature of the representation
induced by the two learning paradigms is similar. We investigate this under the
lens of adversarial robustness. Our analytical treatment of the problem reveals
intrinsic higher sensitivity of CSL over supervised learning. It identifies the
uniform distribution of data representation over a unit hypersphere in the CSL
representation space as the key contributor to this phenomenon. We establish
that this increases model sensitivity to input perturbations in the presence of
false negatives in the training data. Our finding is supported by extensive
experiments for image and video classification using adversarial perturbations
and other input corruptions. Building on the insights, we devise strategies
that are simple, yet effective in improving model robustness with CSL training.
We demonstrate up to 68% reduction in the performance gap between adversarially
attacked CSL and its supervised counterpart. Finally, we contribute to robust
CSL paradigm by incorporating our findings in adversarial self-supervised
learning. We demonstrate an average gain of about 5% over two different
state-of-the-art methods in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation. (arXiv:2207.10866v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10866">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel cost aggregation network, called Volumetric
Aggregation with Transformers (VAT), for few-shot segmentation. The use of
transformers can benefit correlation map aggregation through self-attention
over a global receptive field. However, the tokenization of a correlation map
for transformer processing can be detrimental, because the discontinuity at
token boundaries reduces the local context available near the token edges and
decreases inductive bias. To address this problem, we propose a 4D
Convolutional Swin Transformer, where a high-dimensional Swin Transformer is
preceded by a series of small-kernel convolutions that impart local context to
all pixels and introduce convolutional inductive bias. We additionally boost
aggregation performance by applying transformers within a pyramidal structure,
where aggregation at a coarser level guides aggregation at a finer level. Noise
in the transformer output is then filtered in the subsequent decoder with the
help of the query's appearance embedding. With this model, a new
state-of-the-art is set for all the standard benchmarks in few-shot
segmentation. It is shown that VAT attains state-of-the-art performance for
semantic correspondence as well, where cost aggregation also plays a central
role.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Image Compression via Joint Learning with Denoising. (arXiv:2207.10869v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10869">
<div class="article-summary-box-inner">
<span><p>High levels of noise usually exist in today's captured images due to the
relatively small sensors equipped in the smartphone cameras, where the noise
brings extra challenges to lossy image compression algorithms. Without the
capacity to tell the difference between image details and noise, general image
compression methods allocate additional bits to explicitly store the undesired
image noise during compression and restore the unpleasant noisy image during
decompression. Based on the observations, we optimize the image compression
algorithm to be noise-aware as joint denoising and compression to resolve the
bits misallocation problem. The key is to transform the original noisy images
to noise-free bits by eliminating the undesired noise during compression, where
the bits are later decompressed as clean images. Specifically, we propose a
novel two-branch, weight-sharing architecture with plug-in feature denoisers to
allow a simple and effective realization of the goal with little computational
cost. Experimental results show that our method gains a significant improvement
over the existing baseline methods on both the synthetic and real-world
datasets. Our source code is available at
https://github.com/felixcheng97/DenoiseCompression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Ensemble Approach for Multiple Emotion Descriptors Estimation Using Multi-task Learning. (arXiv:2207.10878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10878">
<div class="article-summary-box-inner">
<span><p>This paper illustrates our submission method to the fourth Affective Behavior
Analysis in-the-Wild (ABAW) Competition. The method is used for the Multi-Task
Learning Challenge. Instead of using only face information, we employ full
information from a provided dataset containing face and the context around the
face. We utilized the InceptionNet V3 model to extract deep features then we
applied the attention mechanism to refine the features. After that, we put
those features into the transformer block and multi-layer perceptron networks
to get the final multiple kinds of emotion. Our model predicts arousal and
valence, classifies the emotional expression and estimates the action units
simultaneously. The proposed system achieves the performance of 0.917 on the
MTL Challenge validation dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">My View is the Best View: Procedure Learning from Egocentric Videos. (arXiv:2207.10883v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10883">
<div class="article-summary-box-inner">
<span><p>Procedure learning involves identifying the key-steps and determining their
logical order to perform a task. Existing approaches commonly use third-person
videos for learning the procedure, making the manipulated object small in
appearance and often occluded by the actor, leading to significant errors. In
contrast, we observe that videos obtained from first-person (egocentric)
wearable cameras provide an unobstructed and clear view of the action. However,
procedure learning from egocentric videos is challenging because (a) the camera
view undergoes extreme changes due to the wearer's head motion, and (b) the
presence of unrelated frames due to the unconstrained nature of the videos. Due
to this, current state-of-the-art methods' assumptions that the actions occur
at approximately the same time and are of the same duration, do not hold.
Instead, we propose to use the signal provided by the temporal correspondences
between key-steps across videos. To this end, we present a novel
self-supervised Correspond and Cut (CnC) framework for procedure learning. CnC
identifies and utilizes the temporal correspondences between the key-steps
across multiple videos to learn the procedure. Our experiments show that CnC
outperforms the state-of-the-art on the benchmark ProceL and CrossTask datasets
by 5.2% and 6.3%, respectively. Furthermore, for procedure learning using
egocentric videos, we propose the EgoProceL dataset consisting of 62 hours of
videos captured by 130 subjects performing 16 tasks. The source code and the
dataset are available on the project page https://sid2697.github.io/egoprocel/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XAI based Performance Preserving Adaptive Image Compression for Efficient Satellite Communication. (arXiv:2207.10885v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10885">
<div class="article-summary-box-inner">
<span><p>In the era of multinational cooperation, gathering and analyzing the
satellite images are getting easier and more important. Typical procedure of
the satellite image analysis include transmission of the bulky image data from
satellite to the ground producing significant overhead. To reduce the amount of
the transmission overhead while making no harm to the analysis result, we
propose a novel image compression scheme RDIC in this paper. RDIC is a
reasoning based image compression scheme that compresses an image according to
the pixel importance score acquired from the analysis model itself. From the
experimental results we showed that our RDIC scheme successfully captures the
important regions in an image showing high compression rate and low accuracy
loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FairGRAPE: Fairness-aware GRAdient Pruning mEthod for Face Attribute Classification. (arXiv:2207.10888v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10888">
<div class="article-summary-box-inner">
<span><p>Existing pruning techniques preserve deep neural networks' overall ability to
make correct predictions but may also amplify hidden biases during the
compression process. We propose a novel pruning method, Fairness-aware GRAdient
Pruning mEthod (FairGRAPE), that minimizes the disproportionate impacts of
pruning on different sub-groups. Our method calculates the per-group importance
of each model weight and selects a subset of weights that maintain the relative
between-group total importance in pruning. The proposed method then prunes
network edges with small importance values and repeats the procedure by
updating importance values. We demonstrate the effectiveness of our method on
four different datasets, FairFace, UTKFace, CelebA, and ImageNet, for the tasks
of face attribute classification where our method reduces the disparity in
performance degradation by up to 90% compared to the state-of-the-art pruning
algorithms. Our method is substantially more effective in a setting with a high
pruning rate (99%). The code and dataset used in the experiments are available
at https://github.com/Bernardo1998/FairGRAPE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-directional Contrastive Learning for Domain Adaptive Semantic Segmentation. (arXiv:2207.10892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10892">
<div class="article-summary-box-inner">
<span><p>We present a novel unsupervised domain adaptation method for semantic
segmentation that generalizes a model trained with source images and
corresponding ground-truth labels to a target domain. A key to domain adaptive
semantic segmentation is to learn domain-invariant and discriminative features
without target ground-truth labels. To this end, we propose a bi-directional
pixel-prototype contrastive learning framework that minimizes intra-class
variations of features for the same object class, while maximizing inter-class
variations for different ones, regardless of domains. Specifically, our
framework aligns pixel-level features and a prototype of the same object class
in target and source images (i.e., positive pairs), respectively, sets them
apart for different classes (i.e., negative pairs), and performs the alignment
and separation processes toward the other direction with pixel-level features
in the source image and a prototype in the target image. The cross-domain
matching encourages domain-invariant feature representations, while the
bidirectional pixel-prototype correspondences aggregate features for the same
object class, providing discriminative features. To establish training pairs
for contrastive learning, we propose to generate dynamic pseudo labels of
target images using a non-parametric label transfer, that is, pixel-prototype
correspondences across different domains. We also present a calibration method
compensating class-wise domain biases of prototypes gradually during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization. (arXiv:2207.10895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10895">
<div class="article-summary-box-inner">
<span><p>Although deep-learning based methods for monocular pedestrian detection have
made great progress, they are still vulnerable to heavy occlusions. Using
multi-view information fusion is a potential solution but has limited
applications, due to the lack of annotated training samples in existing
multi-view datasets, which increases the risk of overfitting. To address this
problem, a data augmentation method is proposed to randomly generate 3D
cylinder occlusions, on the ground plane, which are of the average size of
pedestrians and projected to multiple views, to relieve the impact of
overfitting in the training. Moreover, the feature map of each view is
projected to multiple parallel planes at different heights, by using
homographies, which allows the CNNs to fully utilize the features across the
height of each pedestrian to infer the locations of pedestrians on the ground
plane. The proposed 3DROM method has a greatly improved performance in
comparison with the state-of-the-art deep-learning based methods for multi-view
pedestrian detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Modeling of Future Context for Image Captioning. (arXiv:2207.10897v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10897">
<div class="article-summary-box-inner">
<span><p>Existing approaches to image captioning usually generate the sentence
word-by-word from left to right, with the constraint of conditioned on local
context including the given image and history generated words. There have been
many studies target to make use of global information during decoding, e.g.,
iterative refinement. However, it is still under-explored how to effectively
and efficiently incorporate the future context. To respond to this issue,
inspired by that Non-Autoregressive Image Captioning (NAIC) can leverage
two-side relation with modified mask operation, we aim to graft this advance to
the conventional Autoregressive Image Captioning (AIC) model while maintaining
the inference efficiency without extra time cost. Specifically, AIC and NAIC
models are first trained combined with shared visual encoders, forcing the
visual encoder to contain sufficient and valid future context; then the AIC
model is encouraged to capture the causal dynamics of cross-layer interchanging
from NAIC model on its unconfident words, which follows a teacher-student
paradigm and optimized with the distribution calibration training objective.
Empirical evidences demonstrate that our proposed approach clearly surpass the
state-of-the-art baselines in both automatic metrics and human evaluations on
the MS COCO benchmark. The source code is available at:
https://github.com/feizc/Future-Caption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness. (arXiv:2207.10899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10899">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) for robust representation learning and
self-supervised learning (SSL) for unsupervised representation learning are two
active research fields. Integrating AT into SSL, multiple prior works have
accomplished a highly significant yet challenging task: learning robust
representation without labels. A widely used framework is adversarial
contrastive learning which couples AT and SSL, and thus constitute a very
complex optimization problem. Inspired by the divide-and-conquer philosophy, we
conjecture that it might be simplified as well as improved by solving two
sub-problems: non-robust SSL and pseudo-supervised AT. This motivation shifts
the focus of the task from seeking an optimal integrating strategy for a
coupled problem to finding sub-solutions for sub-problems. With this said, this
work discards prior practices of directly introducing AT to SSL frameworks and
proposed a two-stage framework termed Decoupled Adversarial Contrastive
Learning (DeACL). Extensive experimental results demonstrate that our DeACL
achieves SOTA self-supervised adversarial robustness while significantly
reducing the training time, which validates its effectiveness and efficiency.
Moreover, our DeACL constitutes a more explainable solution, and its success
also bridges the gap with semi-supervised AT for exploiting unlabeled samples
for robust representation learning. The code is publicly accessible at
https://github.com/pantheon5100/DeACL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DBQ-SSD: Dynamic Ball Query for Efficient 3D Object Detection. (arXiv:2207.10909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10909">
<div class="article-summary-box-inner">
<span><p>Many point-based 3D detectors adopt point-feature sampling strategies to drop
some points for efficient inference. These strategies are typically based on
fixed and handcrafted rules, making difficult to handle complicated scenes.
Different from them, we propose a Dynamic Ball Query (DBQ) network to
adaptively select a subset of input points according to the input features, and
assign the feature transform with suitable receptive field for each selected
point. It can be embedded into some state-of-the-art 3D detectors and trained
in an end-to-end manner, which significantly reduces the computational cost.
Extensive experiments demonstrate that our method can reduce latency by 30%-60%
on KITTI and Waymo datasets. Specifically, the inference speed of our detector
can reach 162 FPS and 30 FPS with negligible performance degradation on KITTI
and Waymo datasets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimization of Forcemyography Sensor Placement for Arm Movement Recognition. (arXiv:2207.10915v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10915">
<div class="article-summary-box-inner">
<span><p>How to design an optimal wearable device for human movement recognition is
vital to reliable and accurate human-machine collaboration. Previous works
mainly fabricate wearable devices heuristically. Instead, this paper raises an
academic question: can we design an optimization algorithm to optimize the
fabrication of wearable devices such as figuring out the best sensor
arrangement automatically? Specifically, this work focuses on optimizing the
placement of Forcemyography (FMG) sensors for FMG armbands in the application
of arm movement recognition. Firstly, based on graph theory, the armband is
modeled considering sensors' signals and connectivity. Then, a Graph-based
Armband Modeling Network (GAM-Net) is introduced for arm movement recognition.
Afterward, the sensor placement optimization for FMG armbands is formulated and
an optimization algorithm with greedy local search is proposed. To study the
effectiveness of our optimization algorithm, a dataset for mechanical
maintenance tasks using FMG armbands with 16 sensors is collected. Our
experiments show that using only 4 sensors optimized with our algorithm can
help maintain a comparable recognition accuracy to using all sensors. Finally,
the optimized sensor placement result is verified from a physiological view.
This work would like to shed light on the automatic fabrication of wearable
devices considering downstream tasks, such as human biological signal
collection and movement recognition. Our code and dataset are available at
https://github.com/JerryX1110/IROS22-FMG-Sensor-Optimization
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLD-SLAM: A Real-Time Visual SLAM Using Points and Line Segments in Dynamic Scenes. (arXiv:2207.10916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10916">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problems in the practical application of
visual simultaneous localization and mapping (SLAM). With the popularization
and application of the technology in wide scope, the practicability of SLAM
system has become a new hot topic after the accuracy and robustness, e.g., how
to keep the stability of the system and achieve accurate pose estimation in the
low-texture and dynamic environment, and how to improve the universality and
real-time performance of the system in the real scenes, etc. This paper
proposes a real-time stereo indirect visual SLAM system, PLD-SLAM, which
combines point and line features, and avoid the impact of dynamic objects in
highly dynamic environments. We also present a novel global gray similarity
(GGS) algorithm to achieve reasonable keyframe selection and efficient loop
closure detection (LCD). Benefiting from the GGS, PLD-SLAM can realize
real-time accurate pose estimation in most real scenes without pre-training and
loading a huge feature dictionary model. To verify the performance of the
proposed system, we compare it with existing state-of-the-art (SOTA) methods on
the public datasets KITTI, EuRoC MAV, and the indoor stereo datasets provided
by us, etc. The experiments show that the PLD-SLAM has better real-time
performance while ensuring stability and accuracy in most scenarios. In
addition, through the analysis of the experimental results of the GGS, we can
find it has excellent performance in the keyframe selection and LCD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-tailed Instance Segmentation using Gumbel Optimized Loss. (arXiv:2207.10936v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10936">
<div class="article-summary-box-inner">
<span><p>Major advancements have been made in the field of object detection and
segmentation recently. However, when it comes to rare categories, the
state-of-the-art methods fail to detect them, resulting in a significant
performance gap between rare and frequent categories. In this paper, we
identify that Sigmoid or Softmax functions used in deep detectors are a major
reason for low performance and are sub-optimal for long-tailed detection and
segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for
long-tailed detection and segmentation. It aligns with the Gumbel distribution
of rare classes in imbalanced datasets, considering the fact that most classes
in long-tailed detection have low expected probability. The proposed GOL
significantly outperforms the best state-of-the-art method by 1.1% on AP , and
boosts the overall segmentation by 9.0% and detection by 8.0%, particularly
improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS
dataset. Code available at: https://github.com/kostas1515/GOL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense RGB-D-Inertial SLAM with Map Deformations. (arXiv:2207.10940v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10940">
<div class="article-summary-box-inner">
<span><p>While dense visual SLAM methods are capable of estimating dense
reconstructions of the environment, they suffer from a lack of robustness in
their tracking step, especially when the optimisation is poorly initialised.
Sparse visual SLAM systems have attained high levels of accuracy and robustness
through the inclusion of inertial measurements in a tightly-coupled fusion.
Inspired by this performance, we propose the first tightly-coupled dense
RGB-D-inertial SLAM system.
</p>
<p>Our system has real-time capability while running on a GPU. It jointly
optimises for the camera pose, velocity, IMU biases and gravity direction while
building up a globally consistent, fully dense surfel-based 3D reconstruction
of the environment. Through a series of experiments on both synthetic and real
world datasets, we show that our dense visual-inertial SLAM system is more
robust to fast motions and periods of low texture and low geometric variation
than a related RGB-D-only SLAM system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Local Aggregation Network with Adaptive Clusterer for Anomaly Detection. (arXiv:2207.10948v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10948">
<div class="article-summary-box-inner">
<span><p>Existing methods for anomaly detection based on memory-augmented autoencoder
(AE) have the following drawbacks: (1) Establishing a memory bank requires
additional memory space. (2) The fixed number of prototypes from subjective
assumptions ignores the data feature differences and diversity. To overcome
these drawbacks, we introduce DLAN-AC, a Dynamic Local Aggregation Network with
Adaptive Clusterer, for anomaly detection. First, The proposed DLAN can
automatically learn and aggregate high-level features from the AE to obtain
more representative prototypes, while freeing up extra memory space. Second,
The proposed AC can adaptively cluster video data to derive initial prototypes
with prior information. In addition, we also propose a dynamic redundant
clustering strategy (DRCS) to enable DLAN for automatically eliminating feature
clusters that do not contribute to the construction of prototypes. Extensive
experiments on benchmarks demonstrate that DLAN-AC outperforms most existing
methods, validating the effectiveness of our method. Our code is publicly
available at https://github.com/Beyond-Zw/DLAN-AC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale dependant layer for self-supervised nuclei encoding. (arXiv:2207.10950v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10950">
<div class="article-summary-box-inner">
<span><p>Recent developments in self-supervised learning give us the possibility to
further reduce human intervention in multi-step pipelines where the focus
evolves around particular objects of interest. In the present paper, the focus
lays in the nuclei in histopathology images. In particular we aim at extracting
cellular information in an unsupervised manner for a downstream task. As nuclei
present themselves in a variety of sizes, we propose a new Scale-dependant
convolutional layer to bypass scaling issues when resizing nuclei. On three
nuclei datasets, we benchmark the following methods: handcrafted, pre-trained
ResNet, supervised ResNet and self-supervised features. We show that the
proposed convolution layer boosts performance and that this layer combined with
Barlows-Twins allows for better nuclei encoding compared to the supervised
paradigm in the low sample setting and outperforms all other proposed
unsupervised methods. In addition, we extend the existing TNBC dataset to
incorporate nuclei class annotation in order to enrich and publicly release a
small sample setting dataset for nuclei segmentation and classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-based Human Fall Detection Systems using Deep Learning: A Review. (arXiv:2207.10952v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10952">
<div class="article-summary-box-inner">
<span><p>Human fall is one of the very critical health issues, especially for elders
and disabled people living alone. The number of elder populations is increasing
steadily worldwide. Therefore, human fall detection is becoming an effective
technique for assistive living for those people. For assistive living, deep
learning and computer vision have been used largely. In this review article, we
discuss deep learning (DL)-based state-of-the-art non-intrusive (vision-based)
fall detection techniques. We also present a survey on fall detection benchmark
datasets. For a clear understanding, we briefly discuss different metrics which
are used to evaluate the performance of the fall detection systems. This
article also gives a future direction on vision-based human fall detection
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visible and Near Infrared Image Fusion Based on Texture Information. (arXiv:2207.10953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10953">
<div class="article-summary-box-inner">
<span><p>Multi-sensor fusion is widely used in the environment perception system of
the autonomous vehicle. It solves the interference caused by environmental
changes and makes the whole driving system safer and more reliable. In this
paper, a novel visible and near-infrared fusion method based on texture
information is proposed to enhance unstructured environmental images. It aims
at the problems of artifact, information loss and noise in traditional visible
and near infrared image fusion methods. Firstly, the structure information of
the visible image (RGB) and the near infrared image (NIR) after texture removal
is obtained by relative total variation (RTV) calculation as the base layer of
the fused image; secondly, a Bayesian classification model is established to
calculate the noise weight and the noise information and the noise information
in the visible image is adaptively filtered by joint bilateral filter; finally,
the fused image is acquired by color space conversion. The experimental results
demonstrate that the proposed algorithm can preserve the spectral
characteristics and the unique information of visible and near-infrared images
without artifacts and color distortion, and has good robustness as well as
preserving the unique texture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faster VoxelPose: Real-time 3D Human Pose Estimation by Orthographic Projection. (arXiv:2207.10955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10955">
<div class="article-summary-box-inner">
<span><p>While the voxel-based methods have achieved promising results for
multi-person 3D pose estimation from multi-cameras, they suffer from heavy
computation burdens, especially for large scenes. We present Faster VoxelPose
to address the challenge by re-projecting the feature volume to the three
two-dimensional coordinate planes and estimating X, Y, Z coordinates from them
separately. To that end, we first localize each person by a 3D bounding box by
estimating a 2D box and its height based on the volume features projected to
the xy-plane and z-axis, respectively. Then for each person, we estimate
partial joint coordinates from the three coordinate planes separately which are
then fused to obtain the final 3D pose. The method is free from costly 3D-CNNs
and improves the speed of VoxelPose by ten times and meanwhile achieves
competitive accuracy as the state-of-the-art methods, proving its potential in
real-time applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QueryProp: Object Query Propagation for High-Performance Video Object Detection. (arXiv:2207.10959v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10959">
<div class="article-summary-box-inner">
<span><p>Video object detection has been an important yet challenging topic in
computer vision. Traditional methods mainly focus on designing the image-level
or box-level feature propagation strategies to exploit temporal information.
This paper argues that with a more effective and efficient feature propagation
framework, video object detectors can gain improvement in terms of both
accuracy and speed. For this purpose, this paper studies object-level feature
propagation, and proposes an object query propagation (QueryProp) framework for
high-performance video object detection. The proposed QueryProp contains two
propagation strategies: 1) query propagation is performed from sparse key
frames to dense non-key frames to reduce the redundant computation on non-key
frames; 2) query propagation is performed from previous key frames to the
current key frame to improve feature representation by temporal context
modeling. To further facilitate query propagation, an adaptive propagation gate
is designed to achieve flexible key frame selection. We conduct extensive
experiments on the ImageNet VID dataset. QueryProp achieves comparable accuracy
with state-of-the-art methods and strikes a decent accuracy/speed trade-off.
Code is available at https://github.com/hf1995/QueryProp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Principal Geodesic Analysis of Merge Trees (and Persistence Diagrams). (arXiv:2207.10960v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10960">
<div class="article-summary-box-inner">
<span><p>This paper presents a computational framework for the Principal Geodesic
Analysis of merge trees (MT-PGA), a novel adaptation of the celebrated
Principal Component Analysis (PCA) framework [87] to the Wasserstein metric
space of merge trees [92]. We formulate MT-PGA computation as a constrained
optimization problem, aiming at adjusting a basis of orthogonal geodesic axes,
while minimizing a fitting energy. We introduce an efficient, iterative
algorithm which exploits shared-memory parallelism, as well as an analytic
expression of the fitting energy gradient, to ensure fast iterations. Our
approach also trivially extends to extremum persistence diagrams. Extensive
experiments on public ensembles demonstrate the efficiency of our approach -
with MT-PGA computations in the orders of minutes for the largest examples. We
show the utility of our contributions by extending to merge trees two typical
PCA applications. First, we apply MT-PGA to data reduction and reliably
compress merge trees by concisely representing them by their first coordinates
in the MT-PGA basis. Second, we present a dimensionality reduction framework
exploiting the first two directions of the MT-PGA basis to generate
two-dimensional layouts of the ensemble. We augment these layouts with
persistence correlation views, enabling global and local visual inspections of
the feature variability in the ensemble. In both applications, quantitative
experiments assess the relevance of our framework. Finally, we provide a
lightweight C++ implementation that can be used to reproduce our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opportunistic hip fracture risk prediction in Men from X-ray: Findings from the Osteoporosis in Men (MrOS) Study. (arXiv:2207.10970v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10970">
<div class="article-summary-box-inner">
<span><p>Osteoporosis is a common disease that increases fracture risk. Hip fractures,
especially in elderly people, lead to increased morbidity, decreased quality of
life and increased mortality. Being a silent disease before fracture,
osteoporosis often remains undiagnosed and untreated. Areal bone mineral
density (aBMD) assessed by dual-energy X-ray absorptiometry (DXA) is the
gold-standard method for osteoporosis diagnosis and hence also for future
fracture prediction (prognostic). However, the required special equipment is
not broadly available everywhere, in particular not to patients in developing
countries. We propose a deep learning classification model (FORM) that can
directly predict hip fracture risk from either plain radiographs (X-ray) or 2D
projection images of computed tomography (CT) data. Our method is fully
automated and therefore well suited for opportunistic screening settings,
identifying high risk patients in a broader population without additional
screening. FORM was trained and evaluated on X-rays and CT projections from the
Osteoporosis in Men (MrOS) study. 3108 X-rays (89 incident hip fractures) or
2150 CTs (80 incident hip fractures) with a 80/20 split were used. We show that
FORM can correctly predict the 10-year hip fracture risk with a validation AUC
of 81.44 +- 3.11% / 81.04 +- 5.54% (mean +- STD) including additional
information like age, BMI, fall history and health background across a 5-fold
cross validation on the X-ray and CT cohort, respectively. Our approach
significantly (p &lt; 0.01) outperforms previous methods like Cox
Proportional-Hazards Model and \frax with 70.19 +- 6.58 and 74.72 +- 7.21
respectively on the X-ray cohort. Our model outperform on both cohorts hip aBMD
based predictions. We are confident that FORM can contribute on improving
osteoporosis diagnosis at an early stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Human Kinematics by Modeling Temporal Correlations between Joints for Video-based Human Pose Estimation. (arXiv:2207.10971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10971">
<div class="article-summary-box-inner">
<span><p>Estimating human poses from videos is critical in human-computer interaction.
By precisely estimating human poses, the robot can provide an appropriate
response to the human. Most existing approaches use the optical flow, RNNs, or
CNNs to extract temporal features from videos. Despite the positive results of
these attempts, most of them only straightforwardly integrate features along
the temporal dimension, ignoring temporal correlations between joints. In
contrast to previous methods, we propose a plug-and-play kinematics modeling
module (KMM) based on the domain-cross attention mechanism to model the
temporal correlation between joints across different frames explicitly.
Specifically, the proposed KMM models the temporal correlation between any two
joints by calculating their temporal similarity. In this way, KMM can learn the
motion cues of each joint. Using the motion cues (temporal domain) and
historical positions of joints (spatial domain), KMM can infer the initial
positions of joints in the current frame in advance. In addition, we present a
kinematics modeling network (KIMNet) based on the KMM for obtaining the final
positions of joints by combining pose features and initial positions of joints.
By explicitly modeling temporal correlations between joints, KIMNet can infer
the occluded joints at present according to all joints at the previous moment.
Furthermore, the KMM is achieved through an attention mechanism, which allows
it to maintain the high resolution of features. Therefore, it can transfer rich
historical pose information to the current frame, which provides effective pose
information for locating occluded joints. Our approach achieves
state-of-the-art results on two standard video-based pose estimation
benchmarks. Moreover, the proposed KIMNet shows some robustness to the
occlusion, demonstrating the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeurAR: Neural Uncertainty for Autonomous 3D Reconstruction. (arXiv:2207.10985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10985">
<div class="article-summary-box-inner">
<span><p>Implicit neural representations have shown compelling results in offline 3D
reconstruction and also recently demonstrated the potential for online SLAM
systems. However, applying them to autonomous 3D reconstruction, where robots
are required to explore a scene and plan a view path for the reconstruction,
has not been studied. In this paper, we explore for the first time the
possibility of using implicit neural representations for autonomous 3D scene
reconstruction by addressing two key challenges: 1) seeking a criterion to
measure the quality of the candidate viewpoints for the view planning based on
the new representations, and 2) learning the criterion from data that can
generalize to different scenes instead of hand-crafting one. For the first
challenge, a proxy of Peak Signal-to-Noise Ratio (PSNR) is proposed to quantify
a viewpoint quality. The proxy is acquired by treating the color of a spatial
point in a scene as a random variable under a Gaussian distribution rather than
a deterministic one; the variance of the distribution quantifies the
uncertainty of the reconstruction and composes the proxy. For the second
challenge, the proxy is optimized jointly with the parameters of an implicit
neural network for the scene. With the proposed view quality criterion, we can
then apply the new representations to autonomous 3D reconstruction. Our method
demonstrates significant improvements on various metrics for the rendered image
quality and the geometry quality of the reconstructed 3D models when compared
with variants using TSDF or reconstruction without view planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Object Counting and Detection. (arXiv:2207.10988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10988">
<div class="article-summary-box-inner">
<span><p>We tackle a new task of few-shot object counting and detection. Given a few
exemplar bounding boxes of a target object class, we seek to count and detect
all objects of the target class. This task shares the same supervision as the
few-shot object counting but additionally outputs the object bounding boxes
along with the total object count. To address this challenging problem, we
introduce a novel two-stage training strategy and a novel uncertainty-aware
few-shot object detector: Counting-DETR. The former is aimed at generating
pseudo ground-truth bounding boxes to train the latter. The latter leverages
the pseudo ground-truth provided by the former but takes the necessary steps to
account for the imperfection of pseudo ground-truth. To validate the
performance of our method on the new task, we introduce two new datasets named
FSCD-147 and FSCD-LVIS. Both datasets contain images with complex scenes,
multiple object classes per image, and a huge variation in object shapes,
sizes, and appearance. Our proposed approach outperforms very strong baselines
adapted from few-shot object counting and few-shot object detection with a
large margin in both counting and detection metrics. The code and models are
available at \url{https://github.com/VinAIResearch/Counting-DETR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taguchi based Design of Sequential Convolution Neural Network for Classification of Defective Fasteners. (arXiv:2207.10992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10992">
<div class="article-summary-box-inner">
<span><p>Fasteners play a critical role in securing various parts of machinery.
Deformations such as dents, cracks, and scratches on the surface of fasteners
are caused by material properties and incorrect handling of equipment during
production processes. As a result, quality control is required to ensure safe
and reliable operations. The existing defect inspection method relies on manual
examination, which consumes a significant amount of time, money, and other
resources; also, accuracy cannot be guaranteed due to human error. Automatic
defect detection systems have proven impactful over the manual inspection
technique for defect analysis. However, computational techniques such as
convolutional neural networks (CNN) and deep learning-based approaches are
evolutionary methods. By carefully selecting the design parameter values, the
full potential of CNN can be realised. Using Taguchi-based design of
experiments and analysis, an attempt has been made to develop a robust
automatic system in this study. The dataset used to train the system has been
created manually for M14 size nuts having two labeled classes: Defective and
Non-defective. There are a total of 264 images in the dataset. The proposed
sequential CNN comes up with a 96.3% validation accuracy, 0.277 validation loss
at 0.001 learning rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Generalized Non-Rigid Multimodal Biomedical Image Registration from Generic Point Set Data. (arXiv:2207.10994v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10994">
<div class="article-summary-box-inner">
<span><p>Free Point Transformer (FPT) has been proposed as a data-driven, non-rigid
point set registration approach using deep neural networks. As FPT does not
assume constraints based on point vicinity or correspondence, it may be trained
simply and in a flexible manner by minimizing an unsupervised loss based on the
Chamfer Distance. This makes FPT amenable to real-world medical imaging
applications where ground-truth deformations may be infeasible to obtain, or in
scenarios where only a varying degree of completeness in the point sets to be
aligned is available. To test the limit of the correspondence finding ability
of FPT and its dependency on training data sets, this work explores the
generalizability of the FPT from well-curated non-medical data sets to medical
imaging data sets. First, we train FPT on the ModelNet40 dataset to demonstrate
its effectiveness and the superior registration performance of FPT over
iterative and learning-based point set registration methods. Second, we
demonstrate superior performance in rigid and non-rigid registration and
robustness to missing data. Last, we highlight the interesting generalizability
of the ModelNet-trained FPT by registering reconstructed freehand ultrasound
scans of the spine and generic spine models without additional training,
whereby the average difference to the ground truth curvatures is 1.3 degrees,
across 13 patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Registration: Learning Test-Time Optimization for Single-Pair Image Registration. (arXiv:2207.10996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10996">
<div class="article-summary-box-inner">
<span><p>Neural networks have been proposed for medical image registration by
learning, with a substantial amount of training data, the optimal
transformations between image pairs. These trained networks can further be
optimized on a single pair of test images - known as test-time optimization.
This work formulates image registration as a meta-learning algorithm. Such
networks can be trained by aligning the training image pairs while
simultaneously improving test-time optimization efficacy; tasks which were
previously considered two independent training and optimization processes. The
proposed meta-registration is hypothesized to maximize the efficiency and
effectiveness of the test-time optimization in the "outer" meta-optimization of
the networks. For image guidance applications that often are time-critical yet
limited in training data, the potentially gained speed and accuracy are
compared with classical registration algorithms, registration networks without
meta-learning, and single-pair optimization without test-time optimization
data. Experiments are presented in this paper using clinical transrectal
ultrasound image data from 108 prostate cancer patients. These experiments
demonstrate the effectiveness of a meta-registration protocol, which yields
significantly improved performance relative to existing learning-based methods.
Furthermore, the meta-registration achieves comparable results to classical
iterative methods in a fraction of the time, owing to its rapid test-time
optimization process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rapid Lung Ultrasound COVID-19 Severity Scoring with Resource-Efficient Deep Feature Extraction. (arXiv:2207.10998v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10998">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence-based analysis of lung ultrasound imaging has been
demonstrated as an effective technique for rapid diagnostic decision support
throughout the COVID-19 pandemic. However, such techniques can require days- or
weeks-long training processes and hyper-parameter tuning to develop intelligent
deep learning image analysis models. This work focuses on leveraging
'off-the-shelf' pre-trained models as deep feature extractors for scoring
disease severity with minimal training time. We propose using pre-trained
initializations of existing methods ahead of simple and compact neural networks
to reduce reliance on computational capacity. This reduction of computational
capacity is of critical importance in time-limited or resource-constrained
circumstances, such as the early stages of a pandemic. On a dataset of 49
patients, comprising over 20,000 images, we demonstrate that the use of
existing methods as feature extractors results in the effective classification
of COVID-19-related pneumonia severity while requiring only minutes of training
time. Our methods can achieve an accuracy of over 0.93 on a 4-level severity
score scale and provides comparable per-patient region and global scores
compared to expert annotated ground truths. These results demonstrate the
capability for rapid deployment and use of such minimally-adapted methods for
progress monitoring, patient stratification and management in clinical practice
for COVID-19 patients, and potentially in other respiratory diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POP: Mining POtential Performance of new fashion products via webly cross-modal query expansion. (arXiv:2207.11001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11001">
<div class="article-summary-box-inner">
<span><p>We propose a data-centric pipeline able to generate exogenous observation
data for the New Fashion Product Performance Forecasting (NFPPF) problem, i.e.,
predicting the performance of a brand-new clothing probe with no available past
observations. Our pipeline manufactures the missing past starting from a
single, available image of the clothing probe. It starts by expanding textual
tags associated with the image, querying related fashionable or unfashionable
images uploaded on the web at a specific time in the past. A binary classifier
is robustly trained on these web images by confident learning, to learn what
was fashionable in the past and how much the probe image conforms to this
notion of fashionability. This compliance produces the POtential Performance
(POP) time series, indicating how performing the probe could have been if it
were available earlier. POP proves to be highly predictive for the probe's
future performance, ameliorating the sales forecasts of all state-of-the-art
models on the recent VISUELLE fast-fashion dataset. We also show that POP
reflects the ground-truth popularity of new styles (ensembles of clothing
items) on the Fashion Forward benchmark, demonstrating that our webly-learned
signal is a truthful expression of popularity, accessible by everyone and
generalizable to any time of analysis. Forecasting code, data and the POP time
series are available at:
https://github.com/HumaticsLAB/POP-Mining-POtential-Performance
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fact sheet: Automatic Self-Reported Personality Recognition Track. (arXiv:2207.11012v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11012">
<div class="article-summary-box-inner">
<span><p>We propose an informed baseline to help disentangle the various contextual
factors of influence in this type of case studies. For this purpose, we
analysed the correlation between the given metadata and the self-assigned
personality trait scores and developed a model based solely on this
information. Further, we compared the performance of this informed baseline
with models based on state-of-the-art visual, linguistic and audio features.
For the present dataset, a model trained solely on simple metadata features
(age, gender and number of sessions) proved to have superior or similar
performance when compared with simple audio, linguistic or visual
features-based systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open video data sharing in developmental and behavioural science. (arXiv:2207.11020v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11020">
<div class="article-summary-box-inner">
<span><p>Video recording is a widely used method for documenting infant and child
behaviours in research and clinical practice. Video data has rarely been shared
due to ethical concerns of confidentiality, although the need of shared
large-scaled datasets remains increasing. This demand is even more imperative
when data-driven computer-based approaches are involved, such as screening
tools to complement clinical assessments. To share data while abiding by
privacy protection rules, a critical question arises whether efforts at data
de-identification reduce data utility? We addressed this question by showcasing
the Prechtl's general movements assessment (GMA), an established and globally
practised video-based diagnostic tool in early infancy for detecting
neurological deficits, such as cerebral palsy. To date, no shared
expert-annotated large data repositories for infant movement analyses exist.
Such datasets would massively benefit training and recalibration of human
assessors and the development of computer-based approaches. In the current
study, sequences from a prospective longitudinal infant cohort with a total of
19451 available general movements video snippets were randomly selected for
human clinical reasoning and computer-based analysis. We demonstrated for the
first time that pseudonymisation by face-blurring video recordings is a viable
approach. The video redaction did not affect classification accuracy for either
human assessors or computer vision methods, suggesting an adequate and
easy-to-apply solution for sharing movement video data. We call for further
explorations into efficient and privacy rule-conforming approaches for
deidentifying video data in scientific and clinical fields beyond movement
assessments. These approaches shall enable sharing and merging stand-alone
video datasets into large data pools to advance science and public health.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Custom Structure Preservation in Face Aging. (arXiv:2207.11025v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11025">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a novel architecture for face age editing that can
produce structural modifications while maintaining relevant details present in
the original image. We disentangle the style and content of the input image and
propose a new decoder network that adopts a style-based strategy to combine the
style and content representations of the input image while conditioning the
output on the target age. We go beyond existing aging methods allowing users to
adjust the degree of structure preservation in the input image during
inference. To this purpose, we introduce a masking mechanism, the CUstom
Structure Preservation module, that distinguishes relevant regions in the input
image from those that should be discarded. CUSP requires no additional
supervision. Finally, our quantitative and qualitative analysis which include a
user study, show that our method outperforms prior art and demonstrates the
effectiveness of our strategy regarding image editing and adjustable structure
preservation. Code and pretrained models are available at
https://github.com/guillermogotre/CUSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobileDenseNet: A new approach to object detection on mobile devices. (arXiv:2207.11031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11031">
<div class="article-summary-box-inner">
<span><p>Object detection problem solving has developed greatly within the past few
years. There is a need for lighter models in instances where hardware
limitations exist, as well as a demand for models to be tailored to mobile
devices. In this article, we will assess the methods used when creating
algorithms that address these issues. The main goal of this article is to
increase accuracy in state-of-the-art algorithms while maintaining speed and
real-time efficiency. The most significant issues in one-stage object detection
pertains to small objects and inaccurate localization. As a solution, we
created a new network by the name of MobileDenseNet suitable for embedded
systems. We also developed a light neck FCPNLite for mobile devices that will
aid with the detection of small objects. Our research revealed that very few
papers cited necks in embedded systems. What differentiates our network from
others is our use of concatenation features. A small yet significant change to
the head of the network amplified accuracy without increasing speed or limiting
parameters. In short, our focus on the challenging CoCo and Pascal VOC datasets
were 24.8 and 76.8 in percentage terms respectively - a rate higher than that
recorded by other state-of-the-art systems thus far. Our network is able to
increase accuracy while maintaining real-time efficiency on mobile devices. We
calculated operational speed on Pixel 3 (Snapdragon 845) to 22.8 fps. The
source code of this research is available on
https://github.com/hajizadeh/MobileDenseNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GesSure -- A Robust Face-Authentication enabled Dynamic Gesture Recognition GUI Application. (arXiv:2207.11033v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11033">
<div class="article-summary-box-inner">
<span><p>Using physical interactive devices like mouse and keyboards hinders
naturalistic human-machine interaction and increases the probability of surface
contact during a pandemic. Existing gesture-recognition systems do not possess
user authentication, making them unreliable. Static gestures in current
gesture-recognition technology introduce long adaptation periods and reduce
user compatibility. Our technology places a strong emphasis on user recognition
and safety. We use meaningful and relevant gestures for task operation,
resulting in a better user experience. This paper aims to design a robust,
face-verification-enabled gesture recognition system that utilizes a graphical
user interface and primarily focuses on security through user recognition and
authorization. The face model uses MTCNN and FaceNet to verify the user, and
our LSTM-CNN architecture for gesture recognition, achieving an accuracy of 95%
with five classes of gestures. The prototype developed through our research has
successfully executed context-dependent tasks like save, print, control
video-player operations and exit, and context-free operating system tasks like
sleep, shut-down, and unlock intuitively. Our application and dataset are
available as open source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Spatio-Spectral Total Variation Model for Hyperspectral Image Denoising. (arXiv:2207.11050v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11050">
<div class="article-summary-box-inner">
<span><p>The spatio-spectral total variation (SSTV) model has been widely used as an
effective regularization of hyperspectral images (HSI) for various applications
such as mixed noise removal. However, since SSTV computes local spatial
differences uniformly, it is difficult to remove noise while preserving complex
spatial structures with fine edges and textures, especially in situations of
high noise intensity. To solve this problem, we propose a new TV-type
regularization called Graph-SSTV (GSSTV), which generates a graph explicitly
reflecting the spatial structure of the target HSI from noisy HSIs and
incorporates a weighted spatial difference operator designed based on this
graph. Furthermore, we formulate the mixed noise removal problem as a convex
optimization problem involving GSSTV and develop an efficient algorithm based
on the primal-dual splitting method to solve this problem. Finally, we
demonstrate the effectiveness of GSSTV compared with existing HSI
regularization models through experiments on mixed noise removal. The source
code will be available at https://www.mdi.c.titech.ac.jp/publications/gsstv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal. (arXiv:2207.11061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11061">
<div class="article-summary-box-inner">
<span><p>Estimating 3D interacting hand pose from a single RGB image is essential for
understanding human actions. Unlike most previous works that directly predict
the 3D poses of two interacting hands simultaneously, we propose to decompose
the challenging interacting hand pose estimation task and estimate the pose of
each hand separately. In this way, it is straightforward to take advantage of
the latest research progress on the single-hand pose estimation system.
However, hand pose estimation in interacting scenarios is very challenging, due
to (1) severe hand-hand occlusion and (2) ambiguity caused by the homogeneous
appearance of hands. To tackle these two challenges, we propose a novel Hand
De-occlusion and Removal (HDR) framework to perform hand de-occlusion and
distractor removal. We also propose the first large-scale synthetic amodal hand
dataset, termed Amodal InterHand Dataset (AIH), to facilitate model training
and promote the development of the related research. Experiments show that the
proposed method significantly outperforms previous state-of-the-art interacting
hand pose estimation approaches. Codes and data are available at
https://github.com/MengHao666/HDR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RealFlow: EM-based Realistic Optical Flow Dataset Generation from Videos. (arXiv:2207.11075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11075">
<div class="article-summary-box-inner">
<span><p>Obtaining the ground truth labels from a video is challenging since the
manual annotation of pixel-wise flow labels is prohibitively expensive and
laborious. Besides, existing approaches try to adapt the trained model on
synthetic datasets to authentic videos, which inevitably suffers from domain
discrepancy and hinders the performance for real-world applications. To solve
these problems, we propose RealFlow, an Expectation-Maximization based
framework that can create large-scale optical flow datasets directly from any
unlabeled realistic videos. Specifically, we first estimate optical flow
between a pair of video frames, and then synthesize a new image from this pair
based on the predicted flow. Thus the new image pairs and their corresponding
flows can be regarded as a new training set. Besides, we design a Realistic
Image Pair Rendering (RIPR) module that adopts softmax splatting and
bi-directional hole filling techniques to alleviate the artifacts of the image
synthesis. In the E-step, RIPR renders new images to create a large quantity of
training data. In the M-step, we utilize the generated training data to train
an optical flow network, which can be used to estimate optical flows in the
next E-step. During the iterative learning steps, the capability of the flow
network is gradually improved, so is the accuracy of the flow, as well as the
quality of the synthesized dataset. Experimental results show that RealFlow
outperforms previous dataset generation methods by a considerably large margin.
Moreover, based on the generated dataset, our approach achieves
state-of-the-art performance on two standard benchmarks compared with both
supervised and unsupervised optical flow methods. Our code and dataset are
available at https://github.com/megvii-research/RealFlow
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Expression Recognition using Vanilla ViT backbones with MAE Pretraining. (arXiv:2207.11081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11081">
<div class="article-summary-box-inner">
<span><p>Humans usually convey emotions voluntarily or involuntarily by facial
expressions. Automatically recognizing the basic expression (such as happiness,
sadness, and neutral) from a facial image, i.e., facial expression recognition
(FER), is extremely challenging and attracts much research interests. Large
scale datasets and powerful inference models have been proposed to address the
problem. Though considerable progress has been made, most of the state of the
arts employing convolutional neural networks (CNNs) or elaborately modified
Vision Transformers (ViTs) depend heavily on upstream supervised pretraining.
Transformers are taking place the domination of CNNs in more and more computer
vision tasks. But they usually need much more data to train, since they use
less inductive biases compared with CNNs. To explore whether a vanilla ViT
without extra training samples from upstream tasks is able to achieve
competitive accuracy, we use a plain ViT with MAE pretraining to perform the
FER task. Specifically, we first pretrain the original ViT as a Masked
Autoencoder (MAE) on a large facial expression dataset without expression
labels. Then, we fine-tune the ViT on popular facial expression datasets with
expression labels. The presented method is quite competitive with 90.22\% on
RAF-DB, 61.73\% on AfectNet and can serve as a simple yet strong ViT-based
baseline for FER studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Speech-Aware Perceptual 3D Facial Expression Reconstruction from Videos. (arXiv:2207.11094v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11094">
<div class="article-summary-box-inner">
<span><p>The recent state of the art on monocular 3D face reconstruction from image
data has made some impressive advancements, thanks to the advent of Deep
Learning. However, it has mostly focused on input coming from a single RGB
image, overlooking the following important factors: a) Nowadays, the vast
majority of facial image data of interest do not originate from single images
but rather from videos, which contain rich dynamic information. b) Furthermore,
these videos typically capture individuals in some form of verbal communication
(public talks, teleconferences, audiovisual human-computer interactions,
interviews, monologues/dialogues in movies, etc). When existing 3D face
reconstruction methods are applied in such videos, the artifacts in the
reconstruction of the shape and motion of the mouth area are often severe,
since they do not match well with the speech audio.
</p>
<p>To overcome the aforementioned limitations, we present the first method for
visual speech-aware perceptual reconstruction of 3D mouth expressions. We do
this by proposing a "lipread" loss, which guides the fitting process so that
the elicited perception from the 3D reconstructed talking head resembles that
of the original video footage. We demonstrate that, interestingly, the lipread
loss is better suited for 3D reconstruction of mouth movements compared to
traditional landmark losses, and even direct 3D supervision. Furthermore, the
devised method does not rely on any text transcriptions or corresponding audio,
rendering it ideal for training in unlabeled datasets. We verify the efficiency
of our method through exhaustive objective evaluations on three large-scale
datasets, as well as subjective evaluation with two web-based user studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11095">
<div class="article-summary-box-inner">
<span><p>Speckle filtering is generally a prerequisite to the analysis of synthetic
aperture radar (SAR) images. Tremendous progress has been achieved in the
domain of single-image despeckling. Latest techniques rely on deep neural
networks to restore the various structures and textures peculiar to SAR images.
The availability of time series of SAR images offers the possibility of
improving speckle filtering by combining different speckle realizations over
the same area. The supervised training of deep neural networks requires
ground-truth speckle-free images. Such images can only be obtained indirectly
through some form of averaging, by spatial or temporal integration, and are
imperfect. Given the potential of very high quality restoration reachable by
multi-temporal speckle filtering, the limitations of ground-truth images need
to be circumvented. We extend a recent self-supervised training strategy for
single-look complex SAR images, called MERLIN, to the case of multi-temporal
filtering. This requires modeling the sources of statistical dependencies in
the spatial and temporal dimensions as well as between the real and imaginary
components of the complex amplitudes. Quantitative analysis on datasets with
simulated speckle indicates a clear improvement of speckle reduction when
additional SAR images are included. Our method is then applied to stacks of
TerraSAR-X images and shown to outperform competing multi-temporal speckle
filtering approaches. The code of the trained models is made freely available
on the
$\href{https://gitlab.telecom-paris.fr/ring/multi-temporal-merlin/}{\text{GitLab}}$
of the IMAGES team of the LTCI Lab, T\'el\'ecom Paris Institut Polytechnique de
Paris.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Video Captioning with Evolving Pseudo-Tokens. (arXiv:2207.11100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11100">
<div class="article-summary-box-inner">
<span><p>We introduce a zero-shot video captioning method that employs two frozen
networks: the GPT-2 language model and the CLIP image-text matching model. The
matching score is used to steer the language model toward generating a sentence
that has a high average matching score to a subset of the video frames. Unlike
zero-shot image captioning methods, our work considers the entire sentence at
once. This is achieved by optimizing, during the generation process, part of
the prompt from scratch, by modifying the representation of all other tokens in
the prompt, and by repeating the process iteratively, gradually improving the
specificity and comprehensiveness of the generated sentence. Our experiments
show that the generated captions are coherent and display a broad range of
real-world knowledge. Our code is available at:
https://github.com/YoadTew/zero-shot-video-to-text
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs. (arXiv:2207.11102v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11102">
<div class="article-summary-box-inner">
<span><p>Optical coherence tomography angiography (OCTA) can non-invasively image the
eye's circulatory system. In order to reliably characterize the retinal
vasculature, there is a need to automatically extract quantitative metrics from
these images. The calculation of such biomarkers requires a precise semantic
segmentation of the blood vessels. However, deep-learning-based methods for
segmentation mostly rely on supervised training with voxel-level annotations,
which are costly to obtain. In this work, we present a pipeline to synthesize
large amounts of realistic OCTA images with intrinsically matching ground truth
labels; thereby obviating the need for manual annotation of training data. Our
proposed method is based on two novel components: 1) a physiology-based
simulation that models the various retinal vascular plexuses and 2) a suite of
physics-based image augmentations that emulate the OCTA image acquisition
process including typical artifacts. In extensive benchmarking experiments, we
demonstrate the utility of our synthetic data by successfully training retinal
vessel segmentation algorithms. Encouraged by our method's competitive
quantitative and superior qualitative performance, we believe that it
constitutes a versatile tool to advance the quantitative analysis of OCTA
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeVIS: Making Deformable Transformers Work for Video Instance Segmentation. (arXiv:2207.11103v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11103">
<div class="article-summary-box-inner">
<span><p>Video Instance Segmentation (VIS) jointly tackles multi-object detection,
tracking, and segmentation in video sequences. In the past, VIS methods
mirrored the fragmentation of these subtasks in their architectural design,
hence missing out on a joint solution. Transformers recently allowed to cast
the entire VIS task as a single set-prediction problem. Nevertheless, the
quadratic complexity of existing Transformer-based methods requires long
training times, high memory requirements, and processing of low-single-scale
feature maps. Deformable attention provides a more efficient alternative but
its application to the temporal domain or the segmentation task have not yet
been explored.
</p>
<p>In this work, we present Deformable VIS (DeVIS), a VIS method which
capitalizes on the efficiency and performance of deformable Transformers. To
reason about all VIS subtasks jointly over multiple frames, we present temporal
multi-scale deformable attention with instance-aware object queries. We further
introduce a new image and video instance mask head with multi-scale features,
and perform near-online video processing with multi-cue clip tracking. DeVIS
reduces memory as well as training time requirements, and achieves
state-of-the-art results on the YouTube-VIS 2021, as well as the challenging
OVIS dataset.
</p>
<p>Code is available at https://github.com/acaelles97/DeVIS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast strategies for multi-temporal speckle reduction of Sentinel-1 GRD images. (arXiv:2207.11111v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11111">
<div class="article-summary-box-inner">
<span><p>Reducing speckle and limiting the variations of the physical parameters in
Synthetic Aperture Radar (SAR) images is often a key-step to fully exploit the
potential of such data. Nowadays, deep learning approaches produce state of the
art results in single-image SAR restoration. Nevertheless, huge multi-temporal
stacks are now often available and could be efficiently exploited to further
improve image quality. This paper explores two fast strategies employing a
single-image despeckling algorithm, namely SAR2SAR, in a multi-temporal
framework. The first one is based on Quegan filter and replaces the local
reflectivity pre-estimation by SAR2SAR. The second one uses SAR2SAR to suppress
speckle from a ratio image encoding the multi-temporal information under the
form of a "super-image", i.e. the temporal arithmetic mean of a time series.
Experimental results on Sentinel-1 GRD data show that these two multi-temporal
strategies provide improved filtering results while adding a limited
computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Reference-based Distinctive Image Captioning. (arXiv:2207.11118v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11118">
<div class="article-summary-box-inner">
<span><p>Distinctive Image Captioning (DIC) -- generating distinctive captions that
describe the unique details of a target image -- has received considerable
attention over the last few years. A recent DIC work proposes to generate
distinctive captions by comparing the target image with a set of
semantic-similar reference images, i.e., reference-based DIC (Ref-DIC). It aims
to make the generated captions can tell apart the target and reference images.
Unfortunately, reference images used by existing Ref-DIC works are easy to
distinguish: these reference images only resemble the target image at
scene-level and have few common objects, such that a Ref-DIC model can
trivially generate distinctive captions even without considering the reference
images. To ensure Ref-DIC models really perceive the unique objects (or
attributes) in target images, we first propose two new Ref-DIC benchmarks.
Specifically, we design a two-stage matching mechanism, which strictly controls
the similarity between the target and reference images at object-/attribute-
level (vs. scene-level). Secondly, to generate distinctive captions, we develop
a strong Transformer-based Ref-DIC baseline, dubbed as TransDIC. It not only
extracts visual features from the target image, but also encodes the
differences between objects in the target and reference images. Finally, for
more trustworthy benchmarking, we propose a new evaluation metric named
DisCIDEr for Ref-DIC, which evaluates both the accuracy and distinctiveness of
the generated captions. Experimental results demonstrate that our TransDIC can
generate distinctive captions. Besides, it outperforms several state-of-the-art
models on the two new benchmarks over different metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Graph-Based Feature Normalization for Facial Expression Recognition. (arXiv:2207.11123v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11123">
<div class="article-summary-box-inner">
<span><p>Facial Expression Recognition (FER) suffers from data uncertainties caused by
ambiguous facial images and annotators' subjectiveness, resulting in excursive
semantic and feature covariate shifting problem. Existing works usually correct
mislabeled data by estimating noise distribution, or guide network training
with knowledge learned from clean data, neglecting the associative relations of
expressions. In this work, we propose an Adaptive Graph-based Feature
Normalization (AGFN) method to protect FER models from data uncertainties by
normalizing feature distributions with the association of expressions.
Specifically, we propose a Poisson graph generator to adaptively construct
topological graphs for samples in each mini-batches via a sampling process, and
correspondingly design a coordinate descent strategy to optimize proposed
network. Our method outperforms state-of-the-art works with accuracies of
91.84% and 91.11% on the benchmark datasets FERPlus and RAF-DB, respectively,
and when the percentage of mislabeled data increases (e.g., to 20%), our
network surpasses existing works significantly by 3.38% and 4.52%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VTrackIt: A Synthetic Self-Driving Dataset with Infrastructure and Pooled Vehicle Information. (arXiv:2207.11146v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11146">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence solutions for Autonomous Vehicles (AVs) have been
developed using publicly available datasets such as Argoverse, ApolloScape,
Level5, and NuScenes. One major limitation of these datasets is the absence of
infrastructure and/or pooled vehicle information like lane line type, vehicle
speed, traffic signs, and intersections. Such information is necessary and not
complementary to eliminating high-risk edge cases. The rapid advancements in
Vehicle-to-Infrastructure and Vehicle-to-Vehicle technologies show promise that
infrastructure and pooled vehicle information will soon be accessible in near
real-time. Taking a leap in the future, we introduce the first comprehensive
synthetic dataset with intelligent infrastructure and pooled vehicle
information for advancing the next generation of AVs, named VTrackIt. We also
introduce the first deep learning model (InfraGAN) for trajectory predictions
that considers such information. Our experiments with InfraGAN show that the
comprehensive information offered by VTrackIt reduces the number of high-risk
edge cases. The VTrackIt dataset is available upon request under the Creative
Commons CC BY-NC-SA 4.0 license at <a href="http://vtrackit.irda.club.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images. (arXiv:2207.11148v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11148">
<div class="article-summary-box-inner">
<span><p>We present a method for learning to generate unbounded flythrough videos of
natural scenes starting from a single view, where this capability is learned
from a collection of single photographs, without requiring camera poses or even
multiple views of each scene. To achieve this, we propose a novel
self-supervised view generation training paradigm, where we sample and
rendering virtual camera trajectories, including cyclic ones, allowing our
model to learn stable view generation from a collection of single views. At
test time, despite never seeing a video during training, our approach can take
a single image and generate long camera trajectories comprised of hundreds of
new views with realistic and diverse content. We compare our approach with
recent state-of-the-art supervised view generation methods that require posed
multi-view videos and demonstrate superior performance and synthesis quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Soft Contrastive Learning. (arXiv:2207.11163v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11163">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has recently achieved great success in
representation learning without human annotations. The dominant method -- that
is contrastive learning, is generally based on instance discrimination tasks,
i.e., individual samples are treated as independent categories. However,
presuming all the samples are different contradicts the natural grouping of
similar samples in common visual datasets, e.g., multiple views of the same
dog. To bridge the gap, this paper proposes an adaptive method that introduces
soft inter-sample relations, namely Adaptive Soft Contrastive Learning (ASCL).
More specifically, ASCL transforms the original instance discrimination task
into a multi-instance soft discrimination task, and adaptively introduces
inter-sample relations. As an effective and concise plug-in module for existing
self-supervised learning frameworks, ASCL achieves the best performance on
several benchmarks in terms of both performance and efficiency. Code is
available at https://github.com/MrChenFeng/ASCL_ICPR2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">METER-ML: A Multi-sensor Earth Observation Benchmark for Automated Methane Source Mapping. (arXiv:2207.11166v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11166">
<div class="article-summary-box-inner">
<span><p>Reducing methane emissions is essential for mitigating global warming. To
attribute methane emissions to their sources, a comprehensive dataset of
methane source infrastructure is necessary. Recent advancements with deep
learning on remotely sensed imagery have the potential to identify the
locations and characteristics of methane sources, but there is a substantial
lack of publicly available data to enable machine learning researchers and
practitioners to build automated mapping approaches. To help fill this gap, we
construct a multi-sensor dataset called METER-ML containing 86,625
georeferenced NAIP, Sentinel-1, and Sentinel-2 images in the U.S. labeled for
the presence or absence of methane source facilities including concentrated
animal feeding operations, coal mines, landfills, natural gas processing
plants, oil refineries and petroleum terminals, and wastewater treatment
plants. We experiment with a variety of models that leverage different spatial
resolutions, spatial footprints, image products, and spectral bands. We find
that our best model achieves an area under the precision recall curve of 0.915
for identifying concentrated animal feeding operations and 0.821 for oil
refineries and petroleum terminals on an expert-labeled test set, suggesting
the potential for large-scale mapping. We make METER-ML freely available at
https://stanfordmlgroup.github.io/projects/meter-ml/ to support future work on
automated methane source mapping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Few-Shot Object Detection on a Multi-Domain Benchmark. (arXiv:2207.11169v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11169">
<div class="article-summary-box-inner">
<span><p>Most existing works on few-shot object detection (FSOD) focus on a setting
where both pre-training and few-shot learning datasets are from a similar
domain. However, few-shot algorithms are important in multiple domains; hence
evaluation needs to reflect the broad applications. We propose a Multi-dOmain
Few-Shot Object Detection (MoFSOD) benchmark consisting of 10 datasets from a
wide range of domains to evaluate FSOD algorithms. We comprehensively analyze
the impacts of freezing layers, different architectures, and different
pre-training datasets on FSOD performance. Our empirical results show several
key factors that have not been explored in previous works: 1) contrary to
previous belief, on a multi-domain benchmark, fine-tuning (FT) is a strong
baseline for FSOD, performing on par or better than the state-of-the-art (SOTA)
algorithms; 2) utilizing FT as the baseline allows us to explore multiple
architectures, and we found them to have a significant impact on down-stream
few-shot tasks, even with similar pre-training performances; 3) by decoupling
pre-training and few-shot learning, MoFSOD allows us to explore the impact of
different pre-training datasets, and the right choice can boost the performance
of the down-stream tasks significantly. Based on these findings, we list
possible avenues of investigation for improving FSOD performance and propose
two simple modifications to existing algorithms that lead to SOTA performance
on the MoFSOD benchmark. The code is available at
https://github.com/amazon-research/few-shot-object-detection-benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Certifiably Robust Neural Networks Against Semantic Perturbations. (arXiv:2207.11177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11177">
<div class="article-summary-box-inner">
<span><p>Semantic image perturbations, such as scaling and rotation, have been shown
to easily deceive deep neural networks (DNNs). Hence, training DNNs to be
certifiably robust to these perturbations is critical. However, no prior work
has been able to incorporate the objective of deterministic semantic robustness
into the training procedure, as existing deterministic semantic verifiers are
exceedingly slow. To address these challenges, we propose Certified Semantic
Training (CST), the first training framework for deterministic certified
robustness against semantic image perturbations. Our framework leverages a
novel GPU-optimized verifier that, unlike existing works, is fast enough for
use in training. Our results show that networks trained via CST consistently
achieve both better provable semantic robustness and clean accuracy, compared
to networks trained via baselines based on existing works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Faceted Distillation of Base-Novel Commonality for Few-shot Object Detection. (arXiv:2207.11184v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11184">
<div class="article-summary-box-inner">
<span><p>Most of existing methods for few-shot object detection follow the fine-tuning
paradigm, which potentially assumes that the class-agnostic generalizable
knowledge can be learned and transferred implicitly from base classes with
abundant samples to novel classes with limited samples via such a two-stage
training strategy. However, it is not necessarily true since the object
detector can hardly distinguish between class-agnostic knowledge and
class-specific knowledge automatically without explicit modeling. In this work
we propose to learn three types of class-agnostic commonalities between base
and novel classes explicitly: recognition-related semantic commonalities,
localization-related semantic commonalities and distribution commonalities. We
design a unified distillation framework based on a memory bank, which is able
to perform distillation of all three types of commonalities jointly and
efficiently. Extensive experiments demonstrate that our method can be readily
integrated into most of existing fine-tuning based methods and consistently
improve the performance by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to identify cracks on wind turbine blade surfaces using drone-based inspection images. (arXiv:2207.11186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11186">
<div class="article-summary-box-inner">
<span><p>Wind energy is expected to be one of the leading ways to achieve the goals of
the Paris Agreement but it in turn heavily depends on effective management of
its operations and maintenance (O&amp;M) costs. Blade failures account for
one-third of all O&amp;M costs thus making accurate detection of blade damages,
especially cracks, very important for sustained operations and cost savings.
Traditionally, damage inspection has been a completely manual process thus
making it subjective, error-prone, and time-consuming. Hence in this work, we
bring more objectivity, scalability, and repeatability in our damage inspection
process, using deep learning, to miss fewer cracks. We build a deep learning
model trained on a large dataset of blade damages, collected by our drone-based
inspection, to correctly detect cracks. Our model is already in production and
has processed more than a million damages with a recall of 0.96. We also focus
on model interpretability using class activation maps to get a peek into the
model workings. The model not only performs as good as human experts but also
better in certain tricky cases. Thus, in this work, we aim to increase wind
energy adoption by decreasing one of its major hurdles - the O\&amp;M costs
resulting from missing blade failures like cracks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised-RCNN for Medical Image Segmentation with Limited Data Annotation. (arXiv:2207.11191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11191">
<div class="article-summary-box-inner">
<span><p>Many successful methods developed for medical image analysis that are based
on machine learning use supervised learning approaches, which often require
large datasets annotated by experts to achieve high accuracy. However, medical
data annotation is time-consuming and expensive, especially for segmentation
tasks. To solve the problem of learning with limited labeled medical image
data, an alternative deep learning training strategy based on self-supervised
pretraining on unlabeled MRI scans is proposed in this work. Our pretraining
approach first, randomly applies different distortions to random areas of
unlabeled images and then predicts the type of distortions and loss of
information. To this aim, an improved version of Mask-RCNN architecture has
been adapted to localize the distortion location and recover the original image
pixels. The effectiveness of the proposed method for segmentation tasks in
different pre-training and fine-tuning scenarios is evaluated based on the
Osteoarthritis Initiative dataset. Using this self-supervised pretraining
method improved the Dice score by 20% compared to training from scratch. The
proposed self-supervised learning is simple, effective, and suitable for
different ranges of medical image analysis tasks including anomaly detection,
segmentation, and classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis. (arXiv:2207.11192v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11192">
<div class="article-summary-box-inner">
<span><p>Recently, diffusion models have shown remarkable results in image synthesis
by gradually removing noise and amplifying signals. Although the simple
generative process surprisingly works well, is this the best way to generate
image data? For instance, despite the fact that human perception is more
sensitive to the low frequencies of an image, diffusion models themselves do
not consider any relative importance of each frequency component. Therefore, to
incorporate the inductive bias for image data, we propose a novel generative
process that synthesizes images in a coarse-to-fine manner. First, we
generalize the standard diffusion models by enabling diffusion in a rotated
coordinate system with different velocities for each component of the vector.
We further propose a blur diffusion as a special case, where each frequency
component of an image is diffused at different speeds. Specifically, the
proposed blur diffusion consists of a forward process that blurs an image and
adds noise gradually, after which a corresponding reverse process deblurs an
image and removes noise progressively. Experiments show that the proposed model
outperforms the previous method in FID on LSUN bedroom and church datasets.
Code is available at https://github.com/sangyun884/blur-diffusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target-Driven Structured Transformer Planner for Vision-Language Navigation. (arXiv:2207.11201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11201">
<div class="article-summary-box-inner">
<span><p>Vision-language navigation is the task of directing an embodied agent to
navigate in 3D scenes with natural language instructions. For the agent,
inferring the long-term navigation target from visual-linguistic clues is
crucial for reliable path planning, which, however, has rarely been studied
before in literature. In this article, we propose a Target-Driven Structured
Transformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware
navigation. Specifically, we devise an Imaginary Scene Tokenization mechanism
for explicit estimation of the long-term target (even located in unexplored
environments). In addition, we design a Structured Transformer Planner which
elegantly incorporates the explored room layout into a neural attention
architecture for structured and global planning. Experimental results
demonstrate that our TD-STP substantially improves previous best methods'
success rate by 2% and 5% on the test set of R2R and REVERIE benchmarks,
respectively. Our code is available at https://github.com/YushengZhao/TD-STP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Treelike Tubular Structure Segmentation: A Comprehensive Review and Future Perspectives. (arXiv:2207.11203v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11203">
<div class="article-summary-box-inner">
<span><p>Various structures in human physiology follow a treelike morphology, which
often expresses complexity at very fine scales. Examples of such structures are
intrathoracic airways, retinal blood vessels, and hepatic blood vessels. Large
collections of 2D and 3D images have been made available by medical imaging
modalities such as magnetic resonance imaging (MRI), computed tomography (CT),
Optical coherence tomography (OCT) and ultrasound in which the spatial
arrangement can be observed. Segmentation of these structures in medical
imaging is of great importance since the analysis of the structure provides
insights into disease diagnosis, treatment planning, and prognosis. Manually
labelling extensive data by radiologists is often time-consuming and
error-prone. As a result, automated or semi-automated computational models have
become a popular research field of medical imaging in the past two decades, and
many have been developed to date. In this survey, we aim to provide a
comprehensive review of currently publicly available datasets, segmentation
algorithms, and evaluation metrics. In addition, current challenges and future
research directions are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise Binarization. (arXiv:2207.11209v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11209">
<div class="article-summary-box-inner">
<span><p>Instance segmentation on point clouds is crucially important for 3D scene
understanding. Distance clustering is commonly used in state-of-the-art methods
(SOTAs), which is typically effective but does not perform well in segmenting
adjacent objects with the same semantic label (especially when they share
neighboring points). Due to the uneven distribution of offset points, these
existing methods can hardly cluster all instance points. To this end, we design
a novel divide and conquer strategy and propose an end-to-end network named
PBNet that binarizes each point and clusters them separately to segment
instances. PBNet divides offset instance points into two categories: high and
low density points (HPs vs.LPs), which are then conquered separately. Adjacent
objects can be clearly separated by removing LPs, and then be completed and
refined by assigning LPs via a neighbor voting method. To further reduce
clustering errors, we develop an iterative merging algorithm based on mean size
to aggregate fragment instances. Experiments on ScanNetV2 and S3DIS datasets
indicate the superiority of our model. In particular, PBNet achieves so far the
best AP50 and AP25 on the ScanNetV2 official benchmark challenge (Validation
Set) while demonstrating high efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Predictive Performance and Calibration by Weight Fusion in Semantic Segmentation. (arXiv:2207.11211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11211">
<div class="article-summary-box-inner">
<span><p>Averaging predictions of a deep ensemble of networks is apopular and
effective method to improve predictive performance andcalibration in various
benchmarks and Kaggle competitions. However, theruntime and training cost of
deep ensembles grow linearly with the size ofthe ensemble, making them
unsuitable for many applications. Averagingensemble weights instead of
predictions circumvents this disadvantageduring inference and is typically
applied to intermediate checkpoints ofa model to reduce training cost. Albeit
effective, only few works haveimproved the understanding and the performance of
weight averaging.Here, we revisit this approach and show that a simple weight
fusion (WF)strategy can lead to a significantly improved predictive performance
andcalibration. We describe what prerequisites the weights must meet interms of
weight space, functional space and loss. Furthermore, we presenta new test
method (called oracle test) to measure the functional spacebetween weights. We
demonstrate the versatility of our WF strategy acrossstate of the art
segmentation CNNs and Transformers as well as real worlddatasets such as
BDD100K and Cityscapes. We compare WF with similarapproaches and show our
superiority for in- and out-of-distribution datain terms of predictive
performance and calibration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target Identification and Bayesian Model Averaging with Probabilistic Hierarchical Factor Probabilities. (arXiv:2207.11212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11212">
<div class="article-summary-box-inner">
<span><p>Target detection in hyperspectral imagery is the process of locating pixels
from an image which are likely to contain target, typically done by comparing
one or more spectra for the desired target material to each pixel in the image.
Target identification is the process of target detection incorporating an
additional process to identify more specifically the material that is present
in each pixel that scored high in detection. Detection is generally a 2-class
problem of target vs. background, and identification is a many class problem
including target, background, and additional know materials. The identification
process we present is probabilistic and hierarchical which provides
transparency to the process and produces trustworthy output. In this paper we
show that target identification has a much lower false alarm rate than
detection alone, and provide a detailed explanation of a robust identification
method using probabilistic hierarchical classification that handles the vague
categories of materials that depend on users which are different than the
specific physical categories of chemical constituents. Identification is often
done by comparing mixtures of materials including the target spectra to
mixtures of materials that do not include the target spectra, possibly with
other steps. (band combinations, feature checking, background removal, etc.)
Standard linear regression does not handle these problems well because the
number of regressors (identification spectra) is greater than the number of
feature variables (bands), and there are multiple correlated spectra. Our
proposed method handles these challenges efficiently and provides additional
important practical information in the form of hierarchical probabilities
computed from Bayesian model averaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Class-Incremental Learning via Entropy-Regularized Data-Free Replay. (arXiv:2207.11213v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11213">
<div class="article-summary-box-inner">
<span><p>Few-shot class-incremental learning (FSCIL) has been proposed aiming to
enable a deep learning system to incrementally learn new classes with limited
data. Recently, a pioneer claims that the commonly used replay-based method in
class-incremental learning (CIL) is ineffective and thus not preferred for
FSCIL. This has, if truth, a significant influence on the fields of FSCIL. In
this paper, we show through empirical results that adopting the data replay is
surprisingly favorable. However, storing and replaying old data can lead to a
privacy concern. To address this issue, we alternatively propose using
data-free replay that can synthesize data by a generator without accessing real
data. In observing the the effectiveness of uncertain data for knowledge
distillation, we impose entropy regularization in the generator training to
encourage more uncertain examples. Moreover, we propose to relabel the
generated data with one-hot-like labels. This modification allows the network
to learn by solely minimizing the cross-entropy loss, which mitigates the
problem of balancing different objectives in the conventional knowledge
distillation approach. Finally, we show extensive experimental results and
analysis on CIFAR-100, miniImageNet and CUB-200 to demonstrate the
effectiveness of our proposed one.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization for Activity Recognition via Adaptive Feature Fusion. (arXiv:2207.11221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11221">
<div class="article-summary-box-inner">
<span><p>Human activity recognition requires the efforts to build a generalizable
model using the training datasets with the hope to achieve good performance in
test datasets. However, in real applications, the training and testing datasets
may have totally different distributions due to various reasons such as
different body shapes, acting styles, and habits, damaging the model's
generalization performance. While such a distribution gap can be reduced by
existing domain adaptation approaches, they typically assume that the test data
can be accessed in the training stage, which is not realistic. In this paper,
we consider a more practical and challenging scenario: domain-generalized
activity recognition (DGAR) where the test dataset \emph{cannot} be accessed
during training. To this end, we propose \emph{Adaptive Feature Fusion for
Activity Recognition~(AFFAR)}, a domain generalization approach that learns to
fuse the domain-invariant and domain-specific representations to improve the
model's generalization performance. AFFAR takes the best of both worlds where
domain-invariant representations enhance the transferability across domains and
domain-specific representations leverage the model discrimination power from
each domain. Extensive experiments on three public HAR datasets show its
effectiveness. Furthermore, we apply AFFAR to a real application, i.e., the
diagnosis of Children's Attention Deficit Hyperactivity Disorder~(ADHD), which
also demonstrates the superiority of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forest and Water Bodies Segmentation Through Satellite Images Using U-Net. (arXiv:2207.11222v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11222">
<div class="article-summary-box-inner">
<span><p>Global environment monitoring is a task that requires additional attention in
the contemporary rapid climate change environment. This includes monitoring the
rate of deforestation and areas affected by flooding. Satellite imaging has
greatly helped monitor the earth, and deep learning techniques have helped to
automate this monitoring process. This paper proposes a solution for observing
the area covered by the forest and water. To achieve this task UNet model has
been proposed, which is an image segmentation model. The model achieved a
validation accuracy of 82.55% and 82.92% for the segmentation of areas covered
by forest and water, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved $\alpha$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning. (arXiv:2207.11223v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11223">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have gained significant attention in
several computer vision tasks for generating high-quality synthetic data.
Various medical applications including diagnostic imaging and radiation therapy
can benefit greatly from synthetic data generation due to data scarcity in the
domain. However, medical image data is typically kept in 3D space, and
generative models suffer from the curse of dimensionality issues in generating
such synthetic data. In this paper, we investigate the potential of GANs for
generating connected 3D volumes. We propose an improved version of 3D
$\alpha$-GAN by incorporating various architectural enhancements. On a
synthetic dataset of connected 3D spheres and ellipsoids, our model can
generate fully connected 3D shapes with similar geometrical characteristics to
that of training data. We also show that our 3D GAN model can successfully
generate high-quality 3D tumor volumes and associated treatment specifications
(e.g., isocenter locations). Similar moment invariants to the training data as
well as fully connected 3D shapes confirm that improved 3D $\alpha$-GAN
implicitly learns the training data distribution, and generates
realistic-looking samples. The capability of improved 3D $\alpha$-GAN makes it
a valuable source for generating synthetic medical image data that can help
future research in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Kernel Attention for 3D Medical Image Segmentation. (arXiv:2207.11225v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11225">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of multiple organs and tumors from 3D medical images
such as magnetic resonance imaging (MRI) and computed tomography (CT) scans
using deep learning methods can aid in diagnosing and treating cancer. However,
organs often overlap and are complexly connected, characterized by extensive
anatomical variation and low contrast. In addition, the diversity of tumor
shape, location, and appearance, coupled with the dominance of background
voxels, makes accurate 3D medical image segmentation difficult. In this paper,
a novel large-kernel (LK) attention module is proposed to address these
problems to achieve accurate multi-organ segmentation and tumor segmentation.
The advantages of convolution and self-attention are combined in the proposed
LK attention module, including local contextual information, long-range
dependence, and channel adaptation. The module also decomposes the LK
convolution to optimize the computational cost and can be easily incorporated
into FCNs such as U-Net. Comprehensive ablation experiments demonstrated the
feasibility of convolutional decomposition and explored the most efficient and
effective network design. Among them, the best Mid-type LK attention-based
U-Net network was evaluated on CT-ORG and BraTS 2020 datasets, achieving
state-of-the-art segmentation performance. The performance improvement due to
the proposed LK attention module was also statistically validated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewGAN: Generating from the Joint Distribution of a Few Images. (arXiv:2207.11226v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11226">
<div class="article-summary-box-inner">
<span><p>We introduce FewGAN, a generative model for generating novel, high-quality
and diverse images whose patch distribution lies in the joint patch
distribution of a small number of N&gt;1 training samples. The method is, in
essence, a hierarchical patch-GAN that applies quantization at the first coarse
scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully
convolutional GANs at finer scales. Our key idea is to first use quantization
to learn a fixed set of patch embeddings for training images. We then use a
separate set of side images to model the structure of generated images using an
autoregressive model trained on the learned patch embeddings of training
images. Using quantization at the coarsest scale allows the model to generate
both conditional and unconditional novel images. Subsequently, a patch-GAN
renders the fine details, resulting in high-quality images. In an extensive set
of experiments, it is shown that FewGAN outperforms baselines both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face editing with GAN -- A Review. (arXiv:2207.11227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11227">
<div class="article-summary-box-inner">
<span><p>In recent years, Generative Adversarial Networks (GANs) have become a hot
topic among researchers and engineers that work with deep learning. It has been
a ground-breaking technique which can generate new pieces of content of data in
a consistent way. The topic of GANs has exploded in popularity due to its
applicability in fields like image generation and synthesis, and music
production and composition. GANs have two competing neural networks: a
generator and a discriminator. The generator is used to produce new samples or
pieces of content, while the discriminator is used to recognize whether the
piece of content is real or generated. What makes it different from other
generative models is its ability to learn unlabeled samples. In this review
paper, we will discuss the evolution of GANs, several improvements proposed by
the authors and a brief comparison between the different models. Index Terms
generative adversarial networks, unsupervised learning, deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifying Crop Types using Gaussian Bayesian Models and Neural Networks on GHISACONUS USGS data from NASA Hyperspectral Satellite Imagery. (arXiv:2207.11228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11228">
<div class="article-summary-box-inner">
<span><p>Hyperspectral Imagining is a type of digital imaging in which each pixel
contains typically hundreds of wavelengths of light providing spectroscopic
information about the materials present in the pixel. In this paper we provide
classification methods for determining crop type in the USGS GHISACONUS data,
which contains around 7,000 pixel spectra from the five major U.S. agricultural
crops (winter wheat, rice, corn, soybeans, and cotton) collected by the NASA
Hyperion satellite, and includes the spectrum, geolocation, crop type, and
stage of growth for each pixel. We apply standard LDA and QDA as well as
Bayesian custom versions that compute the joint probability of crop type and
stage, and then the marginal probability for crop type, outperforming the
non-Bayesian methods. We also test a single layer neural network with dropout
on the data, which performs comparable to LDA and QDA but not as well as the
Bayesian methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine. (arXiv:2207.11230v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11230">
<div class="article-summary-box-inner">
<span><p>Layout Analysis (the identification of zones and their classification) is the
first step along line segmentation in Optical Character Recognition and similar
tasks. The ability of identifying main body of text from marginal text or
running titles makes the difference between extracting the work full text of a
digitized book and noisy outputs. We show that most segmenters focus on pixel
classification and that polygonization of this output has not been used as a
target for the latest competition on historical document (ICDAR 2017 and
onwards), despite being the focus in the early 2010s. We propose to shift, for
efficiency, the task from a pixel classification-based polygonization to an
object detection using isothetic rectangles. We compare the output of Kraken
and YOLOv5 in terms of segmentation and show that the later severely
outperforms the first on small datasets (1110 samples and below). We release
two datasets for training and evaluation on historical documents as well as a
new package, YALTAi, which injects YOLOv5 in the segmentation pipeline of
Kraken 4.1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeing 3D Objects in a Single Image via Self-Supervised Static-Dynamic Disentanglement. (arXiv:2207.11232v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11232">
<div class="article-summary-box-inner">
<span><p>Human perception reliably identifies movable and immovable parts of 3D
scenes, and completes the 3D structure of objects and background from
incomplete observations. We learn this skill not via labeled examples, but
simply by observing objects move. In this work, we propose an approach that
observes unlabeled multi-view videos at training time and learns to map a
single image observation of a complex scene, such as a street with cars, to a
3D neural scene representation that is disentangled into movable and immovable
parts while plausibly completing its 3D structure. We separately parameterize
movable and immovable scene parts via 2D neural ground plans. These ground
plans are 2D grids of features aligned with the ground plane that can be
locally decoded into 3D neural radiance fields. Our model is trained
self-supervised via neural rendering. We demonstrate that the structure
inherent to our disentangled 3D representation enables a variety of downstream
tasks in street-scale 3D scenes using simple heuristics, such as extraction of
object-centric 3D representations, novel view synthesis, instance segmentation,
and 3D bounding box prediction, highlighting its value as a backbone for
data-efficient 3D scene understanding models. This disentanglement further
enables scene editing via object manipulation such as deletion, insertion, and
rigid-body motion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A System-driven Automatic Ground Truth Generation Method for DL Inner-City Driving Corridor Detectors. (arXiv:2207.11234v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11234">
<div class="article-summary-box-inner">
<span><p>Data-driven perception approaches are well-established in automated driving
systems. In many fields even super-human performance is reached. Unlike
prediction and planning approaches, mainly supervised learning algorithms are
used for the perception domain. Therefore, a major remaining challenge is the
efficient generation of ground truth data. As perception modules are positioned
close to the sensor, they typically run on raw sensor data of high bandwidth.
Due to that, the generation of ground truth labels typically causes a
significant manual effort, which leads to high costs for the labelling itself
and the necessary quality control. In this contribution, we propose an
automatic labeling approach for semantic segmentation of the drivable ego
corridor that reduces the manual effort by a factor of 150 and more. The
proposed holistic approach could be used in an automated data loop, allowing a
continuous improvement of the depending perception modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved lightweight identification of agricultural diseases based on MobileNetV3. (arXiv:2207.11238v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11238">
<div class="article-summary-box-inner">
<span><p>At present, the identification of agricultural pests and diseases has the
problem that the model is not lightweight enough and difficult to apply. Based
on MobileNetV3, this paper introduces the Coordinate Attention block. The
parameters of MobileNetV3-large are reduced by 22%, the model size is reduced
by 19.7%, and the accuracy is improved by 0.92%. The parameters of
MobileNetV3-small are reduced by 23.4%, the model size is reduced by 18.3%, and
the accuracy is increased by 0.40%. In addition, the improved MobileNetV3-small
was migrated to Jetson Nano for testing. The accuracy increased by 2.48% to
98.31%, and the inference speed increased by 7.5%. It provides a reference for
deploying the agricultural pest identification model to embedded devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiface: A Dataset for Neural Face Rendering. (arXiv:2207.11243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11243">
<div class="article-summary-box-inner">
<span><p>Photorealistic avatars of human faces have come a long way in recent years,
yet research along this area is limited by a lack of publicly available,
high-quality datasets covering both, dense multi-view camera captures, and rich
facial expressions of the captured subjects. In this work, we present
Multiface, a new multi-view, high-resolution human face dataset collected from
13 identities at Reality Labs Research for neural face rendering. We introduce
Mugsy, a large scale multi-camera apparatus to capture high-resolution
synchronized videos of a facial performance. The goal of Multiface is to close
the gap in accessibility to high quality data in the academic community and to
enable research in VR telepresence. Along with the release of the dataset, we
conduct ablation studies on the influence of different model architectures
toward the model's interpolation capacity of novel viewpoint and expressions.
With a conditional VAE model serving as our baseline, we found that adding
spatial bias, texture warp field, and residual connections improves performance
on novel view synthesis. Our code and data is available at:
https://github.com/facebookresearch/multiface
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Hyperparameter Optimization for Breast Mass Detection in Mammograms. (arXiv:2207.11244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11244">
<div class="article-summary-box-inner">
<span><p>Accurate breast cancer diagnosis through mammography has the potential to
save millions of lives around the world. Deep learning (DL) methods have shown
to be very effective for mass detection in mammograms. Additional improvements
of current DL models will further improve the effectiveness of these methods. A
critical issue in this context is how to pick the right hyperparameters for DL
models. In this paper, we present GA-E2E, a new approach for tuning the
hyperparameters of DL models for brest cancer detection using Genetic
Algorithms (GAs). Our findings reveal that differences in parameter values can
considerably alter the area under the curve (AUC), which is used to determine a
classifier's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic Scene Graph Generation. (arXiv:2207.11247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11247">
<div class="article-summary-box-inner">
<span><p>Existing research addresses scene graph generation (SGG) -- a critical
technology for scene understanding in images -- from a detection perspective,
i.e., objects are detected using bounding boxes followed by prediction of their
pairwise relationships. We argue that such a paradigm causes several problems
that impede the progress of the field. For instance, bounding box-based labels
in current datasets usually contain redundant classes like hairs, and leave out
background information that is crucial to the understanding of context. In this
work, we introduce panoptic scene graph generation (PSG), a new problem task
that requires the model to generate a more comprehensive scene graph
representation based on panoptic segmentations rather than rigid bounding
boxes. A high-quality PSG dataset, which contains 49k well-annotated
overlapping images from COCO and Visual Genome, is created for the community to
keep track of its progress. For benchmarking, we build four two-stage
baselines, which are modified from classic methods in SGG, and two one-stage
baselines called PSGTR and PSGFormer, which are based on the efficient
Transformer-based detector, i.e., DETR. While PSGTR uses a set of queries to
directly learn triplets, PSGFormer separately models the objects and relations
in the form of queries from two Transformer decoders, followed by a
prompting-like relation-object matching mechanism. In the end, we share
insights on open challenges and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NASA: Neural Articulated Shape Approximation. (arXiv:1912.03207v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.03207">
<div class="article-summary-box-inner">
<span><p>Efficient representation of articulated objects such as human bodies is an
important problem in computer vision and graphics. To efficiently simulate
deformation, existing approaches represent 3D objects using polygonal meshes
and deform them using skinning techniques. This paper introduces neural
articulated shape approximation (NASA), an alternative framework that enables
efficient representation of articulated deformable objects using neural
indicator functions that are conditioned on pose. Occupancy testing using NASA
is straightforward, circumventing the complexity of meshes and the issue of
water-tightness. We demonstrate the effectiveness of NASA for 3D tracking
applications, and discuss other potential extensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Trustworthy are Performance Evaluations for Basic Vision Tasks?. (arXiv:2008.03533v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.03533">
<div class="article-summary-box-inner">
<span><p>This paper examines performance evaluation criteria for basic vision tasks
involving sets of objects namely, object detection, instance-level segmentation
and multi-object tracking. The rankings of algorithms by an existing criterion
can fluctuate with different choices of parameters, e.g. Intersection over
Union (IoU) threshold, making their evaluations unreliable. More importantly,
there is no means to verify whether we can trust the evaluations of a
criterion. This work suggests a notion of trustworthiness for performance
criteria, which requires (i) robustness to parameters for reliability, (ii)
contextual meaningfulness in sanity tests, and (iii) consistency with
mathematical requirements such as the metric properties. We observe that these
requirements were overlooked by many widely-used criteria, and explore
alternative criteria using metrics for sets of shapes. We also assess all these
criteria based on the suggested requirements for trustworthiness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Energy-Based Models With Adversarial Training. (arXiv:2012.06568v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06568">
<div class="article-summary-box-inner">
<span><p>We study a new approach to learning energy-based models (EBMs) based on
adversarial training (AT). We show that (binary) AT learns a special kind of
energy function that models the support of the data distribution, and the
learning process is closely related to MCMC-based maximum likelihood learning
of EBMs. We further propose improved techniques for generative modeling with
AT, and demonstrate that this new approach is capable of generating diverse and
realistic images. Aside from having competitive image generation performance to
explicit EBMs, the studied approach is stable to train, is well-suited for
image translation tasks, and exhibits strong out-of-distribution adversarial
robustness. Our results demonstrate the viability of the AT approach to
generative modeling, suggesting that AT is a competitive alternative approach
to learning EBMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FoV-NeRF: Foveated Neural Radiance Fields for Virtual Reality. (arXiv:2103.16365v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16365">
<div class="article-summary-box-inner">
<span><p>Virtual Reality (VR) is becoming ubiquitous with the rise of consumer
displays and commercial VR platforms. Such displays require low latency and
high quality rendering of synthetic imagery with reduced compute overheads.
Recent advances in neural rendering showed promise of unlocking new
possibilities in 3D computer graphics via image-based representations of
virtual or physical environments. Specifically, the neural radiance fields
(NeRF) demonstrated that photo-realistic quality and continuous view changes of
3D scenes can be achieved without loss of view-dependent effects. While NeRF
can significantly benefit rendering for VR applications, it faces unique
challenges posed by high field-of-view, high resolution, and
stereoscopic/egocentric viewing, typically causing low quality and high latency
of the rendered images. In VR, this not only harms the interaction experience
but may also cause sickness. To tackle these problems toward
six-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first
gaze-contingent 3D neural representation and view synthesis method. We
incorporate the human psychophysics of visual- and stereo-acuity into an
egocentric neural representation of 3D scenery. We then jointly optimize the
latency/performance and visual quality while mutually bridging human perception
and neural scene synthesis to achieve perceptually high-quality immersive
interaction. We conducted both objective analysis and subjective studies to
evaluate the effectiveness of our approach. We find that our method
significantly reduces latency (up to 99% time reduction compared with NeRF)
without loss of high-fidelity rendering (perceptually identical to
full-resolution ground truth). The presented approach may serve as the first
step toward future VR/AR systems that capture, teleport, and visualize remote
environments in real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14944">
<div class="article-summary-box-inner">
<span><p>Explaining the decisions of an Artificial Intelligence (AI) model is
increasingly critical in many real-world, high-stake applications. Hundreds of
papers have either proposed new feature attribution methods, discussed or
harnessed these tools in their work. However, despite humans being the target
end-users, most attribution methods were only evaluated on proxy
automatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et
al. 2018). In this paper, we conduct the first user study to measure
attribution map effectiveness in assisting humans in ImageNet classification
and Stanford Dogs fine-grained classification, and when an image is natural or
adversarial (i.e., contains adversarial perturbations). Overall, feature
attribution is surprisingly not more effective than showing humans nearest
training-set examples. On a harder task of fine-grained dog categorization,
presenting attribution maps to humans does not help, but instead hurts the
performance of human-AI teams compared to AI alone. Importantly, we found
automatic attribution-map evaluation measures to correlate poorly with the
actual human-AI team performance. Our findings encourage the community to
rigorously test their methods on the downstream human-in-the-loop applications
and to rethink the existing evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-level Feature Alignment for Versatile Image Translation and Manipulation. (arXiv:2107.03021v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03021">
<div class="article-summary-box-inner">
<span><p>Generative adversarial networks (GANs) have achieved great success in image
translation and manipulation. However, high-fidelity image generation with
faithful style control remains a grand challenge in computer vision. This paper
presents a versatile image translation and manipulation framework that achieves
accurate semantic and style guidance in image generation by explicitly building
a correspondence. To handle the quadratic complexity incurred by building the
dense correspondences, we introduce a bi-level feature alignment strategy that
adopts a top-$k$ operation to rank block-wise features followed by dense
attention between block features which reduces memory cost substantially. As
the top-$k$ operation involves index swapping which precludes the gradient
propagation, we approximate the non-differentiable top-$k$ operation with a
regularized earth mover's problem so that its gradient can be effectively
back-propagated. In addition, we design a novel semantic position encoding
mechanism that builds up coordinate for each individual semantic region to
preserve texture structures while building correspondences. Further, we design
a novel confidence feature injection module which mitigates mismatch problem by
fusing features adaptively according to the reliability of built
correspondences. Extensive experiments show that our method achieves superior
performance qualitatively and quantitatively as compared with the
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Predict Diverse Human Motions from a Single Image via Mixture Density Networks. (arXiv:2109.05776v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05776">
<div class="article-summary-box-inner">
<span><p>Human motion prediction, which plays a key role in computer vision, generally
requires a past motion sequence as input. However, in real applications, a
complete and correct past motion sequence can be too expensive to achieve. In
this paper, we propose a novel approach to predicting future human motions from
a much weaker condition, i.e., a single image, with mixture density networks
(MDN) modeling. Contrary to most existing deep human motion prediction
approaches, the multimodal nature of MDN enables the generation of diverse
future motion hypotheses, which well compensates for the strong stochastic
ambiguity aggregated by the single input and human motion uncertainty. In
designing the loss function, we further introduce the energy-based formulation
to flexibly impose prior losses over the learnable parameters of MDN to
maintain motion coherence as well as improve the prediction accuracy by
customizing the energy functions. Our trained model directly takes an image as
input and generates multiple plausible motions that satisfy the given
condition. Extensive experiments on two standard benchmark datasets demonstrate
the effectiveness of our method in terms of prediction diversity and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-aware direct monocular odometry. (arXiv:2109.10077v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10077">
<div class="article-summary-box-inner">
<span><p>We present a generic framework for scale-aware direct monocular odometry
based on depth prediction from a deep neural network. In contrast with previous
methods where depth information is only partially exploited, we formulate a
novel depth prediction residual which allows us to incorporate multi-view depth
information. In addition, we propose to use a truncated robust cost function
which prevents considering inconsistent depth estimations. The photometric and
depth-prediction measurements are integrated into a tightly-coupled
optimization leading to a scale-aware monocular system which does not
accumulate scale drift. Our proposal does not particularize for a concrete
neural network, being able to work along with the vast majority of the existing
depth prediction solutions. We demonstrate the validity and generality of our
proposal evaluating it on the KITTI odometry dataset, using two publicly
available neural networks and comparing it with similar approaches and the
state-of-the-art for monocular and stereo SLAM. Experiments show that our
proposal largely outperforms classic monocular SLAM, being 5 to 9 times more
precise, beating similar approaches and having an accuracy which is closer to
that of stereo systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bounding-box deep calibration for high performance face detection. (arXiv:2110.03892v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03892">
<div class="article-summary-box-inner">
<span><p>Modern convolutional neural networks (CNNs)-based face detectors have
achieved tremendous strides due to large annotated datasets. However,
misaligned results with high detection confidence but low localization accuracy
restrict the further improvement of detection performance. In this paper, the
authors first predict high confidence detection results on the training set
itself. Surprisingly, a considerable part of them exist in the same
misalignment problem. Then, the authors carefully examine these cases and point
out that annotation misalignment is the main reason. Later, a comprehensive
discussion is given for the replacement rationality between predicted and
annotated bounding-boxes. Finally, the authors propose a novel Bounding-Box
Deep Calibration (BDC) method to reasonably replace misaligned annotations with
model predicted bounding-boxes and offer calibrated annotations for the
training set. Extensive experiments on multiple detectors and two popular
benchmark datasets show the effectiveness of BDC on improving models' precision
and recall rate, without adding extra inference time and memory consumption.
Our simple and effective method provides a general strategy for improving face
detection, especially for light-weight detectors in real-time situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00232">
<div class="article-summary-box-inner">
<span><p>In visual recognition tasks, few-shot learning requires the ability to learn
object categories with few support examples. Its re-popularity in light of the
deep learning development is mainly in image classification. This work focuses
on few-shot semantic segmentation, which is still a largely unexplored field. A
few recent advances are often restricted to single-class few-shot segmentation.
In this paper, we first present a novel multi-way (class) encoding and decoding
architecture which effectively fuses multi-scale query information and
multi-class support information into one query-support embedding. Multi-class
segmentation is directly decoded upon this embedding. For better feature
fusion, a multi-level attention mechanism is proposed within the architecture,
which includes the attention for support feature modulation and attention for
multi-scale combination. Last, to enhance the embedding space learning, an
additional pixel-wise metric learning module is introduced with triplet loss
formulated on the pixel-level embedding of the input image. Extensive
experiments on standard benchmarks PASCAL-5i and COCO-20i show clear benefits
of our method over the state of the art in few-shot segmentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Dynamics of DNNs Using Graph Modularity. (arXiv:2111.12485v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12485">
<div class="article-summary-box-inner">
<span><p>There are good arguments to support the claim that deep neural networks
(DNNs) capture better feature representations than the previous hand-crafted
feature engineering, which leads to a significant performance improvement. In
this paper, we move a tiny step towards understanding the dynamics of feature
representations over layers. Specifically, we model the process of class
separation of intermediate representations in pre-trained DNNs as the evolution
of communities in dynamic graphs. Then, we introduce modularity, a generic
metric in graph theory, to quantify the evolution of communities. In the
preliminary experiment, we find that modularity roughly tends to increase as
the layer goes deeper and the degradation and plateau arise when the model
complexity is great relative to the dataset. Through an asymptotic analysis, we
prove that modularity can be broadly used for different applications. For
example, modularity provides new insights to quantify the difference between
feature representations. More crucially, we demonstrate that the degradation
and plateau in modularity curves represent redundant layers in DNNs and can be
pruned with minimal impact on performance, which provides theoretical guidance
for layer pruning. Our code is available at
https://github.com/yaolu-zjut/Dynamic-Graphs-Construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Shape Part Slot Machine: Contact-based Reasoning for Generating 3D Shapes from Parts. (arXiv:2112.00584v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00584">
<div class="article-summary-box-inner">
<span><p>We present the Shape Part Slot Machine, a new method for assembling novel 3D
shapes from existing parts by performing contact-based reasoning. Our method
represents each shape as a graph of ``slots,'' where each slot is a region of
contact between two shape parts. Based on this representation, we design a
graph-neural-network-based model for generating new slot graphs and retrieving
compatible parts, as well as a gradient-descent-based optimization scheme for
assembling the retrieved parts into a complete shape that respects the
generated slot graph. This approach does not require any semantic part labels;
interestingly, it also does not require complete part geometries -- reasoning
about the slots proves sufficient to generate novel, high-quality 3D shapes. We
demonstrate that our method generates shapes that outperform existing
modeling-by-assembly approaches regarding quality, diversity, and structural
complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding. (arXiv:2112.01551v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01551">
<div class="article-summary-box-inner">
<span><p>Recent studies on dense captioning and visual grounding in 3D have achieved
impressive results. Despite developments in both areas, the limited amount of
available 3D vision-language data causes overfitting issues for 3D visual
grounding and 3D dense captioning methods. Also, how to discriminatively
describe objects in complex 3D environments is not fully studied yet. To
address these challenges, we present D3Net, an end-to-end neural
speaker-listener architecture that can detect, describe and discriminate. Our
D3Net unifies dense captioning and visual grounding in 3D in a self-critical
manner. This self-critical property of D3Net also introduces discriminability
during object caption generation and enables semi-supervised training on
ScanNet data with partially annotated descriptions. Our method outperforms SOTA
methods in both tasks on the ScanRefer dataset, surpassing the SOTA 3D dense
captioning method by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoFaNeRF: Morphable Facial Neural Radiance Field. (arXiv:2112.02308v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02308">
<div class="article-summary-box-inner">
<span><p>We propose a parametric model that maps free-view images into a vector space
of coded facial shape, expression and appearance with a neural radiance field,
namely Morphable Facial NeRF. Specifically, MoFaNeRF takes the coded facial
shape, expression and appearance along with space coordinate and view direction
as input to an MLP, and outputs the radiance of the space point for
photo-realistic image synthesis. Compared with conventional 3D morphable models
(3DMM), MoFaNeRF shows superiority in directly synthesizing photo-realistic
facial details even for eyes, mouths, and beards. Also, continuous face
morphing can be easily achieved by interpolating the input shape, expression
and appearance codes. By introducing identity-specific modulation and texture
encoder, our model synthesizes accurate photometric details and shows strong
representation ability. Our model shows strong ability on multiple applications
including image-based fitting, random generation, face rigging, face editing,
and novel view synthesis. Experiments show that our method achieves higher
representation ability than previous parametric models, and achieves
competitive performance in several applications. To the best of our knowledge,
our work is the first facial parametric model built upon a neural radiance
field that can be used in fitting, generation and manipulation. The code and
data is available at https://github.com/zhuhao-nju/mofanerf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">4DContrast: Contrastive Learning with Dynamic Correspondences for 3D Scene Understanding. (arXiv:2112.02990v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02990">
<div class="article-summary-box-inner">
<span><p>We present a new approach to instill 4D dynamic object priors into learned 3D
representations by unsupervised pre-training. We observe that dynamic movement
of an object through an environment provides important cues about its
objectness, and thus propose to imbue learned 3D representations with such
dynamic understanding, that can then be effectively transferred to improved
performance in downstream 3D semantic scene understanding tasks. We propose a
new data augmentation scheme leveraging synthetic 3D shapes moving in static 3D
environments, and employ contrastive learning under 3D-4D constraints that
encode 4D invariances into the learned 3D representations. Experiments
demonstrate that our unsupervised representation learning results in
improvement in downstream 3D semantic segmentation, object detection, and
instance segmentation tasks, and moreover, notably improves performance in
data-scarce scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector. (arXiv:2112.06102v3 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06102">
<div class="article-summary-box-inner">
<span><p>Vertebrate retinas are highly-efficient in processing trivial visual tasks
such as detecting moving objects, yet a complex challenges for modern
computers. In vertebrates, the detection of object motion is performed by
specialised retinal cells named Object Motion Sensitive Ganglion Cells
(OMS-GC). OMS-GC process continuous visual signals and generate spike patterns
that are post-processed by the Visual Cortex. Our previous Hybrid Sensitive
Motion Detector (HSMD) algorithm was the first hybrid algorithm to enhance
Background subtraction (BS) algorithms with a customised 3-layer Spiking Neural
Network (SNN) that generates OMS-GC spiking-like responses. In this work, we
present a Neuromorphic Hybrid Sensitive Motion Detector (NeuroHSMD) algorithm
that accelerates our HSMD algorithm using Field-Programmable Gate Arrays
(FPGAs). The NeuroHSMD was compared against the HSMD algorithm, using the same
2012 Change Detection (CDnet2012) and 2014 Change Detection (CDnet2014)
benchmark datasets. When tested against the CDnet2012 and CDnet2014 datasets,
NeuroHSMD performs object motion detection at 720x480 at 28.06 Frames Per
Second (fps) and 720x480 at 28.71 fps, respectively, with no degradation of
quality. Moreover, the NeuroHSMD proposed in this paper was completely
implemented in Open Computer Language (OpenCL) and therefore is easily
replicated in other devices such as Graphical Processing Units (GPUs) and
clusters of Central Processing Units (CPUs).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formulating Event-based Image Reconstruction as a Linear Inverse Problem using Optical Flow. (arXiv:2112.06242v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06242">
<div class="article-summary-box-inner">
<span><p>Event cameras are novel bio-inspired sensors that measure per-pixel
brightness differences asynchronously. Recovering brightness from events is
appealing since the reconstructed images inherit the high dynamic range (HDR)
and high-speed properties of events; hence they can be used in many robotic
vision applications and to generate slow-motion HDR videos. However,
state-of-the-art methods tackle this problem by training an event-to-image
recurrent neural network (RNN), which lacks explainability and is difficult to
tune. In this work we show, for the first time, how tackling the joint problem
of motion and brightness estimation leads us to formulate event-based image
reconstruction as a linear inverse problem that can be solved without training
an image reconstruction RNN. Instead, classical and learning-based image priors
can be used to solve the problem and remove artifacts from the reconstructed
images. The experiments show that the proposed approach generates images with
visual quality on par with state-of-the-art methods despite only using data
from a short time interval. The proposed linear formulation and solvers have a
unifying character because they can be applied also to reconstruct brightness
from the second derivative. Additionally, the linear formulation is attractive
because it can be naturally combined with super-resolution, motion-segmentation
and color demosaicing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAGA: Stochastic Whole-Body Grasping with Contact. (arXiv:2112.10103v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10103">
<div class="article-summary-box-inner">
<span><p>The synthesis of human grasping has numerous applications including AR/VR,
video games and robotics. While methods have been proposed to generate
realistic hand-object interaction for object grasping and manipulation, these
typically only consider interacting hand alone. Our goal is to synthesize
whole-body grasping motions. Starting from an arbitrary initial pose, we aim to
generate diverse and natural whole-body human motions to approach and grasp a
target object in 3D space. This task is challenging as it requires modeling
both whole-body dynamics and dexterous finger movements. To this end, we
propose SAGA (StochAstic whole-body Grasping with contAct), a framework which
consists of two key components: (a) Static whole-body grasping pose generation.
Specifically, we propose a multi-task generative model, to jointly learn static
whole-body grasping poses and human-object contacts. (b) Grasping motion
infilling. Given an initial pose and the generated whole-body grasping pose as
the start and end of the motion respectively, we design a novel contact-aware
generative motion infilling module to generate a diverse set of grasp-oriented
motions. We demonstrate the effectiveness of our method, which is a novel
generative framework to synthesize realistic and expressive whole-body motions
that approach and grasp randomly placed unseen objects. Code and models are
available at https://jiahaoplus.github.io/SAGA/saga.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RadioTransformer: A Cascaded Global-Focal Transformer for Visual Attention-guided Disease Classification. (arXiv:2202.11781v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11781">
<div class="article-summary-box-inner">
<span><p>In this work, we present RadioTransformer, a novel visual attention-driven
transformer framework, that leverages radiologists' gaze patterns and models
their visuo-cognitive behavior for disease diagnosis on chest radiographs.
Domain experts, such as radiologists, rely on visual information for medical
image interpretation. On the other hand, deep neural networks have demonstrated
significant promise in similar tasks even where visual interpretation is
challenging. Eye-gaze tracking has been used to capture the viewing behavior of
domain experts, lending insights into the complexity of visual search. However,
deep learning frameworks, even those that rely on attention mechanisms, do not
leverage this rich domain information. RadioTransformer fills this critical gap
by learning from radiologists' visual search patterns, encoded as 'human visual
attention regions' in a cascaded global-focal transformer framework. The
overall 'global' image characteristics and the more detailed 'local' features
are captured by the proposed global and focal modules, respectively. We
experimentally validate the efficacy of our student-teacher approach for 8
datasets involving different disease classification tasks where eye-gaze data
is not available during the inference phase. Code:
https://github.com/bmi-imaginelab/radiotransformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing Local Similarities for Retrieval-based 3D Orientation Estimation of Unseen Objects. (arXiv:2203.08472v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08472">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the task of estimating the 3D orientation of
previously-unseen objects from monocular images. This task contrasts with the
one considered by most existing deep learning methods which typically assume
that the testing objects have been observed during training. To handle the
unseen objects, we follow a retrieval-based strategy and prevent the network
from learning object-specific features by computing multi-scale local
similarities between the query image and synthetically-generated reference
images. We then introduce an adaptive fusion module that robustly aggregates
the local similarities into a global similarity score of pairwise images.
Furthermore, we speed up the retrieval process by developing a fast retrieval
strategy. Our experiments on the LineMOD, LineMOD-Occluded, and T-LESS datasets
show that our method yields a significantly better generalization to unseen
objects than previous works. Our code and pre-trained models are available at
https://sailor-z.github.io/projects/Unseen_Object_Pose.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A workflow for segmenting soil and plant X-ray CT images with deep learning in Googles Colaboratory. (arXiv:2203.09674v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09674">
<div class="article-summary-box-inner">
<span><p>X-ray micro-computed tomography (X-ray microCT) has enabled the
characterization of the properties and processes that take place in plants and
soils at the micron scale. Despite the widespread use of this advanced
technique, major limitations in both hardware and software limit the speed and
accuracy of image processing and data analysis. Recent advances in machine
learning, specifically the application of convolutional neural networks to
image analysis, have enabled rapid and accurate segmentation of image data.
Yet, challenges remain in applying convolutional neural networks to the
analysis of environmentally and agriculturally relevant images. Specifically,
there is a disconnect between the computer scientists and engineers, who build
these AI/ML tools, and the potential end users in agricultural research, who
may be unsure of how to apply these tools in their work. Additionally, the
computing resources required for training and applying deep learning models are
unique, more common to computer gaming systems or graphics design work, than to
traditional computational systems. To navigate these challenges, we developed a
modular workflow for applying convolutional neural networks to X-ray microCT
images, using low-cost resources in Googles Colaboratory web application. Here
we present the results of the workflow, illustrating how parameters can be
optimized to achieve best results using example scans from walnut leaves,
almond flower buds, and a soil aggregate. We expect that this framework will
accelerate the adoption and use of emerging deep learning techniques within the
plant and soil sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization by Mutual-Information Regularization with Pre-trained Models. (arXiv:2203.10789v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10789">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) aims to learn a generalized model to an unseen
target domain using only limited source domains. Previous attempts to DG fail
to learn domain-invariant representations only from the source domains due to
the significant domain shifts between training and test domains. Instead, we
re-formulate the DG objective using mutual information with the oracle model, a
model generalized to any possible domain. We derive a tractable variational
lower bound via approximating the oracle model by a pre-trained model, called
Mutual Information Regularization with Oracle (MIRO). Our extensive experiments
show that MIRO significantly improves the out-of-distribution performance.
Furthermore, our scaling experiments show that the larger the scale of the
pre-trained model, the greater the performance improvement of MIRO. Source code
is available at https://github.com/kakaobrain/miro.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Portrait Delighting. (arXiv:2203.12088v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12088">
<div class="article-summary-box-inner">
<span><p>We present a deep neural network for removing undesirable shading features
from an unconstrained portrait image, recovering the underlying texture. Our
training scheme incorporates three regularization strategies: masked loss, to
emphasize high-frequency shading features; soft-shadow loss, which improves
sensitivity to subtle changes in lighting; and shading-offset estimation, to
supervise separation of shading and texture. Our method demonstrates improved
delighting quality and generalization when compared with the state-of-the-art.
We further demonstrate how our delighting method can enhance the performance of
light-sensitive computer vision tasks such as face relighting and semantic
parsing, allowing them to handle extreme lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What to Hide from Your Students: Attention-Guided Masked Image Modeling. (arXiv:2203.12719v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12719">
<div class="article-summary-box-inner">
<span><p>Transformers and masked language modeling are quickly being adopted and
explored in computer vision as vision transformers and masked image modeling
(MIM). In this work, we argue that image token masking differs from token
masking in text, due to the amount and correlation of tokens in an image. In
particular, to generate a challenging pretext task for MIM, we advocate a shift
from random masking to informed masking. We develop and exhibit this idea in
the context of distillation-based MIM, where a teacher transformer encoder
generates an attention map, which we use to guide masking for the student. We
thus introduce a novel masking strategy, called attention-guided masking
(AttMask), and we demonstrate its effectiveness over random masking for dense
distillation-based MIM as well as plain distillation-based self-supervised
learning on classification tokens. We confirm that AttMask accelerates the
learning process and improves the performance on a variety of downstream tasks.
We provide the implementation code at https://github.com/gkakogeorgiou/attmask.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Video-centralised Transformer for Video Face Clustering. (arXiv:2203.13166v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13166">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel method for face clustering in videos using a
video-centralised transformer. Previous works often employed contrastive
learning to learn frame-level representation and used average pooling to
aggregate the features along the temporal dimension. This approach may not
fully capture the complicated video dynamics. In addition, despite the recent
progress in video-based contrastive learning, few have attempted to learn a
self-supervised clustering-friendly face representation that benefits the video
face clustering task. To overcome these limitations, our method employs a
transformer to directly learn video-level representations that can better
reflect the temporally-varying property of faces in videos, while we also
propose a video-centralised self-supervised framework to train the transformer
model. We also investigate face clustering in egocentric videos, a
fast-emerging field that has not been studied yet in works related to face
clustering. To this end, we present and release the first large-scale
egocentric video face clustering dataset named EasyCom-Clustering. We evaluate
our proposed method on both the widely used Big Bang Theory (BBT) dataset and
the new EasyCom-Clustering dataset. Results show the performance of our
video-centralised transformer has surpassed all previous state-of-the-art
methods on both benchmarks, exhibiting a self-attentive understanding of face
videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mc-BEiT: Multi-choice Discretization for Image BERT Pre-training. (arXiv:2203.15371v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15371">
<div class="article-summary-box-inner">
<span><p>Image BERT pre-training with masked image modeling (MIM) becomes a popular
practice to cope with self-supervised representation learning. A seminal work,
BEiT, casts MIM as a classification task with a visual vocabulary, tokenizing
the continuous visual signals into discrete vision tokens using a pre-learned
dVAE. Despite a feasible solution, the improper discretization hinders further
improvements of image pre-training. Since image discretization has no
ground-truth answers, we believe that the masked patch should not be assigned
with a unique token id even if a better ``tokenizer'' can be obtained. In this
work, we introduce an improved BERT-style image pre-training method, namely
mc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice
training objectives. Specifically, the multi-choice supervision for the masked
image patches is formed by the soft probability vectors of the discrete token
ids, which are predicted by the off-the-shelf image ``tokenizer'' and further
refined by high-level inter-patch perceptions resorting to the observation that
similar patches should share their choices. Extensive experiments on
classification, segmentation, and detection tasks demonstrate the superiority
of our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning
accuracy on ImageNet-1K classification, 49.2% AP^b and 44.0% AP^m of object
detection and instance segmentation on COCO, 50.8% mIOU on ADE20K semantic
segmentation, outperforming the competitive counterparts. The code will be
available at https://github.com/lixiaotong97/mc-BEiT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images. (arXiv:2204.01341v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01341">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel pixel interval down-sampling network (PID-Net)
for dense tiny object (yeast cells) counting tasks with higher accuracy. The
PID-Net is an end-to-end convolutional neural network (CNN) model with an
encoder--decoder architecture. The pixel interval down-sampling operations are
concatenated with max-pooling operations to combine the sparse and dense
features. This addresses the limitation of contour conglutination of dense
objects while counting. The evaluation was conducted using classical
segmentation metrics (the Dice, Jaccard and Hausdorff distance) as well as
counting metrics. The experimental results show that the proposed PID-Net had
the best performance and potential for dense tiny object counting tasks, which
achieved 96.97\% counting accuracy on the dataset with 2448 yeast cell images.
By comparing with the state-of-the-art approaches, such as Attention U-Net,
Swin U-Net and Trans U-Net, the proposed PID-Net can segment dense tiny objects
with clearer boundaries and fewer incorrect debris, which shows the great
potential of PID-Net in the task of accurate counting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHORE: Contact, Human and Object REconstruction from a single RGB image. (arXiv:2204.02445v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02445">
<div class="article-summary-box-inner">
<span><p>Most prior works in perceiving 3D humans from images reason human in
isolation without their surroundings. However, humans are constantly
interacting with the surrounding objects, thus calling for models that can
reason about not only the human but also the object and their interaction. The
problem is extremely challenging due to heavy occlusions between humans and
objects, diverse interaction types and depth ambiguity. In this paper, we
introduce CHORE, a novel method that learns to jointly reconstruct the human
and the object from a single RGB image. CHORE takes inspiration from recent
advances in implicit surface learning and classical model-based fitting. We
compute a neural reconstruction of human and object represented implicitly with
two unsigned distance fields, a correspondence field to a parametric body and
an object pose field. This allows us to robustly fit a parametric body model
and a 3D object template, while reasoning about interactions. Furthermore,
prior pixel-aligned implicit learning methods use synthetic data and make
assumptions that are not met in the real data. We propose a elegant depth-aware
scaling that allows more efficient shape learning on real data. Experiments
show that our joint reconstruction learned with the proposed strategy
significantly outperforms the SOTA. Our code and models are available at
https://virtualhumans.mpi-inf.mpg.de/chore
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stripformer: Strip Transformer for Fast Image Deblurring. (arXiv:2204.04627v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04627">
<div class="article-summary-box-inner">
<span><p>Images taken in dynamic scenes may contain unwanted motion blur, which
significantly degrades visual quality. Such blur causes short- and long-range
region-specific smoothing artifacts that are often directional and non-uniform,
which is difficult to be removed. Inspired by the current success of
transformers on computer vision and image processing tasks, we develop,
Stripformer, a transformer-based architecture that constructs intra- and
inter-strip tokens to reweight image features in the horizontal and vertical
directions to catch blurred patterns with different orientations. It stacks
interlaced intra-strip and inter-strip attention layers to reveal blur
magnitudes. In addition to detecting region-specific blurred patterns of
various orientations and magnitudes, Stripformer is also a token-efficient and
parameter-efficient transformer model, demanding much less memory usage and
computation cost than the vanilla transformer but works better without relying
on tremendous training data. Experimental results show that Stripformer
performs favorably against state-of-the-art models in dynamic scene deblurring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEMOS: Generating diverse human motions from textual descriptions. (arXiv:2204.14109v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14109">
<div class="article-summary-box-inner">
<span><p>We address the problem of generating diverse 3D human motions from textual
descriptions. This challenging task requires joint modeling of both modalities:
understanding and extracting useful human-centric information from the text,
and then generating plausible and realistic sequences of human poses. In
contrast to most previous work which focuses on generating a single,
deterministic, motion from a textual description, we design a variational
approach that can produce multiple diverse human motions. We propose TEMOS, a
text-conditioned generative model leveraging variational autoencoder (VAE)
training with human motion data, in combination with a text encoder that
produces distribution parameters compatible with the VAE latent space. We show
the TEMOS framework can produce both skeleton-based animations as in prior
work, as well more expressive SMPL body motions. We evaluate our approach on
the KIT Motion-Language benchmark and, despite being relatively
straightforward, demonstrate significant improvements over the state of the
art. Code and models are available on our webpage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Detection of Unknown Objects on Roads for Autonomous Driving. (arXiv:2205.01414v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01414">
<div class="article-summary-box-inner">
<span><p>Tremendous progress in deep learning over the last years has led towards a
future with autonomous vehicles on our roads. Nevertheless, the performance of
their perception systems is strongly dependent on the quality of the utilized
training data. As these usually only cover a fraction of all object classes an
autonomous driving system will face, such systems struggle with handling the
unexpected. In order to safely operate on public roads, the identification of
objects from unknown classes remains a crucial task. In this paper, we propose
a novel pipeline to detect unknown objects. Instead of focusing on a single
sensor modality, we make use of lidar and camera data by combining state-of-the
art detection models in a sequential manner. We evaluate our approach on the
Waymo Open Perception Dataset and point out current research gaps in anomaly
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers. (arXiv:2205.03436v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03436">
<div class="article-summary-box-inner">
<span><p>Self-attention based models such as vision transformers (ViTs) have emerged
as a very competitive architecture alternative to convolutional neural networks
(CNNs) in computer vision. Despite increasingly stronger variants with
ever-higher recognition accuracies, due to the quadratic complexity of
self-attention, existing ViTs are typically demanding in computation and model
size. Although several successful design choices (e.g., the convolutions and
hierarchical multi-stage structure) of prior CNNs have been reintroduced into
recent ViTs, they are still not sufficient to meet the limited resource
requirements of mobile devices. This motivates a very recent attempt to develop
light ViTs based on the state-of-the-art MobileNet-v2, but still leaves a
performance gap behind. In this work, pushing further along this under-studied
direction we introduce EdgeViTs, a new family of light-weight ViTs that, for
the first time, enable attention-based vision models to compete with the best
light-weight CNNs in the tradeoff between accuracy and on-device efficiency.
This is realized by introducing a highly cost-effective local-global-local
(LGL) information exchange bottleneck based on optimal integration of
self-attention and convolutions. For device-dedicated evaluation, rather than
relying on inaccurate proxies like the number of FLOPs or parameters, we adopt
a practical approach of focusing directly on on-device latency and, for the
first time, energy efficiency. Specifically, we show that our models are
Pareto-optimal when both accuracy-latency and accuracy-energy trade-offs are
considered, achieving strict dominance over other ViTs in almost all cases and
competing with the most efficient CNNs. Code is available at
https://github.com/saic-fi/edgevit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Class-incremental Learning for 3D Point Cloud Objects. (arXiv:2205.15225v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15225">
<div class="article-summary-box-inner">
<span><p>Few-shot class-incremental learning (FSCIL) aims to incrementally fine-tune a
model (trained on base classes) for a novel set of classes using a few examples
without forgetting the previous training. Recent efforts address this problem
primarily on 2D images. However, due to the advancement of camera technology,
3D point cloud data has become more available than ever, which warrants
considering FSCIL on 3D data. This paper addresses FSCIL in the 3D domain. In
addition to well-known issues of catastrophic forgetting of past knowledge and
overfitting of few-shot data, 3D FSCIL can bring newer challenges. For example,
base classes may contain many synthetic instances in a realistic scenario. In
contrast, only a few real-scanned samples (from RGBD sensors) of novel classes
are available in incremental steps. Due to the data variation from synthetic to
real, FSCIL endures additional challenges, degrading performance in later
incremental steps. We attempt to solve this problem using Microshapes
(orthogonal basis vectors) by describing any 3D objects using a pre-defined set
of rules. It supports incremental training with few-shot examples minimizing
synthetic to real data variation. We propose new test protocols for 3D FSCIL
using popular synthetic datasets (ModelNet and ShapeNet) and 3D real-scanned
datasets (ScanObjectNN and CO3D). By comparing state-of-the-art methods, we
establish the effectiveness of our approach in the 3D domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01191">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have shown rapid progress in computer vision tasks,
achieving promising results on various benchmarks. However, due to the massive
number of parameters and model design, e.g., attention mechanism, ViT-based
models are generally times slower than lightweight convolutional networks.
Therefore, the deployment of ViT for real-time applications is particularly
challenging, especially on resource-constrained hardware such as mobile
devices. Recent efforts try to reduce the computation complexity of ViT through
network architecture search or hybrid design with MobileNet block, yet the
inference speed is still unsatisfactory. This leads to an important question:
can transformers run as fast as MobileNet while obtaining high performance? To
answer this, we first revisit the network architecture and operators used in
ViT-based models and identify inefficient designs. Then we introduce a
dimension-consistent pure transformer (without MobileNet blocks) as a design
paradigm. Finally, we perform latency-driven slimming to get a series of final
models dubbed EfficientFormer. Extensive experiments show the superiority of
EfficientFormer in performance and speed on mobile devices. Our fastest model,
EfficientFormer-L1, achieves $79.2\%$ top-1 accuracy on ImageNet-1K with only
$1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which { runs as
fast as MobileNetV2$\times 1.4$ ($1.6$ ms, $74.7\%$ top-1),} and our largest
model, EfficientFormer-L7, obtains $83.3\%$ accuracy with only $7.0$ ms
latency. Our work proves that properly designed transformers can reach
extremely low latency on mobile devices while maintaining high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01612">
<div class="article-summary-box-inner">
<span><p>We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python
library of eXplainable AI (XAI), which offers omni-way explainable AI
capabilities and various interpretable machine learning techniques to address
the pain points of understanding and interpreting the decisions made by machine
learning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library
that makes explainable AI easy for data scientists, ML researchers and
practitioners who need explanation for various types of data, models and
explanation methods at different stages of ML process (data exploration,
feature engineering, model development, evaluation, and decision-making, etc).
In particular, our library includes a rich family of explanation methods
integrated in a unified interface, which supports multiple data types (tabular
data, images, texts, time-series), multiple types of ML models (traditional ML
in Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of
diverse explanation methods including "model-specific" and "model-agnostic"
ones (such as feature-attribution explanation, counterfactual explanation,
gradient-based explanation, etc). For practitioners, the library provides an
easy-to-use unified interface to generate the explanations for their
applications by only writing a few lines of codes, and also a GUI dashboard for
visualization of different explanations for more insights about decisions. In
this technical report, we present OmniXAI's design principles, system
architectures, and major functionalities, and also demonstrate several example
use cases across different types of data, tasks, and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2206.10878v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10878">
<div class="article-summary-box-inner">
<span><p>Whole slide image (WSI) classification is a fundamental task for the
diagnosis and treatment of diseases; but, curation of accurate labels is
time-consuming and limits the application of fully-supervised methods. To
address this, multiple instance learning (MIL) is a popular method that poses
classification as a weakly supervised learning task with slide-level labels
only. While current MIL methods apply variants of the attention mechanism to
re-weight instance features with stronger models, scant attention is paid to
the properties of the data distribution. In this work, we propose to
re-calibrate the distribution of a WSI bag (instances) by using the statistics
of the max-instance (critical) feature. We assume that in binary MIL, positive
bags have larger feature magnitudes than negatives, thus we can enforce the
model to maximize the discrepancy between bags with a metric feature loss that
models positive bags as out-of-distribution. To achieve this, unlike existing
MIL methods that use single-batch training modes, we propose balanced-batch
sampling to effectively use the feature loss i.e., (+/-) bags simultaneously.
Further, we employ a position encoding module (PEM) to model
spatial/morphological information, and perform pooling by multi-head
self-attention (PSMA) with a Transformer encoder. Experimental results on
existing benchmark datasets show our approach is effective and improves over
state-of-the-art MIL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning approach for Classifying Trusses and Runners of Strawberries. (arXiv:2207.02721v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02721">
<div class="article-summary-box-inner">
<span><p>The use of artificial intelligence in the agricultural sector has been
growing at a rapid rate to automate farming activities. Emergent farming
technologies focus on mapping and classification of plants, fruits, diseases,
and soil types. Although, assisted harvesting and pruning applications using
deep learning algorithms are in the early development stages, there is a demand
for solutions to automate such processes. This paper proposes the use of Deep
Learning for the classification of trusses and runners of strawberry plants
using semantic segmentation and dataset augmentation. The proposed approach is
based on the use of noises (i.e. Gaussian, Speckle, Poisson and
Salt-and-Pepper) to artificially augment the dataset and compensate the low
number of data samples and increase the overall classification performance. The
results are evaluated using mean average of precision, recall and F1 score. The
proposed approach achieved 91%, 95% and 92% on precision, recall and F1 score,
respectively, for truss detection using the ResNet101 with dataset augmentation
utilising Salt-and-Pepper noise; and 83%, 53% and 65% on precision, recall and
F1 score, respectively, for truss detection using the ResNet50 with dataset
augmentation utilising Poisson noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Video Compression via Heterogeneous Deformable Compensation Network. (arXiv:2207.04589v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04589">
<div class="article-summary-box-inner">
<span><p>Learned video compression has recently emerged as an essential research topic
in developing advanced video compression technologies, where motion
compensation is considered one of the most challenging issues. In this paper,
we propose a learned video compression framework via heterogeneous deformable
compensation strategy (HDCVC) to tackle the problems of unstable compression
performance caused by single-size deformable kernels in downsampled feature
domain. More specifically, instead of utilizing optical flow warping or
single-size-kernel deformable alignment, the proposed algorithm extracts
features from the two adjacent frames to estimate content-adaptive
heterogeneous deformable (HetDeform) kernel offsets. Then we transform the
reference features with the HetDeform convolution to accomplish motion
compensation. Moreover, we design a Spatial-Neighborhood-Conditioned Divisive
Normalization (SNCDN) to achieve more effective data Gaussianization combined
with the Generalized Divisive Normalization. Furthermore, we propose a
multi-frame enhanced reconstruction module for exploiting context and temporal
information for final quality enhancement. Experimental results indicate that
HDCVC achieves superior performance than the recent state-of-the-art learned
video compression approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Average Precision Training for Pertinent Image Retrieval. (arXiv:2207.04873v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04873">
<div class="article-summary-box-inner">
<span><p>Image Retrieval is commonly evaluated with Average Precision (AP) or
Recall@k. Yet, those metrics, are limited to binary labels and do not take into
account errors' severity. This paper introduces a new hierarchical AP training
method for pertinent image retrieval (HAP-PIER). HAPPIER is based on a new H-AP
metric, which leverages a concept hierarchy to refine AP by integrating errors'
importance and better evaluate rankings. To train deep models with H-AP, we
carefully study the problem's structure and design a smooth lower bound
surrogate combined with a clustering loss that ensures consistent ordering.
Extensive experiments on 6 datasets show that HAPPIER significantly outperforms
state-of-the-art methods for hierarchical retrieval, while being on par with
the latest approaches when evaluating fine-grained ranking performances.
Finally, we show that HAPPIER leads to better organization of the embedding
space, and prevents most severe failure cases of non-hierarchical methods. Our
code is publicly available at: https://github.com/elias-ramzi/HAPPIER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Transformer for Traffic Sign Detection. (arXiv:2207.06067v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06067">
<div class="article-summary-box-inner">
<span><p>Traffic sign detection is a vital task in the visual system of self-driving
cars and the automated driving system. Recently, novel Transformer-based models
have achieved encouraging results for various computer vision tasks. We still
observed that vanilla ViT could not yield satisfactory results in traffic sign
detection because the overall size of the datasets is very small and the class
distribution of traffic signs is extremely unbalanced. To overcome this
problem, a novel Pyramid Transformer with locality mechanisms is proposed in
this paper. Specifically, Pyramid Transformer has several spatial pyramid
reduction layers to shrink and embed the input image into tokens with rich
multi-scale context by using atrous convolutions. Moreover, it inherits an
intrinsic scale invariance inductive bias and is able to learn local feature
representation for objects at various scales, thereby enhancing the network
robustness against the size discrepancy of traffic signs. The experiments are
conducted on the German Traffic Sign Detection Benchmark (GTSDB). The results
demonstrate the superiority of the proposed model in the traffic sign detection
tasks. More specifically, Pyramid Transformer achieves 77.8% mAP on GTSDB when
applied to the Cascade RCNN as the backbone, which surpasses most well-known
and widely-used state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarially-Aware Robust Object Detector. (arXiv:2207.06202v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06202">
<div class="article-summary-box-inner">
<span><p>Object detection, as a fundamental computer vision task, has achieved a
remarkable progress with the emergence of deep neural networks. Nevertheless,
few works explore the adversarial robustness of object detectors to resist
adversarial attacks for practical applications in various real-world scenarios.
Detectors have been greatly challenged by unnoticeable perturbation, with sharp
performance drop on clean images and extremely poor performance on adversarial
images. In this work, we empirically explore the model training for adversarial
robustness in object detection, which greatly attributes to the conflict
between learning clean images and adversarial images. To mitigate this issue,
we propose a Robust Detector (RobustDet) based on adversarially-aware
convolution to disentangle gradients for model learning on clean and
adversarial images. RobustDet also employs the Adversarial Image Discriminator
(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable
robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that
our model effectively disentangles gradients and significantly enhances the
detection robustness with maintaining the detection ability on clean images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sample-dependent Adaptive Temperature Scaling for Improved Calibration. (arXiv:2207.06211v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06211">
<div class="article-summary-box-inner">
<span><p>It is now well known that neural networks can be wrong with high confidence
in their predictions, leading to poor calibration. The most common post-hoc
approach to compensate for this is to perform temperature scaling, which
adjusts the confidences of the predictions on any input by scaling the logits
by a fixed value. Whilst this approach typically improves the average
calibration across the whole test dataset, this improvement typically reduces
the individual confidences of the predictions irrespective of whether the
classification of a given input is correct or incorrect. With this insight, we
base our method on the observation that different samples contribute to the
calibration error by varying amounts, with some needing to increase their
confidence and others needing to decrease it. Therefore, for each input, we
propose to predict a different temperature value, allowing us to adjust the
mismatch between confidence and accuracy at a finer granularity. Furthermore,
we observe improved results on OOD detection and can also extract a notion of
hardness for the data-points. Our method is applied post-hoc, consequently
using very little computation time and with a negligible memory footprint and
is applied to off-the-shelf pre-trained classifiers. We test our method on the
ResNet50 and WideResNet28-10 architectures using the CIFAR10/100 and
Tiny-ImageNet datasets, showing that producing per-data-point temperatures is
beneficial also for the expected calibration error across the whole test set.
Code is available at: https://github.com/thwjoy/adats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHREC 2022 Track on Online Detection of Heterogeneous Gestures. (arXiv:2207.06706v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06706">
<div class="article-summary-box-inner">
<span><p>This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDM:Visual Explanations for Neural Networks via Multiple Dynamic Mask. (arXiv:2207.08046v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08046">
<div class="article-summary-box-inner">
<span><p>The active region lookup of a neural network tells us which regions the
neural network focuses on when making a decision, which gives us a basis for
interpretability when the neural network makes a classification decision. We
propose an algorithm Multiple Dynamic Mask(MDM), which is a general saliency
graph query method with interpretability of the inference process. Its proposal
is based on an assumption: when a picture is input to a neural network that has
been trained, the activation features related to classification will affect the
classification results of the neural network, and the features unrelated to
classification will hardly affect the classification results of the network.
MDM: A learning-based end-to-end algorithm for finding regions of interest for
neural network classification. It has the following advantages: 1. It has the
interpretability of the reasoning process. 2. It is universal, it can be used
for any neural network and does not depend on the internal structure of the
neural network. 3. The search performance is better. Because the algorithm is
based on learning to generate masks and has the ability to adapt to different
data and networks, the performance is better than the method proposed in the
previous paper. For the MDM saliency map search algorithm, we experimentally
compared the performance indicators of various saliency map search methods and
the MDM with ResNet and DenseNet as the trained neural networks. The search
effect performance of the MDM reached the state of the art. We applied the MDM
to the interpretable neural network ProtoPNet and XProtoNet, which improved the
interpretability of the model and the prototype search performance. We
visualize the performance of convolutional neural architecture and Transformer
architecture on saliency map search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection. (arXiv:2207.08531v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08531">
<div class="article-summary-box-inner">
<span><p>Monocular 3D detection has drawn much attention from the community due to its
low cost and setup simplicity. It takes an RGB image as input and predicts 3D
boxes in the 3D space. The most challenging sub-task lies in the instance depth
estimation. Previous works usually use a direct estimation method. However, in
this paper we point out that the instance depth on the RGB image is
non-intuitive. It is coupled by visual depth clues and instance attribute
clues, making it hard to be directly learned in the network. Therefore, we
propose to reformulate the instance depth to the combination of the instance
visual surface depth (visual depth) and the instance attribute depth (attribute
depth). The visual depth is related to objects' appearances and positions on
the image. By contrast, the attribute depth relies on objects' inherent
attributes, which are invariant to the object affine transformation on the
image. Correspondingly, we decouple the 3D location uncertainty into visual
depth uncertainty and attribute depth uncertainty. By combining different types
of depths and associated uncertainties, we can obtain the final instance depth.
Furthermore, data augmentation in monocular 3D detection is usually limited due
to the physical nature, hindering the boost of performance. Based on the
proposed instance depth disentanglement strategy, we can alleviate this
problem. Evaluated on KITTI, our method achieves new state-of-the-art results,
and extensive ablation studies validate the effectiveness of each component in
our method. The codes are released at https://github.com/SPengLiang/DID-M3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latency-Aware Collaborative Perception. (arXiv:2207.08560v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08560">
<div class="article-summary-box-inner">
<span><p>Collaborative perception has recently shown great potential to improve
perception capabilities over single-agent perception. Existing collaborative
perception methods usually consider an ideal communication environment.
However, in practice, the communication system inevitably suffers from latency
issues, causing potential performance degradation and high risks in
safety-critical applications, such as autonomous driving. To mitigate the
effect caused by the inevitable latency, from a machine learning perspective,
we present the first latency-aware collaborative perception system, which
actively adapts asynchronous perceptual features from multiple agents to the
same time stamp, promoting the robustness and effectiveness of collaboration.
To achieve such a feature-level synchronization, we propose a novel latency
compensation module, called SyncNet, which leverages feature-attention
symbiotic estimation and time modulation techniques. Experiments results show
that the proposed latency aware collaborative perception system with SyncNet
can outperforms the state-of-the-art collaborative perception method by 15.6%
in the communication latency scenario and keep collaborative perception being
superior to single agent perception under severe latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AiATrack: Attention in Attention for Transformer Visual Tracking. (arXiv:2207.09603v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09603">
<div class="article-summary-box-inner">
<span><p>Transformer trackers have achieved impressive advancements recently, where
the attention mechanism plays an important role. However, the independent
correlation computation in the attention mechanism could result in noisy and
ambiguous attention weights, which inhibits further performance improvement. To
address this issue, we propose an attention in attention (AiA) module, which
enhances appropriate correlations and suppresses erroneous ones by seeking
consensus among all correlation vectors. Our AiA module can be readily applied
to both self-attention blocks and cross-attention blocks to facilitate feature
aggregation and information propagation for visual tracking. Moreover, we
propose a streamlined Transformer tracking framework, dubbed AiATrack, by
introducing efficient feature reuse and target-background embeddings to make
full use of temporal references. Experiments show that our tracker achieves
state-of-the-art performance on six tracking benchmarks while running at a
real-time speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERA: Expert Retrieval and Assembly for Early Action Prediction. (arXiv:2207.09675v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09675">
<div class="article-summary-box-inner">
<span><p>Early action prediction aims to successfully predict the class label of an
action before it is completely performed. This is a challenging task because
the beginning stages of different actions can be very similar, with only minor
subtle differences for discrimination. In this paper, we propose a novel Expert
Retrieval and Assembly (ERA) module that retrieves and assembles a set of
experts most specialized at using discriminative subtle differences, to
distinguish an input sample from other highly similar samples. To encourage our
model to effectively use subtle differences for early action prediction, we
push experts to discriminate exclusively between samples that are highly
similar, forcing these experts to learn to use subtle differences that exist
between those samples. Additionally, we design an effective Expert Learning
Rate Optimization method that balances the experts' optimization and leads to
better performance. We evaluate our ERA module on four public action datasets
and achieve state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AU-Supervised Convolutional Vision Transformers for Synthetic Facial Expression Recognition. (arXiv:2207.09777v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09777">
<div class="article-summary-box-inner">
<span><p>The paper describes our proposed methodology for the six basic expression
classification track of Affective Behavior Analysis in-the-wild (ABAW)
Competition 2022. In Learing from Synthetic Data(LSD) task, facial expression
recognition (FER) methods aim to learn the representation of expression from
the artificially generated data and generalise to real data. Because of the
ambiguous of the synthetic data and the objectivity of the facial Action Unit
(AU), we resort to the AU information for performance boosting, and make
contributions as follows. First, to adapt the model to synthetic scenarios, we
use the knowledge from pre-trained large-scale face recognition data. Second,
we propose a conceptually-new framework, termed as AU-Supervised Convolutional
Vision Transformers (AU-CVT), which clearly improves the performance of FER by
jointly training auxiliary datasets with AU or pseudo AU labels. Our AU-CVT
achieved F1 score as $0.6863$, accuracy as $0.7433$ on the validation set. The
source code of our work is publicly available online:
https://github.com/msy1412/ABAW4
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09933">
<div class="article-summary-box-inner">
<span><p>In clinical procedures of angioplasty (i.e., open clogged coronary arteries),
devices such as balloons and stents need to be placed and expanded in arteries
under the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,
the resulting images are often noisy. To check the correct placement of these
devices, typically multiple motion-compensated frames are averaged to enhance
the view. Therefore, device tracking is a necessary procedure for this purpose.
Even though angioplasty devices are designed to have radiopaque markers for the
ease of tracking, current methods struggle to deliver satisfactory results due
to the small marker size and complex scenes in angioplasty. In this paper, we
propose an end-to-end deep learning framework for single stent tracking, which
consists of three hierarchical modules: U-Net based landmark detection, ResNet
based stent proposal and feature extraction, and graph convolutional neural
network (GCN) based stent tracking that temporally aggregates both spatial
information and appearance features. The experiments show that our method
performs significantly better in detection compared with the state-of-the-art
point-based tracking models. In addition, its fast inference speed satisfies
clinical requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10106">
<div class="article-summary-box-inner">
<span><p>Tidying up a household environment using a mobile manipulator poses various
challenges in robotics, such as adaptation to large real-world environmental
variations, and safe and robust deployment in the presence of humans.The
Partner Robot Challenge in World Robot Challenge (WRC) 2020, a global
competition held in September 2021, benchmarked tidying tasks in the real home
environments, and importantly, tested for full system performances.For this
challenge, we developed an entire household service robot system, which
leverages a data-driven approach to adapt to numerous edge cases that occur
during the execution, instead of classical manual pre-programmed solutions. In
this paper, we describe the core ingredients of the proposed robot system,
including visual recognition, object manipulation, and motion planning. Our
robot system won the second prize, verifying the effectiveness and potential of
data-driven robot systems for mobile manipulation in home environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis. (arXiv:2207.10120v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10120">
<div class="article-summary-box-inner">
<span><p>Generative models for audio-conditioned dance motion synthesis map music
features to dance movements. Models are trained to associate motion patterns to
audio patterns, usually without an explicit knowledge of the human body. This
approach relies on a few assumptions: strong music-dance correlation,
controlled motion data and relatively simple poses and movements. These
characteristics are found in all existing datasets for dance motion synthesis,
and indeed recent methods can achieve good results.We introduce a new dataset
aiming to challenge these common assumptions, compiling a set of dynamic dance
sequences displaying complex human poses. We focus on breakdancing which
features acrobatic moves and tangled postures. We source our data from the Red
Bull BC One competition videos. Estimating human keypoints from these videos is
difficult due to the complexity of the dance, as well as the multiple moving
cameras recording setup. We adopt a hybrid labelling pipeline leveraging deep
estimation models as well as manual annotations to obtain good quality keypoint
sequences at a reduced cost. Our efforts produced the BRACE dataset, which
contains over 3 hours and 30 minutes of densely annotated poses. We test
state-of-the-art methods on BRACE, showing their limitations when evaluated on
complex sequences. Our dataset can readily foster advance in dance motion
synthesis. With intricate poses and swift movements, models are forced to go
beyond learning a mapping between modalities and reason more effectively about
body structure and movements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Knowledge Tracing. (arXiv:2207.10157v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10157">
<div class="article-summary-box-inner">
<span><p>Each year, thousands of people learn new visual categorization tasks --
radiologists learn to recognize tumors, birdwatchers learn to distinguish
similar species, and crowd workers learn how to annotate valuable data for
applications like autonomous driving. As humans learn, their brain updates the
visual features it extracts and attend to, which ultimately informs their final
classification decisions. In this work, we propose a novel task of tracing the
evolving classification behavior of human learners as they engage in
challenging visual classification tasks. We propose models that jointly extract
the visual features used by learners as well as predicting the classification
functions they utilize. We collect three challenging new datasets from real
human learners in order to evaluate the performance of different visual
knowledge tracing methods. Our results show that our recurrent models are able
to predict the classification behavior of human learners on three challenging
medical image and species identification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles. (arXiv:2207.10172v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10172">
<div class="article-summary-box-inner">
<span><p>Video Anomaly Detection (VAD) is an important topic in computer vision.
Motivated by the recent advances in self-supervised learning, this paper
addresses VAD by solving an intuitive yet challenging pretext task, i.e.,
spatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained
classification problem. Our method exhibits several advantages over existing
works: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial
and temporal dimensions, responsible for capturing highly discriminative
appearance and motion features, respectively; 2) full permutations are used to
provide abundant jigsaw puzzles covering various difficulty levels, allowing
the network to distinguish subtle spatio-temporal differences between normal
and abnormal events; and 3) the pretext task is tackled in an end-to-end manner
without relying on any pre-trained models. Our method outperforms
state-of-the-art counterparts on three public benchmarks. Especially on
ShanghaiTech Campus, the result is superior to reconstruction and
prediction-based methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aware Fine-Grained Correspondence. (arXiv:2207.10456v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10456">
<div class="article-summary-box-inner">
<span><p>Establishing visual correspondence across images is a challenging and
essential task. Recently, an influx of self-supervised methods have been
proposed to better learn representations for visual correspondence. However, we
find that these methods often fail to leverage semantic information and
over-rely on the matching of low-level features. In contrast, human vision is
capable of distinguishing between distinct objects as a pretext to tracking.
Inspired by this paradigm, we propose to learn semantic-aware fine-grained
correspondence. Firstly, we demonstrate that semantic correspondence is
implicitly available through a rich set of image-level self-supervised methods.
We further design a pixel-level self-supervised learning objective which
specifically targets fine-grained correspondence. For downstream tasks, we fuse
these two kinds of complementary correspondence representations together,
demonstrating that they boost performance synergistically. Our method surpasses
previous state-of-the-art self-supervised methods using convolutional networks
on a variety of visual correspondence tasks, including video object
segmentation, human pose tracking, and human part tracking.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-25 23:08:08.686655711 UTC">2022-07-25 23:08:08 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>