<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-22T01:30:00Z">07-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The Birth of Bias: A case study on the evolution of gender bias in an English language model. (arXiv:2207.10245v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10245">
<div class="article-summary-box-inner">
<span><p>Detecting and mitigating harmful biases in modern language models are widely
recognized as crucial, open problems. In this paper, we take a step back and
investigate how language models come to be biased in the first place. We use a
relatively small language model, using the LSTM architecture trained on an
English Wikipedia corpus. With full access to the data and to the model
parameters as they change during every step while training, we can map in
detail how the representation of gender develops, what patterns in the dataset
drive this, and how the model's internal state relates to the bias in a
downstream task (semantic textual similarity). We find that the representation
of gender is dynamic and identify different phases during training.
Furthermore, we show that gender information is represented increasingly
locally in the input embeddings of the model and that, as a consequence,
debiasing these can be effective in reducing the downstream bias. Monitoring
the training dynamics, allows us to detect an asymmetry in how the female and
male gender are represented in the input embeddings. This is important, as it
may cause naive mitigation strategies to introduce new undesirable biases. We
discuss the relevance of the findings for mitigation strategies more generally
and the prospects of generalizing our methods to larger language models, the
Transformer architecture, other languages and other undesirable biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi Resolution Analysis (MRA) for Approximate Self-Attention. (arXiv:2207.10284v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10284">
<div class="article-summary-box-inner">
<span><p>Transformers have emerged as a preferred model for many tasks in natural
langugage processing and vision. Recent efforts on training and deploying
Transformers more efficiently have identified many strategies to approximate
the self-attention matrix, a key module in a Transformer architecture.
Effective ideas include various prespecified sparsity patterns, low-rank basis
expansions and combinations thereof. In this paper, we revisit classical
Multiresolution Analysis (MRA) concepts such as Wavelets, whose potential value
in this setting remains underexplored thus far. We show that simple
approximations based on empirical feedback and design choices informed by
modern hardware and implementation challenges, eventually yield a MRA-based
approach for self-attention with an excellent performance profile across most
criteria of interest. We undertake an extensive set of experiments and
demonstrate that this multi-resolution scheme outperforms most efficient
self-attention proposals and is favorable for both short and long sequences.
Code is available at \url{https://github.com/mlpen/mra-attention}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Visual Representations with Texts for Domain Generalization. (arXiv:2207.10285v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10285">
<div class="article-summary-box-inner">
<span><p>Reducing the representational discrepancy between source and target domains
is a key component to maximize the model generalization. In this work, we
advocate for leveraging natural language supervision for the domain
generalization task. We introduce two modules to ground visual representations
with texts containing typical reasoning of humans: (1) Visual and Textual Joint
Embedder and (2) Textual Explanation Generator. The former learns the
image-text joint embedding space where we can ground high-level
class-discriminative information into the model. The latter leverages an
explainable model and generates explanations justifying the rationale behind
its decision. To the best of our knowledge, this is the first work to leverage
the vision-and-language cross-modality approach for the domain generalization
task. Our experiments with a newly created CUB-DG benchmark dataset demonstrate
that cross-modality supervision can be successfully used to ground
domain-invariant visual representations and improve the model generalization.
Furthermore, in the large-scale DomainBed benchmark, our proposed method
achieves state-of-the-art results and ranks 1st in average performance for five
multi-domain datasets. The dataset and codes are available at
https://github.com/mswzeus/GVRT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Cascades. (arXiv:2207.10342v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10342">
<div class="article-summary-box-inner">
<span><p>Prompted models have demonstrated impressive few-shot learning abilities.
Repeated interactions at test-time with a single model, or the composition of
multiple models together, further expands capabilities. These compositions are
probabilistic models, and may be expressed in the language of graphical models
with random variables whose values are complex data types such as strings.
Cases with control flow and dynamic structure require techniques from
probabilistic programming, which allow implementing disparate model structures
and inference strategies in a unified language. We formalize several existing
techniques from this perspective, including scratchpads / chain of thought,
verifiers, STaR, selection-inference, and tool use. We refer to the resulting
programs as language model cascades.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeT: Code Generation with Generated Tests. (arXiv:2207.10397v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10397">
<div class="article-summary-box-inner">
<span><p>Given a programming problem, pre-trained language models such as Codex have
demonstrated the ability to generate multiple different code solutions via
sampling. However, selecting a correct or best solution from those samples
still remains a challenge. While an easy way to verify the correctness of a
code solution is through executing test cases, producing high-quality test
cases is prohibitively expensive. In this paper, we explore the use of
pre-trained language models to automatically generate test cases, calling our
method CodeT: Code generation with generated Tests. CodeT executes the code
solutions using the generated test cases, and then chooses the best solution
based on a dual execution agreement with both the generated test cases and
other generated solutions. We evaluate CodeT on five different pre-trained
models with both HumanEval and MBPP benchmarks. Extensive experimental results
demonstrate CodeT can achieve significant, consistent, and surprising
improvements over previous methods. For example, CodeT improves the pass@1 on
HumanEval to 65.8%, an increase of absolute 18.8% on the code-davinci-002
model, and an absolute 20+% improvement over previous state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian Languages. (arXiv:2207.10524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10524">
<div class="article-summary-box-inner">
<span><p>At the center of the underlying issues that halt Indonesian natural language
processing (NLP) research advancement, we find data scarcity. Resources in
Indonesian languages, especially the local ones, are extremely scarce and
underrepresented. Many Indonesian researchers do not publish their dataset.
Furthermore, the few public datasets that we have are scattered across
different platforms, thus makes performing reproducible and data-centric
research in Indonesian NLP even more arduous. Rising to this challenge, we
initiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd
strives to provide the largest datasheets aggregation with standardized data
loading for NLP tasks in all Indonesian languages. By enabling open and
centralized access to Indonesian NLP resources, we hope NusaCrowd can tackle
the data scarcity problem hindering NLP progress in Indonesia and bring NLP
practitioners to move towards collaboration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?. (arXiv:2207.10551v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10551">
<div class="article-summary-box-inner">
<span><p>There have been a lot of interest in the scaling properties of Transformer
models. However, not much has been done on the front of investigating the
effect of scaling properties of different inductive biases and model
architectures. Do model architectures scale differently? If so, how does
inductive bias affect scaling behaviour? How does this influence upstream
(pretraining) and downstream (transfer)? This paper conducts a systematic study
of scaling behaviour of ten diverse model architectures such as Transformers,
Switch Transformers, Universal Transformers, Dynamic convolutions, Performers,
and recently proposed MLP-Mixers. Via extensive experiments, we show that (1)
architecture is an indeed an important consideration when performing scaling
and (2) the best performing model can fluctuate at different scales. We believe
that the findings outlined in this work has significant implications to how
model architectures are currently evaluated in the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Reinforcement Learning-based Offensive semantics Censorship System for Chatbots. (arXiv:2207.10569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10569">
<div class="article-summary-box-inner">
<span><p>The rapid development of artificial intelligence (AI) technology has enabled
large-scale AI applications to land in the market and practice. However, while
AI technology has brought many conveniences to people in the productization
process, it has also exposed many security issues. Especially, attacks against
online learning vulnerabilities of chatbots occur frequently. Therefore, this
paper proposes a semantics censorship chatbot system based on reinforcement
learning, which is mainly composed of two parts: the Offensive semantics
censorship model and the semantics purification model. Offensive semantics
review can combine the context of user input sentences to detect the rapid
evolution of Offensive semantics and respond to Offensive semantics responses.
The semantics purification model For the case of chatting robot models, it has
been contaminated by large numbers of offensive semantics, by strengthening the
offensive reply learned by the learning algorithm, rather than rolling back to
the early versions. In addition, by integrating a once-through learning
approach, the speed of semantics purification is accelerated while reducing the
impact on the quality of replies. The experimental results show that our
proposed approach reduces the probability of the chat model generating
offensive replies and that the integration of the few-shot learning algorithm
improves the training speed rapidly while effectively slowing down the decline
in BLEU values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Big Data and Education: using big data analytics in language learning. (arXiv:2207.10572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10572">
<div class="article-summary-box-inner">
<span><p>Working with big data using data mining tools is rapidly becoming a trend in
education industry. The combination of the current capacity to collect, store,
manage and process data in a timely manner, and data from online educational
platforms represents an unprecedented opportunity for educational institutes,
learners, educators, and researchers. In this position paper, we consider some
basic concepts as well as most popular tools, methods and techniques regarding
Educational Data Mining and Learning Analytics, and discuss big data
applications in language learning, in particular.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Based Chatbot: An Approach of Utilizing On Customer Service Assistance. (arXiv:2207.10573v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10573">
<div class="article-summary-box-inner">
<span><p>Providing the best customer experience is one of the primary concerns for the
firms that are based online. The advancement of machine learning is
revolutionising the company's attitude towards the client through improving the
service quality by implementing chatbot solutions, which gives the user instant
and satisfactory answers to their enquiries. The acceptance of this technology
is increasing with the new improvements and efficiency of the chatbot system.
This thesis paper will cover the concept of chatbot system for the company, as
a use case we took AK traders Ltd. It involves the research work on various
chatbot technologies available and based on research, use them to develop a
chatbot system for the company. This system will work based on the text as a
conversational agent that can interact with humans by natural language. The
main objective project is to develop the chatbot solution that could comply
with complex questions and logical output answers in a well-defined approach.
The ultimate goal is to give high-quality results (answers) based on user input
(question). For the successful implementation of this project, we have
undertaken an in-depth analysis of the various machine learning techniques
available and followed well-structured implementation to figure out the best
solution for the company. The primary concern of this project includes natural
language processing (NLP), machine learning and the vector space model (VSM).
The outcome of the project shows the problem-solving technique for the
implementation of the chatbot system for the company at a reasonable quality
level
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Democratizing Ethical Assessment of Natural Language Generation Models. (arXiv:2207.10576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10576">
<div class="article-summary-box-inner">
<span><p>Natural language generation models are computer systems that generate
coherent language when prompted with a sequence of words as context. Despite
their ubiquity and many beneficial applications, language generation models
also have the potential to inflict social harms by generating discriminatory
language, hateful speech, profane content, and other harmful material. Ethical
assessment of these models is therefore critical. But it is also a challenging
task, requiring an expertise in several specialized domains, such as
computational linguistics and social justice. While significant strides have
been made by the research community in this domain, accessibility of such
ethical assessments to the wider population is limited due to the high entry
barriers. This article introduces a new tool to democratize and standardize
ethical assessment of natural language generation models: Tool for Ethical
Assessment of Language generation models (TEAL), a component of Credo AI Lens,
an open-source assessment framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Natural Supervision for Language Representation Learning and Generation. (arXiv:2207.10617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10617">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs in Natural Language Processing (NLP) have been driven by
language models trained on a massive amount of plain text. While powerful,
deriving supervision from textual resources is still an open question. For
example, language model pretraining often neglects the rich, freely-available
structures in textual data. In this thesis, we describe three lines of work
that seek to improve the training and evaluation of neural models using
naturally-occurring supervision.
</p>
<p>We first investigate self-supervised training losses to help enhance the
performance of pretrained language models for various NLP tasks. Specifically,
we alter the sentence prediction loss to make it better suited to other
pretraining losses and more challenging to solve. We design an intermediate
finetuning step that uses self-supervised training to promote models' ability
in cross-task generalization.
</p>
<p>Then we describe methods to leverage the structures in Wikipedia and
paraphrases. In particular, we propose training losses to exploit hyperlinks,
article structures, and article category graphs for entity-, discourse-,
entailment-related knowledge. We propose a framework that uses paraphrase pairs
to disentangle semantics and syntax in sentence representations. We extend the
framework for a novel generation task that controls the syntax of output text
with a sentential exemplar.
</p>
<p>Lastly, we discuss our work on tailoring textual resources for establishing
challenging evaluation tasks. We introduce three datasets by defining novel
tasks using various fan-contributed websites, including a long-form
data-to-text generation dataset, a screenplay summarization dataset, and a
long-form story generation dataset. These datasets have unique characteristics
offering challenges to future work in their respective task settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Session-based Cyberbullying Detection in Social Media: A Survey. (arXiv:2207.10639v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10639">
<div class="article-summary-box-inner">
<span><p>Cyberbullying is a pervasive problem in online social media, where a bully
abuses a victim through a social media session. By investigating cyberbullying
perpetrated through social media sessions, recent research has looked into
mining patterns and features for modeling and understanding the two defining
characteristics of cyberbullying: repetitive behavior and power imbalance. In
this survey paper, we define the Session-based Cyberbullying Detection
framework that encapsulates the different steps and challenges of the problem.
Based on this framework, we provide a comprehensive overview of session-based
cyberbullying detection in social media, delving into existing efforts from a
data and methodological perspective. Our review leads us to propose
evidence-based criteria for a set of best practices to create session-based
cyberbullying datasets. In addition, we perform benchmark experiments comparing
the performance of state-of-the-art session-based cyberbullying detection
models as well as large pre-trained language models across two different
datasets. Through our review, we also put forth a set of open challenges as
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Reveals Patterns of Diverse and Changing Sentiments Towards COVID-19 Vaccines Based on 11 Million Tweets. (arXiv:2207.10641v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10641">
<div class="article-summary-box-inner">
<span><p>Over 12 billion doses of COVID-19 vaccines have been administered at the time
of writing. However, public perceptions of vaccines have been complex. We
analyzed COVID-19 vaccine-related tweets to understand the evolving perceptions
of COVID-19 vaccines. We finetuned a deep learning classifier using a
state-of-the-art model, XLNet, to detect each tweet's sentiment automatically.
We employed validated methods to extract the users' race or ethnicity, gender,
age, and geographical locations from user profiles. Incorporating multiple data
sources, we assessed the sentiment patterns among subpopulations and juxtaposed
them against vaccine uptake data to unravel their interactive patterns.
11,211,672 COVID-19 vaccine-related tweets corresponding to 2,203,681 users
over two years were analyzed. The finetuned model for sentiment classification
yielded an accuracy of 0.92 on testing set. Users from various demographic
groups demonstrated distinct patterns in sentiments towards COVID-19 vaccines.
User sentiments became more positive over time, upon which we observed
subsequent upswing in the population-level vaccine uptake. Surrounding dates
where positive sentiments crest, we detected encouraging news or events
regarding vaccine development and distribution. Positive sentiments in
pregnancy-related tweets demonstrated a delayed pattern compared with trends in
general population, with postponed vaccine uptake trends. Distinctive patterns
across subpopulations suggest the need of tailored strategies. Global news and
events profoundly involved in shaping users' thoughts on social media.
Populations with additional concerns, such as pregnancy, demonstrated more
substantial hesitancy since lack of timely recommendations. Feature analysis
revealed hesitancies of various subpopulations stemmed from clinical trial
logics, risks and complications, and urgency of scientific evidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STOP: A dataset for Spoken Task Oriented Semantic Parsing. (arXiv:2207.10643v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10643">
<div class="article-summary-box-inner">
<span><p>End-to-end spoken language understanding (SLU) predicts intent directly from
audio using a single model. It promises to improve the performance of assistant
systems by leveraging acoustic information lost in the intermediate textual
representation and preventing cascading errors from Automatic Speech
Recognition (ASR). Further, having one unified model has efficiency advantages
when deploying assistant systems on-device. However, the limited number of
public audio datasets with semantic parse labels hinders the research progress
in this area. In this paper, we release the Spoken Task-Oriented semantic
Parsing (STOP) dataset, the largest and most complex SLU dataset to be publicly
available. Additionally, we define low-resource splits to establish a benchmark
for improving SLU when limited labeled data is available. Furthermore, in
addition to the human-recorded audio, we are releasing a TTS-generated version
to benchmark the performance for low-resource domain adaptation of end-to-end
SLU systems. Initial experimentation show end-to-end SLU models performing
slightly worse than their cascaded counterparts, which we hope encourages
future work in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for the Single-Corpus and Cross-Corpus Speech Emotion Recognition. (arXiv:2207.10644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10644">
<div class="article-summary-box-inner">
<span><p>Speech Emotion Recognition (SER) has become a growing focus of research in
human-computer interaction. An essential challenge in SER is to extract common
attributes from different speakers or languages, especially when a specific
source corpus has to be trained to recognize the unknown data coming from
another speech corpus. To address this challenge, a Capsule Network (CapsNet)
and Transfer Learning based Mixed Task Net (CTLMTNet) are proposed to deal with
both the singlecorpus and cross-corpus SER tasks simultaneously in this paper.
For the single-corpus task, the combination of Convolution-Pooling and
Attention CapsNet module CPAC) is designed by embedding the self-attention
mechanism to the CapsNet, guiding the module to focus on the important features
that can be fed into different capsules. The extracted high-level features by
CPAC provide sufficient discriminative ability. Furthermore, to handle the
cross-corpus task, CTL-MTNet employs a Corpus Adaptation Adversarial Module
(CAAM) by combining CPAC with Margin Disparity Discrepancy (MDD), which can
learn the domain-invariant emotion representations through extracting the
strong emotion commonness. Experiments including ablation studies and
visualizations on both singleand cross-corpus tasks using four well-known SER
datasets in different languages are conducted for performance evaluation and
comparison. The results indicate that in both tasks the CTL-MTNet showed better
performance in all cases compared to a number of state-of-the-art methods. The
source code and the supplementary materials are available at:
https://github.com/MLDMXM2017/CTLMTNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wide & Deep Learning for Judging Student Performance in Online One-on-one Math Classes. (arXiv:2207.10645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10645">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the opportunities of automating the judgment
process in online one-on-one math classes. We build a Wide &amp; Deep framework to
learn fine-grained predictive representations from a limited amount of noisy
classroom conversation data that perform better student judgments. We conducted
experiments on the task of predicting students' levels of mastery of example
questions and the results demonstrate the superiority and availability of our
model in terms of various evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A No-Code Low-Code Paradigm for Authoring Business Automations Using Natural Language. (arXiv:2207.10648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10648">
<div class="article-summary-box-inner">
<span><p>Most business process automation is still developed using traditional
automation technologies such as workflow engines. These systems provide domain
specific languages that require both business knowledge and programming skills
to effectively use. As such, business users often lack adequate programming
skills to fully leverage these code oriented environments. We propose a
paradigm for the construction of business automations using natural language.
The approach applies a large language model to translate business rules and
automations described in natural language, into a domain specific language
interpretable by a business rule engine. We compare the performance of various
language model configurations, across various target domains, and explore the
use of constrained decoding to ensure syntactically correct generation of
output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Disinformation Detection for Digital Advertising. (arXiv:2207.10649v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10649">
<div class="article-summary-box-inner">
<span><p>In today's world, the presence of online disinformation and propaganda is
more widespread than ever. Independent publishers are funded mostly via digital
advertising, which is unfortunately also the case for those publishing
disinformation content. The question of how to remove such publishers from
advertising inventory has long been ignored, despite the negative impact on the
open internet. In this work, we make the first step towards quickly detecting
and red-flagging websites that potentially manipulate the public with
disinformation. We build a machine learning model based on multilingual text
embeddings that first determines whether the page mentions a topic of interest,
then estimates the likelihood of the content being malicious, creating a
shortlist of publishers that will be reviewed by human experts. Our system
empowers internal teams to proactively, rather than defensively, blacklist
unsafe content, thus protecting the reputation of the advertisement provider.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">O-Dang! The Ontology of Dangerous Speech Messages. (arXiv:2207.10652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10652">
<div class="article-summary-box-inner">
<span><p>Inside the NLP community there is a considerable amount of language resources
created, annotated and released every day with the aim of studying specific
linguistic phenomena. Despite a variety of attempts in order to organize such
resources has been carried on, a lack of systematic methods and of possible
interoperability between resources are still present. Furthermore, when storing
linguistic information, still nowadays, the most common practice is the concept
of "gold standard", which is in contrast with recent trends in NLP that aim at
stressing the importance of different subjectivities and points of view when
training machine learning and deep learning methods. In this paper we present
O-Dang!: The Ontology of Dangerous Speech Messages, a systematic and
interoperable Knowledge Graph (KG) for the collection of linguistic annotated
data. O-Dang! is designed to gather and organize Italian datasets into a
structured KG, according to the principles shared within the Linguistic Linked
Open Data community. The ontology has also been designed to account for a
perspectivist approach, since it provides a model for encoding both gold
standard and single-annotator labels in the KG. The paper is structured as
follows. In Section 1 the motivations of our work are outlined. Section 2
describes the O-Dang! Ontology, that provides a common semantic model for the
integration of datasets in the KG. The Ontology Population stage with
information about corpora, users, and annotations is presented in Section 3.
Finally, in Section 4 an analysis of offensiveness across corpora is provided
as a first case study for the resource.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion detection of social data: APIs comparative study. (arXiv:2207.10654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10654">
<div class="article-summary-box-inner">
<span><p>The development of emotion detection technology has emerged as a highly
valuable possibility in the corporate sector due to the nearly limitless uses
of this new discipline, particularly with the unceasing propagation of social
data. In recent years, the electronic marketplace has witnessed the
establishment of a large number of start-up businesses with an almost sole
focus on building new commercial and open-source tools and APIs for emotion
detection and recognition. Yet, these tools and APIs must be continuously
reviewed and evaluated, and their performances should be reported and
discussed. There is a lack of research to empirically compare current emotion
detection technologies in terms of the results obtained from each model using
the same textual dataset. Also, there is a lack of comparative studies that
apply benchmark comparison to social data. This study compares eight
technologies; IBM Watson NLU, ParallelDots, Symanto-Ekman, Crystalfeel, Text to
Emotion, Senpy, Textprobe, and NLP Cloud. The comparison was undertaken using
two different datasets. The emotions from the chosen datasets were then derived
using the incorporated APIs. The performance of these APIs was assessed using
the aggregated scores that they delivered as well as the theoretically proven
evaluation metrics such as the micro-average of accuracy, classification error,
precision, recall, and f1-score. Lastly, the assessment of these APIs
incorporating the evaluation measures is reported and discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Triple Extraction with Generative Transformer. (arXiv:2009.06207v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06207">
<div class="article-summary-box-inner">
<span><p>Triple extraction is an essential task in information extraction for natural
language processing and knowledge graph construction. In this paper, we revisit
the end-to-end triple extraction task for sequence generation. Since generative
triple extraction may struggle to capture long-term dependencies and generate
unfaithful triples, we introduce a novel model, contrastive triple extraction
with a generative transformer. Specifically, we introduce a single shared
transformer module for encoder-decoder-based generation. To generate faithful
results, we propose a novel triplet contrastive training object. Moreover, we
introduce two mechanisms to further improve model performance (i.e., batch-wise
dynamic attention-masking and triple-wise calibration). Experimental results on
three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves
better performance than that of baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion analysis and detection during COVID-19. (arXiv:2107.11020v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11020">
<div class="article-summary-box-inner">
<span><p>Crises such as natural disasters, global pandemics, and social unrest
continuously threaten our world and emotionally affect millions of people
worldwide in distinct ways. Understanding emotions that people express during
large-scale crises helps inform policy makers and first responders about the
emotional states of the population as well as provide emotional support to
those who need such support. We present CovidEmo, ~3K English tweets labeled
with emotions and temporally distributed across 18 months. Our analyses reveal
the emotional toll caused by COVID-19, and changes of the social narrative and
associated emotions over time. Motivated by the time-sensitive nature of crises
and the cost of large-scale annotation efforts, we examine how well large
pre-trained language models generalize across domains and timeline in the task
of perceived emotion prediction in the context of COVID-19. Our analyses
suggest that cross-domain information transfers occur, yet there are still
significant gaps. We propose semi-supervised learning as a way to bridge this
gap, obtaining significantly better performance using unlabeled data from the
target domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Learning the Transformer Kernel. (arXiv:2110.08323v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08323">
<div class="article-summary-box-inner">
<span><p>In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data
driven framework for learning the kernel function in Transformers. Our
framework approximates the Transformer kernel as a dot product between spectral
feature maps and learns the kernel by learning the spectral distribution. This
not only helps in learning a generic kernel end-to-end, but also reduces the
time and space complexity of Transformers from quadratic to linear. We show
that KERNELIZED TRANSFORMERS achieve performance comparable to existing
efficient Transformer architectures, both in terms of accuracy as well as
computational efficiency. Our study also demonstrates that the choice of the
kernel has a substantial impact on performance, and kernel learning variants
are competitive alternatives to fixed kernel Transformers, both in long as well
as short sequence tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02080">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) such as GPT-3 have the surprising ability to do
in-context learning, where the model learns to do a downstream task simply by
conditioning on a prompt consisting of input-output examples. The LM learns
from these examples without being explicitly pretrained to learn. Thus, it is
unclear what enables in-context learning. In this paper, we study how
in-context learning can emerge when pretraining documents have long-range
coherence. Here, the LM must infer a latent document-level concept to generate
coherent next tokens during pretraining. At test time, in-context learning
occurs when the LM also infers a shared latent concept between examples in a
prompt. We prove when this occurs despite a distribution mismatch between
prompts and pretraining data in a setting where the pretraining distribution is
a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs
capable of in-context learning, we generate a small-scale synthetic dataset
(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond
the theory, experiments on GINC exhibit large-scale real-world phenomena
including improved in-context performance with model scaling (despite the same
pretraining loss), sensitivity to example order, and instances where zero-shot
is better than few-shot in-context learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP2TV: Align, Match and Distill for Video-Text Retrieval. (arXiv:2111.05610v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05610">
<div class="article-summary-box-inner">
<span><p>Modern video-text retrieval frameworks basically consist of three parts:
video encoder, text encoder and the similarity head. With the success on both
visual and textual representation learning, transformer based encoders and
fusion methods have also been adopted in the field of video-text retrieval. In
this report, we present CLIP2TV, aiming at exploring where the critical
elements lie in transformer based methods. To achieve this, We first revisit
some recent works on multi-modal learning, then introduce some techniques into
video-text retrieval, finally evaluate them through extensive experiments in
different configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,
outperforming the previous SOTA result by 4.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds. (arXiv:2112.08879v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08879">
<div class="article-summary-box-inner">
<span><p>Most models tasked to ground referential utterances in 2D and 3D scenes learn
to select the referred object from a pool of object proposals provided by a
pre-trained detector. This is limiting because an utterance may refer to visual
entities at various levels of granularity, such as the chair, the leg of the
chair, or the tip of the front leg of the chair, which may be missed by the
detector. We propose a language grounding model that attends on the referential
utterance and on the object proposal pool computed from a pre-trained detector
to decode referenced objects with a detection head, without selecting them from
the pool. In this way, it is helped by powerful pre-trained object detectors
without being restricted by their misses. We call our model Bottom Up Top Down
DEtection TRansformers (BUTD-DETR) because it uses both language guidance (top
down) and objectness guidance (bottom-up) to ground referential utterances in
images and point clouds. Moreover, BUTD-DETR casts object detection as
referential grounding and uses object labels as language prompts to be grounded
in the visual scene, augmenting supervision for the referential grounding task
in this way. The proposed model sets a new state-of-the-art across popular 3D
language grounding benchmarks with significant performance gains over previous
3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When
applied in 2D images, it performs on par with the previous state of the art. We
ablate the design choices of our model and quantify their contribution to
performance. Our code and checkpoints can be found at the project website
https://butd-detr.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Webly Supervised Concept Expansion for General Purpose Vision Models. (arXiv:2202.02317v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02317">
<div class="article-summary-box-inner">
<span><p>General Purpose Vision (GPV) systems are models that are designed to solve a
wide array of visual tasks without requiring architectural changes. Today, GPVs
primarily learn both skills and concepts from large fully supervised datasets.
Scaling GPVs to tens of thousands of concepts by acquiring data to learn each
concept for every skill quickly becomes prohibitive. This work presents an
effective and inexpensive alternative: learn skills from supervised datasets,
learn concepts from web image search, and leverage a key characteristic of
GPVs: the ability to transfer visual knowledge across skills. We use a dataset
of 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised
concept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5
COCO-based datasets (80 primary concepts), a newly curated series of 5 datasets
based on the OpenImages and VisualGenome repositories (~500 concepts), and the
Web-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2
that supports a variety of tasks -- from vision tasks like classification and
localization to vision+language tasks like QA and captioning, to more niche
ones like human-object interaction detection. GPV-2 benefits hugely from web
data and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code,
and web demo are available at https://prior.allenai.org/projects/gpv2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02874">
<div class="article-summary-box-inner">
<span><p>We introduce an audiovisual method for long-range text-to-video retrieval.
Unlike previous approaches designed for short video retrieval (e.g., 5-15
seconds in duration), our approach aims to retrieve minute-long videos that
capture complex human actions. One challenge of standard video-only approaches
is the large computational cost associated with processing hundreds of densely
extracted frames from such long videos. To address this issue, we propose to
replace parts of the video with compact audio cues that succinctly summarize
dynamic audio events and are cheap to process. Our method, named ECLIPSE
(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an
audiovisual video setting, by adding a unified audiovisual transformer block
that captures complementary cues from the video and audio streams. In addition
to being 2.92x faster and 2.34x memory-efficient than long-range video-only
approaches, our method also achieves better text-to-video retrieval accuracy on
several diverse long-range video datasets such as ActivityNet, QVHighlights,
YouCook2, DiDeMo and Charades.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09817">
<div class="article-summary-box-inner">
<span><p>Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13749">
<div class="article-summary-box-inner">
<span><p>Classifiers are biased when trained on biased datasets. As a remedy, we
propose Learning to Split (ls), an algorithm for automatic bias detection.
Given a dataset with input-label pairs, ls learns to split this dataset so that
predictors trained on the training split cannot generalize to the testing
split. This performance gap suggests that the testing split is
under-represented in the dataset, which is a signal of potential bias.
Identifying non-generalizable splits is challenging since we have no
annotations about the bias. In this work, we show that the prediction
correctness of each example in the testing split can be used as a source of
weak supervision: generalization performance will drop if we move examples that
are predicted correctly away from the testing split, leaving only those that
are mis-predicted. ls is task-agnostic and can be applied to any supervised
learning problem, ranging from natural language understanding and image
classification to molecular property prediction. Empirical results show that ls
is able to generate astonishingly challenging splits that correlate with
human-identified biases. Moreover, we demonstrate that combining robust
learning algorithms (such as group DRO) with splits identified by ls enables
automatic de-biasing. Compared to previous state-of-the-art, we substantially
improve the worst-group performance (23.4% on average) when the source of
biases is unknown during training and validation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nominal Metaphor Generation with Multitask Learning. (arXiv:2206.05195v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05195">
<div class="article-summary-box-inner">
<span><p>Metaphor generation is a challenging task which can impact many downstream
tasks such as improving user satisfaction with dialogue systems and story
generation. This paper tackles the problem of Chinese nominal metaphor
generation by introducing a multitask metaphor generation framework with
self-training and metaphor identification mechanisms. Self-training addresses
the data scarcity issue of metaphor datasets. That is, instead of solely
relying on labelled metaphor datasets which are usually small in size,
self-training helps identify potential metaphors from a large-scale unlabelled
corpus for metaphor generation. The metaphor weighting mechanism enables our
model to focus on the metaphor-related parts of the input (e.g., the comparison
of the metaphor and comparator) during model learning and thus improves the
metaphoricity of the generated metaphors. Our model is trained on an annotated
corpus consisting of 6.3k sentences that contain diverse metaphorical
expressions. Experimental results show that our model is able to generate
metaphors with better readability and creativity compared to the baseline
models, even in the situation where training data is insufficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapping a User-Centered Task-Oriented Dialogue System. (arXiv:2207.05223v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05223">
<div class="article-summary-box-inner">
<span><p>We present TacoBot, a task-oriented dialogue system built for the inaugural
Alexa Prize TaskBot Challenge, which assists users in completing multi-step
cooking and home improvement tasks. TacoBot is designed with a user-centered
principle and aspires to deliver a collaborative and accessible dialogue
experience. Towards that end, it is equipped with accurate language
understanding, flexible dialogue management, and engaging response generation.
Furthermore, TacoBot is backed by a strong search engine and an automated
end-to-end test suite. In bootstrapping the development of TacoBot, we explore
a series of data augmentation strategies to train advanced neural language
processing models and continuously improve the dialogue experience with
collected real conversations. At the end of the semifinals, TacoBot achieved an
average rating of 3.55/5.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReFactorGNNs: Revisiting Factorisation-based Models from a Message-Passing Perspective. (arXiv:2207.09980v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09980">
<div class="article-summary-box-inner">
<span><p>Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and to generalise to unseen nodes in inductive settings. Our work
bridges the gap between FMs and GNNs by proposing ReFactorGNNs. This new
architecture draws upon both modelling paradigms, which previously were largely
thought of as disjoint. Concretely, using a message-passing formalism, we show
how FMs can be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our ReFactorGNNs. Across a
multitude of well-established KGC benchmarks, our ReFactorGNNs achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Compression for Resource-Constrained Mobile Robots. (arXiv:2207.10082v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10082">
<div class="article-summary-box-inner">
<span><p>The number of mobile robots with constrained computing resources that need to
execute complex machine learning models has been increasing during the past
decade. Commonly, these robots rely on edge infrastructure accessible over
wireless communication to execute heavy computational complex tasks. However,
the edge might become unavailable and, consequently, oblige the execution of
the tasks on the robot. This work focuses on making it possible to execute the
tasks on the robots by reducing the complexity and the total number of
parameters of pre-trained computer vision models. This is achieved by using
model compression techniques such as Pruning and Knowledge Distillation. These
compression techniques have strong theoretical and practical foundations, but
their combined usage has not been widely explored in the literature. Therefore,
this work especially focuses on investigating the effects of combining these
two compression techniques. The results of this work reveal that up to 90% of
the total number of parameters of a computer vision model can be removed
without any considerable reduction in the model's accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10106">
<div class="article-summary-box-inner">
<span><p>Tidying up a household environment using a mobile manipulator poses various
challenges in robotics, such as adaptation to large real-world environmental
variations, and safe and robust deployment in the presence of humans.The
Partner Robot Challenge in World Robot Challenge (WRC) 2020, a global
competition held in September 2021, benchmarked tidying tasks in the real home
environments, and importantly, tested for full system performances.For this
challenge, we developed an entire household service robot system, which
leverages a data-driven approach to adapt to numerous edge cases that occur
during the execution, instead of classical manual pre-programmed solutions.In
this paper, we describe the core ingredients of the proposed robot system,
including visual recognition, object manipulation, and motion planning. Our
robot system won the second prize, verifying the effectiveness and potential of
data-driven robot systems for mobile manipulation in home environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis. (arXiv:2207.10120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10120">
<div class="article-summary-box-inner">
<span><p>Generative models for audio-conditioned dance motion synthesis map music
features to dance movements. Models are trained to associate motion patterns to
audio patterns, usually without an explicit knowledge of the human body. This
approach relies on a few assumptions: strong music-dance correlation,
controlled motion data and relatively simple poses and movements. These
characteristics are found in all existing datasets for dance motion synthesis,
and indeed recent methods can achieve good results.We introduce a new dataset
aiming to challenge these common assumptions, compiling a set of dynamic dance
sequences displaying complex human poses. We focus on breakdancing which
features acrobatic moves and tangled postures. We source our data from the Red
Bull BC One competition videos. Estimating human keypoints from these videos is
difficult due to the complexity of the dance, as well as the multiple moving
cameras recording setup. We adopt a hybrid labelling pipeline leveraging deep
estimation models as well as manual annotations to obtain good quality keypoint
sequences at a reduced cost. Our efforts produced the BRACE dataset, which
contains over 3 hours and 30 minutes of densely annotated poses. We test
state-of-the-art methods on BRACE, showing their limitations when evaluated on
complex sequences. Our dataset can readily foster advance in dance motion
synthesis. With intricate poses and swift movements, models are forced to go
beyond learning a mapping between modalities and reason more effectively about
body structure and movements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance. (arXiv:2207.10123v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10123">
<div class="article-summary-box-inner">
<span><p>We study the challenging problem of recovering detailed motion from a single
motion-blurred image. Existing solutions to this problem estimate a single
image sequence without considering the motion ambiguity for each region.
Therefore, the results tend to converge to the mean of the multi-modal
possibilities. In this paper, we explicitly account for such motion ambiguity,
allowing us to generate multiple plausible solutions all in sharp detail. The
key idea is to introduce a motion guidance representation, which is a compact
quantization of 2D optical flow with only four discrete motion directions.
Conditioned on the motion guidance, the blur decomposition is led to a
specific, unambiguous solution by using a novel two-stage decomposition
network. We propose a unified framework for blur decomposition, which supports
various interfaces for generating our motion guidance, including human input,
motion information from adjacent video frames, and learning from a video
dataset. Extensive experiments on synthesized datasets and real-world data show
that the proposed framework is qualitatively and quantitatively superior to
previous methods, and also offers the merit of producing physically plausible
and diverse solutions. Code is available at
https://github.com/zzh-tech/Animation-from-Blur.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Discriminant deterministic Uncertainty. (arXiv:2207.10130v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10130">
<div class="article-summary-box-inner">
<span><p>Predictive uncertainty estimation is essential for deploying Deep Neural
Networks in real-world autonomous systems. However, most successful approaches
are computationally intensive. In this work, we attempt to address these
challenges in the context of autonomous driving perception tasks. Recently
proposed Deterministic Uncertainty Methods (DUM) can only partially meet such
requirements as their scalability to complex computer vision tasks is not
obvious. In this work we advance a scalable and effective DUM for
high-resolution semantic segmentation, that relaxes the Lipschitz constraint
typically hindering practicality of such architectures. We learn a discriminant
latent space by leveraging a distinction maximization layer over an
arbitrarily-sized set of trainable prototypes. Our approach achieves
competitive results over Deep Ensembles, the state-of-the-art for uncertainty
prediction, on image classification, segmentation and monocular depth
estimation tasks. Our code is available at https://github.com/ENSTA-U2IS/LDU
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Variational Autoencoder Learning via Online Cooperative Memorization. (arXiv:2207.10131v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10131">
<div class="article-summary-box-inner">
<span><p>Due to their inference, data representation and reconstruction properties,
Variational Autoencoders (VAE) have been successfully used in continual
learning classification tasks. However, their ability to generate images with
specifications corresponding to the classes and databases learned during
Continual Learning (CL) is not well understood and catastrophic forgetting
remains a significant challenge. In this paper, we firstly analyze the
forgetting behaviour of VAEs by developing a new theoretical framework that
formulates CL as a dynamic optimal transport problem. This framework proves
approximate bounds to the data likelihood without requiring the task
information and explains how the prior knowledge is lost during the training
process. We then propose a novel memory buffering approach, namely the Online
Cooperative Memorization (OCM) framework, which consists of a Short-Term Memory
(STM) that continually stores recent samples to provide future information for
the model, and a Long-Term Memory (LTM) aiming to preserve a wide diversity of
samples. The proposed OCM transfers certain samples from STM to LTM according
to the information diversity selection criterion without requiring any
supervised signals. The OCM framework is then combined with a dynamic VAE
expansion mixture network for further enhancing its performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation. (arXiv:2207.10137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10137">
<div class="article-summary-box-inner">
<span><p>In temporal action segmentation, Timestamp supervision requires only a
handful of labelled frames per video sequence. For unlabelled frames, previous
works rely on assigning hard labels, and performance rapidly collapses under
subtle violations of the annotation assumptions. We propose a novel
Expectation-Maximization (EM) based approach that leverages the label
uncertainty of unlabelled frames and is robust enough to accommodate possible
annotation errors. With accurate timestamp annotations, our proposed method
produces SOTA results and even exceeds the fully-supervised setup in several
metrics and datasets. When applied to timestamp annotations with missing action
segments, our method presents stable performance. To further test our
formulation's robustness, we introduce the new challenging annotation setup of
Skip-tag supervision. This setup relaxes constraints and requires annotations
of any fixed number of random frames in a video, making it more flexible than
Timestamp supervision while remaining competitive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AudioScopeV2: Audio-Visual Attention Architectures for Calibrated Open-Domain On-Screen Sound Separation. (arXiv:2207.10141v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10141">
<div class="article-summary-box-inner">
<span><p>We introduce AudioScopeV2, a state-of-the-art universal audio-visual
on-screen sound separation system which is capable of learning to separate
sounds and associate them with on-screen objects by looking at in-the-wild
videos. We identify several limitations of previous work on audio-visual
on-screen sound separation, including the coarse resolution of spatio-temporal
attention, poor convergence of the audio separation model, limited variety in
training and evaluation data, and failure to account for the trade off between
preservation of on-screen sounds and suppression of off-screen sounds. We
provide solutions to all of these issues. Our proposed cross-modal and
self-attention network architectures capture audio-visual dependencies at a
finer resolution over time, and we also propose efficient separable variants
that are capable of scaling to longer videos without sacrificing much
performance. We also find that pre-training the separation model only on audio
greatly improves results. For training and evaluation, we collected new human
annotations of onscreen sounds from a large database of in-the-wild videos
(YFCC100M). This new dataset is more diverse and challenging. Finally, we
propose a calibration procedure that allows exact tuning of on-screen
reconstruction versus off-screen suppression, which greatly simplifies
comparing performance between models with different operating points. Overall,
our experimental results show marked improvements in on-screen separation
performance under much more general conditions than previous methods with
minimal additional computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Long-Tailed Category Distribution Under Domain Shifts. (arXiv:2207.10150v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10150">
<div class="article-summary-box-inner">
<span><p>Machine learning models fail to perform well on real-world applications when
1) the category distribution P(Y) of the training dataset suffers from
long-tailed distribution and 2) the test data is drawn from different
conditional distributions P(X|Y). Existing approaches cannot handle the
scenario where both issues exist, which however is common for real-world
applications. In this study, we took a step forward and looked into the problem
of long-tailed classification under domain shifts. We designed three novel core
functional blocks including Distribution Calibrated Classification Loss,
Visual-Semantic Mapping and Semantic-Similarity Guided Augmentation.
Furthermore, we adopted a meta-learning framework which integrates these three
blocks to improve domain generalization on unseen target domains. Two new
datasets were proposed for this problem, named AWA2-LTS and ImageNet-LTS. We
evaluated our method on the two datasets and extensive experimental results
demonstrate that our proposed method can achieve superior performance over
state-of-the-art long-tailed/domain generalization approaches and the
combinations. Source codes and datasets can be found at our project page
https://xiaogu.site/LTDS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of the Effect of Low-Overhead Lossy Image Compression on the Performance of Visual Crowd Counting for Smart City Applications. (arXiv:2207.10155v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10155">
<div class="article-summary-box-inner">
<span><p>Images and video frames captured by cameras placed throughout smart cities
are often transmitted over the network to a server to be processed by deep
neural networks for various tasks. Transmission of raw images, i.e., without
any form of compression, requires high bandwidth and can lead to congestion
issues and delays in transmission. The use of lossy image compression
techniques can reduce the quality of the images, leading to accuracy
degradation. In this paper, we analyze the effect of applying low-overhead
lossy image compression methods on the accuracy of visual crowd counting, and
measure the trade-off between bandwidth reduction and the obtained accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Causal 3D Reconstruction. (arXiv:2207.10156v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10156">
<div class="article-summary-box-inner">
<span><p>This paper considers the problem of unsupervised 3D object reconstruction
from in-the-wild single-view images. Due to ambiguity and intrinsic
ill-posedness, this problem is inherently difficult to solve and therefore
requires strong regularization to achieve disentanglement of different latent
factors. Unlike existing works that introduce explicit regularizations into
objective functions, we look into a different space for implicit regularization
-- the structure of latent space. Specifically, we restrict the structure of
latent space to capture a topological causal ordering of latent factors (i.e.,
representing causal dependency as a directed acyclic graph). We first show that
different causal orderings matter for 3D reconstruction, and then explore
several approaches to find a task-dependent causal factor ordering. Our
experiments demonstrate that the latent space structure indeed serves as an
implicit regularization and introduces an inductive bias beneficial for
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Knowledge Tracing. (arXiv:2207.10157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10157">
<div class="article-summary-box-inner">
<span><p>Each year, thousands of people learn new visual categorization tasks --
radiologists learn to recognize tumors, birdwatchers learn to distinguish
similar species, and crowd workers learn how to annotate valuable data for
applications like autonomous driving. As humans learn, their brain updates the
visual features it extracts and attend to, which ultimately informs their final
classification decisions. In this work, we propose a novel task of tracing the
evolving classification behavior of human learners as they engage in
challenging visual classification tasks. We propose models that jointly extract
the visual features used by learners as well as predicting the classification
functions they utilize. We collect three challenging new datasets from real
human learners in order to evaluate the performance of different visual
knowledge tracing methods. Our results show that our recurrent models are able
to predict the classification behavior of human learners on three challenging
medical image and species identification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOCA: Guided Online Cluster Assignment for Self-Supervised Video Representation Learning. (arXiv:2207.10158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10158">
<div class="article-summary-box-inner">
<span><p>Clustering is a ubiquitous tool in unsupervised learning. Most of the
existing self-supervised representation learning methods typically cluster
samples based on visually dominant features. While this works well for
image-based self-supervision, it often fails for videos, which require
understanding motion rather than focusing on background. Using optical flow as
complementary information to RGB can alleviate this problem. However, we
observe that a naive combination of the two views does not provide meaningful
gains. In this paper, we propose a principled way to combine two views.
Specifically, we propose a novel clustering strategy where we use the initial
cluster assignment of each view as prior to guide the final cluster assignment
of the other view. This idea will enforce similar cluster structures for both
views, and the formed clusters will be semantically abstract and robust to
noisy inputs coming from each individual view. Additionally, we propose a novel
regularization strategy to address the feature collapse problem, which is
common in cluster-based self-supervised learning methods. Our extensive
evaluation shows the effectiveness of our learned representations on downstream
tasks, e.g., video retrieval and action recognition. Specifically, we
outperform the state of the art by 7% on UCF and 4% on HMDB for video
retrieval, and 5% on UCF and 6% on HMDB for video classification
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Liver Segmentation using Turbolift Learning for CT and Cone-beam C-arm Perfusion Imaging. (arXiv:2207.10167v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10167">
<div class="article-summary-box-inner">
<span><p>Model-based reconstruction employing the time separation technique (TST) was
found to improve dynamic perfusion imaging of the liver using C-arm cone-beam
computed tomography (CBCT). To apply TST using prior knowledge extracted from
CT perfusion data, the liver should be accurately segmented from the CT scans.
Reconstructions of primary and model-based CBCT data need to be segmented for
proper visualisation and interpretation of perfusion maps. This research
proposes Turbolift learning, which trains a modified version of the multi-scale
Attention UNet on different liver segmentation tasks serially, following the
order of the trainings CT, CBCT, CBCT TST - making the previous trainings act
as pre-training stages for the subsequent ones - addressing the problem of
limited number of datasets for training. For the final task of liver
segmentation from CBCT TST, the proposed method achieved an overall Dice scores
of 0.874$\pm$0.031 and 0.905$\pm$0.007 in 6-fold and 4-fold cross-validation
experiments, respectively - securing statistically significant improvements
over the model, which was trained only for that task. Experiments revealed that
Turbolift not only improves the overall performance of the model but also makes
it robust against artefacts originating from the embolisation materials and
truncation artefacts. Additionally, in-depth analyses confirmed the order of
the segmentation tasks. This paper shows the potential of segmenting the liver
from CT, CBCT, and CBCT TST, learning from the available limited training data,
which can possibly be used in the future for the visualisation and evaluation
of the perfusion maps for the treatment evaluation of liver diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pediatric Bone Age Assessment using Deep Learning Models. (arXiv:2207.10169v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10169">
<div class="article-summary-box-inner">
<span><p>Bone age assessment (BAA) is a standard method for determining the age
difference between skeletal and chronological age. Manual processes are
complicated and necessitate the expertise of experts. This is where deep
learning comes into play. In this study, pre-trained models like VGG-16,
InceptionV3, XceptionNet, and MobileNet are used to assess the bone age of the
input data, and their mean average errors are compared and evaluated to see
which model predicts the best.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles. (arXiv:2207.10172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10172">
<div class="article-summary-box-inner">
<span><p>Video Anomaly Detection (VAD) is an important topic in computer vision.
Motivated by the recent advances in self-supervised learning, this paper
addresses VAD by solving an intuitive yet challenging pretext task, i.e.,
spatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained
classification problem. Our method exhibits several advantages over existing
works: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial
and temporal dimensions, responsible for capturing highly discriminative
appearance and motion features, respectively; 2) full permutations are used to
provide abundant jigsaw puzzles covering various difficulty levels, allowing
the network to distinguish subtle spatio-temporal differences between normal
and abnormal events; and 3) the pretext task is tackled in an end-to-end manner
without relying on any pre-trained models. Our method outperforms
state-of-the-art counterparts on three public benchmarks. Especially on
ShanghaiTech Campus, the result is superior to reconstruction and
prediction-based methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Recognition with Objectness, Attribute and Category Learning. (arXiv:2207.10174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10174">
<div class="article-summary-box-inner">
<span><p>Scene classification has established itself as a challenging research
problem. Compared to images of individual objects, scene images could be much
more semantically complex and abstract. Their difference mainly lies in the
level of granularity of recognition. Yet, image recognition serves as a key
pillar for the good performance of scene recognition as the knowledge attained
from object images can be used for accurate recognition of scenes. The existing
scene recognition methods only take the category label of the scene into
consideration. However, we find that the contextual information that contains
detailed local descriptions are also beneficial in allowing the scene
recognition model to be more discriminative. In this paper, we aim to improve
scene recognition using attribute and category label information encoded in
objects. Based on the complementarity of attribute and category labels, we
propose a Multi-task Attribute-Scene Recognition (MASR) network which learns a
category embedding and at the same time predicts scene attributes. Attribute
acquisition and object annotation are tedious and time consuming tasks. We
tackle the problem by proposing a partially supervised annotation strategy in
which human intervention is significantly reduced. The strategy provides a much
more cost-effective solution to real world scenarios, and requires considerably
less annotation efforts. Moreover, we re-weight the attribute predictions
considering the level of importance indicated by the object detected scores.
Using the proposed method, we efficiently annotate attribute labels for four
large-scale datasets, and systematically investigate how scene and attribute
recognition benefit from each other. The experimental results demonstrate that
MASR learns a more discriminative representation and achieves competitive
recognition performance compared to the state-of-the-art methods
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable and Guided Face Synthesis for Unconstrained Face Recognition. (arXiv:2207.10180v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10180">
<div class="article-summary-box-inner">
<span><p>Although significant advances have been made in face recognition (FR), FR in
unconstrained environments remains challenging due to the domain gap between
the semi-constrained training datasets and unconstrained testing scenarios. To
address this problem, we propose a controllable face synthesis model (CFSM)
that can mimic the distribution of target datasets in a style latent space.
CFSM learns a linear subspace with orthogonal bases in the style latent space
with precise control over the diversity and degree of synthesis. Furthermore,
the pre-trained synthesis model can be guided by the FR model, making the
resulting images more beneficial for FR model training. Besides, target dataset
distributions are characterized by the learned orthogonal bases, which can be
utilized to measure the distributional similarity among face datasets. Our
approach yields significant performance gains on unconstrained benchmarks, such
as IJB-B, IJB-C, TinyFace and IJB-S (+5.76% Rank1).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flow-based Visual Quality Enhancer for Super-resolution Magnetic Resonance Spectroscopic Imaging. (arXiv:2207.10181v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10181">
<div class="article-summary-box-inner">
<span><p>Magnetic Resonance Spectroscopic Imaging (MRSI) is an essential tool for
quantifying metabolites in the body, but the low spatial resolution limits its
clinical applications. Deep learning-based super-resolution methods provided
promising results for improving the spatial resolution of MRSI, but the
super-resolved images are often blurry compared to the experimentally-acquired
high-resolution images. Attempts have been made with the generative adversarial
networks to improve the image visual quality. In this work, we consider another
type of generative model, the flow-based model, of which the training is more
stable and interpretable compared to the adversarial networks. Specifically, we
propose a flow-based enhancer network to improve the visual quality of
super-resolution MRSI. Different from previous flow-based models, our enhancer
network incorporates anatomical information from additional image modalities
(MRI) and uses a learnable base distribution. In addition, we impose a guide
loss and a data-consistency loss to encourage the network to generate images
with high visual quality while maintaining high fidelity. Experiments on a
1H-MRSI dataset acquired from 25 high-grade glioma patients indicate that our
enhancer network outperforms the adversarial networks and the baseline
flow-based methods. Our method also allows visual quality adjustment and
uncertainty estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2D GANs Meet Unsupervised Single-view 3D Reconstruction. (arXiv:2207.10183v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10183">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that controllable image generation based on
pre-trained GANs can benefit a wide range of computer vision tasks. However,
less attention has been devoted to 3D vision tasks. In light of this, we
propose a novel image-conditioned neural implicit field, which can leverage 2D
supervisions from GAN-generated multi-view images and perform the single-view
reconstruction of generic objects. Firstly, a novel offline StyleGAN-based
generator is presented to generate plausible pseudo images with full control
over the viewpoint. Then, we propose to utilize a neural implicit function,
along with a differentiable renderer to learn 3D geometry from pseudo images
with object masks and rough pose initializations. To further detect the
unreliable supervisions, we introduce a novel uncertainty module to predict
uncertainty maps, which remedy the negative effect of uncertain regions in
pseudo images, leading to a better reconstruction performance. The
effectiveness of our approach is demonstrated through superior single-view 3D
reconstruction results of generic objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bitwidth-Adaptive Quantization-Aware Neural Network Training: A Meta-Learning Approach. (arXiv:2207.10188v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10188">
<div class="article-summary-box-inner">
<span><p>Deep neural network quantization with adaptive bitwidths has gained
increasing attention due to the ease of model deployment on various platforms
with different resource budgets. In this paper, we propose a meta-learning
approach to achieve this goal. Specifically, we propose MEBQAT, a simple yet
effective way of bitwidth-adaptive quantization aware training (QAT) where
meta-learning is effectively combined with QAT by redefining meta-learning
tasks to incorporate bitwidths. After being deployed on a platform, MEBQAT
allows the (meta-)trained model to be quantized to any candidate bitwidth then
helps to conduct inference without much accuracy drop from quantization.
Moreover, with a few-shot learning scenario, MEBQAT can also adapt a model to
any bitwidth as well as any unseen target classes by adding conventional
optimization or metric-based meta-learning. We design variants of MEBQAT to
support both (1) a bitwidth-adaptive quantization scenario and (2) a new
few-shot learning scenario where both quantization bitwidths and target classes
are jointly adapted. We experimentally demonstrate their validity in multiple
QAT schemes. By comparing their performance to (bitwidth-dedicated) QAT,
existing bitwidth adaptive QAT and vanilla meta-learning, we find that merging
bitwidths into meta-learning tasks achieves a higher level of robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Hotels-50K and Hotel-ID. (arXiv:2207.10200v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10200">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose revisited versions for two recent hotel recognition
datasets: Hotels50K and Hotel-ID. The revisited versions provide evaluation
setups with different levels of difficulty to better align with the intended
real-world application, i.e. countering human trafficking. Real-world scenarios
involve hotels and locations that are not captured in the current data sets,
therefore it is important to consider evaluation settings where classes are
truly unseen. We test this setup using multiple state-of-the-art image
retrieval models and show that as expected, the models' performances decrease
as the evaluation gets closer to the real-world unseen settings. The rankings
of the best performing models also change across the different evaluation
settings, which further motivates using the proposed revisited datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid CNN-Transformer Model For Facial Affect Recognition In the ABAW4 Challenge. (arXiv:2207.10201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10201">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to the fourth Affective Behavior Analysis
(ABAW) competition. We proposed a hybrid CNN-Transformer model for the
Multi-Task-Learning (MTL) and Learning from Synthetic Data (LSD) task.
Experimental results on validation dataset shows that our method achieves
better performance than baseline model, which verifies that the effectiveness
of proposed network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Robustness of 3D Object Detectors. (arXiv:2207.10205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10205">
<div class="article-summary-box-inner">
<span><p>In recent years, significant progress has been achieved for 3D object
detection on point clouds thanks to the advances in 3D data collection and deep
learning techniques. Nevertheless, 3D scenes exhibit a lot of variations and
are prone to sensor inaccuracies as well as information loss during
pre-processing. Thus, it is crucial to design techniques that are robust
against these variations. This requires a detailed analysis and understanding
of the effect of such variations. This work aims to analyze and benchmark
popular point-based 3D object detectors against several data corruptions. To
the best of our knowledge, we are the first to investigate the robustness of
point-based 3D object detectors. To this end, we design and evaluate
corruptions that involve data addition, reduction, and alteration. We further
study the robustness of different modules against local and global variations.
Our experimental results reveal several intriguing findings. For instance, we
show that methods that integrate Transformers at a patch or object level lead
to increased robustness, compared to using Transformers at the point level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spotting Temporally Precise, Fine-Grained Events in Video. (arXiv:2207.10213v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10213">
<div class="article-summary-box-inner">
<span><p>We introduce the task of spotting temporally precise, fine-grained events in
video (detecting the precise moment in time events occur). Precise spotting
requires models to reason globally about the full-time scale of actions and
locally to identify subtle frame-to-frame appearance and motion differences
that identify events during these actions. Surprisingly, we find that top
performing solutions to prior video understanding tasks such as action
detection and segmentation do not simultaneously meet both requirements. In
response, we propose E2E-Spot, a compact, end-to-end model that performs well
on the precise spotting task and can be trained quickly on a single GPU. We
demonstrate that E2E-Spot significantly outperforms recent baselines adapted
from the video action detection, segmentation, and spotting literature to the
precise spotting task. Finally, we contribute new annotations and splits to
several fine-grained sports action datasets to make these datasets suitable for
future work on precise spotting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Label Granularity and Object Localization. (arXiv:2207.10225v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10225">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object localization (WSOL) aims to learn representations
that encode object location using only image-level category labels. However,
many objects can be labeled at different levels of granularity. Is it an
animal, a bird, or a great horned owl? Which image-level labels should we use?
In this paper we study the role of label granularity in WSOL. To facilitate
this investigation we introduce iNatLoc500, a new large-scale fine-grained
benchmark dataset for WSOL. Surprisingly, we find that choosing the right
training label granularity provides a much larger performance boost than
choosing the best WSOL algorithm. We also show that changing the label
granularity can significantly improve data efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeshMAE: Masked Autoencoders for 3D Mesh Data Analysis. (arXiv:2207.10228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10228">
<div class="article-summary-box-inner">
<span><p>Recently, self-supervised pre-training has advanced Vision Transformers on
various tasks w.r.t. different data modalities, e.g., image and 3D point cloud
data. In this paper, we explore this learning paradigm for 3D mesh data
analysis based on Transformers. Since applying Transformer architectures to new
modalities is usually non-trivial, we first adapt Vision Transformer to 3D mesh
data processing, i.e., Mesh Transformer. In specific, we divide a mesh into
several non-overlapping local patches with each containing the same number of
faces and use the 3D position of each patch's center point to form positional
embeddings. Inspired by MAE, we explore how pre-training on 3D mesh data with
the Transformer-based structure benefits downstream 3D mesh analysis tasks. We
first randomly mask some patches of the mesh and feed the corrupted mesh into
Mesh Transformers. Then, through reconstructing the information of masked
patches, the network is capable of learning discriminative representations for
mesh data. Therefore, we name our method MeshMAE, which can yield
state-of-the-art or comparable performance on mesh analysis tasks, i.e.,
classification and segmentation. In addition, we also conduct comprehensive
ablation studies to show the effectiveness of key designs in our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPIN: An Empirical Evaluation on Sharing Parameters of Isotropic Networks. (arXiv:2207.10237v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10237">
<div class="article-summary-box-inner">
<span><p>Recent isotropic networks, such as ConvMixer and vision transformers, have
found significant success across visual recognition tasks, matching or
outperforming non-isotropic convolutional neural networks (CNNs). Isotropic
architectures are particularly well-suited to cross-layer weight sharing, an
effective neural network compression technique. In this paper, we perform an
empirical evaluation on methods for sharing parameters in isotropic networks
(SPIN). We present a framework to formalize major weight sharing design
decisions and perform a comprehensive empirical evaluation of this design
space. Guided by our experimental results, we propose a weight sharing strategy
to generate a family of models with better overall efficiency, in terms of
FLOPs and parameters versus accuracy, compared to traditional scaling methods
alone, for example compressing ConvMixer by 1.9x while improving accuracy on
ImageNet. Finally, we perform a qualitative study to further understand the
behavior of weight sharing in isotropic architectures. The code is available at
https://github.com/apple/ml-spin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection. (arXiv:2207.10246v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10246">
<div class="article-summary-box-inner">
<span><p>Facial forgery by deepfakes has raised severe societal concerns. Several
solutions have been proposed by the vision community to effectively combat the
misinformation on the internet via automated deepfake detection systems. Recent
studies have demonstrated that facial analysis-based deep learning models can
discriminate based on protected attributes. For the commercial adoption and
massive roll-out of the deepfake detection technology, it is vital to evaluate
and understand the fairness (the absence of any prejudice or favoritism) of
deepfake detectors across demographic variations such as gender and race. As
the performance differential of deepfake detectors between demographic
subgroups would impact millions of people of the deprived sub-group. This paper
aims to evaluate the fairness of the deepfake detectors across males and
females. However, existing deepfake datasets are not annotated with demographic
labels to facilitate fairness analysis. To this aim, we manually annotated
existing popular deepfake datasets with gender labels and evaluated the
performance differential of current deepfake detectors across gender. Our
analysis on the gender-labeled version of the datasets suggests (a) current
deepfake datasets have skewed distribution across gender, and (b) commonly
adopted deepfake detectors obtain unequal performance across gender with mostly
males outperforming females. Finally, we contributed a gender-balanced and
annotated deepfake dataset, GBDF, to mitigate the performance differential and
to promote research and development towards fairness-aware deep fake detectors.
The GBDF dataset is publicly available at: https://github.com/aakash4305/GBDF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10255">
<div class="article-summary-box-inner">
<span><p>We present SplitMixer, a simple and lightweight isotropic MLP-like
architecture, for visual recognition. It contains two types of interleaving
convolutional operations to mix information across spatial locations (spatial
mixing) and channels (channel mixing). The first one includes sequentially
applying two depthwise 1D kernels, instead of a 2D kernel, to mix spatial
information. The second one is splitting the channels into overlapping or
non-overlapping segments, with or without shared parameters, and applying our
proposed channel mixing approaches or 3D convolution to mix channel
information. Depending on design choices, a number of SplitMixer variants can
be constructed to balance accuracy, the number of parameters, and speed. We
show, both theoretically and experimentally, that SplitMixer performs on par
with the state-of-the-art MLP-like models while having a significantly lower
number of parameters and FLOPS. For example, without strong data augmentation
and optimization, SplitMixer achieves around 94% accuracy on CIFAR-10 with only
0.28M parameters, while ConvMixer achieves the same accuracy with about 0.6M
parameters. The well-known MLP-Mixer achieves 85.45% with 17.1M parameters. On
CIFAR-100 dataset, SplitMixer achieves around 73% accuracy, on par with
ConvMixer, but with about 52% fewer parameters and FLOPS. We hope that our
results spark further research towards finding more efficient vision
architectures and facilitate the development of MLP-like models. Code is
available at https://github.com/aliborji/splitmixer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGBANet: Semantic GAN and Balanced Attention Network for Arbitrarily Oriented Scene Text Recognition. (arXiv:2207.10256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10256">
<div class="article-summary-box-inner">
<span><p>Scene text recognition is a challenging task due to the complex backgrounds
and diverse variations of text instances. In this paper, we propose a novel
Semantic GAN and Balanced Attention Network (SGBANet) to recognize the texts in
scene images. The proposed method first generates the simple semantic feature
using Semantic GAN and then recognizes the scene text with the Balanced
Attention Module. The Semantic GAN aims to align the semantic feature
distribution between the support domain and target domain. Different from the
conventional image-to-image translation methods that perform at the image
level, the Semantic GAN performs the generation and discrimination on the
semantic level with the Semantic Generator Module (SGM) and Semantic
Discriminator Module (SDM). For target images (scene text images), the Semantic
Generator Module generates simple semantic features that share the same feature
distribution with support images (clear text images). The Semantic
Discriminator Module is used to distinguish the semantic features between the
support domain and target domain. In addition, a Balanced Attention Module is
designed to alleviate the problem of attention drift. The Balanced Attention
Module first learns a balancing parameter based on the visual glimpse vector
and semantic glimpse vector, and then performs the balancing operation for
obtaining a balanced glimpse vector. Experiments on six benchmarks, including
regular datasets, i.e., IIIT5K, SVT, ICDAR2013, and irregular datasets, i.e.,
ICDAR2015, SVTP, CUTE80, validate the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis. (arXiv:2207.10257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10257">
<div class="article-summary-box-inner">
<span><p>Over the years, 2D GANs have achieved great successes in photorealistic
portrait generation. However, they lack 3D understanding in the generation
process, thus they suffer from multi-view inconsistency problem. To alleviate
the issue, many 3D-aware GANs have been proposed and shown notable results, but
3D GANs struggle with editing semantic attributes. The controllability and
interpretability of 3D GANs have not been much explored. In this work, we
propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware
GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of
discovering semantic attributes during training and controlling them in an
unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN
to obtain a high-fidelity 3D-controllable generator. Unlike existing
latent-based methods allowing implicit pose control, the proposed
3D-controllable StyleGAN enables explicit pose control over portrait
generation. This distillation allows direct compatibility between 3D control
and many StyleGAN-based techniques (e.g., inversion and stylization), and also
brings an advantage in terms of computational resources. Our codes are
available at https://github.com/jgkwak95/SURF-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region Aware Video Object Segmentation with Deep Motion Modeling. (arXiv:2207.10258v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10258">
<div class="article-summary-box-inner">
<span><p>Current semi-supervised video object segmentation (VOS) methods usually
leverage the entire features of one frame to predict object masks and update
memory. This introduces significant redundant computations. To reduce
redundancy, we present a Region Aware Video Object Segmentation (RAVOS)
approach that predicts regions of interest (ROIs) for efficient object
segmentation and memory storage. RAVOS includes a fast object motion tracker to
predict their ROIs in the next frame. For efficient segmentation, object
features are extracted according to the ROIs, and an object decoder is designed
for object-level segmentation. For efficient memory storage, we propose motion
path memory to filter out redundant context by memorizing the features within
the motion path of objects between two frames. Besides RAVOS, we also propose a
large-scale dataset, dubbed OVOS, to benchmark the performance of VOS models
under occlusions. Evaluation on DAVIS and YouTube-VOS benchmarks and our new
OVOS dataset show that our method achieves state-of-the-art performance with
significantly faster inference time, e.g., 86.1 J&amp;F at 42 FPS on DAVIS and 84.4
J&amp;F at 23 FPS on YouTube-VOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-centric Image Cropping with Partition-aware and Content-preserving Features. (arXiv:2207.10269v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10269">
<div class="article-summary-box-inner">
<span><p>Image cropping aims to find visually appealing crops in an image, which is an
important yet challenging task. In this paper, we consider a specific and
practical application: human-centric image cropping, which focuses on the
depiction of a person. To this end, we propose a human-centric image cropping
method with two novel feature designs for the candidate crop: partition-aware
feature and content-preserving feature. For partition-aware feature, we divide
the whole image into nine partitions based on the human bounding box and treat
different partitions in a candidate crop differently conditioned on the human
information. For content-preserving feature, we predict a heatmap indicating
the important content to be included in a good crop, and extract the geometric
relation between the heatmap and a candidate crop. Extensive experiments
demonstrate that our method can perform favorably against state-of-the-art
image cropping methods on human-centric image cropping task. Code is available
at https://github.com/bcmi/Human-Centric-Image-Cropping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2207.10271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10271">
<div class="article-summary-box-inner">
<span><p>Learning to generate new images for a novel category based on only a few
images, named as few-shot image generation, has attracted increasing research
interest. Several state-of-the-art works have yielded impressive results, but
the diversity is still limited. In this work, we propose a novel Delta
Generative Adversarial Network (DeltaGAN), which consists of a reconstruction
subnetwork and a generation subnetwork. The reconstruction subnetwork captures
intra-category transformation, i.e., delta, between same-category pairs. The
generation subnetwork generates sample-specific delta for an input image, which
is combined with this input image to generate a new image within the same
category. Besides, an adversarial delta matching loss is designed to link the
above two subnetworks together. Extensive experiments on six benchmark datasets
demonstrate the effectiveness of our proposed method. Our code is available at
https://github.com/bcmi/DeltaGAN-Few-Shot-Image-Generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Forget Me: Accurate Background Recovery for Text Removal via Modeling Local-Global Context. (arXiv:2207.10273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10273">
<div class="article-summary-box-inner">
<span><p>Text removal has attracted increasingly attention due to its various
applications on privacy protection, document restoration, and text editing. It
has shown significant progress with deep neural network. However, most of the
existing methods often generate inconsistent results for complex background. To
address this issue, we propose a Contextual-guided Text Removal Network, termed
as CTRNet. CTRNet explores both low-level structure and high-level
discriminative context feature as prior knowledge to guide the process of
background restoration. We further propose a Local-global Content Modeling
(LGCM) block with CNNs and Transformer-Encoder to capture local features and
establish the long-term relationship among pixels globally. Finally, we
incorporate LGCM with context guidance for feature modeling and decoding.
Experiments on benchmark datasets, SCUT-EnsText and SCUT-Syn show that CTRNet
significantly outperforms the existing state-of-the-art methods. Furthermore, a
qualitative experiment on examination papers also demonstrates the
generalization ability of our method. The codes and supplement materials are
available at https://github.com/lcy0604/CTRNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond single receptive field: A receptive field fusion-and-stratification network for airborne laser scanning point cloud classification. (arXiv:2207.10278v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10278">
<div class="article-summary-box-inner">
<span><p>The classification of airborne laser scanning (ALS) point clouds is a
critical task of remote sensing and photogrammetry fields. Although recent deep
learning-based methods have achieved satisfactory performance, they have
ignored the unicity of the receptive field, which makes the ALS point cloud
classification remain challenging for the distinguishment of the areas with
complex structures and extreme scale variations. In this article, for the
objective of configuring multi-receptive field features, we propose a novel
receptive field fusion-and-stratification network (RFFS-Net). With a novel
dilated graph convolution (DGConv) and its extension annular dilated
convolution (ADConv) as basic building blocks, the receptive field fusion
process is implemented with the dilated and annular graph fusion (DAGFusion)
module, which obtains multi-receptive field feature representation through
capturing dilated and annular graphs with various receptive regions. The
stratification of the receptive fields with point sets of different resolutions
as the calculation bases is performed with Multi-level Decoders nested in
RFFS-Net and driven by the multi-level receptive field aggregation loss
(MRFALoss) to drive the network to learn in the direction of the supervision
labels with different resolutions. With receptive field
fusion-and-stratification, RFFS-Net is more adaptable to the classification of
regions with complex structures and extreme scale variations in large-scale ALS
point clouds. Evaluated on the ISPRS Vaihingen 3D dataset, our RFFS-Net
significantly outperforms the baseline approach by 5.3% on mF1 and 5.4% on
mIoU, accomplishing an overall accuracy of 82.1%, an mF1 of 71.6%, and an mIoU
of 58.2%. Furthermore, experiments on the LASDU dataset and the 2019 IEEE-GRSS
Data Fusion Contest dataset show that RFFS-Net achieves a new state-of-the-art
classification performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-based Point Cloud Denoising with Uniformity. (arXiv:2207.10279v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10279">
<div class="article-summary-box-inner">
<span><p>Point clouds captured by depth sensors are often contaminated by noises,
obstructing further analysis and applications. In this paper, we emphasize the
importance of point distribution uniformity to downstream tasks. We demonstrate
that point clouds produced by existing gradient-based denoisers lack uniformity
despite having achieved promising quantitative results. To this end, we propose
GPCD++, a gradient-based denoiser with an ultra-lightweight network named
UniNet to address uniformity. Compared with previous state-of-the-art methods,
our approach not only generates competitive or even better denoising results,
but also significantly improves uniformity which largely benefits applications
such as surface reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Visual Representations with Texts for Domain Generalization. (arXiv:2207.10285v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10285">
<div class="article-summary-box-inner">
<span><p>Reducing the representational discrepancy between source and target domains
is a key component to maximize the model generalization. In this work, we
advocate for leveraging natural language supervision for the domain
generalization task. We introduce two modules to ground visual representations
with texts containing typical reasoning of humans: (1) Visual and Textual Joint
Embedder and (2) Textual Explanation Generator. The former learns the
image-text joint embedding space where we can ground high-level
class-discriminative information into the model. The latter leverages an
explainable model and generates explanations justifying the rationale behind
its decision. To the best of our knowledge, this is the first work to leverage
the vision-and-language cross-modality approach for the domain generalization
task. Our experiments with a newly created CUB-DG benchmark dataset demonstrate
that cross-modality supervision can be successfully used to ground
domain-invariant visual representations and improve the model generalization.
Furthermore, in the large-scale DomainBed benchmark, our proposed method
achieves state-of-the-art results and ranks 1st in average performance for five
multi-domain datasets. The dataset and codes are available at
https://github.com/mswzeus/GVRT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Accurate Open-Set Recognition via Background-Class Regularization. (arXiv:2207.10287v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10287">
<div class="article-summary-box-inner">
<span><p>In open-set recognition (OSR), classifiers should be able to reject
unknown-class samples while maintaining high closed-set classification
accuracy. To effectively solve the OSR problem, previous studies attempted to
limit latent feature space and reject data located outside the limited space
via offline analyses, e.g., distance-based feature analyses, or complicated
network architectures. To conduct OSR via a simple inference process (without
offline analyses) in standard classifier architectures, we use distance-based
classifiers instead of conventional Softmax classifiers. Afterwards, we design
a background-class regularization strategy, which uses background-class data as
surrogates of unknown-class ones during training phase. Specifically, we
formulate a novel regularization loss suitable for distance-based classifiers,
which reserves sufficiently large class-wise latent feature spaces for known
classes and forces background-class samples to be located far away from the
limited spaces. Through our extensive experiments, we show that the proposed
method provides robust OSR results, while maintaining high closed-set
classification accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AugRmixAT: A Data Processing and Training Method for Improving Multiple Robustness and Generalization Performance. (arXiv:2207.10290v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10290">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are powerful, but they also have shortcomings such as
their sensitivity to adversarial examples, noise, blur, occlusion, etc.
Moreover, ensuring the reliability and robustness of deep neural network models
is crucial for their application in safety-critical areas. Much previous work
has been proposed to improve specific robustness. However, we find that the
specific robustness is often improved at the sacrifice of the additional
robustness or generalization ability of the neural network model. In
particular, adversarial training methods significantly hurt the generalization
performance on unperturbed data when improving adversarial robustness. In this
paper, we propose a new data processing and training method, called AugRmixAT,
which can simultaneously improve the generalization ability and multiple
robustness of neural network models. Finally, we validate the effectiveness of
AugRmixAT on the CIFAR-10/100 and Tiny-ImageNet datasets. The experiments
demonstrate that AugRmixAT can improve the model's generalization performance
while enhancing the white-box robustness, black-box robustness, common
corruption robustness, and partial occlusion robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Generation Network for Covert Transmission in Online Social Network. (arXiv:2207.10292v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10292">
<div class="article-summary-box-inner">
<span><p>Online social networks have stimulated communications over the Internet more
than ever, making it possible for secret message transmission over such noisy
channels. In this paper, we propose a Coverless Image Steganography Network,
called CIS-Net, that synthesizes a high-quality image directly conditioned on
the secret message to transfer. CIS-Net is composed of four modules, namely,
the Generation, Adversarial, Extraction, and Noise Module. The receiver can
extract the hidden message without any loss even the images have been distorted
by JPEG compression attacks. To disguise the behaviour of steganography, we
collected images in the context of profile photos and stickers and train our
network accordingly. As such, the generated images are more inclined to escape
from malicious detection and attack. The distinctions from previous image
steganography methods are majorly the robustness and losslessness against
diverse attacks. Experiments over diverse public datasets have manifested the
superior ability of anti-steganalysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Cross Attention Network in Facial Behavior Analysis. (arXiv:2207.10293v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10293">
<div class="article-summary-box-inner">
<span><p>Facial behavior analysis is a broad topic with various categories such as
facial emotion recognition, age and gender recognition, ... Many studies focus
on individual tasks while the multi-task learning approach is still open and
requires more research. In this paper, we present our solution and experiment
result for the Multi-Task Learning challenge of the Affective Behavior Analysis
in-the-wild competition. The challenge is a combination of three tasks: action
unit detection, facial expression recognition and valance-arousal estimation.
To address this challenge, we introduce a cross-attentive module to improve
multi-task learning performance. Additionally, a facial graph is applied to
capture the association among action units. As a result, we achieve the
evaluation measure of 1.24 on the validation data provided by the organizers,
which is better than the baseline result of 0.30.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Unsupervised Anomaly Localization in Industrial Images: A Survey. (arXiv:2207.10298v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10298">
<div class="article-summary-box-inner">
<span><p>Currently, deep learning-based visual inspection has been highly successful
with the help of supervised learning methods. However, in real industrial
scenarios, the scarcity of defect samples, the cost of annotation, and the lack
of a priori knowledge of defects may render supervised-based methods
ineffective. In recent years, unsupervised anomaly localization algorithms have
become more widely used in industrial inspection tasks. This paper aims to help
researchers in this field by comprehensively surveying recent achievements in
unsupervised anomaly localization in industrial images using deep learning. The
survey reviews more than 120 significant publications covering different
aspects of anomaly localization, mainly covering various concepts, challenges,
taxonomies, benchmark datasets, and quantitative performance comparisons of the
methods reviewed. In reviewing the achievements to date, this paper provides
detailed predictions and analysis of several future research directions. This
review provides detailed technical information for researchers interested in
industrial anomaly localization and who wish to apply it to the localization of
anomalies in other fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn From All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition. (arXiv:2207.10299v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10299">
<div class="article-summary-box-inner">
<span><p>Noisy label Facial Expression Recognition (FER) is more challenging than
traditional noisy label classification tasks due to the inter-class similarity
and the annotation ambiguity. Recent works mainly tackle this problem by
filtering out large-loss samples. In this paper, we explore dealing with noisy
labels from a new feature-learning perspective. We find that FER models
remember noisy samples by focusing on a part of the features that can be
considered related to the noisy labels instead of learning from the whole
features that lead to the latent truth. Inspired by that, we propose a novel
Erasing Attention Consistency (EAC) method to suppress the noisy samples during
the training process automatically. Specifically, we first utilize the flip
semantic consistency of facial images to design an imbalanced framework. We
then randomly erase input images and use flip attention consistency to prevent
the model from focusing on a part of the features. EAC significantly
outperforms state-of-the-art noisy label FER methods and generalizes well to
other tasks with a large number of classes like CIFAR100 and Tiny-ImageNet. The
code is available at
https://github.com/zyh-uaiaaaa/Erasing-Attention-Consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On an Edge-Preserving Variational Model for Optical Flow Estimation. (arXiv:2207.10302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10302">
<div class="article-summary-box-inner">
<span><p>It is well known that classical formulations resembling the Horn and Schunck
model are still largely competitive due to the modern implementation practices.
In most cases, these models outperform many modern flow estimation methods. In
view of this, we propose an effective implementation design for an
edge-preserving $L^1$ regularization approach to optical flow. The mathematical
well-posedness of our proposed model is studied in the space of functions of
bounded variations $BV(\Omega,\mathbb{R}^2)$. The implementation scheme is
designed in multiple steps. The flow field is computed using the robust
Chambolle-Pock primal-dual algorithm. Motivated by the recent studies of Castro
and Donoho we extend the heuristic of iterated median filtering to our flow
estimation. Further, to refine the flow edges we use the weighted median filter
established by Li and Osher as a post-processing step. Our experiments on the
Middlebury dataset show that the proposed method achieves the best average
angular and end-point errors compared to some of the state-of-the-art Horn and
Schunck based variational methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Leveraging Pre-trained Generative Adversarial Networks for Image Editing and Restoration. (arXiv:2207.10309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10309">
<div class="article-summary-box-inner">
<span><p>Generative adversarial networks (GANs) have drawn enormous attention due to
the simple yet effective training mechanism and superior image generation
quality. With the ability to generate photo-realistic high-resolution (e.g.,
$1024\times1024$) images, recent GAN models have greatly narrowed the gaps
between the generated images and the real ones. Therefore, many recent works
show emerging interest to take advantage of pre-trained GAN models by
exploiting the well-disentangled latent space and the learned GAN priors. In
this paper, we briefly review recent progress on leveraging pre-trained
large-scale GAN models from three aspects, i.e., 1) the training of large-scale
generative adversarial networks, 2) exploring and understanding the pre-trained
GAN models, and 3) leveraging these models for subsequent tasks like image
restoration and editing. More information about relevant methods and
repositories can be found at https://github.com/csmliu/pretrained-GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields. (arXiv:2207.10312v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10312">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis has recently been revolutionized by learning neural
radiance fields directly from sparse observations. However, rendering images
with this new paradigm is slow due to the fact that an accurate quadrature of
the volume rendering equation requires a large number of samples for each ray.
Previous work has mainly focused on speeding up the network evaluations that
are associated with each sample point, e.g., via caching of radiance values
into explicit spatial data structures, but this comes at the expense of model
compactness. In this paper, we propose a novel dual-network architecture that
takes an orthogonal direction by learning how to best reduce the number of
required sample points. To this end, we split our network into a sampling and
shading network that are jointly trained. Our training scheme employs fixed
sample positions along each ray, and incrementally introduces sparsity
throughout training to achieve high quality even at low sample counts. After
fine-tuning with the target number of samples, the resulting compact neural
representation can be rendered in real-time. Our experiments demonstrate that
our approach outperforms concurrent compact neural representations in terms of
quality and frame rate and performs on par with highly efficient hybrid
representations. Code and supplementary material is available at
https://thomasneff.github.io/adanerf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Learning of Optical Flow by Flow Supervisor. (arXiv:2207.10314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10314">
<div class="article-summary-box-inner">
<span><p>A training pipeline for optical flow CNNs consists of a pretraining stage on
a synthetic dataset followed by a fine tuning stage on a target dataset.
However, obtaining ground truth flows from a target video requires a tremendous
effort. This paper proposes a practical fine tuning method to adapt a
pretrained model to a target dataset without ground truth flows, which has not
been explored extensively. Specifically, we propose a flow supervisor for
self-supervision, which consists of parameter separation and a student output
connection. This design is aimed at stable convergence and better accuracy over
conventional self-supervision methods which are unstable on the fine tuning
task. Experimental results show the effectiveness of our method compared to
different self-supervision methods for semi-supervised learning. In addition,
we achieve meaningful improvements over state-of-the-art optical flow models on
Sintel and KITTI benchmarks by exploiting additional unlabeled datasets. Code
is available at https://github.com/iwbn/flow-supervisor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer. (arXiv:2207.10315v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10315">
<div class="article-summary-box-inner">
<span><p>Point cloud completion has become increasingly popular among generation tasks
of 3D point clouds, as it is a challenging yet indispensable problem to recover
the complete shape of a 3D object from its partial observation. In this paper,
we propose a novel SeedFormer to improve the ability of detail preservation and
recovery in point cloud completion. Unlike previous methods based on a global
feature vector, we introduce a new shape representation, namely Patch Seeds,
which not only captures general structures from partial inputs but also
preserves regional information of local patterns. Then, by integrating seed
features into the generation process, we can recover faithful details for
complete point clouds in a coarse-to-fine manner. Moreover, we devise an
Upsample Transformer by extending the transformer structure into basic
operations of point generators, which effectively incorporates spatial and
semantic relationships between neighboring points. Qualitative and quantitative
evaluations demonstrate that our method outperforms state-of-the-art completion
networks on several benchmark datasets. Our code is available at
https://github.com/hrzhou2/seedformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection. (arXiv:2207.10316v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10316">
<div class="article-summary-box-inner">
<span><p>Point clouds and RGB images are two general perceptional sources in
autonomous driving. The former can provide accurate localization of objects,
and the latter is denser and richer in semantic information. Recently,
AutoAlign presents a learnable paradigm in combining these two modalities for
3D object detection. However, it suffers from high computational cost
introduced by the global-wise attention. To solve the problem, we propose
Cross-Domain DeformCAFA module in this work. It attends to sparse learnable
sampling points for cross-modal relational modeling, which enhances the
tolerance to calibration error and greatly speeds up the feature aggregation
across different modalities. To overcome the complex GT-AUG under multi-modal
settings, we design a simple yet effective cross-modal augmentation strategy on
convex combination of image patches given their depth information. Moreover, by
carrying out a novel image-level dropout training scheme, our model is able to
infer in a dynamic manner. To this end, we propose AutoAlignV2, a faster and
stronger multi-modal 3D detection framework, built on top of AutoAlign.
Extensive experiments on nuScenes benchmark demonstrate the effectiveness and
efficiency of AutoAlignV2. Notably, our best model reaches 72.4 NDS on nuScenes
test leaderboard, achieving new state-of-the-art results among all published
multi-modal 3D object detectors. Code will be available at
https://github.com/zehuichen123/AutoAlignV2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient CNN Architecture Design Guided by Visualization. (arXiv:2207.10318v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10318">
<div class="article-summary-box-inner">
<span><p>Modern efficient Convolutional Neural Networks(CNNs) always use Depthwise
Separable Convolutions(DSCs) and Neural Architecture Search(NAS) to reduce the
number of parameters and the computational complexity. But some inherent
characteristics of networks are overlooked. Inspired by visualizing feature
maps and N$\times$N(N$&gt;$1) convolution kernels, several guidelines are
introduced in this paper to further improve parameter efficiency and inference
speed. Based on these guidelines, our parameter-efficient CNN architecture,
called \textit{VGNetG}, achieves better accuracy and lower latency than
previous networks with about 30%$\thicksim$50% parameters reduction. Our
VGNetG-1.0MP achieves 67.7% top-1 accuracy with 0.99M parameters and 69.2%
top-1 accuracy with 1.14M parameters on ImageNet classification dataset.
</p>
<p>Furthermore, we demonstrate that edge detectors can replace learnable
depthwise convolution layers to mix features by replacing the N$\times$N
kernels with fixed edge detection kernels. And our VGNetF-1.5MP archives
64.4%(-3.2%) top-1 accuracy and 66.2%(-1.4%) top-1 accuracy with additional
Gaussian kernels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OIMNet++: Prototypical Normalization and Localization-aware Learning for Person Search. (arXiv:2207.10320v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10320">
<div class="article-summary-box-inner">
<span><p>We address the task of person search, that is, localizing and re-identifying
query persons from a set of raw scene images. Recent approaches are typically
built upon OIMNet, a pioneer work on person search, that learns joint person
representations for performing both detection and person re-identification
(reID) tasks. To obtain the representations, they extract features from
pedestrian proposals, and then project them on a unit hypersphere with L2
normalization. These methods also incorporate all positive proposals, that
sufficiently overlap with the ground truth, equally to learn person
representations for reID. We have found that 1) the L2 normalization without
considering feature distributions degenerates the discriminative power of
person representations, and 2) positive proposals often also depict background
clutter and person overlaps, which could encode noisy features to person
representations. In this paper, we introduce OIMNet++ that addresses the
aforementioned limitations. To this end, we introduce a novel normalization
layer, dubbed ProtoNorm, that calibrates features from pedestrian proposals,
while considering a long-tail distribution of person IDs, enabling L2
normalized person representations to be discriminative. We also propose a
localization-aware feature learning scheme that encourages better-aligned
proposals to contribute more in learning discriminative representations.
Experimental results and analysis on standard person search benchmarks
demonstrate the effectiveness of OIMNet++.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Generative Model for Weakly Supervised Chest Anomaly Localization via Pseudo-paired Registration with Bilaterally Symmetrical Data Augmentation. (arXiv:2207.10324v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10324">
<div class="article-summary-box-inner">
<span><p>Image translation based on a generative adversarial network (GAN-IT) is a
promising method for precise localization of abnormal regions in chest X-ray
images (AL-CXR). However, heterogeneous unpaired datasets undermine existing
methods to extract key features and distinguish normal from abnormal cases,
resulting in inaccurate and unstable AL-CXR. To address this problem, we
propose an improved two-stage GAN-IT involving registration and data
augmentation. For the first stage, we introduce an invertible
deep-learning-based registration technique that virtually and reasonably
converts unpaired data into paired data for learning registration maps. This
novel approach achieves high registration performance. For the second stage, we
apply data augmentation to diversify anomaly locations by swapping the left and
right lung regions on the uniform registered frames, further improving the
performance by alleviating imbalance in data distribution showing left and
right lung lesions. Our method is intended for application to existing GAN-IT
models, allowing existing architecture to benefit from key features for
translation. By showing that the AL-CXR performance is uniformly improved when
applying the proposed method, we believe that GAN-IT for AL-CXR can be deployed
in clinical environments, even if learning data are scarce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UFO: Unified Feature Optimization. (arXiv:2207.10341v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10341">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel Unified Feature Optimization (UFO) paradigm for
training and deploying deep models under real-world and large-scale scenarios,
which requires a collection of multiple AI functions. UFO aims to benefit each
single task with a large-scale pretraining on all tasks. Compared with the well
known foundation model, UFO has two different points of emphasis, i.e.,
relatively smaller model size and NO adaptation cost: 1) UFO squeezes a wide
range of tasks into a moderate-sized unified model in a multi-task learning
manner and further trims the model size when transferred to down-stream tasks.
2) UFO does not emphasize transfer to novel tasks. Instead, it aims to make the
trimmed model dedicated for one or more already-seen task. With these two
characteristics, UFO provides great convenience for flexible deployment, while
maintaining the benefits of large-scale pretraining. A key merit of UFO is that
the trimming process not only reduces the model size and inference consumption,
but also even improves the accuracy on certain tasks. Specifically, UFO
considers the multi-task training and brings two-fold impact on the unified
model: some closely related tasks have mutual benefits, while some tasks have
conflicts against each other. UFO manages to reduce the conflicts and to
preserve the mutual benefits through a novel Network Architecture Search (NAS)
method. Experiments on a wide range of deep representation learning tasks
(i.e., face recognition, person re-identification, vehicle re-identification
and product retrieval) show that the model trimmed from UFO achieves higher
accuracy than its single-task-trained counterpart and yet has smaller model
size, validating the concept of UFO. Besides, UFO also supported the release of
17 billion parameters computer vision (CV) foundation model which is the
largest CV model in the industry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution. (arXiv:2207.10345v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10345">
<div class="article-summary-box-inner">
<span><p>Despite breakthrough advances in image super-resolution (SR) with
convolutional neural networks (CNNs), SR has yet to enjoy ubiquitous
applications due to the high computational complexity of SR networks.
Quantization is one of the promising approaches to solve this problem. However,
existing methods fail to quantize SR models with a bit-width lower than 8 bits,
suffering from severe accuracy loss due to fixed bit-width quantization applied
everywhere. In this work, to achieve high average bit-reduction with less
accuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ)
method for SR networks that allocates optimal bits to local regions and layers
adaptively based on the local contents of an input image. To this end, a
trainable bit selector module is introduced to determine the proper bit-width
and quantization level for each layer and a given local image patch. This
module is governed by the quantization sensitivity that is estimated by using
both the average magnitude of image gradient of the patch and the standard
deviation of the input feature of the layer. The proposed quantization pipeline
has been tested on various SR networks and evaluated on several standard
benchmarks extensively. Significant reduction in computational complexity and
the elevated restoration accuracy clearly demonstrate the effectiveness of the
proposed CADyQ framework for SR. Codes are available at
https://github.com/Cheeun/CADyQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto Machine Learning for Medical Image Analysis by Unifying the Search on Data Augmentation and Neural Architecture. (arXiv:2207.10351v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10351">
<div class="article-summary-box-inner">
<span><p>Automated data augmentation, which aims at engineering augmentation policy
automatically, recently draw a growing research interest. Many previous
auto-augmentation methods utilized a Density Matching strategy by evaluating
policies in terms of the test-time augmentation performance. In this paper, we
theoretically and empirically demonstrated the inconsistency between the train
and validation set of small-scale medical image datasets, referred to as
in-domain sampling bias. Next, we demonstrated that the in-domain sampling bias
might cause the inefficiency of Density Matching. To address the problem, an
improved augmentation search strategy, named Augmented Density Matching, was
proposed by randomly sampling policies from a prior distribution for training.
Moreover, an efficient automatical machine learning(AutoML) algorithm was
proposed by unifying the search on data augmentation and neural architecture.
Experimental results indicated that the proposed methods outperformed
state-of-the-art approaches on MedMNIST, a pioneering benchmark designed for
AutoML in medical image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Data with Noisy Labels Using Temporal Self-Ensemble. (arXiv:2207.10354v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10354">
<div class="article-summary-box-inner">
<span><p>There are inevitably many mislabeled data in real-world datasets. Because
deep neural networks (DNNs) have an enormous capacity to memorize noisy labels,
a robust training scheme is required to prevent labeling errors from degrading
the generalization performance of DNNs. Current state-of-the-art methods
present a co-training scheme that trains dual networks using samples associated
with small losses. In practice, however, training two networks simultaneously
can burden computing resources. In this study, we propose a simple yet
effective robust training scheme that operates by training only a single
network. During training, the proposed method generates temporal self-ensemble
by sampling intermediate network parameters from the weight trajectory formed
by stochastic gradient descent optimization. The loss sum evaluated with these
self-ensembles is used to identify incorrectly labeled samples. In parallel,
our method generates multi-view predictions by transforming an input data into
various forms and considers their agreement to identify incorrectly labeled
samples. By combining the aforementioned metrics, we present the proposed {\it
self-ensemble-based robust training} (SRT) method, which can filter the samples
with noisy labels to reduce their influence on training. Experiments on
widely-used public datasets demonstrate that the proposed method achieves a
state-of-the-art performance in some categories without training the dual
networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LocVTP: Video-Text Pre-training for Temporal Localization. (arXiv:2207.10362v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10362">
<div class="article-summary-box-inner">
<span><p>Video-Text Pre-training (VTP) aims to learn transferable representations for
various downstream tasks from large-scale web videos. To date, almost all
existing VTP methods are limited to retrieval-based downstream tasks, e.g.,
video retrieval, whereas their transfer potentials on localization-based tasks,
e.g., temporal grounding, are under-explored. In this paper, we experimentally
analyze and demonstrate the incompatibility of current VTP methods with
localization tasks, and propose a novel Localization-oriented Video-Text
Pre-training framework, dubbed as LocVTP. Specifically, we perform the
fine-grained contrastive alignment as a complement to the coarse-grained one by
a clip-word correspondence discovery scheme. To further enhance the temporal
reasoning ability of the learned feature, we propose a context projection head
and a temporal aware contrastive loss to perceive the contextual relationships.
Extensive experiments on four downstream tasks across six datasets demonstrate
that our LocVTP achieves state-of-the-art performance on both retrieval-based
and localization-based tasks. Furthermore, we conduct comprehensive ablation
studies and thorough analyses to explore the optimum model designs and training
strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Land Classification in Satellite Images by Injecting Traditional Features to CNN Models. (arXiv:2207.10368v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10368">
<div class="article-summary-box-inner">
<span><p>Deep learning methods have been successfully applied to remote sensing
problems for several years. Among these methods, CNN based models have high
accuracy in solving the land classification problem using satellite or aerial
images. Although these models have high accuracy, this generally comes with
large memory size requirements. On the other hand, it is desirable to have
small-sized models for applications, such as the ones implemented on unmanned
aerial vehicles, with low memory space. Unfortunately, small-sized CNN models
do not provide high accuracy as with their large-sized versions. In this study,
we propose a novel method to improve the accuracy of CNN models, especially the
ones with small size, by injecting traditional features to them. To test the
effectiveness of the proposed method, we applied it to the CNN models
SqueezeNet, MobileNetV2, ShuffleNetV2, VGG16, and ResNet50V2 having size 0.5 MB
to 528 MB. We used the sample mean, gray level co-occurrence matrix features,
Hu moments, local binary patterns, histogram of oriented gradients, and color
invariants as traditional features for injection. We tested the proposed method
on the EuroSAT dataset to perform land classification. Our experimental results
show that the proposed method significantly improves the land classification
accuracy especially when applied to small-sized CNN models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Saliency Query Network for Efficient Video Recognition. (arXiv:2207.10379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10379">
<div class="article-summary-box-inner">
<span><p>Efficient video recognition is a hot-spot research topic with the explosive
growth of multimedia data on the Internet and mobile devices. Most existing
methods select the salient frames without awareness of the class-specific
saliency scores, which neglect the implicit association between the saliency of
frames and its belonging category. To alleviate this issue, we devise a novel
Temporal Saliency Query (TSQ) mechanism, which introduces class-specific
information to provide fine-grained cues for saliency measurement.
Specifically, we model the class-specific saliency measuring process as a
query-response task. For each category, the common pattern of it is employed as
a query and the most salient frames are responded to it. Then, the calculated
similarities are adopted as the frame saliency scores. To achieve it, we
propose a Temporal Saliency Query Network (TSQNet) that includes two
instantiations of the TSQ mechanism based on visual appearance similarities and
textual event-object relations. Afterward, cross-modality interactions are
imposed to promote the information exchange between them. Finally, we use the
class-specific saliencies of the most confident categories generated by two
modalities to perform the selection of salient frames. Extensive experiments
demonstrate the effectiveness of our method by achieving state-of-the-art
results on ActivityNet, FCVID and Mini-Kinetics datasets. Our project page is
at https://lawrencexia2008.github.io/projects/tsqnet .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose for Everything: Towards Category-Agnostic Pose Estimation. (arXiv:2207.10387v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10387">
<div class="article-summary-box-inner">
<span><p>Existing works on 2D pose estimation mainly focus on a certain category, e.g.
human, animal, and vehicle. However, there are lots of application scenarios
that require detecting the poses/keypoints of the unseen class of objects. In
this paper, we introduce the task of Category-Agnostic Pose Estimation (CAPE),
which aims to create a pose estimation model capable of detecting the pose of
any class of object given only a few samples with keypoint definition. To
achieve this goal, we formulate the pose estimation problem as a keypoint
matching problem and design a novel CAPE framework, termed POse Matching
Network (POMNet). A transformer-based Keypoint Interaction Module (KIM) is
proposed to capture both the interactions among different keypoints and the
relationship between the support and query images. We also introduce
Multi-category Pose (MP-100) dataset, which is a 2D pose dataset of 100 object
categories containing over 20K instances and is well-designed for developing
CAPE algorithms. Experiments show that our method outperforms other baseline
approaches by a large margin. Codes and data are available at
https://github.com/luminxu/Pose-for-Everything.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition. (arXiv:2207.10388v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10388">
<div class="article-summary-box-inner">
<span><p>It is challenging for artificial intelligence systems to achieve accurate
video recognition under the scenario of low computation costs. Adaptive
inference based efficient video recognition methods typically preview videos
and focus on salient parts to reduce computation costs. Most existing works
focus on complex networks learning with video classification based objectives.
Taking all frames as positive samples, few of them pay attention to the
discrimination between positive samples (salient frames) and negative samples
(non-salient frames) in supervisions. To fill this gap, in this paper, we
propose a novel Non-saliency Suppression Network (NSNet), which effectively
suppresses the responses of non-salient frames. Specifically, on the frame
level, effective pseudo labels that can distinguish between salient and
non-salient frames are generated to guide the frame saliency learning. On the
video level, a temporal attention module is learned under dual video-level
supervisions on both the salient and the non-salient representations. Saliency
measurements from both two levels are combined for exploitation of
multi-granularity complementary information. Extensive experiments conducted on
four well-known benchmarks verify our NSNet not only achieves the
state-of-the-art accuracy-efficiency trade-off but also present a significantly
faster (2.4~4.3x) practical inference speed than state-of-the-art methods. Our
project page is at https://lawrencexia2008.github.io/projects/nsnet .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Error Compensation Framework for Flow-Guided Video Inpainting. (arXiv:2207.10391v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10391">
<div class="article-summary-box-inner">
<span><p>The key to video inpainting is to use correlation information from as many
reference frames as possible. Existing flow-based propagation methods split the
video synthesis process into multiple steps: flow completion -&gt; pixel
propagation -&gt; synthesis. However, there is a significant drawback that the
errors in each step continue to accumulate and amplify in the next step. To
this end, we propose an Error Compensation Framework for Flow-guided Video
Inpainting (ECFVI), which takes advantage of the flow-based method and offsets
its weaknesses. We address the weakness with the newly designed flow completion
module and the error compensation network that exploits the error guidance map.
Our approach greatly improves the temporal consistency and the visual quality
of the completed videos. Experimental results show the superior performance of
our proposed method with the speed up of x6, compared to the state-of-the-art
methods. In addition, we present a new benchmark dataset for evaluation by
supplementing the weaknesses of existing test datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FADE: Fusing the Assets of Decoder and Encoder for Task-Agnostic Upsampling. (arXiv:2207.10392v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10392">
<div class="article-summary-box-inner">
<span><p>We consider the problem of task-agnostic feature upsampling in dense
prediction where an upsampling operator is required to facilitate both
region-sensitive tasks like semantic segmentation and detail-sensitive tasks
such as image matting. Existing upsampling operators often can work well in
either type of the tasks, but not both. In this work, we present FADE, a novel,
plug-and-play, and task-agnostic upsampling operator. FADE benefits from three
design choices: i) considering encoder and decoder features jointly in
upsampling kernel generation; ii) an efficient semi-shift convolutional
operator that enables granular control over how each feature point contributes
to upsampling kernels; iii) a decoder-dependent gating mechanism for enhanced
detail delineation. We first study the upsampling properties of FADE on toy
data and then evaluate it on large-scale semantic segmentation and image
matting. In particular, FADE reveals its effectiveness and task-agnostic
characteristic by consistently outperforming recent dynamic upsampling
operators in different tasks. It also generalizes well across convolutional and
transformer architectures with little computational overhead. Our work
additionally provides thoughtful insights on what makes for task-agnostic
upsampling. Code is available at: <a href="http://lnkiy.in/fade_in">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sobolev Training for Implicit Neural Representations with Approximated Image Derivatives. (arXiv:2207.10395v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10395">
<div class="article-summary-box-inner">
<span><p>Recently, Implicit Neural Representations (INRs) parameterized by neural
networks have emerged as a powerful and promising tool to represent different
kinds of signals due to its continuous, differentiable properties, showing
superiorities to classical discretized representations. However, the training
of neural networks for INRs only utilizes input-output pairs, and the
derivatives of the target output with respect to the input, which can be
accessed in some cases, are usually ignored. In this paper, we propose a
training paradigm for INRs whose target output is image pixels, to encode image
derivatives in addition to image values in the neural network. Specifically, we
use finite differences to approximate image derivatives. We show how the
training paradigm can be leveraged to solve typical INRs problems, i.e., image
regression and inverse rendering, and demonstrate this training paradigm can
improve the data-efficiency and generalization capabilities of INRs. The code
of our method is available at
\url{https://github.com/megvii-research/Sobolev_INRs}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights. (arXiv:2207.10398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10398">
<div class="article-summary-box-inner">
<span><p>A profound understanding of inter-agent relationships and motion behaviors is
important to achieve high-quality planning when navigating in complex
scenarios, especially at urban traffic intersections. We present a trajectory
prediction approach with respect to traffic lights, D2-TPred, which uses a
spatial dynamic interaction graph (SDG) and a behavior dependency graph (BDG)
to handle the problem of discontinuous dependency in the spatial-temporal
space. Specifically, the SDG is used to capture spatial interactions by
reconstructing sub-graphs for different agents with dynamic and changeable
characteristics during each frame. The BDG is used to infer motion tendency by
modeling the implicit dependency of the current state on priors behaviors,
especially the discontinuous motions corresponding to acceleration,
deceleration, or turning direction. Moreover, we present a new dataset for
vehicle trajectory prediction under traffic lights called VTP-TL. Our
experimental results show that our model achieves more than {20.45% and 20.78%
}improvement in terms of ADE and FDE, respectively, on VTP-TL as compared to
other trajectory prediction algorithms. The dataset and code are available at:
https://github.com/VTP-TL/D2-TPred.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correspondence Matters for Video Referring Expression Comprehension. (arXiv:2207.10400v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10400">
<div class="article-summary-box-inner">
<span><p>We investigate the problem of video Referring Expression Comprehension (REC),
which aims to localize the referent objects described in the sentence to visual
regions in the video frames. Despite the recent progress, existing methods
suffer from two problems: 1) inconsistent localization results across video
frames; 2) confusion between the referent and contextual objects. To this end,
we propose a novel Dual Correspondence Network (dubbed as DCNet) which
explicitly enhances the dense associations in both the inter-frame and
cross-modal manners. Firstly, we aim to build the inter-frame correlations for
all existing instances within the frames. Specifically, we compute the
inter-frame patch-wise cosine similarity to estimate the dense alignment and
then perform the inter-frame contrastive learning to map them close in feature
space. Secondly, we propose to build the fine-grained patch-word alignment to
associate each patch with certain words. Due to the lack of this kind of
detailed annotations, we also predict the patch-word correspondence through the
cosine similarity. Extensive experiments demonstrate that our DCNet achieves
state-of-the-art performance on both video and image REC benchmarks.
Furthermore, we conduct comprehensive ablation studies and thorough analyses to
explore the optimal model designs. Notably, our inter-frame and cross-modal
contrastive losses are plug-and-play functions and are applicable to any video
REC architectures. For example, by building on top of Co-grounding, we boost
the performance by 1.48% absolute improvement on Accu.@0.5 for VID-Sentence
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Deepfake by Creating Spatio-Temporal Regularity Disruption. (arXiv:2207.10402v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10402">
<div class="article-summary-box-inner">
<span><p>Despite encouraging progress in deepfake detection, generalization to unseen
forgery types remains a significant challenge due to the limited forgery clues
explored during training. In contrast, we notice a common phenomenon in
deepfake: fake video creation inevitably disrupts the statistical regularity in
original videos. Inspired by this observation, we propose to boost the
generalization of deepfake detection by distinguishing the "regularity
disruption" that does not appear in real videos. Specifically, by carefully
examining the spatial and temporal properties, we propose to disrupt a real
video through a Pseudo-fake Generator and create a wide range of pseudo-fake
videos for training. Such practice allows us to achieve deepfake detection
without using fake videos and improves the generalization ability in a simple
and efficient manner. To jointly capture the spatial and temporal disruptions,
we propose a Spatio-Temporal Enhancement block to learn the regularity
disruption across space and time on our self-created videos. Through
comprehensive experiments, our method exhibits excellent performance on several
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-aware Modular Capsule Routing for Visual Question Answering. (arXiv:2207.10404v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10404">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) is fundamentally compositional in nature, and
many questions are simply answered by decomposing them into modular
sub-problems. The recent proposed Neural Module Network (NMN) employ this
strategy to question answering, whereas heavily rest with off-the-shelf layout
parser or additional expert policy regarding the network architecture design
instead of learning from the data. These strategies result in the
unsatisfactory adaptability to the semantically-complicated variance of the
inputs, thereby hindering the representational capacity and generalizability of
the model. To tackle this problem, we propose a Semantic-aware modUlar caPsulE
Routing framework, termed as SUPER, to better capture the instance-specific
vision-semantic characteristics and refine the discriminative representations
for prediction. Particularly, five powerful specialized modules as well as
dynamic routers are tailored in each layer of the SUPER network, and the
compact routing spaces are constructed such that a variety of customizable
routes can be sufficiently exploited and the vision-semantic representations
can be explicitly calibrated. We comparatively justify the effectiveness and
generalization ability of our proposed SUPER scheme over five benchmark
datasets, as well as the parametric-efficient advantage. It is worth
emphasizing that this work is not to pursue the state-of-the-art results in
VQA. Instead, we expect that our model is responsible to provide a novel
perspective towards architecture learning and representation calibration for
VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence Models for Drone vs Bird Classification. (arXiv:2207.10409v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10409">
<div class="article-summary-box-inner">
<span><p>Drone detection has become an essential task in object detection as drone
costs have decreased and drone technology has improved. It is, however,
difficult to detect distant drones when there is weak contrast, long range, and
low visibility. In this work, we propose several sequence classification
architectures to reduce the detected false-positive ratio of drone tracks.
Moreover, we propose a new drone vs. bird sequence classification dataset to
train and evaluate the proposed architectures. 3D CNN, LSTM, and Transformer
based sequence classification architectures have been trained on the proposed
dataset to show the effectiveness of the proposed idea. As experiments show,
using sequence information, bird classification and overall F1 scores can be
increased by up to 73% and 35%, respectively. Among all sequence classification
models, R(2+1)D-based fully convolutional model yields the best transfer
learning and fine-tuning results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KD-MVS: Knowledge Distillation Based Self-supervised Learning for MVS. (arXiv:2207.10425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10425">
<div class="article-summary-box-inner">
<span><p>Supervised multi-view stereo (MVS) methods have achieved remarkable progress
in terms of reconstruction quality, but suffer from the challenge of collecting
large-scale ground-truth depth. In this paper, we propose a novel
self-supervised training pipeline for MVS based on knowledge distillation,
termed \textit{KD-MVS}, which mainly consists of self-supervised teacher
training and distillation-based student training. Specifically, the teacher
model is trained in a self-supervised fashion using both photometric and
featuremetric consistency. Then we distill the knowledge of the teacher model
to the student model through probabilistic knowledge transferring. With the
supervision of validated knowledge, the student model is able to outperform its
teacher by a large margin. Extensive experiments performed on multiple datasets
show our method can even outperform supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StreamYOLO: Real-time Object Detection for Streaming Perception. (arXiv:2207.10433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10433">
<div class="article-summary-box-inner">
<span><p>The perceptive models of autonomous driving require fast inference within a
low latency for safety. While existing works ignore the inevitable
environmental changes after processing, streaming perception jointly evaluates
the latency and accuracy into a single metric for video online perception,
guiding the previous works to search trade-offs between accuracy and speed. In
this paper, we explore the performance of real time models on this metric and
endow the models with the capacity of predicting the future, significantly
improving the results for streaming perception. Specifically, we build a simple
framework with two effective modules. One is a Dual Flow Perception module
(DFP). It consists of dynamic flow and static flow in parallel to capture
moving tendency and basic detection feature, respectively. Trend Aware Loss
(TAL) is the other module which adaptively generates loss weight for each
object with its moving speed. Realistically, we consider multiple velocities
driving scene and further propose Velocity-awared streaming AP (VsAP) to
jointly evaluate the accuracy. In this realistic setting, we design a efficient
mix-velocity training strategy to guide detector perceive any velocities. Our
simple method achieves the state-of-the-art performance on Argoverse-HD dataset
and improves the sAP and VsAP by 4.7% and 8.2% respectively compared to the
strong baseline, validating its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using Unsupervised Domain-Classifier Guided Network. (arXiv:2207.10434v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10434">
<div class="article-summary-box-inner">
<span><p>Shadow removal from a single image is generally still an open problem. Most
existing learning-based methods use supervised learning and require a large
number of paired images (shadow and corresponding non-shadow images) for
training. A recent unsupervised method, Mask-ShadowGAN, addresses this
limitation. However, it requires a binary mask to represent shadow regions,
making it inapplicable to soft shadows. To address the problem, in this paper,
we propose an unsupervised domain-classifier guided shadow removal network,
DC-ShadowNet. Specifically, we propose to integrate a shadow/shadow-free domain
classifier into a generator and its discriminator, enabling them to focus on
shadow regions. To train our network, we introduce novel losses based on
physics-based shadow-free chromaticity, shadow-robust perceptual features, and
boundary smoothness. Moreover, we show that our unsupervised network can be
used for test-time training that further improves the results. Our experiments
show that all these novel components allow our method to handle soft shadows,
and also to perform better on hard shadows both quantitatively and
qualitatively than the existing state-of-the-art shadow removal methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Trajectory Prediction via Neural Social Physics. (arXiv:2207.10435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10435">
<div class="article-summary-box-inner">
<span><p>Trajectory prediction has been widely pursued in many fields, and many
model-based and model-free methods have been explored. The former include
rule-based, geometric or optimization-based models, and the latter are mainly
comprised of deep learning approaches. In this paper, we propose a new method
combining both methodologies based on a new Neural Differential Equation model.
Our new model (Neural Social Physics or NSP) is a deep neural network within
which we use an explicit physics model with learnable parameters. The explicit
physics model serves as a strong inductive bias in modeling pedestrian
behaviors, while the rest of the network provides a strong data-fitting
capability in terms of system parameter estimation and dynamics stochasticity
modeling. We compare NSP with 15 recent deep learning methods on 6 datasets and
improve the state-of-the-art performance by 5.56%-70%. Besides, we show that
NSP has better generalizability in predicting plausible trajectories in
drastically different scenarios where the density is 2-5 times as high as the
testing data. Finally, we show that the physics model in NSP can provide
plausible explanations for pedestrian behaviors, as opposed to black-box deep
learning. Code is available:
https://github.com/realcrane/Human-Trajectory-Prediction-via-Neural-Social-Physics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Relations among Cross-Frame Affinities for Video Semantic Segmentation. (arXiv:2207.10436v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10436">
<div class="article-summary-box-inner">
<span><p>The essence of video semantic segmentation (VSS) is how to leverage temporal
information for prediction. Previous efforts are mainly devoted to developing
new techniques to calculate the cross-frame affinities such as optical flow and
attention. Instead, this paper contributes from a different angle by mining
relations among cross-frame affinities, upon which better temporal information
aggregation could be achieved. We explore relations among affinities in two
aspects: single-scale intrinsic correlations and multi-scale relations.
Inspired by traditional feature processing, we propose Single-scale Affinity
Refinement (SAR) and Multi-scale Affinity Aggregation (MAA). To make it
feasible to execute MAA, we propose a Selective Token Masking (STM) strategy to
select a subset of consistent reference tokens for different scales when
calculating affinities, which also improves the efficiency of our method. At
last, the cross-frame affinities strengthened by SAR and MAA are adopted for
adaptively aggregating temporal information. Our experiments demonstrate that
the proposed method performs favorably against state-of-the-art VSS methods.
The code is publicly available at https://github.com/GuoleiSun/VSS-MRCFA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COBRA: Cpu-Only aBdominal oRgan segmentAtion. (arXiv:2207.10446v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10446">
<div class="article-summary-box-inner">
<span><p>Abdominal organ segmentation is a difficult and time-consuming task. To
reduce the burden on clinical experts, fully-automated methods are highly
desirable. Current approaches are dominated by Convolutional Neural Networks
(CNNs) however the computational requirements and the need for large data sets
limit their application in practice. By implementing a small and efficient
custom 3D CNN, compiling the trained model and optimizing the computational
graph: our approach produces high accuracy segmentations (Dice Similarity
Coefficient (%): Liver: 97.3$\pm$1.3, Kidneys: 94.8$\pm$3.6, Spleen:
96.4$\pm$3.0, Pancreas: 80.9$\pm$10.1) at a rate of 1.6 seconds per image.
Crucially, we are able to perform segmentation inference solely on CPU (no GPU
required), thereby facilitating easy and widespread deployment of the model
without specialist hardware.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration. (arXiv:2207.10447v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10447">
<div class="article-summary-box-inner">
<span><p>Weakly Supervised Object Localization (WSOL), which aims to localize objects
by only using image-level labels, has attracted much attention because of its
low annotation cost in real applications. Recent studies leverage the advantage
of self-attention in visual Transformer for long-range dependency to re-active
semantic regions, aiming to avoid partial activation in traditional class
activation mapping (CAM). However, the long-range modeling in Transformer
neglects the inherent spatial coherence of the object, and it usually diffuses
the semantic-aware regions far from the object boundary, making localization
results significantly larger or far smaller. To address such an issue, we
introduce a simple yet effective Spatial Calibration Module (SCM) for accurate
WSOL, incorporating semantic similarities of patch tokens and their spatial
relationships into a unified diffusion model. Specifically, we introduce a
learnable parameter to dynamically adjust the semantic correlations and spatial
context intensities for effective information propagation. In practice, SCM is
designed as an external module of Transformer, and can be removed during
inference to reduce the computation cost. The object-sensitive localization
ability is implicitly embedded into the Transformer encoder through
optimization in the training phase. It enables the generated attention maps to
capture the sharper object boundaries and filter the object-irrelevant
background area. Extensive experimental results demonstrate the effectiveness
of the proposed method, which significantly outperforms its counterpart TS-CAM
on both CUB-200 and ImageNet-1K benchmarks. The code is available at
https://github.<a href="/abs/com/1641407">com/1641407</a>57/SCM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Spatio-Temporal Pyramid Transformer for Action Detection. (arXiv:2207.10448v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10448">
<div class="article-summary-box-inner">
<span><p>The task of action detection aims at deducing both the action category and
localization of the start and end moment for each action instance in a long,
untrimmed video. While vision Transformers have driven the recent advances in
video understanding, it is non-trivial to design an efficient architecture for
action detection due to the prohibitively expensive self-attentions over a long
sequence of video clips. To this end, we present an efficient hierarchical
Spatio-Temporal Pyramid Transformer (STPT) for action detection, building upon
the fact that the early self-attention layers in Transformers still focus on
local patterns. Specifically, we propose to use local window attention to
encode rich local spatio-temporal representations in the early stages while
applying global attention modules to capture long-term space-time dependencies
in the later stages. In this way, our STPT can encode both locality and
dependency with largely reduced redundancy, delivering a promising trade-off
between accuracy and efficiency. For example, with only RGB input, the proposed
STPT achieves 53.6% mAP on THUMOS14, surpassing I3D+AFSD RGB model by over 10%
and performing favorably against state-of-the-art AFSD that uses additional
flow features with 31% fewer GFLOPs, which serves as an effective and efficient
end-to-end Transformer-based framework for action detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Magic ELF: Image Deraining Meets Association Learning and Transformer. (arXiv:2207.10455v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10455">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) and Transformer have achieved great
success in multimedia applications. However, little effort has been made to
effectively and efficiently harmonize these two architectures to satisfy image
deraining. This paper aims to unify these two architectures to take advantage
of their learning merits for image deraining. In particular, the local
connectivity and translation equivariance of CNN and the global aggregation
ability of self-attention (SA) in Transformer are fully exploited for specific
local context and global structure representations. Based on the observation
that rain distribution reveals the degradation location and degree, we
introduce degradation prior to help background recovery and accordingly present
the association refinement deraining scheme. A novel multi-input attention
module (MAM) is proposed to associate rain perturbation removal and background
recovery. Moreover, we equip our model with effective depth-wise separable
convolutions to learn the specific feature representations and trade off
computational complexity. Extensive experiments show that our proposed method
(dubbed as ELF) outperforms the state-of-the-art approach (MPRNet) by 0.25 dB
on average, but only accounts for 11.7\% and 42.1\% of its computational cost
and parameters. The source code is available at
https://github.com/kuijiang94/Magic-ELF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aware Fine-Grained Correspondence. (arXiv:2207.10456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10456">
<div class="article-summary-box-inner">
<span><p>Establishing visual correspondence across images is a challenging and
essential task. Recently, an influx of self-supervised methods have been
proposed to better learn representations for visual correspondence. However, we
find that these methods often fail to leverage semantic information and
over-rely on the matching of low-level features. In contrast, human vision is
capable of distinguishing between distinct objects as a pretext to tracking.
Inspired by this paradigm, we propose to learn semantic-aware fine-grained
correspondence. Firstly, we demonstrate that semantic correspondence is
implicitly available through a rich set of image-level self-supervised methods.
We further design a pixel-level self-supervised learning objective which
specifically targets fine-grained correspondence. For downstream tasks, we fuse
these two kinds of complementary correspondence representations together,
demonstrating that they boost performance synergistically. Our method surpasses
previous state-of-the-art self-supervised methods using convolutional networks
on a variety of visual correspondence tasks, including video object
segmentation, human pose tracking, and human part tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Data Driven Estimation of Cluster Number in Multiplex Images using Embedded Density Outliers. (arXiv:2207.10469v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10469">
<div class="article-summary-box-inner">
<span><p>The usage of chemical imaging technologies is becoming a routine
accompaniment to traditional methods in pathology. Significant technological
advances have developed these next generation techniques to provide rich,
spatially resolved, multidimensional chemical images. The rise of digital
pathology has significantly enhanced the synergy of these imaging modalities
with optical microscopy and immunohistochemistry, enhancing our understanding
of the biological mechanisms and progression of diseases. Techniques such as
imaging mass cytometry provide labelled multidimensional (multiplex) images of
specific components used in conjunction with digital pathology techniques.
These powerful techniques generate a wealth of high dimensional data that
create significant challenges in data analysis. Unsupervised methods such as
clustering are an attractive way to analyse these data, however, they require
the selection of parameters such as the number of clusters. Here we propose a
methodology to estimate the number of clusters in an automatic data-driven
manner using a deep sparse autoencoder to embed the data into a lower
dimensional space. We compute the density of regions in the embedded space, the
majority of which are empty, enabling the high density regions to be detected
as outliers and provide an estimate for the number of clusters. This framework
provides a fully unsupervised and data-driven method to analyse
multidimensional data. In this work we demonstrate our method using 45
multiplex imaging mass cytometry datasets. Moreover, our model is trained using
only one of the datasets and the learned embedding is applied to the remaining
44 images providing an efficient process for data analysis. Finally, we
demonstrate the high computational efficiency of our method which is two orders
of magnitude faster than estimating via computing the sum squared distances as
a function of cluster number.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LPYOLO: Low Precision YOLO for Face Detection on FPGA. (arXiv:2207.10482v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10482">
<div class="article-summary-box-inner">
<span><p>In recent years, number of edge computing devices and artificial intelligence
applications on them have advanced excessively. In edge computing, decision
making processes and computations are moved from servers to edge devices.
Hence, cheap and low power devices are required. FPGAs are very low power,
inclined to do parallel operations and deeply suitable devices for running
Convolutional Neural Networks (CNN) which are the fundamental unit of an
artificial intelligence application. Face detection on surveillance systems is
the most expected application on the security market. In this work, TinyYolov3
architecture is redesigned and deployed for face detection. It is a CNN based
object detection method and developed for embedded systems. PYNQ-Z2 is selected
as a target board which has low-end Xilinx Zynq 7020 System-on-Chip (SoC) on
it. Redesigned TinyYolov3 model is defined in numerous bit width precisions
with Brevitas library which brings fundamental CNN layers and activations in
integer quantized form. Then, the model is trained in a quantized structure
with WiderFace dataset. In order to decrease latency and power consumption,
onchip memory of the FPGA is configured as a storage of whole network
parameters and the last activation function is modified as rescaled HardTanh
instead of Sigmoid. Also, high degree of parallelism is applied to logical
resources of the FPGA. The model is converted to an HLS based application with
using FINN framework and FINN-HLS library which includes the layer definitions
in C++. Later, the model is synthesized and deployed. CPU of the SoC is
employed with multithreading mechanism and responsible for preprocessing,
postprocessing and TCP/IP streaming operations. Consequently, 2.4 Watt total
board power consumption, 18 Frames-Per-Second (FPS) throughput and 0.757 mAP
accuracy rate on Easy category of the WiderFace are achieved with 4 bits
precision model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Confident Detection of Prostate Cancer using High Resolution Micro-ultrasound. (arXiv:2207.10485v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10485">
<div class="article-summary-box-inner">
<span><p>MOTIVATION: Detection of prostate cancer during transrectal ultrasound-guided
biopsy is challenging. The highly heterogeneous appearance of cancer, presence
of ultrasound artefacts, and noise all contribute to these difficulties. Recent
advancements in high-frequency ultrasound imaging - micro-ultrasound - have
drastically increased the capability of tissue imaging at high resolution. Our
aim is to investigate the development of a robust deep learning model
specifically for micro-ultrasound-guided prostate cancer biopsy. For the model
to be clinically adopted, a key challenge is to design a solution that can
confidently identify the cancer, while learning from coarse histopathology
measurements of biopsy samples that introduce weak labels. METHODS: We use a
dataset of micro-ultrasound images acquired from 194 patients, who underwent
prostate biopsy. We train a deep model using a co-teaching paradigm to handle
noise in labels, together with an evidential deep learning method for
uncertainty estimation. We evaluate the performance of our model using the
clinically relevant metric of accuracy vs. confidence. RESULTS: Our model
achieves a well-calibrated estimation of predictive uncertainty with area under
the curve of 88$\%$. The use of co-teaching and evidential deep learning in
combination yields significantly better uncertainty estimation than either
alone. We also provide a detailed comparison against state-of-the-art in
uncertainty estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Localisation and Colored Mesh Reconstruction Architecture for 3D Visual Feedback in Robotic Exploration Missions. (arXiv:2207.10489v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10489">
<div class="article-summary-box-inner">
<span><p>This paper introduces an Online Localisation and Colored Mesh Reconstruction
(OLCMR) ROS perception architecture for ground exploration robots aiming to
perform robust Simultaneous Localisation And Mapping (SLAM) in challenging
unknown environments and provide an associated colored 3D mesh representation
in real time. It is intended to be used by a remote human operator to easily
visualise the mapped environment during or after the mission or as a
development base for further researches in the field of exploration robotics.
The architecture is mainly composed of carefully-selected open-source ROS
implementations of a LiDAR-based SLAM algorithm alongside a colored surface
reconstruction procedure using a point cloud and RGB camera images projected
into the 3D space. The overall performances are evaluated on the Newer College
handheld LiDAR-Vision reference dataset and on two experimental trajectories
gathered on board of representative wheeled robots in respectively urban and
countryside outdoor environments. Index Terms: Field Robots, Mapping, SLAM,
Colored Surface Reconstruction
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Event-Camera Depth Estimation and Outlier Rejection by Refocused Events Fusion. (arXiv:2207.10494v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10494">
<div class="article-summary-box-inner">
<span><p>Event cameras are bio-inspired sensors that offer advantages over traditional
cameras. They work asynchronously, sampling the scene with microsecond
resolution and producing a stream of brightness changes. This unconventional
output has sparked novel computer vision methods to unlock the camera's
potential. We tackle the problem of event-based stereo 3D reconstruction for
SLAM. Most event-based stereo methods try to exploit the camera's high temporal
resolution and event simultaneity across cameras to establish matches and
estimate depth. By contrast, we investigate how to estimate depth without
explicit data association by fusing Disparity Space Images (DSIs) originated in
efficient monocular methods. We develop fusion theory and apply it to design
multi-camera 3D reconstruction algorithms that produce state-of-the-art
results, as we confirm by comparing against four baseline methods and testing
on a variety of available datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient Adversarial Training on Vision Transformers. (arXiv:2207.10498v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10498">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT), as a powerful alternative to Convolutional Neural
Network (CNN), has received much attention. Recent work showed that ViTs are
also vulnerable to adversarial examples like CNNs. To build robust ViTs, an
intuitive way is to apply adversarial training since it has been shown as one
of the most effective ways to accomplish robust CNNs. However, one major
limitation of adversarial training is its heavy computational cost. The
self-attention mechanism adopted by ViTs is a computationally intense operation
whose expense increases quadratically with the number of input patches, making
adversarial training on ViTs even more time-consuming. In this work, we first
comprehensively study fast adversarial training on a variety of vision
transformers and illustrate the relationship between the efficiency and
robustness. Then, to expediate adversarial training on ViTs, we propose an
efficient Attention Guided Adversarial Training mechanism. Specifically,
relying on the specialty of self-attention, we actively remove certain patch
embeddings of each layer with an attention-guided dropping strategy during
adversarial training. The slimmed self-attention modules accelerate the
adversarial training on ViTs significantly. With only 65\% of the fast
adversarial training time, we match the state-of-the-art results on the
challenging ImageNet benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Retinal Image Registration Using a Keypoint-Based Vessel Structure Aligning Network. (arXiv:2207.10506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10506">
<div class="article-summary-box-inner">
<span><p>In ophthalmological imaging, multiple imaging systems, such as color fundus,
infrared, fluorescein angiography, optical coherence tomography (OCT) or OCT
angiography, are often involved to make a diagnosis of retinal disease.
Multi-modal retinal registration techniques can assist ophthalmologists by
providing a pixel-based comparison of aligned vessel structures in images from
different modalities or acquisition times. To this end, we propose an
end-to-end trainable deep learning method for multi-modal retinal image
registration. Our method extracts convolutional features from the vessel
structure for keypoint detection and description and uses a graph neural
network for feature matching. The keypoint detection and description network
and graph neural network are jointly trained in a self-supervised manner using
synthetic multi-modal image pairs and are guided by synthetically sampled
ground truth homographies. Our method demonstrates higher registration accuracy
as competing methods for our synthetic retinal dataset and generalizes well for
our real macula dataset and a public fundus dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Elderly Monitoring for Senior Safety by Lightweight Human Action Recognition. (arXiv:2207.10519v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10519">
<div class="article-summary-box-inner">
<span><p>With an increasing number of elders living alone, care-giving from a distance
becomes a compelling need, particularly for safety. Real-time monitoring and
action recognition are essential to raise an alert timely when abnormal
behaviors or unusual activities occur. While wearable sensors are widely
recognized as a promising solution, highly depending on user's ability and
willingness makes them inefficient. In contrast, video streams collected
through non-contact optical cameras provide richer information and release the
burden on elders. In this paper, leveraging the Independently-Recurrent neural
Network (IndRNN) we propose a novel Real-time Elderly Monitoring for senior
Safety (REMS) based on lightweight human action recognition (HAR) technology.
Using captured skeleton images, the REMS scheme is able to recognize abnormal
behaviors or actions and preserve the user's privacy. To achieve high accuracy,
the HAR module is trained and fine-tuned using multiple databases. An extensive
experimental study verified that REMS system performs action recognition
accurately and timely. REMS meets the design goals as a privacy-preserving
elderly safety monitoring system and possesses the potential to be adopted in
various smart monitoring systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Network Learning of Chemical Bond Representations in Spectral Indices and Features. (arXiv:2207.10530v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10530">
<div class="article-summary-box-inner">
<span><p>In this paper we investigate neural networks for classification in
hyperspectral imaging with a focus on connecting the architecture of the
network with the physics of the sensing and materials present. Spectroscopy is
the process of measuring light reflected or emitted by a material as a function
wavelength. Molecular bonds present in the material have vibrational
frequencies which affect the amount of light measured at each wavelength. Thus
the measured spectrum contains information about the particular chemical
constituents and types of bonds. For example, chlorophyll reflects more light
in the near-IR rage (800-900nm) than in the red (625-675nm) range, and this
difference can be measured using a normalized vegetation difference index
(NDVI), which is commonly used to detect vegetation presence, health, and type
in imagery collected at these wavelengths. In this paper we show that the
weights in a Neural Network trained on different vegetation classes learn to
measure this difference in reflectance. We then show that a Neural Network
trained on a more complex set of ten different polymer materials will learn
spectral 'features' evident in the weights for the network, and these features
can be used to reliably distinguish between the different types of polymers.
Examination of the weights provides a human-interpretable understanding of the
network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Primer on Topological Data Analysis to Support Image Analysis Tasks in Environmental Science. (arXiv:2207.10552v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10552">
<div class="article-summary-box-inner">
<span><p>Topological data analysis (TDA) is a tool from data science and mathematics
that is beginning to make waves in environmental science. In this work, we seek
to provide an intuitive and understandable introduction to a tool from TDA that
is particularly useful for the analysis of imagery, namely persistent homology.
We briefly discuss the theoretical background but focus primarily on
understanding the output of this tool and discussing what information it can
glean. To this end, we frame our discussion around a guiding example of
classifying satellite images from the Sugar, Fish, Flower, and Gravel Dataset
produced for the study of mesocale organization of clouds by Rasp et. al. in
2020 (arXiv:1906:01906). We demonstrate how persistent homology and its
vectorization, persistence landscapes, can be used in a workflow with a simple
machine learning algorithm to obtain good results, and explore in detail how we
can explain this behavior in terms of image-level features. One of the core
strengths of persistent homology is how interpretable it can be, so throughout
this paper we discuss not just the patterns we find, but why those results are
to be expected given what we know about the theory of persistent homology. Our
goal is that a reader of this paper will leave with a better understanding of
TDA and persistent homology, be able to identify problems and datasets of their
own for which persistent homology could be helpful, and gain an understanding
of results they obtain from applying the included GitHub example code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior. (arXiv:2207.10553v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10553">
<div class="article-summary-box-inner">
<span><p>Real-world behavior is often shaped by complex interactions between multiple
agents. To scalably study multi-agent behavior, advances in unsupervised and
self-supervised learning have enabled a variety of different behavioral
representations to be learned from trajectory data. To date, there does not
exist a unified set of benchmarks that can enable comparing methods
quantitatively and systematically across a broad set of behavior analysis
settings. We aim to address this by introducing a large-scale, multi-agent
trajectory dataset from real-world behavioral neuroscience experiments that
covers a range of behavior analysis tasks. Our dataset consists of trajectory
data from common model organisms, with 9.6 million frames of mouse data and 4.4
million frames of fly data, in a variety of experimental settings, such as
different strains, lengths of interaction, and optogenetic stimulation. A
subset of the frames also consist of expert-annotated behavior labels.
Improvements on our dataset corresponds to behavioral representations that work
across multiple organisms and is able to capture differences for common
behavior analysis tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Night Image Enhancement: When Layer Decomposition Meets Light-Effects Suppression. (arXiv:2207.10564v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10564">
<div class="article-summary-box-inner">
<span><p>Night images suffer not only from low light, but also from uneven
distributions of light. Most existing night visibility enhancement methods
focus mainly on enhancing low-light regions. This inevitably leads to over
enhancement and saturation in bright regions, such as those regions affected by
light effects (glare, floodlight, etc). To address this problem, we need to
suppress the light effects in bright regions while, at the same time, boosting
the intensity of dark regions. With this idea in mind, we introduce an
unsupervised method that integrates a layer decomposition network and a
light-effects suppression network. Given a single night image as input, our
decomposition network learns to decompose shading, reflectance and
light-effects layers, guided by unsupervised layer-specific prior losses. Our
light-effects suppression network further suppresses the light effects and, at
the same time, enhances the illumination in dark regions. This light-effects
suppression network exploits the estimated light-effects layer as the guidance
to focus on the light-effects regions. To recover the background details and
reduce hallucination/artefacts, we propose structure and high-frequency
consistency losses. Our quantitative and qualitative evaluations on real images
show that our method outperforms state-of-the-art methods in suppressing night
light effects and boosting the intensity of dark regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face-to-Face Co-Located Human-Human Social Interaction Analysis using Nonverbal Cues: A Survey. (arXiv:2207.10574v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10574">
<div class="article-summary-box-inner">
<span><p>This work presents a systematic review of recent efforts (since 2010) aimed
at automatic analysis of nonverbal cues displayed in face-to-face co-located
human-human social interactions. The main reason for focusing on nonverbal cues
is that these are the physical, machine detectable traces of social and
psychological phenomena. Therefore, detecting and understanding nonverbal cues
means, at least to a certain extent, to detect and understand social and
psychological phenomena. The covered topics are categorized into three as: a)
modeling social traits, such as leadership, dominance, personality traits, b)
social role recognition and social relations detection and c) interaction
dynamics analysis in terms of group cohesion, empathy, rapport and so forth. We
target the co-located interactions, in which the interactants are always
humans. The survey covers a wide spectrum of settings and scenarios, including
free-standing interactions, meetings, indoor and outdoor social exchanges,
dyadic conversations, and crowd dynamics. For each of them, the survey
considers the three main elements of nonverbal cues analysis, namely data,
sensing approaches and computational methodologies. The goal is to highlight
the main advances of the last decade, to point out existing limitations, and to
outline future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Designing An Illumination-Aware Network for Deep Image Relighting. (arXiv:2207.10582v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10582">
<div class="article-summary-box-inner">
<span><p>Lighting is a determining factor in photography that affects the style,
expression of emotion, and even quality of images. Creating or finding
satisfying lighting conditions, in reality, is laborious and time-consuming, so
it is of great value to develop a technology to manipulate illumination in an
image as post-processing. Although previous works have explored techniques
based on the physical viewpoint for relighting images, extensive supervisions
and prior knowledge are necessary to generate reasonable images, restricting
the generalization ability of these works. In contrast, we take the viewpoint
of image-to-image translation and implicitly merge ideas of the conventional
physical viewpoint. In this paper, we present an Illumination-Aware Network
(IAN) which follows the guidance from hierarchical sampling to progressively
relight a scene from a single image with high efficiency. In addition, an
Illumination-Aware Residual Block (IARB) is designed to approximate the
physical rendering process and to extract precise descriptors of light sources
for further manipulations. We also introduce a depth-guided geometry encoder
for acquiring valuable geometry- and structure-related representations once the
depth information is available. Experimental results show that our proposed
method produces better quantitative and qualitative relighting results than
previous state-of-the-art methods. The code and models are publicly available
on https://github.com/NK-CS-ZZL/IAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting 3D Object Detection via Object-Focused Image Fusion. (arXiv:2207.10589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10589">
<div class="article-summary-box-inner">
<span><p>3D object detection has achieved remarkable progress by taking point clouds
as the only input. However, point clouds often suffer from incomplete geometric
structures and the lack of semantic information, which makes detectors hard to
accurately classify detected objects. In this work, we focus on how to
effectively utilize object-level information from images to boost the
performance of point-based 3D detector. We present DeMF, a simple yet effective
method to fuse image information into point features. Given a set of point
features and image feature maps, DeMF adaptively aggregates image features by
taking the projected 2D location of the 3D point as reference. We evaluate our
method on the challenging SUN RGB-D dataset, improving state-of-the-art results
by a large margin (+2.1 mAP@0.25 and +2.3mAP@0.5). Code is available at
https://github.com/haoy945/DeMF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximate Differentiable Rendering with Algebraic Surfaces. (arXiv:2207.10606v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10606">
<div class="article-summary-box-inner">
<span><p>Differentiable renderers provide a direct mathematical link between an
object's 3D representation and images of that object. In this work, we develop
an approximate differentiable renderer for a compact, interpretable
representation, which we call Fuzzy Metaballs. Our approximate renderer focuses
on rendering shapes via depth maps and silhouettes. It sacrifices fidelity for
utility, producing fast runtimes and high-quality gradient information that can
be used to solve vision tasks. Compared to mesh-based differentiable renderers,
our method has forward passes that are 5x faster and backwards passes that are
30x faster. The depth maps and silhouette images generated by our method are
smooth and defined everywhere. In our evaluation of differentiable renderers
for pose estimation, we show that our method is the only one comparable to
classic techniques. In shape from silhouette, our method performs well using
only gradient descent and a per-pixel loss, without any surrogate losses or
regularization. These reconstructions work well even on natural video sequences
with segmentation artifacts. Project page:
https://leonidk.github.io/fuzzy-metaballs
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Statistic Shape Model for Myocardium Segmentation. (arXiv:2207.10607v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10607">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation and motion estimation of myocardium have always been
important in clinic field, which essentially contribute to the downstream
diagnosis. However, existing methods cannot always guarantee the shape
integrity for myocardium segmentation. In addition, motion estimation requires
point correspondence on the myocardium region across different frames. In this
paper, we propose a novel end-to-end deep statistic shape model to focus on
myocardium segmentation with both shape integrity and boundary correspondence
preserving. Specifically, myocardium shapes are represented by a fixed number
of points, whose variations are extracted by Principal Component Analysis
(PCA). Deep neural network is used to predict the transformation parameters
(both affine and deformation), which are then used to warp the mean point cloud
to the image domain. Furthermore, a differentiable rendering layer is
introduced to incorporate mask supervision into the framework to learn more
accurate point clouds. In this way, the proposed method is able to consistently
produce anatomically reasonable segmentation mask without post processing.
Additionally, the predicted point cloud guarantees boundary correspondence for
sequential images, which contributes to the downstream tasks, such as the
motion estimation of myocardium. We conduct several experiments to demonstrate
the effectiveness of the proposed method on several benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing. (arXiv:2207.10614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10614">
<div class="article-summary-box-inner">
<span><p>A key algorithm for understanding the world is material segmentation, which
assigns a label (metal, glass, etc.) to each pixel. We find that a model
trained on existing data underperforms in some settings and propose to address
this with a large-scale dataset of 3.2 million dense segments on 44,560 indoor
and outdoor images, which is 23x more segments than existing data. Our data
covers a more diverse set of scenes, objects, viewpoints and materials, and
contains a more fair distribution of skin types. We show that a model trained
on our data outperforms a state-of-the-art model across datasets and
viewpoints. We propose a large-scale scene parsing benchmark and baseline of
0.729 per-pixel accuracy, 0.585 mean class accuracy and 0.420 mean IoU across
46 materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaComp: Learning to Adapt for Online Depth Completion. (arXiv:2207.10623v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10623">
<div class="article-summary-box-inner">
<span><p>Relying on deep supervised or self-supervised learning, previous methods for
depth completion from paired single image and sparse depth data have achieved
impressive performance in recent years. However, facing a new environment where
the test data occurs online and differs from the training data in the RGB image
content and depth sparsity, the trained model might suffer severe performance
drop. To encourage the trained model to work well in such conditions, we expect
it to be capable of adapting to the new environment continuously and
effectively. To achieve this, we propose MetaComp. It utilizes the
meta-learning technique to simulate adaptation policies during the training
phase, and then adapts the model to new environments in a self-supervised
manner in testing. Considering that the input is multi-modal data, it would be
challenging to adapt a model to variations in two modalities simultaneously,
due to significant differences in structure and form of the two modal data.
Therefore, we further propose to disentangle the adaptation procedure in the
basic meta-learning training into two steps, the first one focusing on the
depth sparsity while the second attending to the image content. During testing,
we take the same strategy to adapt the model online to new multi-modal data.
Experimental results and comprehensive ablations show that our MetaComp is
capable of adapting to the depth completion in a new environment effectively
and robust to changes in different modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dynamical Systems Algorithm for Clustering in Hyperspectral Imagery. (arXiv:2207.10625v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10625">
<div class="article-summary-box-inner">
<span><p>In this paper we present a new dynamical systems algorithm for clustering in
hyperspectral images. The main idea of the algorithm is that data points are
\`pushed\' in the direction of increasing density and groups of pixels that end
up in the same dense regions belong to the same class. This is essentially a
numerical solution of the differential equation defined by the gradient of the
density of data points on the data manifold. The number of classes is automated
and the resulting clustering can be extremely accurate. In addition to
providing a accurate clustering, this algorithm presents a new tool for
understanding hyperspectral data in high dimensions. We evaluate the algorithm
on the Urban (Available at www.tec.ary.mil/Hypercube/) scene comparing
performance against the k-means algorithm using pre-identified classes of
materials as ground truth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Multiplane Images: Making a 2D GAN 3D-Aware. (arXiv:2207.10642v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10642">
<div class="article-summary-box-inner">
<span><p>What is really needed to make an existing 2D GAN 3D-aware? To answer this
question, we modify a classical GAN, i.e., StyleGANv2, as little as possible.
We find that only two modifications are absolutely necessary: 1) a multiplane
image style generator branch which produces a set of alpha maps conditioned on
their depth; 2) a pose-conditioned discriminator. We refer to the generated
output as a 'generative multiplane image' (GMPI) and emphasize that its
renderings are not only high-quality but also guaranteed to be view-consistent,
which makes GMPIs different from many prior works. Importantly, the number of
alpha maps can be dynamically adjusted and can differ between training and
inference, alleviating memory concerns and enabling fast training of GMPIs in
less than half a day at a resolution of $1024^2$. Our findings are consistent
across three challenging and common high-resolution datasets, including FFHQ,
AFHQv2, and MetFaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novel Class Discovery without Forgetting. (arXiv:2207.10659v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10659">
<div class="article-summary-box-inner">
<span><p>Humans possess an innate ability to identify and differentiate instances that
they are not familiar with, by leveraging and adapting the knowledge that they
have acquired so far. Importantly, they achieve this without deteriorating the
performance on their earlier learning. Inspired by this, we identify and
formulate a new, pragmatic problem setting of NCDwF: Novel Class Discovery
without Forgetting, which tasks a machine learning model to incrementally
discover novel categories of instances from unlabeled data, while maintaining
its performance on the previously seen categories. We propose 1) a method to
generate pseudo-latent representations which act as a proxy for (no longer
available) labeled data, thereby alleviating forgetting, 2) a
mutual-information based regularizer which enhances unsupervised discovery of
novel classes, and 3) a simple Known Class Identifier which aids generalized
inference when the testing data contains instances form both seen and unseen
categories. We introduce experimental protocols based on CIFAR-10, CIFAR-100
and ImageNet-1000 to measure the trade-off between knowledge retention and
novel class discovery. Our extensive evaluations reveal that existing models
catastrophically forget previously seen categories while identifying novel
categories, while our method is able to effectively balance between the
competing objectives. We hope our work will attract further research into this
newly identified pragmatic problem setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild. (arXiv:2207.10660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10660">
<div class="article-summary-box-inner">
<span><p>Recognizing scenes and objects in 3D from a single image is a longstanding
goal of computer vision with applications in robotics and AR/VR. For 2D
recognition, large datasets and scalable solutions have led to unprecedented
advances. In 3D, existing benchmarks are small in size and approaches
specialize in few object categories and specific domains, e.g. urban driving
scenes. Motivated by the success of 2D recognition, we revisit the task of 3D
object detection by introducing a large benchmark, called Omni3D. Omni3D
re-purposes and combines existing datasets resulting in 234k images annotated
with more than 3 million instances and 97 categories.3D detection at such scale
is challenging due to variations in camera intrinsics and the rich diversity of
scene and object types. We propose a model, called Cube R-CNN, designed to
generalize across camera and scene types with a unified approach. We show that
Cube R-CNN outperforms prior works on the larger Omni3D and existing
benchmarks. Finally, we prove that Omni3D is a powerful dataset for 3D object
recognition, show that it improves single-dataset performance and can
accelerate learning on new smaller datasets via pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In Defense of Online Models for Video Instance Segmentation. (arXiv:2207.10661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10661">
<div class="article-summary-box-inner">
<span><p>In recent years, video instance segmentation (VIS) has been largely advanced
by offline models, while online models gradually attracted less attention
possibly due to their inferior performance. However, online methods have their
inherent advantage in handling long video sequences and ongoing videos while
offline models fail due to the limit of computational resources. Therefore, it
would be highly desirable if online models can achieve comparable or even
better performance than offline models. By dissecting current online models and
offline models, we demonstrate that the main cause of the performance gap is
the error-prone association between frames caused by the similar appearance
among different instances in the feature space. Observing this, we propose an
online framework based on contrastive learning that is able to learn more
discriminative instance embeddings for association and fully exploit history
information for stability. Despite its simplicity, our method outperforms all
online and offline methods on three benchmarks. Specifically, we achieve 49.5
AP on YouTube-VIS 2019, a significant improvement of 13.2 AP and 2.1 AP over
the prior online and offline art, respectively. Moreover, we achieve 30.2 AP on
OVIS, a more challenging dataset with significant crowding and occlusions,
surpassing the prior art by 14.8 AP. The proposed method won first place in the
video instance segmentation track of the 4th Large-scale Video Object
Segmentation Challenge (CVPR2022). We hope the simplicity and effectiveness of
our method, as well as our insight into current methods, could shed light on
the exploration of VIS models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Patch-Based Neural Rendering. (arXiv:2207.10662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10662">
<div class="article-summary-box-inner">
<span><p>Neural rendering has received tremendous attention since the advent of Neural
Radiance Fields (NeRF), and has pushed the state-of-the-art on novel-view
synthesis considerably. The recent focus has been on models that overfit to a
single scene, and the few attempts to learn models that can synthesize novel
views of unseen scenes mostly consist of combining deep convolutional features
with a NeRF-like model. We propose a different paradigm, where no deep features
and no NeRF-like volume rendering are needed. Our method is capable of
predicting the color of a target ray in a novel scene directly, just from a
collection of patches sampled from the scene. We first leverage epipolar
geometry to extract patches along the epipolar lines of each reference view.
Each patch is linearly projected into a 1D feature vector and a sequence of
transformers process the collection. For positional encoding, we parameterize
rays as in a light field representation, with the crucial difference that the
coordinates are canonicalized with respect to the target ray, which makes our
method independent of the reference frame and improves generalization. We show
that our approach outperforms the state-of-the-art on novel view synthesis of
unseen scenes even when being trained with considerably less data than prior
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Pixel Composition: 3D-4D View Synthesis from Multi-Views. (arXiv:2207.10663v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10663">
<div class="article-summary-box-inner">
<span><p>We present Neural Pixel Composition (NPC), a novel approach for continuous
3D-4D view synthesis given only a discrete set of multi-view observations as
input. Existing state-of-the-art approaches require dense multi-view
supervision and an extensive computational budget. The proposed formulation
reliably operates on sparse and wide-baseline multi-view imagery and can be
trained efficiently within a few seconds to 10 minutes for hi-res (12MP)
content, i.e., 200-400X faster convergence than existing methods. Crucial to
our approach are two core novelties: 1) a representation of a pixel that
contains color and depth information accumulated from multi-views for a
particular location and time along a line of sight, and 2) a multi-layer
perceptron (MLP) that enables the composition of this rich information provided
for a pixel location to obtain the final color output. We experiment with a
large variety of multi-view sequences, compare to existing approaches, and
achieve better results in diverse and challenging settings. Finally, our
approach enables dense 3D reconstruction from sparse multi-views, where COLMAP,
a state-of-the-art 3D reconstruction approach, struggles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset. (arXiv:2207.10664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10664">
<div class="article-summary-box-inner">
<span><p>We present a new benchmark dataset, Sapsucker Woods 60 (SSW60), for advancing
research on audiovisual fine-grained categorization. While our community has
made great strides in fine-grained visual categorization on images, the
counterparts in audio and video fine-grained categorization are relatively
unexplored. To encourage advancements in this space, we have carefully
constructed the SSW60 dataset to enable researchers to experiment with
classifying the same set of categories in three different modalities: images,
audio, and video. The dataset covers 60 species of birds and is comprised of
images from existing datasets, and brand new, expert-curated audio and video
datasets. We thoroughly benchmark audiovisual classification performance and
modality fusion experiments through the use of state-of-the-art transformer
methods. Our findings show that performance of audiovisual fusion methods is
better than using exclusively image or audio based methods for the task of
video classification. We also present interesting modality transfer
experiments, enabled by the unique construction of SSW60 to encompass three
different modalities. We hope the SSW60 dataset and accompanying baselines spur
research in this fascinating area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TinyViT: Fast Pretraining Distillation for Small Vision Transformers. (arXiv:2207.10666v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10666">
<div class="article-summary-box-inner">
<span><p>Vision transformer (ViT) recently has drawn great attention in computer
vision due to its remarkable model capability. However, most prevailing ViT
models suffer from huge number of parameters, restricting their applicability
on devices with limited resources. To alleviate this issue, we propose TinyViT,
a new family of tiny and efficient small vision transformers pretrained on
large-scale datasets with our proposed fast distillation framework. The central
idea is to transfer knowledge from large pretrained models to small ones, while
enabling small models to get the dividends of massive pretraining data. More
specifically, we apply distillation during pretraining for knowledge transfer.
The logits of large teacher models are sparsified and stored in disk in advance
to save the memory cost and computation overheads. The tiny student
transformers are automatically scaled down from a large pretrained model with
computation and parameter constraints. Comprehensive experiments demonstrate
the efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k
with only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k
while using 4.2 times fewer parameters. Moreover, increasing image resolutions,
TinyViT can reach 86.5% accuracy, being slightly better than Swin-L while using
only 11% parameters. Last but not the least, we demonstrate a good transfer
ability of TinyViT on various downstream tasks. Code and models are available
at https://github.com/microsoft/Cream/tree/main/TinyViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Domain Adaptation for Semantic Segmentation in Ever-Changing Conditions. (arXiv:2207.10667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10667">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between
training and testing data and is, in most cases, carried out in offline manner.
However, domain changes may occur continuously and unpredictably during
deployment (e.g. sudden weather changes). In such conditions, deep neural
networks witness dramatic drops in accuracy and offline adaptation may not be
enough to contrast it. In this paper, we tackle Online Domain Adaptation (OnDA)
for semantic segmentation. We design a pipeline that is robust to continuous
domain shifts, either gradual or sudden, and we evaluate it in the case of
rainy and foggy scenarios. Our experiments show that our framework can
effectively adapt to new domains during deployment, while not being affected by
catastrophic forgetting of the previous domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Imitating Collaborative Manipulation Plans from YouTube Cooking Videos. (arXiv:1911.10686v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.10686">
<div class="article-summary-box-inner">
<span><p>People often watch videos on the web to learn how to cook new recipes,
assemble furniture or repair a computer. We wish to enable robots with the very
same capability. This is challenging; there is a large variation in
manipulation actions and some videos even involve multiple persons, who
collaborate by sharing and exchanging objects and tools. Furthermore, the
learned representations need to be general enough to be transferable to robotic
systems. On the other hand, previous work has shown that the space of human
manipulation actions has a linguistic, hierarchical structure that relates
actions to manipulated objects and tools. Building upon this theory of language
for action, we propose a system for understanding and executing demonstrated
action sequences from full-length, real-world cooking videos on the web. The
system takes as input a new, previously unseen cooking video annotated with
object labels and bounding boxes, and outputs a collaborative manipulation
action plan for one or more robotic arms. We demonstrate performance of the
system in a standardized dataset of 100 YouTube cooking videos, as well as in
six full-length Youtube videos that include collaborative actions between two
participants. We compare our system with a baseline system that consists of a
state-of-the-art action detection baseline and show our system achieves higher
action detection accuracy. We additionally propose an open-source platform for
executing the learned plans in a simulation environment as well as with an
actual robotic arm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Photorealism in Driving Simulations: Blending Generative Adversarial Image Synthesis with Rendering. (arXiv:2007.15820v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15820">
<div class="article-summary-box-inner">
<span><p>Driving simulators play a large role in developing and testing new
intelligent vehicle systems. The visual fidelity of the simulation is critical
for building vision-based algorithms and conducting human driver experiments.
Low visual fidelity breaks immersion for human-in-the-loop driving experiments.
Conventional computer graphics pipelines use detailed 3D models, meshes,
textures, and rendering engines to generate 2D images from 3D scenes. These
processes are labor-intensive, and they do not generate photorealistic imagery.
Here we introduce a hybrid generative neural graphics pipeline for improving
the visual fidelity of driving simulations. Given a 3D scene, we partially
render only important objects of interest, such as vehicles, and use generative
adversarial processes to synthesize the background and the rest of the image.
To this end, we propose a novel image formation strategy to form 2D semantic
images from 3D scenery consisting of simple object models without textures.
These semantic images are then converted into photorealistic RGB images with a
state-of-the-art Generative Adversarial Network (GAN) trained on real-world
driving scenes. This replaces repetitiveness with randomly generated but
photorealistic surfaces. Finally, the partially-rendered and GAN synthesized
images are blended with a blending GAN. We show that the photorealism of images
generated with the proposed method is more similar to real-world driving
datasets such as Cityscapes and KITTI than conventional approaches. This
comparison is made using semantic retention analysis and Frechet Inception
Distance (FID) measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Registration for Unsupervised Medical Image Segmentation. (arXiv:2011.08894v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08894">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation is a relevant task as it serves as the first step
for several diagnosis processes, thus it is indispensable in clinical usage.
Whilst major success has been reported using supervised techniques, they assume
a large and well-representative labelled set. This is a strong assumption in
the medical domain where annotations are expensive, time-consuming, and
inherent to human bias. To address this problem, unsupervised techniques have
been proposed in the literature yet it is still an open problem due to the
difficulty of learning any transformation pattern. In this work, we present a
novel optimisation model framed into a new CNN-based contrastive registration
architecture for unsupervised medical image segmentation. The core of our
approach is to exploit image-level registration and feature-level from a
contrastive learning mechanism, to perform registration-based segmentation.
Firstly, we propose an architecture to capture the image-to-image
transformation pattern via registration for unsupervised medical image
segmentation. Secondly, we embed a contrastive learning mechanism into the
registration architecture to enhance the discriminating capacity of the network
in the feature-level. We show that our proposed technique mitigates the major
drawbacks of existing unsupervised techniques. We demonstrate, through
numerical and visual experiments, that our technique substantially outperforms
the current state-of-the-art unsupervised segmentation methods on two major
medical image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identification. (arXiv:2101.08238v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08238">
<div class="article-summary-box-inner">
<span><p>Cross-modal person re-identification (Re-ID) is critical for modern video
surveillance systems. The key challenge is to align cross-modality
representations induced by the semantic information present for a person and
ignore background information. This work presents a novel convolutional neural
network (CNN) based architecture designed to learn semantically aligned
cross-modal visual and textual representations. The underlying building block,
named AXM-Block, is a unified multi-layer network that dynamically exploits the
multi-scale knowledge from both modalities and re-calibrates each modality
according to shared semantics. To complement the convolutional design,
contextual attention is applied in the text branch to manipulate long-term
dependencies. Moreover, we propose a unique design to enhance visual part-based
feature coherence and locality information. Our framework is novel in its
ability to implicitly learn aligned semantics between modalities during the
feature learning stage. The unified feature learning effectively utilizes
textual data as a super-annotation signal for visual representation learning
and automatically rejects irrelevant information. The entire AXM-Net is trained
end-to-end on CUHK-PEDES data. We report results on two tasks, person search
and cross-modal Re-ID. The AXM-Net outperforms the current state-of-the-art
(SOTA) methods and achieves 64.44\% Rank@1 on the CUHK-PEDES test set. It also
outperforms its competitors by $&gt;$10\% in cross-viewpoint text-to-image Re-ID
scenarios on CrossRe-ID and CUHK-SYSU datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehensive Multi-Modal Interactions for Referring Image Segmentation. (arXiv:2104.10412v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10412">
<div class="article-summary-box-inner">
<span><p>We investigate Referring Image Segmentation (RIS), which outputs a
segmentation map corresponding to the natural language description. Addressing
RIS efficiently requires considering the interactions happening \emph{across}
visual and linguistic modalities and the interactions \emph{within} each
modality. Existing methods are limited because they either compute different
forms of interactions \emph{sequentially} (leading to error propagation) or
\emph{ignore} intramodal interactions. We address this limitation by performing
all three interactions \emph{simultaneously} through a Synchronous Multi-Modal
Fusion Module (SFM). Moreover, to produce refined segmentation masks, we
propose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where
linguistic features facilitate the exchange of contextual information across
the visual hierarchy. We present thorough ablation studies and validate our
approach's performance on four benchmark datasets, showing considerable
performance gains over the existing state-of-the-art (SOTA) methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Well Does Self-Supervised Pre-Training Perform with Streaming Data?. (arXiv:2104.12081v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12081">
<div class="article-summary-box-inner">
<span><p>Prior works on self-supervised pre-training focus on the joint training
scenario, where massive unlabeled data are assumed to be given as input all at
once, and only then is a learner trained. Unfortunately, such a problem setting
is often impractical if not infeasible since many real-world tasks rely on
sequential learning, e.g., data are decentralized or collected in a streaming
fashion. In this paper, we conduct the first thorough and dedicated
investigation on self-supervised pre-training with streaming data, aiming to
shed light on the model behavior under this overlooked setup. Specifically, we
pre-train over 500 models on four categories of pre-training streaming data
from ImageNet and DomainNet and evaluate them on three types of downstream
tasks and 12 different downstream datasets. Our studies show that, somehow
beyond our expectation, with simple data replay or parameter regularization,
sequential self-supervised pre-training turns out to be an efficient
alternative for joint pre-training, as the performances of the former are
mostly on par with those of the latter. Moreover, catastrophic forgetting, a
common issue in sequential supervised learning, is much alleviated in
sequential self-supervised learning (SSL), which is well justified through our
comprehensive empirical analysis on representations and the sharpness of minima
in the loss landscape. Our findings, therefore, suggest that, in practice, for
SSL, the cumbersome joint training can be replaced mainly by sequential
learning, which in turn enables a much broader spectrum of potential
application scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unsupervised Sketch-based Image Retrieval. (arXiv:2105.08237v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08237">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the first attempt at unsupervised SBIR to remove
the labeling cost (category annotations and sketch-photo pairings) that is
conventionally needed for training. Existing single-domain unsupervised
representation learning methods perform poorly in this application, due to the
unique cross-domain (sketch and photo) nature of the problem. We therefore
introduce a novel framework that simultaneously performs unsupervised
representation learning and sketch-photo domain alignment. Technically this is
underpinned by exploiting joint distribution optimal transport (JDOT) to align
data from different domains during representation learning, which we extend
with trainable cluster prototypes and feature memory banks to further improve
scalability and efficacy. Extensive experiments show that our framework
achieves excellent performance in the new unsupervised setting, and performs
comparably or better than state-of-the-art in the zero-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Complex Heterogeneity. (arXiv:2105.09401v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09401">
<div class="article-summary-box-inner">
<span><p>With the advent of big data across multiple high-impact applications, we are
often facing the challenge of complex heterogeneity. The newly collected data
usually consist of multiple modalities and are characterized with multiple
labels, thus exhibiting the co-existence of multiple types of heterogeneity.
Although state-of-the-art techniques are good at modeling complex heterogeneity
with sufficient label information, such label information can be quite
expensive to obtain in real applications. Recently, researchers pay great
attention to contrastive learning due to its prominent performance by utilizing
rich unlabeled data. However, existing work on contrastive learning is not able
to address the problem of false negative pairs, i.e., some `negative' pairs may
have similar representations if they have the same label. To overcome the
issues, in this paper, we propose a unified heterogeneous learning framework,
which combines both the weighted unsupervised contrastive loss and the weighted
supervised contrastive loss to model multiple types of heterogeneity. We first
provide a theoretical analysis showing that the vanilla contrastive learning
loss easily leads to the sub-optimal solution in the presence of false negative
pairs, whereas the proposed weighted loss could automatically adjust the weight
based on the similarity of the learned representations to mitigate this issue.
Experimental results on real-world data sets demonstrate the effectiveness and
the efficiency of the proposed framework modeling multiple types of
heterogeneity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming the Latent Space of StyleGAN for Real Face Editing. (arXiv:2105.14230v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14230">
<div class="article-summary-box-inner">
<span><p>Despite recent advances in semantic manipulation using StyleGAN, semantic
editing of real faces remains challenging. The gap between the $W$ space and
the $W$+ space demands an undesirable trade-off between reconstruction quality
and editing quality. To solve this problem, we propose to expand the latent
space by replacing fully-connected layers in the StyleGAN's mapping network
with attention-based transformers. This simple and effective technique
integrates the aforementioned two spaces and transforms them into one new
latent space called $W$++. Our modified StyleGAN maintains the state-of-the-art
generation quality of the original StyleGAN with moderately better diversity.
But more importantly, the proposed $W$++ space achieves superior performance in
both reconstruction quality and editing quality. Despite these significant
advantages, our $W$++ space supports existing inversion algorithms and editing
methods with only negligible modifications thanks to its structural similarity
with the $W/W$+ space. Extensive experiments on the FFHQ dataset prove that our
proposed $W$++ space is evidently more preferable than the previous $W/W$+
space for real face editing. The code is publicly available for research
purposes at https://github.com/AnonSubm2021/TransStyleGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Knowledge-Transfer for Learned Image Reconstruction. (arXiv:2107.02572v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02572">
<div class="article-summary-box-inner">
<span><p>Deep learning-based image reconstruction approaches have demonstrated
impressive empirical performance in many imaging modalities. These approaches
usually require a large amount of high-quality paired training data, which is
often not available in medical imaging. To circumvent this issue we develop a
novel unsupervised knowledge-transfer paradigm for learned reconstruction
within a Bayesian framework. The proposed approach learns a reconstruction
network in two phases. The first phase trains a reconstruction network with a
set of ordered pairs comprising of ground truth images of ellipses and the
corresponding simulated measurement data. The second phase fine-tunes the
pretrained network to more realistic measurement data without supervision. By
construction, the framework is capable of delivering predictive uncertainty
information over the reconstructed image. We present extensive experimental
results on low-dose and sparse-view computed tomography showing that the
approach is competitive with several state-of-the-art supervised and
unsupervised reconstruction techniques. Moreover, for test data distributed
differently from the training data, the proposed framework can significantly
improve reconstruction quality not only visually, but also quantitatively in
terms of PSNR and SSIM, when compared with learned methods trained on the
synthetic dataset only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DISP6D: Disentangled Implicit Shape and Pose Learning for Scalable 6D Pose Estimation. (arXiv:2107.12549v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12549">
<div class="article-summary-box-inner">
<span><p>Scalable 6D pose estimation for rigid objects from RGB images aims at
handling multiple objects and generalizing to novel objects. Building on a
well-known auto-encoding framework to cope with object symmetry and the lack of
labeled training data, we achieve scalability by disentangling the latent
representation of auto-encoder into shape and pose sub-spaces. The latent shape
space models the similarity of different objects through contrastive metric
learning, and the latent pose code is compared with canonical rotations for
rotation retrieval. Because different object symmetries induce inconsistent
latent pose spaces, we re-entangle the shape representation with canonical
rotations to generate shape-dependent pose codebooks for rotation retrieval. We
show state-of-the-art performance on two benchmarks containing textureless CAD
objects without category and daily objects with categories respectively, and
further demonstrate improved scalability by extending to a more challenging
setting of daily objects across categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Image Representations for Multi-Image Fusion and Layer Separation. (arXiv:2108.01199v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01199">
<div class="article-summary-box-inner">
<span><p>We propose a framework for aligning and fusing multiple images into a single
view using neural image representations (NIRs), also known as implicit or
coordinate-based neural representations. Our framework targets burst images
that exhibit camera ego motion and potential changes in the scene. We describe
different strategies for alignment depending on the nature of the scene motion
-- namely, perspective planar (i.e., homography), optical flow with minimal
scene change, and optical flow with notable occlusion and disocclusion. With
the neural image representation, our framework effectively combines multiple
inputs into a single canonical view without the need for selecting one of the
images as a reference frame. We demonstrate how to use this multi-frame fusion
framework for various layer separation tasks. The code and results are
available at https://shnnam.github.io/research/nir.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Gaze Analysis: A Survey of Deep Learning based Approaches. (arXiv:2108.05479v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05479">
<div class="article-summary-box-inner">
<span><p>Eye gaze analysis is an important research problem in the field of Computer
Vision and Human-Computer Interaction. Even with notable progress in the last
10 years, automatic gaze analysis still remains challenging due to the
uniqueness of eye appearance, eye-head interplay, occlusion, image quality, and
illumination conditions. There are several open questions, including what are
the important cues to interpret gaze direction in an unconstrained environment
without prior knowledge and how to encode them in real-time. We review the
progress across a range of gaze analysis tasks and applications to elucidate
these fundamental questions, identify effective methods in gaze analysis, and
provide possible future directions. We analyze recent gaze estimation and
segmentation methods, especially in the unsupervised and weakly supervised
domain, based on their advantages and reported evaluation metrics. Our analysis
shows that the development of a robust and generic gaze analysis method still
needs to address real-world challenges such as unconstrained setup and learning
with less supervision. We conclude by discussing future research directions for
designing a real-world gaze analysis system that can propagate to other domains
including Computer Vision, Augmented Reality (AR), Virtual Reality (VR), and
Human Computer Interaction (HCI). Project Page:
https://github.com/i-am-shreya/EyeGazeSurvey}{https://github.com/i-am-shreya/EyeGazeSurvey
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying partial mouse brain microscopy images from Allen reference atlas using a contrastively learned semantic space. (arXiv:2109.06662v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06662">
<div class="article-summary-box-inner">
<span><p>Precise identification of mouse brain microscopy images is a crucial first
step when anatomical structures in the mouse brain are to be registered to a
reference atlas. Practitioners usually rely on manual comparison of images or
tools that assume the presence of complete images. This work explores Siamese
Networks as the method for finding corresponding 2D reference atlas plates for
given partial 2D mouse brain images. Siamese networks are a class of
convolutional neural networks (CNNs) that use weight-shared paths to obtain low
dimensional embeddings of pairs of input images. The correspondence between the
partial mouse brain image and reference atlas plate is determined based on the
distance between low dimensional embeddings of brain slices and atlas plates
that are obtained from Siamese networks using contrastive learning. Experiments
showed that Siamese CNNs can precisely identify brain slices using the Allen
mouse brain atlas when training and testing images come from the same source.
They achieved TOP-1 and TOP-5 accuracy of 25% and 100%, respectively, taking
only 7.2 seconds to identify 29 images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Autonomous Visual Navigation in Arable Fields. (arXiv:2109.11936v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11936">
<div class="article-summary-box-inner">
<span><p>Autonomous navigation of a robot in agricultural fields is essential for
every task from crop monitoring to weed management and fertilizer application.
Many current approaches rely on accurate GPS, however, such technology is
expensive and also prone to failure (e.g. through lack of coverage). As such,
autonomous navigation through sensors that can interpret their environment
(such as cameras) is important to achieve the goal of autonomy in agriculture.
In this paper, we introduce a purely vision-based navigation scheme that is
able to reliably guide the robot through row-crop fields without manual
intervention. Independent of any global localization or mapping, this approach
is able to accurately follow the crop-rows and switch between the rows, only
using onboard cameras. With the help of a novel crop-row detection and a novel
crop-row switching technique, our navigation scheme can be deployed in a wide
range of fields with different canopy types in various growth stages with
limited parameter tuning, creating a crop agnostic navigation approach. We have
extensively evaluated our approach in three different fields under various
illumination conditions using our agricultural robotic platform (BonnBot-I).
For navigation, our approach is evaluated on five crop types and achieves an
average navigation accuracy of 3.82cm relative to manual teleoperation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Pedestrian Attribute Recognition Using Group Sparsity for Occlusion Videos. (arXiv:2110.08708v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08708">
<div class="article-summary-box-inner">
<span><p>Occlusion processing is a key issue in pedestrian attribute recognition
(PAR). Nevertheless, several existing video-based PAR methods have not yet
considered occlusion handling in depth. In this paper, we formulate finding
non-occluded frames as sparsity-based temporal attention of a crowded video. In
this manner, a model is guided not to pay attention to the occluded frame.
However, temporal sparsity cannot include a correlation between attributes when
occlusion occurs. For example, "boots" and "shoe color" cannot be recognized
when the foot is invisible. To solve the uncorrelated attention issue, we also
propose a novel group sparsity-based temporal attention module. Group sparsity
is applied across attention weights in correlated attributes. Thus, attention
weights in a group are forced to pay attention to the same frames. Experimental
results showed that the proposed method achieved a higher F1-score than the
state-of-the-art methods on two video-based PAR datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algorithmic encoding of protected characteristics in image-based models for disease detection. (arXiv:2110.14755v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14755">
<div class="article-summary-box-inner">
<span><p>It has been rightfully emphasized that the use of AI for clinical decision
making could amplify health disparities. An algorithm may encode protected
characteristics, and then use this information for making predictions due to
undesirable correlations in the (historical) training data. It remains unclear
how we can establish whether such information is actually used. Besides the
scarcity of data from underserved populations, very little is known about how
dataset biases manifest in predictive models and how this may result in
disparate performance. This article aims to shed some light on these issues by
exploring new methodology for subgroup analysis in image-based disease
detection models. We utilize two publicly available chest X-ray datasets,
CheXpert and MIMIC-CXR, to study performance disparities across race and
biological sex in deep learning models. We explore test set resampling,
transfer learning, multitask learning, and model inspection to assess the
relationship between the encoding of protected characteristics and disease
detection performance across subgroups. We confirm subgroup disparities in
terms of shifted true and false positive rates which are partially removed
after correcting for population and prevalence shifts in the test sets. We
further find a previously used transfer learning method to be insufficient for
establishing whether specific patient information is used for making
predictions. The proposed combination of test-set resampling, multitask
learning, and model inspection reveals valuable new insights about the way
protected characteristics are encoded in the feature representations of deep
neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP2TV: Align, Match and Distill for Video-Text Retrieval. (arXiv:2111.05610v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05610">
<div class="article-summary-box-inner">
<span><p>Modern video-text retrieval frameworks basically consist of three parts:
video encoder, text encoder and the similarity head. With the success on both
visual and textual representation learning, transformer based encoders and
fusion methods have also been adopted in the field of video-text retrieval. In
this report, we present CLIP2TV, aiming at exploring where the critical
elements lie in transformer based methods. To achieve this, We first revisit
some recent works on multi-modal learning, then introduce some techniques into
video-text retrieval, finally evaluate them through extensive experiments in
different configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,
outperforming the previous SOTA result by 4.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnimeCeleb: Large-Scale Animation CelebHeads Dataset for Head Reenactment. (arXiv:2111.07640v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07640">
<div class="article-summary-box-inner">
<span><p>We present a novel Animation CelebHeads dataset (AnimeCeleb) to address an
animation head reenactment. Different from previous animation head datasets, we
utilize 3D animation models as the controllable image samplers, which can
provide a large amount of head images with their corresponding detailed pose
annotations. To facilitate a data creation process, we build a semi-automatic
pipeline leveraging an open 3D computer graphics software with a developed
annotation system. After training with the AnimeCeleb, recent head reenactment
models produce high-quality animation head reenactment results, which are not
achievable with existing datasets. Furthermore, motivated by metaverse
application, we propose a novel pose mapping method and architecture to tackle
a cross-domain head reenactment task. During inference, a user can easily
transfer one's motion to an arbitrary animation head. Experiments demonstrate
the usefulness of the AnimeCeleb to train animation head reenactment models,
and the superiority of our cross-domain head reenactment model compared to
state-of-the-art methods. Our dataset and code are available at
https://github.com/kangyeolk/AnimeCeleb.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layered Controllable Video Generation. (arXiv:2111.12747v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12747">
<div class="article-summary-box-inner">
<span><p>We introduce layered controllable video generation, where we, without any
supervision, decompose the initial frame of a video into foreground and
background layers, with which the user can control the video generation process
by simply manipulating the foreground mask. The key challenges are the
unsupervised foreground-background separation, which is ambiguous, and ability
to anticipate user manipulations with access to only raw video sequences. We
address these challenges by proposing a two-stage learning procedure. In the
first stage, with the rich set of losses and dynamic foreground size prior, we
learn how to separate the frame into foreground and background layers and,
conditioned on these layers, how to generate the next frame using VQ-VAE
generator. In the second stage, we fine-tune this network to anticipate edits
to the mask, by fitting (parameterized) control to the mask from future frame.
We demonstrate the effectiveness of this learning and the more granular control
mechanism, while illustrating state-of-the-art performance on two benchmark
datasets. We provide a video abstract as well as some video results on
https://gabriel-huang.github.io/layered_controllable_video_generation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Invariants to Understand Unsupervised Out-of-Distribution Detection. (arXiv:2111.13362v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13362">
<div class="article-summary-box-inner">
<span><p>Unsupervised out-of-distribution (U-OOD) detection has recently attracted
much attention due its importance in mission-critical systems and broader
applicability over its supervised counterpart. Despite this increase in
attention, U-OOD methods suffer from important shortcomings. By performing a
large-scale evaluation on different benchmarks and image modalities, we show in
this work that most popular state-of-the-art methods are unable to consistently
outperform a simple anomaly detector based on pre-trained features and the
Mahalanobis distance (MahaAD). A key reason for the inconsistencies of these
methods is the lack of a formal description of U-OOD. Motivated by a simple
thought experiment, we propose a characterization of U-OOD based on the
invariants of the training dataset. We show how this characterization is
unknowingly embodied in the top-scoring MahaAD method, thereby explaining its
quality. Furthermore, our approach can be used to interpret predictions of
U-OOD detectors and provides insights into good practices for evaluating future
U-OOD methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Fit Morphable Models. (arXiv:2111.14824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14824">
<div class="article-summary-box-inner">
<span><p>Fitting parametric models of human bodies, hands or faces to sparse input
signals in an accurate, robust, and fast manner has the promise of
significantly improving immersion in AR and VR scenarios. A common first step
in systems that tackle these problems is to regress the parameters of the
parametric model directly from the input data. This approach is fast, robust,
and is a good starting point for an iterative minimization algorithm. The
latter searches for the minimum of an energy function, typically composed of a
data term and priors that encode our knowledge about the problem's structure.
While this is undoubtedly a very successful recipe, priors are often hand
defined heuristics and finding the right balance between the different terms to
achieve high quality results is a non-trivial task. Furthermore, converting and
optimizing these systems to run in a performant way requires custom
implementations that demand significant time investments from both engineers
and domain experts. In this work, we build upon recent advances in learned
optimization and propose an update rule inspired by the classic
Levenberg-Marquardt algorithm. We show the effectiveness of the proposed neural
optimizer on three problems, 3D body estimation from a head-mounted device, 3D
body estimation from sparse 2D keypoints and face surface estimation from dense
2D landmarks. Our method can easily be applied to new model fitting problems
and offers a competitive alternative to well-tuned 'traditional' model fitting
pipelines, both in terms of accuracy and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdiBERT, a generative model for image editing. (arXiv:2111.15264v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15264">
<div class="article-summary-box-inner">
<span><p>Advances in computer vision are pushing the limits of im-age manipulation,
with generative models sampling detailed images on various tasks. However, a
specialized model is often developed and trained for each specific task, even
though many image edition tasks share similarities. In denoising, inpainting,
or image compositing, one always aims at generating a realistic image from a
low-quality one. In this paper, we aim at making a step towards a unified
approach for image editing. To do so, we propose EdiBERT, a bi-directional
transformer trained in the discrete latent space built by a vector-quantized
auto-encoder. We argue that such a bidirectional model is suited for image
manipulation since any patch can be re-sampled conditionally to the whole
image. Using this unique and straightforward training objective, we show that
the resulting model matches state-of-the-art performances on a wide variety of
tasks: image denoising, image completion, and image composition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRF-SR: High-Quality Neural Radiance Fields using Supersampling. (arXiv:2112.01759v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01759">
<div class="article-summary-box-inner">
<span><p>We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis
with mostly low-resolution (LR) inputs. Our method is built upon Neural
Radiance Fields (NeRF) that predicts per-point density and color with a
multi-layer perceptron. While producing images at arbitrary scales, NeRF
struggles with resolutions that go beyond observed images. Our key insight is
that NeRF benefits from 3D consistency, which means an observed pixel absorbs
information from nearby views. We first exploit it by a supersampling strategy
that shoots multiple rays at each image pixel, which further enforces
multi-view constraint at a sub-pixel level. Then, we show that NeRF-SR can
further boost the performance of supersampling by a refinement network that
leverages the estimated depth at hand to hallucinate details from related
patches on only one HR reference image. Experiment results demonstrate that
NeRF-SR generates high-quality results for novel view synthesis at HR on both
synthetic and real-world datasets without any external information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIVE: Evaluating the Human Interpretability of Visual Explanations. (arXiv:2112.03184v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03184">
<div class="article-summary-box-inner">
<span><p>As AI technology is increasingly applied to high-impact, high-risk domains,
there have been a number of new methods aimed at making AI models more human
interpretable. Despite the recent growth of interpretability work, there is a
lack of systematic evaluation of proposed techniques. In this work, we
introduce HIVE (Human Interpretability of Visual Explanations), a novel human
evaluation framework that assesses the utility of explanations to human users
in AI-assisted decision making scenarios, and enables falsifiable hypothesis
testing, cross-method comparison, and human-centered evaluation of visual
interpretability methods. To the best of our knowledge, this is the first work
of its kind. Using HIVE, we conduct IRB-approved human studies with nearly 1000
participants and evaluate four methods that represent the diversity of computer
vision interpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our
results suggest that explanations engender human trust, even for incorrect
predictions, yet are not distinct enough for users to distinguish between
correct and incorrect predictions. We open-source HIVE to enable future studies
and encourage more human-centered approaches to interpretability research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Global and Local Hierarchical Priors for Learned Image Compression. (arXiv:2112.04487v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04487">
<div class="article-summary-box-inner">
<span><p>Recently, learned image compression methods have outperformed traditional
hand-crafted ones including BPG. One of the keys to this success is learned
entropy models that estimate the probability distribution of the quantized
latent representation. Like other vision tasks, most recent learned entropy
models are based on convolutional neural networks (CNNs). However, CNNs have a
limitation in modeling long-range dependencies due to their nature of local
connectivity, which can be a significant bottleneck in image compression where
reducing spatial redundancy is a key point. To overcome this issue, we propose
a novel entropy model called Information Transformer (Informer) that exploits
both global and local information in a content-dependent manner using an
attention mechanism. Our experiments show that Informer improves
rate--distortion performance over the state-of-the-art methods on the Kodak and
Tecnick datasets without the quadratic computational complexity problem. Our
source code is available at https://github.com/naver-ai/informer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRF for Outdoor Scene Relighting. (arXiv:2112.05140v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05140">
<div class="article-summary-box-inner">
<span><p>Photorealistic editing of outdoor scenes from photographs requires a profound
understanding of the image formation process and an accurate estimation of the
scene geometry, reflectance and illumination. A delicate manipulation of the
lighting can then be performed while keeping the scene albedo and geometry
unaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene
relighting based on neural radiance fields. In contrast to the prior art, our
technique allows simultaneous editing of both scene illumination and camera
viewpoint using only a collection of outdoor photos shot in uncontrolled
settings. Moreover, it enables direct control over the scene illumination, as
defined through a spherical harmonics model. For evaluation, we collect a new
benchmark dataset of several outdoor sites photographed from multiple
viewpoints and at different times. For each time, a 360 degree environment map
is captured together with a colour-calibration chequerboard to allow accurate
numerical evaluations on real data against ground truth. Comparisons against
SoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at
higher quality and with realistic self-shadowing reproduction. Our method and
the dataset are publicly available at https://4dqv.mpi-inf.mpg.de/NeRF-OSR/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Triangle Attack: A Query-efficient Decision-based Adversarial Attack. (arXiv:2112.06569v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06569">
<div class="article-summary-box-inner">
<span><p>Decision-based attack poses a severe threat to real-world applications since
it regards the target model as a black box and only accesses the hard
prediction label. Great efforts have been made recently to decrease the number
of queries; however, existing decision-based attacks still require thousands of
queries in order to generate good quality adversarial examples. In this work,
we find that a benign sample, the current and the next adversarial examples can
naturally construct a triangle in a subspace for any iterative attacks. Based
on the law of sines, we propose a novel Triangle Attack (TA) to optimize the
perturbation by utilizing the geometric information that the longer side is
always opposite the larger angle in any triangle. However, directly applying
such information on the input image is ineffective because it cannot thoroughly
explore the neighborhood of the input sample in the high dimensional space. To
address this issue, TA optimizes the perturbation in the low frequency space
for effective dimensionality reduction owing to the generality of such
geometric property. Extensive evaluations on ImageNet dataset show that TA
achieves a much higher attack success rate within 1,000 queries and needs a
much less number of queries to achieve the same attack success rate under
various perturbation budgets than existing decision-based attacks. With such
high efficiency, we further validate the applicability of TA on real-world API,
i.e., Tencent Cloud API.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqFormer: Sequential Transformer for Video Instance Segmentation. (arXiv:2112.08275v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08275">
<div class="article-summary-box-inner">
<span><p>In this work, we present SeqFormer for video instance segmentation. SeqFormer
follows the principle of vision transformer that models instance relationships
among video frames. Nevertheless, we observe that a stand-alone instance query
suffices for capturing a time sequence of instances in a video, but attention
mechanisms shall be done with each frame independently. To achieve this,
SeqFormer locates an instance in each frame and aggregates temporal information
to learn a powerful representation of a video-level instance, which is used to
predict the mask sequences on each frame dynamically. Instance tracking is
achieved naturally without tracking branches or post-processing. On
YouTube-VIS, SeqFormer achieves 47.4 AP with a ResNet-50 backbone and 49.0 AP
with a ResNet-101 backbone without bells and whistles. Such achievement
significantly exceeds the previous state-of-the-art performance by 4.6 and 4.4,
respectively. In addition, integrated with the recently-proposed Swin
transformer, SeqFormer achieves a much higher AP of 59.3. We hope SeqFormer
could be a strong baseline that fosters future research in video instance
segmentation, and in the meantime, advances this field with a more robust,
accurate, neat model. The code is available at
https://github.com/wjf5203/SeqFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mimic Embedding via Adaptive Aggregation: Learning Generalizable Person Re-identification. (arXiv:2112.08684v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08684">
<div class="article-summary-box-inner">
<span><p>Domain generalizable (DG) person re-identification (ReID) aims to test across
unseen domains without access to the target domain data at training time, which
is a realistic but challenging problem. In contrast to methods assuming an
identical model for different domains, Mixture of Experts (MoE) exploits
multiple domain-specific networks for leveraging complementary information
between domains, obtaining impressive results. However, prior MoE-based DG ReID
methods suffer from a large model size with the increase of the number of
source domains, and most of them overlook the exploitation of domain-invariant
characteristics. To handle the two issues above, this paper presents a new
approach called Mimic Embedding via adapTive Aggregation (META) for DG person
ReID. To avoid the large model size, experts in META do not adopt a branch
network for each source domain but share all the parameters except for the
batch normalization layers. Besides multiple experts, META leverages Instance
Normalization (IN) and introduces it into a global branch to pursue invariant
features across domains. Meanwhile, META considers the relevance of an unseen
target sample and source domains via normalization statistics and develops an
aggregation module to adaptively integrate multiple experts for mimicking
unseen target domain. Benefiting from a proposed consistency loss and an
episodic training algorithm, META is expected to mimic embedding for a truly
unseen target domain. Extensive experiments verify that META surpasses
state-of-the-art DG person ReID methods by a large margin. Our code is
available at https://github.com/xbq1994/META.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DProST: Dynamic Projective Spatial Transformer Network for 6D Pose Estimation. (arXiv:2112.08775v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08775">
<div class="article-summary-box-inner">
<span><p>Predicting the object's 6D pose from a single RGB image is a fundamental
computer vision task. Generally, the distance between transformed object
vertices is employed as an objective function for pose estimation methods.
However, projective geometry in the camera space is not considered in those
methods and causes performance degradation. In this regard, we propose a new
pose estimation system based on a projective grid instead of object vertices.
Our pose estimation method, dynamic projective spatial transformer network
(DProST), localizes the region of interest grid on the rays in camera space and
transforms the grid to object space by estimated pose. The transformed grid is
used as both a sampling grid and a new criterion of the estimated pose.
Additionally, because DProST does not require object vertices, our method can
be used in a mesh-less setting by replacing the mesh with a reconstructed
feature. Experimental results show that mesh-less DProST outperforms the
state-of-the-art mesh-based methods on the LINEMOD and LINEMOD-OCCLUSION
dataset, and shows competitive performance on the YCBV dataset with mesh data.
The source code is available at https://github.com/parkjaewoo0611/DProST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds. (arXiv:2112.08879v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08879">
<div class="article-summary-box-inner">
<span><p>Most models tasked to ground referential utterances in 2D and 3D scenes learn
to select the referred object from a pool of object proposals provided by a
pre-trained detector. This is limiting because an utterance may refer to visual
entities at various levels of granularity, such as the chair, the leg of the
chair, or the tip of the front leg of the chair, which may be missed by the
detector. We propose a language grounding model that attends on the referential
utterance and on the object proposal pool computed from a pre-trained detector
to decode referenced objects with a detection head, without selecting them from
the pool. In this way, it is helped by powerful pre-trained object detectors
without being restricted by their misses. We call our model Bottom Up Top Down
DEtection TRansformers (BUTD-DETR) because it uses both language guidance (top
down) and objectness guidance (bottom-up) to ground referential utterances in
images and point clouds. Moreover, BUTD-DETR casts object detection as
referential grounding and uses object labels as language prompts to be grounded
in the visual scene, augmenting supervision for the referential grounding task
in this way. The proposed model sets a new state-of-the-art across popular 3D
language grounding benchmarks with significant performance gains over previous
3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When
applied in 2D images, it performs on par with the previous state of the art. We
ablate the design choices of our model and quantify their contribution to
performance. Our code and checkpoints can be found at the project website
https://butd-detr.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleSwin: Transformer-based GAN for High-resolution Image Generation. (arXiv:2112.10762v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10762">
<div class="article-summary-box-inner">
<span><p>Despite the tantalizing success in a broad of vision tasks, transformers have
not yet demonstrated on-par ability as ConvNets in high-resolution image
generative modeling. In this paper, we seek to explore using pure transformers
to build a generative adversarial network for high-resolution image synthesis.
To this end, we believe that local attention is crucial to strike the balance
between computational efficiency and modeling capacity. Hence, the proposed
generator adopts Swin transformer in a style-based architecture. To achieve a
larger receptive field, we propose double attention which simultaneously
leverages the context of the local and the shifted windows, leading to improved
generation quality. Moreover, we show that offering the knowledge of the
absolute position that has been lost in window-based transformers greatly
benefits the generation quality. The proposed StyleSwin is scalable to high
resolutions, with both the coarse geometry and fine structures benefit from the
strong expressivity of transformers. However, blocking artifacts occur during
high-resolution synthesis because performing the local attention in a
block-wise manner may break the spatial coherency. To solve this, we
empirically investigate various solutions, among which we find that employing a
wavelet discriminator to examine the spectral discrepancy effectively
suppresses the artifacts. Extensive experiments show the superiority over prior
transformer-based GANs, especially on high resolutions, e.g., 1024x1024. The
StyleSwin, without complex training strategies, excels over StyleGAN on
CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the
promise of using transformers for high-resolution image generation. The code
and models will be available at https://github.com/microsoft/StyleSwin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Open-Vocabulary Image Segmentation with Image-Level Labels. (arXiv:2112.12143v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12143">
<div class="article-summary-box-inner">
<span><p>We design an open-vocabulary image segmentation model to organize an image
into meaningful regions indicated by arbitrary texts. Recent works (CLIP and
ALIGN), despite attaining impressive open-vocabulary classification accuracy
with image-level caption labels, are unable to segment visual concepts with
pixels. We argue that these models miss an important step of visual grouping,
which organizes pixels into groups before learning visual-semantic alignments.
We propose OpenSeg to address the above issue while still making use of
scalable image-level supervision of captions. First, it learns to propose
segmentation masks for possible organizations. Then it learns visual-semantic
alignments by aligning each word in a caption to one or a few predicted masks.
We find the mask representations are the key to support learning image
segmentation from captions, making it possible to scale up the dataset and
vocabulary sizes. OpenSeg significantly outperforms the recent open-vocabulary
method of LSeg by +19.9 mIoU on PASCAL dataset, thanks to its scalability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos. (arXiv:2112.13715v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13715">
<div class="article-summary-box-inner">
<span><p>When analyzing human motion videos, the output jitters from existing pose
estimators are highly-unbalanced with varied estimation errors across frames.
Most frames in a video are relatively easy to estimate and only suffer from
slight jitters. In contrast, for rarely seen or occluded actions, the estimated
positions of multiple joints largely deviate from the ground truth values for a
consecutive sequence of frames, rendering significant jitters on them. To
tackle this problem, we propose to attach a dedicated temporal-only refinement
network to existing pose estimators for jitter mitigation, named SmoothNet.
Unlike existing learning-based solutions that employ spatio-temporal models to
co-optimize per-frame precision and temporal smoothness at all the joints,
SmoothNet models the natural smoothness characteristics in body movements by
learning the long-range temporal relations of every joint without considering
the noisy correlations among joints. With a simple yet effective motion-aware
fully-connected network, SmoothNet improves the temporal smoothness of existing
pose estimators significantly and enhances the estimation accuracy of those
challenging frames as a side-effect. Moreover, as a temporal-only model, a
unique advantage of SmoothNet is its strong transferability across various
types of estimators and datasets. Comprehensive experiments on five datasets
with eleven popular backbone networks across 2D and 3D pose estimation and body
recovery tasks demonstrate the efficacy of the proposed solution. Code is
available at https://github.com/cure-lab/SmoothNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Query Video Retrieval. (arXiv:2201.03639v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03639">
<div class="article-summary-box-inner">
<span><p>Retrieving target videos based on text descriptions is a task of great
practical value and has received increasing attention over the past few years.
Despite recent progress, imperfect annotations in existing video retrieval
datasets have posed significant challenges on model evaluation and development.
In this paper, we tackle this issue by focusing on the less-studied setting of
multi-query video retrieval, where multiple descriptions are provided to the
model for searching over the video archive. We first show that multi-query
retrieval task effectively mitigates the dataset noise introduced by imperfect
annotations and better correlates with human judgement on evaluating retrieval
abilities of current models. We then investigate several methods which leverage
multiple queries at training time, and demonstrate that the multi-query
inspired training can lead to superior performance and better generalization.
We hope further investigation in this direction can bring new insights on
building systems that perform better in real-world video retrieval
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Robustness by Enhancing Weak Subnets. (arXiv:2201.12765v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12765">
<div class="article-summary-box-inner">
<span><p>Despite their success, deep networks have been shown to be highly susceptible
to perturbations, often causing significant drops in accuracy. In this paper,
we investigate model robustness on perturbed inputs by studying the performance
of internal sub-networks (subnets). Interestingly, we observe that most subnets
show particularly poor robustness against perturbations. More importantly,
these weak subnets are correlated with the overall lack of robustness. Tackling
this phenomenon, we propose a new training procedure that identifies and
enhances weak subnets (EWS) to improve robustness. Specifically, we develop a
search algorithm to find particularly weak subnets and explicitly strengthen
them via knowledge distillation from the full network. We show that EWS greatly
improves both robustness against corrupted images as well as accuracy on clean
data. Being complementary to popular data augmentation methods, EWS
consistently improves robustness when combined with these approaches. To
highlight the flexibility of our approach, we combine EWS also with popular
adversarial training methods resulting in improved adversarial robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Webly Supervised Concept Expansion for General Purpose Vision Models. (arXiv:2202.02317v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02317">
<div class="article-summary-box-inner">
<span><p>General Purpose Vision (GPV) systems are models that are designed to solve a
wide array of visual tasks without requiring architectural changes. Today, GPVs
primarily learn both skills and concepts from large fully supervised datasets.
Scaling GPVs to tens of thousands of concepts by acquiring data to learn each
concept for every skill quickly becomes prohibitive. This work presents an
effective and inexpensive alternative: learn skills from supervised datasets,
learn concepts from web image search, and leverage a key characteristic of
GPVs: the ability to transfer visual knowledge across skills. We use a dataset
of 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised
concept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5
COCO-based datasets (80 primary concepts), a newly curated series of 5 datasets
based on the OpenImages and VisualGenome repositories (~500 concepts), and the
Web-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2
that supports a variety of tasks -- from vision tasks like classification and
localization to vision+language tasks like QA and captioning, to more niche
ones like human-object interaction detection. GPV-2 benefits hugely from web
data and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code,
and web demo are available at https://prior.allenai.org/projects/gpv2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realistic Blur Synthesis for Learning Image Deblurring. (arXiv:2202.08771v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08771">
<div class="article-summary-box-inner">
<span><p>Training learning-based deblurring methods demands a tremendous amount of
blurred and sharp image pairs. Unfortunately, existing synthetic datasets are
not realistic enough, and deblurring models trained on them cannot handle real
blurred images effectively. While real datasets have recently been proposed,
they provide limited diversity of scenes and camera settings, and capturing
real datasets for diverse settings is still challenging. To resolve this, this
paper analyzes various factors that introduce differences between real and
synthetic blurred images. To this end, we present RSBlur, a novel dataset with
real blurred images and the corresponding sharp image sequences to enable a
detailed analysis of the difference between real and synthetic blur. With the
dataset, we reveal the effects of different factors in the blur generation
process. Based on the analysis, we also present a novel blur synthesis pipeline
to synthesize more realistic blur. We show that our synthesis pipeline can
improve the deblurring performance on real blurred images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context. (arXiv:2203.02113v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02113">
<div class="article-summary-box-inner">
<span><p>We advance sketch research to scenes with the first dataset of freehand scene
sketches, FS-COCO. With practical applications in mind, we collect sketches
that convey scene content well but can be sketched within a few minutes by a
person with any sketching skills. Our dataset comprises 10,000 freehand scene
vector sketches with per point space-time information by 100 non-expert
individuals, offering both object- and scene-level abstraction. Each sketch is
augmented with its text description. Using our dataset, we study for the first
time the problem of fine-grained image retrieval from freehand scene sketches
and sketch captions. We draw insights on: (i) Scene salience encoded in
sketches using the strokes temporal order; (ii) Performance comparison of image
retrieval from a scene sketch and an image caption; (iii) Complementarity of
information in sketches and image captions, as well as the potential benefit of
combining the two modalities. In addition, we extend a popular vector sketch
LSTM-based encoder to handle sketches with larger complexity than was supported
by previous work. Namely, we propose a hierarchical sketch decoder, which we
leverage at a sketch-specific "pre-text" task. Our dataset enables for the
first time research on freehand scene sketch understanding and its practical
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective. (arXiv:2203.03871v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03871">
<div class="article-summary-box-inner">
<span><p>This work simultaneously considers the discriminability and transferability
properties of deep representations in the typical supervised learning task,
i.e., image classification. By a comprehensive temporal analysis, we observe a
trade-off between these two properties. The discriminability keeps increasing
with the training progressing while the transferability intensely diminishes in
the later training period.
</p>
<p>From the perspective of information-bottleneck theory, we reveal that the
incompatibility between discriminability and transferability is attributed to
the over-compression of input information. More importantly, we investigate why
and how the InfoNCE loss can alleviate the over-compression, and further
present a learning framework, named contrastive temporal coding~(CTC), to
counteract the over-compression and alleviate the incompatibility. Extensive
experiments validate that CTC successfully mitigates the incompatibility,
yielding discriminative and transferable representations. Noticeable
improvements are achieved on the image classification task and challenging
transfer learning tasks. We hope that this work will raise the significance of
the transferability property in the conventional supervised learning setting.
Code is available at https://github.com/DTennant/dt-tradeoff.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClearPose: Large-scale Transparent Object Dataset and Benchmark. (arXiv:2203.03890v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03890">
<div class="article-summary-box-inner">
<span><p>Transparent objects are ubiquitous in household settings and pose distinct
challenges for visual sensing and perception systems. The optical properties of
transparent objects leave conventional 3D sensors alone unreliable for object
depth and pose estimation. These challenges are highlighted by the shortage of
large-scale RGB-Depth datasets focusing on transparent objects in real-world
settings. In this work, we contribute a large-scale real-world RGB-Depth
transparent object dataset named ClearPose to serve as a benchmark dataset for
segmentation, scene-level depth completion and object-centric pose estimation
tasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth
frames and 5M instance annotations covering 63 household objects. The dataset
includes object categories commonly used in daily life under various lighting
and occluding conditions as well as challenging test scenarios such as cases of
occlusion by opaque or translucent objects, non-planar orientations, presence
of liquids, etc. We benchmark several state-of-the-art depth completion and
object pose estimation deep neural networks on ClearPose. The dataset and
benchmarking source code is available at https://github.com/opipari/ClearPose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Multimodal Guidance for Medical Image Classification. (arXiv:2203.05683v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05683">
<div class="article-summary-box-inner">
<span><p>Medical imaging is a cornerstone of therapy and diagnosis in modern medicine.
However, the choice of imaging modality for a particular theranostic task
typically involves trade-offs between the feasibility of using a particular
modality (e.g., short wait times, low cost, fast acquisition, reduced
radiation/invasiveness) and the expected performance on a clinical task (e.g.,
diagnostic accuracy, efficacy of treatment planning and guidance). In this
work, we aim to apply the knowledge learned from the less feasible but
better-performing (superior) modality to guide the utilization of the
more-feasible yet under-performing (inferior) modality and steer it towards
improved performance. We focus on the application of deep learning for
image-based diagnosis. We develop a light-weight guidance model that leverages
the latent representation learned from the superior modality, when training a
model that consumes only the inferior modality. We examine the advantages of
our method in the context of two clinical applications: multi-task skin lesion
classification from clinical and dermoscopic images and brain tumor
classification from multi-sequence magnetic resonance imaging (MRI) and
histopathology images. For both these scenarios we show a boost in diagnostic
performance of the inferior modality without requiring the superior modality.
Furthermore, in the case of brain tumor classification, our method outperforms
the model trained on the superior modality while producing comparable results
to the model that uses both modalities during inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PC-SwinMorph: Patch Representation for Unsupervised Medical Image Registration and Segmentation. (arXiv:2203.05684v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05684">
<div class="article-summary-box-inner">
<span><p>Medical image registration and segmentation are critical tasks for several
clinical procedures. Manual realisation of those tasks is time-consuming and
the quality is highly dependent on the level of expertise of the physician. To
mitigate that laborious task, automatic tools have been developed where the
majority of solutions are supervised techniques. However, in medical domain,
the strong assumption of having a well-representative ground truth is far from
being realistic. To overcome this challenge, unsupervised techniques have been
investigated. However, they are still limited in performance and they fail to
produce plausible results. In this work, we propose a novel unified
unsupervised framework for image registration and segmentation that we called
PC-SwinMorph. The core of our framework is two patch-based strategies, where we
demonstrate that patch representation is key for performance gain. We first
introduce a patch-based contrastive strategy that enforces locality conditions
and richer feature representation. Secondly, we utilise a 3D
window/shifted-window multi-head self-attention module as a patch stitching
strategy to eliminate artifacts from the patch splitting. We demonstrate,
through a set of numerical and visual results, that our technique outperforms
current state-of-the-art unsupervised techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit field supervision for robust non-rigid shape matching. (arXiv:2203.07694v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07694">
<div class="article-summary-box-inner">
<span><p>Establishing a correspondence between two non-rigidly deforming shapes is one
of the most fundamental problems in visual computing. Existing methods often
show weak resilience when presented with challenges innate to real-world data
such as noise, outliers, self-occlusion etc. On the other hand, auto-decoders
have demonstrated strong expressive power in learning geometrically meaningful
latent embeddings. However, their use in \emph{shape analysis} has been
limited. In this paper, we introduce an approach based on an auto-decoder
framework, that learns a continuous shape-wise deformation field over a fixed
template. By supervising the deformation field for points on-surface and
regularising for points off-surface through a novel \emph{Signed Distance
Regularisation} (SDR), we learn an alignment between the template and shape
\emph{volumes}. Trained on clean water-tight meshes, \emph{without} any
data-augmentation, we demonstrate compelling performance on compromised data
and real-world scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation. (arXiv:2203.08713v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08713">
<div class="article-summary-box-inner">
<span><p>This paper proposes a simple baseline framework for video-based 2D/3D human
pose estimation that can achieve 10 times efficiency improvement over existing
works without any performance degradation, named DeciWatch. Unlike current
solutions that estimate each frame in a video, DeciWatch introduces a simple
yet effective sample-denoise-recover framework that only watches sparsely
sampled frames, taking advantage of the continuity of human motions and the
lightweight pose representation. Specifically, DeciWatch uniformly samples less
than 10% video frames for detailed estimation, denoises the estimated 2D/3D
poses with an efficient Transformer architecture, and then accurately recovers
the rest of the frames using another Transformer-based network. Comprehensive
experimental results on three video-based human pose estimation and body mesh
recovery tasks with four datasets validate the efficiency and effectiveness of
DeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knee arthritis severity measurement using deep learning: a publicly available algorithm with a multi-institutional validation showing radiologist-level performance. (arXiv:2203.08914v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08914">
<div class="article-summary-box-inner">
<span><p>The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical "black box" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
</p>
<p>The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10157">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis is a long-standing problem. In this work, we consider a
variant of the problem where we are given only a few context views sparsely
covering a scene or an object. The goal is to predict novel viewpoints in the
scene, which requires learning priors. The current state of the art is based on
Neural Radiance Field (NeRF), and while achieving impressive results, the
methods suffer from long training times as they require evaluating millions of
3D point samples via a neural network for each image. We propose a 2D-only
method that maps multiple context views and a query pose to a new image in a
single pass of a neural network. Our model uses a two-stage architecture
consisting of a codebook and a transformer model. The codebook is used to embed
individual images into a smaller latent space, and the transformer solves the
view synthesis task in this more compact space. To train our model efficiently,
we introduce a novel branching attention mechanism that allows us to use the
same model not only for neural rendering but also for camera pose estimation.
Experimental results on real-world scenes show that our approach is competitive
compared to NeRF-based methods while not reasoning explicitly in 3D, and it is
faster to train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields. (arXiv:2203.10821v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10821">
<div class="article-summary-box-inner">
<span><p>Image translation and manipulation have gain increasing attention along with
the rapid development of deep generative models. Although existing approaches
have brought impressive results, they mainly operated in 2D space. In light of
recent advances in NeRF-based 3D-aware generative models, we introduce a new
task, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene
modelled by NeRF, conditioned on one single-view semantic mask as input. To
kick-off this novel task, we propose the Sem2NeRF framework. In particular,
Sem2NeRF addresses the highly challenging task by encoding the semantic mask
into the latent code that controls the 3D scene representation of a pre-trained
decoder. To further improve the accuracy of the mapping, we integrate a new
region-aware learning strategy into the design of both the encoder and the
decoder. We verify the efficacy of the proposed Sem2NeRF and demonstrate that
it outperforms several strong baselines on two benchmark datasets. Code and
video are available at https://donydchen.github.io/sem2nerf/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Network for Future Hand Segmentation from Egocentric Video. (arXiv:2203.11305v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11305">
<div class="article-summary-box-inner">
<span><p>We introduce the novel problem of anticipating a time series of future hand
masks from egocentric video. A key challenge is to model the stochasticity of
future head motions, which globally impact the head-worn camera video analysis.
To this end, we propose a novel deep generative model -- EgoGAN, which uses a
3D Fully Convolutional Network to learn a spatio-temporal video representation
for pixel-wise visual anticipation, generates future head motion using
Generative Adversarial Network (GAN), and then predicts the future hand masks
based on the video representation and the generated future head motion. We
evaluate our method on both the EPIC-Kitchens and the EGTEA Gaze+ datasets. We
conduct detailed ablation studies to validate the design choices of our
approach. Furthermore, we compare our method with previous state-of-the-art
methods on future image segmentation and show that our method can more
accurately predict future hand masks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Patch Exiting for Scalable Single Image Super-Resolution. (arXiv:2203.11589v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11589">
<div class="article-summary-box-inner">
<span><p>Since the future of computing is heterogeneous, scalability is a crucial
problem for single image super-resolution. Recent works try to train one
network, which can be deployed on platforms with different capacities. However,
they rely on the pixel-wise sparse convolution, which is not hardware-friendly
and achieves limited practical speedup. As image can be divided into patches,
which have various restoration difficulties, we present a scalable method based
on Adaptive Patch Exiting (APE) to achieve more practical speedup.
Specifically, we propose to train a regressor to predict the incremental
capacity of each layer for the patch. Once the incremental capacity is below
the threshold, the patch can exit at the specific layer. Our method can easily
adjust the trade-off between performance and efficiency by changing the
threshold of incremental capacity. Furthermore, we propose a novel strategy to
enable the network training of our method. We conduct extensive experiments
across various backbones, datasets and scaling factors to demonstrate the
advantages of our method. Code is available at
https://github.com/littlepure2333/APE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-enabled Assessment of Cardiac Systolic and Diastolic Function from Echocardiography. (arXiv:2203.11726v2 [physics.med-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11726">
<div class="article-summary-box-inner">
<span><p>Left ventricular (LV) function is an important factor in terms of patient
management, outcome, and long-term survival of patients with heart disease. The
most recently published clinical guidelines for heart failure recognise that
over reliance on only one measure of cardiac function (LV ejection fraction) as
a diagnostic and treatment stratification biomarker is suboptimal. Recent
advances in AI-based echocardiography analysis have shown excellent results on
automated estimation of LV volumes and LV ejection fraction. However, from
time-varying 2-D echocardiography acquisition, a richer description of cardiac
function can be obtained by estimating functional biomarkers from the complete
cardiac cycle. In this work we propose for the first time an AI approach for
deriving advanced biomarkers of systolic and diastolic LV function from 2-D
echocardiography based on segmentations of the full cardiac cycle. These
biomarkers will allow clinicians to obtain a much richer picture of the heart
in health and disease. The AI model is based on the 'nn-Unet' framework and was
trained and tested using four different databases. Results show excellent
agreement between manual and automated analysis and showcase the potential of
the advanced systolic and diastolic biomarkers for patient stratification.
Finally, for a subset of 50 cases, we perform a correlation analysis between
clinical biomarkers derived from echocardiography and CMR and we show excellent
agreement between the two modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Broad Study of Pre-training for Domain Generalization and Adaptation. (arXiv:2203.11819v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11819">
<div class="article-summary-box-inner">
<span><p>Deep models must learn robust and transferable representations in order to
perform well on new domains. While domain transfer methods (e.g., domain
adaptation, domain generalization) have been proposed to learn transferable
representations across domains, they are typically applied to ResNet backbones
pre-trained on ImageNet. Thus, existing works pay little attention to the
effects of pre-training on domain transfer tasks. In this paper, we provide a
broad study and in-depth analysis of pre-training for domain adaptation and
generalization, namely: network architectures, size, pre-training loss, and
datasets. We observe that simply using a state-of-the-art backbone outperforms
existing state-of-the-art domain adaptation baselines and set new baselines on
Office-Home and DomainNet improving by 10.7\% and 5.5\%. We hope that this work
can provide more insights for future domain transfer research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11834">
<div class="article-summary-box-inner">
<span><p>Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CM-GAN: Image Inpainting with Cascaded Modulation GAN and Object-Aware Training. (arXiv:2203.11947v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11947">
<div class="article-summary-box-inner">
<span><p>Recent image inpainting methods have made great progress but often struggle
to generate plausible image structures when dealing with large holes in complex
images. This is partially due to the lack of effective network structures that
can capture both the long-range dependency and high-level semantics of an
image. We propose cascaded modulation GAN (CM-GAN), a new network design
consisting of an encoder with Fourier convolution blocks that extract
multi-scale feature representations from the input image with holes and a
dual-stream decoder with a novel cascaded global-spatial modulation block at
each scale level. In each decoder block, global modulation is first applied to
perform coarse and semantic-aware structure synthesis, followed by spatial
modulation to further adjust the feature map in a spatially adaptive fashion.
In addition, we design an object-aware training scheme to prevent the network
from hallucinating new objects inside holes, fulfilling the needs of object
removal tasks in real-world scenarios. Extensive experiments are conducted to
show that our method significantly outperforms existing methods in both
quantitative and qualitative evaluation. Please refer to the project page:
\url{https://github.com/htzheng/CM-GAN-Inpainting}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R-DFCIL: Relation-Guided Representation Learning for Data-Free Class Incremental Learning. (arXiv:2203.13104v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13104">
<div class="article-summary-box-inner">
<span><p>Class-Incremental Learning (CIL) struggles with catastrophic forgetting when
learning new knowledge, and Data-Free CIL (DFCIL) is even more challenging
without access to the training data of previously learned classes. Though
recent DFCIL works introduce techniques such as model inversion to synthesize
data for previous classes, they fail to overcome forgetting due to the severe
domain gap between the synthetic and real data. To address this issue, this
paper proposes relation-guided representation learning (RRL) for DFCIL, dubbed
R-DFCIL. In RRL, we introduce relational knowledge distillation to flexibly
transfer the structural relation of new data from the old model to the current
model. Our RRL-boosted DFCIL can guide the current model to learn
representations of new classes better compatible with representations of
previous classes, which greatly reduces forgetting while improving plasticity.
To avoid the mutual interference between representation and classifier
learning, we employ local rather than global classification loss during RRL.
After RRL, the classification head is refined with global class-balanced
classification loss to address the data imbalance issue as well as learn the
decision boundaries between new and previous classes. Extensive experiments on
CIFAR100, Tiny-ImageNet200, and ImageNet100 demonstrate that our R-DFCIL
significantly surpasses previous approaches and achieves a new state-of-the-art
performance for DFCIL. Code is available at
https://github.com/jianzhangcs/R-DFCIL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlowFormer: A Transformer Architecture for Optical Flow. (arXiv:2203.16194v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16194">
<div class="article-summary-box-inner">
<span><p>We introduce optical Flow transFormer, dubbed as FlowFormer, a
transformer-based neural network architecture for learning optical flow.
FlowFormer tokenizes the 4D cost volume built from an image pair, encodes the
cost tokens into a cost memory with alternate-group transformer (AGT) layers in
a novel latent space, and decodes the cost memory via a recurrent transformer
decoder with dynamic positional cost queries. On the Sintel benchmark,
FlowFormer achieves 1.144 and 2.183 average end-ponit-error (AEPE) on the clean
and final pass, a 17.6% and 11.6% error reduction from the best published
result (1.388 and 2.47). Besides, FlowFormer also achieves strong
generalization performance. Without being trained on Sintel, FlowFormer
achieves 0.95 AEPE on the Sintel training set clean pass, outperforming the
best published result (1.29) by 26.9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation. (arXiv:2204.01599v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01599">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches achieve prominent success in 3D semantic
segmentation. However, collecting densely annotated real-world 3D datasets is
extremely time-consuming and expensive. Training models on synthetic data and
generalizing on real-world scenarios becomes an appealing alternative, but
unfortunately suffers from notorious domain shifts. In this work, we propose a
Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and
context gaps caused by different sensing mechanisms and layout placements
across domains. Our DODA encompasses virtual scan simulation to imitate
real-world point cloud patterns and tail-aware cuboid mixing to alleviate the
interior context gap with a cuboid-based intermediate domain. The first
unsupervised sim-to-real adaptation benchmark on 3D indoor semantic
segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular
Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA
approaches by over 13% on both 3D-FRONT -&gt; ScanNet and 3D-FRONT -&gt; S3DIS. Code
is available at https://github.com/CVMI-Lab/DODA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Movie Clip Classification with State-Space Video Models. (arXiv:2204.01692v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01692">
<div class="article-summary-box-inner">
<span><p>Most modern video recognition models are designed to operate on short video
clips (e.g., 5-10s in length). Thus, it is challenging to apply such models to
long movie understanding tasks, which typically require sophisticated
long-range temporal reasoning. The recently introduced video transformers
partially address this issue by using long-range temporal self-attention.
However, due to the quadratic cost of self-attention, such models are often
costly and impractical to use. Instead, we propose ViS4mer, an efficient
long-range video model that combines the strengths of self-attention and the
recently introduced structured state-space sequence (S4) layer. Our model uses
a standard Transformer encoder for short-range spatiotemporal feature
extraction, and a multi-scale temporal S4 decoder for subsequent long-range
temporal reasoning. By progressively reducing the spatiotemporal feature
resolution and channel dimension at each decoder layer, ViS4mer learns complex
long-range spatiotemporal dependencies in a video. Furthermore, ViS4mer is
$2.63\times$ faster and requires $8\times$ less GPU memory than the
corresponding pure self-attention-based model. Additionally, ViS4mer achieves
state-of-the-art results in $6$ out of $9$ long-form movie video classification
tasks on the Long Video Understanding (LVU) benchmark. Furthermore, we show
that our approach successfully generalizes to other domains, achieving
competitive results on the Breakfast and the COIN procedural activity datasets.
The code is publicly available at: https://github.com/md-mohaiminul/ViS4mer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI. (arXiv:2204.01702v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01702">
<div class="article-summary-box-inner">
<span><p>Precision medicine for chronic diseases such as multiple sclerosis (MS)
involves choosing a treatment which best balances efficacy and side
effects/preferences for individual patients. Making this choice as early as
possible is important, as delays in finding an effective therapy can lead to
irreversible disability accrual. To this end, we present the first deep neural
network model for individualized treatment decisions from baseline magnetic
resonance imaging (MRI) (with clinical information if available) for MS
patients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)
lesion counts on follow-up MRI on multiple treatments and (b) estimates the
conditional average treatment effect (CATE), as defined by the predicted future
suppression of NE-T2 lesions, between different treatment options relative to
placebo. Our model is validated on a proprietary federated dataset of 1817
multi-sequence MRIs acquired from MS patients during four multi-centre
randomized clinical trials. Our framework achieves high average precision in
the binarized regression of future NE-T2 lesions on five different treatments,
identifies heterogeneous treatment effects, and provides a personalized
treatment recommendation that accounts for treatment-associated risk (e.g. side
effects, patient preference, administration difficulties).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D face reconstruction with dense landmarks. (arXiv:2204.02776v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02776">
<div class="article-summary-box-inner">
<span><p>Landmarks often play a key role in face analysis, but many aspects of
identity or expression cannot be represented by sparse landmarks alone. Thus,
in order to reconstruct faces more accurately, landmarks are often combined
with additional signals like depth images or techniques like differentiable
rendering. Can we keep things simple by just using more landmarks? In answer,
we present the first method that accurately predicts 10x as many landmarks as
usual, covering the whole head, including the eyes and teeth. This is
accomplished using synthetic training data, which guarantees perfect landmark
annotations. By fitting a morphable model to these dense landmarks, we achieve
state-of-the-art results for monocular 3D face reconstruction in the wild. We
show that dense landmarks are an ideal signal for integrating face shape
information across frames by demonstrating accurate and expressive facial
performance capture in both monocular and multi-view scenarios. This approach
is also highly efficient: we can predict dense landmarks and fit our 3D face
model at over 150FPS on a single CPU thread. Please see our website:
https://microsoft.github.io/DenseLandmarks/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02874">
<div class="article-summary-box-inner">
<span><p>We introduce an audiovisual method for long-range text-to-video retrieval.
Unlike previous approaches designed for short video retrieval (e.g., 5-15
seconds in duration), our approach aims to retrieve minute-long videos that
capture complex human actions. One challenge of standard video-only approaches
is the large computational cost associated with processing hundreds of densely
extracted frames from such long videos. To address this issue, we propose to
replace parts of the video with compact audio cues that succinctly summarize
dynamic audio events and are cheap to process. Our method, named ECLIPSE
(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an
audiovisual video setting, by adding a unified audiovisual transformer block
that captures complementary cues from the video and audio streams. In addition
to being 2.92x faster and 2.34x memory-efficient than long-range video-only
approaches, our method also achieves better text-to-video retrieval accuracy on
several diverse long-range video datasets such as ActivityNet, QVHighlights,
YouCook2, DiDeMo and Charades.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSGN++: Exploiting Visual-Spatial Relation for Stereo-based 3D Detectors. (arXiv:2204.03039v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03039">
<div class="article-summary-box-inner">
<span><p>Camera-based 3D object detectors are welcome due to their wider deployment
and lower price than LiDAR sensors. We first revisit the prior stereo detector
DSGN for its stereo volume construction ways for representing both 3D geometry
and semantics. We polish the stereo modeling and propose the advanced version,
DSGN++, aiming to enhance effective information flow throughout the 2D-to-3D
pipeline in three main aspects. First, to effectively lift the 2D information
to stereo volume, we propose depth-wise plane sweeping (DPS) that allows denser
connections and extracts depth-guided features. Second, for grasping
differently spaced features, we present a novel stereo volume -- Dual-view
Stereo Volume (DSV) that integrates front-view and top-view features and
reconstructs sub-voxel depth in the camera frustum. Third, as the foreground
region becomes less dominant in 3D space, we propose a multi-modal data editing
strategy -- Stereo-LiDAR Copy-Paste, which ensures cross-modal alignment and
improves data efficiency. Without bells and whistles, extensive experiments in
various modality setups on the popular KITTI benchmark show that our method
consistently outperforms other camera-based 3D detectors for all categories.
Code is available at https://github.com/chenyilun95/DSGN2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking. (arXiv:2204.07049v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07049">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an iterative self-training framework for
sim-to-real 6D object pose estimation to facilitate cost-effective robotic
grasping. Given a bin-picking scenario, we establish a photo-realistic
simulator to synthesize abundant virtual data, and use this to train an initial
pose estimation network. This network then takes the role of a teacher model,
which generates pose predictions for unlabeled real data. With these
predictions, we further design a comprehensive adaptive selection scheme to
distinguish reliable results, and leverage them as pseudo labels to update a
student model for pose estimation on real data. To continuously improve the
quality of pseudo labels, we iterate the above steps by taking the trained
student model as a new teacher and re-label real data using the refined teacher
model. We evaluate our method on a public benchmark and our newly-released
dataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.
Our method is also able to improve robotic bin-picking success by 19.54%,
demonstrating the potential of iterative sim-to-real solutions for robotic
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Level Set Theory for Neural Implicit Evolution under Explicit Flows. (arXiv:2204.07159v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07159">
<div class="article-summary-box-inner">
<span><p>Coordinate-based neural networks parameterizing implicit surfaces have
emerged as efficient representations of geometry. They effectively act as
parametric level sets with the zero-level set defining the surface of interest.
We present a framework that allows applying deformation operations defined for
triangle meshes onto such implicit surfaces. Several of these operations can be
viewed as energy-minimization problems that induce an instantaneous flow field
on the explicit surface. Our method uses the flow field to deform parametric
implicit surfaces by extending the classical theory of level sets. We also
derive a consolidated view for existing methods on differentiable surface
extraction and rendering, by formalizing connections to the level-set theory.
We show that these methods drift from the theory and that our approach exhibits
improvements for applications like surface smoothing, mean-curvature flow,
inverse rendering and user-defined editing on implicit geometry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation. (arXiv:2204.07733v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07733">
<div class="article-summary-box-inner">
<span><p>Birds-eye-view (BEV) semantic segmentation is critical for autonomous driving
for its powerful spatial representation ability. It is challenging to estimate
the BEV semantic maps from monocular images due to the spatial gap, since it is
implicitly required to realize both the perspective-to-BEV transformation and
segmentation. We present a novel two-stage Geometry Prior-based Transformation
framework named GitNet, consisting of (i) the geometry-guided pre-alignment and
(ii) ray-based transformer. In the first stage, we decouple the BEV
segmentation into the perspective image segmentation and geometric prior-based
mapping, with explicit supervision by projecting the BEV semantic labels onto
the image plane to learn visibility-aware features and learnable geometry to
translate into BEV space. Second, the pre-aligned coarse BEV features are
further deformed by ray-based transformers to take visibility knowledge into
account. GitNet achieves the leading performance on the challenging nuScenes
and Argoverse Datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09817">
<div class="article-summary-box-inner">
<span><p>Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13749">
<div class="article-summary-box-inner">
<span><p>Classifiers are biased when trained on biased datasets. As a remedy, we
propose Learning to Split (ls), an algorithm for automatic bias detection.
Given a dataset with input-label pairs, ls learns to split this dataset so that
predictors trained on the training split cannot generalize to the testing
split. This performance gap suggests that the testing split is
under-represented in the dataset, which is a signal of potential bias.
Identifying non-generalizable splits is challenging since we have no
annotations about the bias. In this work, we show that the prediction
correctness of each example in the testing split can be used as a source of
weak supervision: generalization performance will drop if we move examples that
are predicted correctly away from the testing split, leaving only those that
are mis-predicted. ls is task-agnostic and can be applied to any supervised
learning problem, ranging from natural language understanding and image
classification to molecular property prediction. Empirical results show that ls
is able to generate astonishingly challenging splits that correlate with
human-identified biases. Moreover, we demonstrate that combining robust
learning algorithms (such as group DRO) with splits identified by ls enables
automatic de-biasing. Compared to previous state-of-the-art, we substantially
improve the worst-group performance (23.4% on average) when the source of
biases is unknown during training and validation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking. (arXiv:2205.02301v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02301">
<div class="article-summary-box-inner">
<span><p>Estimating human motion from video is an active research area due to its many
potential applications. Most state-of-the-art methods predict human shape and
posture estimates for individual images and do not leverage the temporal
information available in video. Many "in the wild" sequences of human motion
are captured by a moving camera, which adds the complication of conflated
camera and human motion to the estimation. We therefore present BodySLAM, a
monocular SLAM system that jointly estimates the position, shape, and posture
of human bodies, as well as the camera trajectory. We also introduce a novel
human motion model to constrain sequential body postures and observe the scale
of the scene. Through a series of experiments on video sequences of human
motion captured by a moving monocular camera, we demonstrate that BodySLAM
improves estimates of all human body parameters and camera poses when compared
to estimating these separately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints. (arXiv:2205.04992v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04992">
<div class="article-summary-box-inner">
<span><p>Image-based volumetric humans using pixel-aligned features promise
generalization to unseen poses and identities. Prior work leverages global
spatial encodings and multi-view geometric consistency to reduce spatial
ambiguity. However, global encodings often suffer from overfitting to the
distribution of the training data, and it is difficult to learn multi-view
consistent reconstruction from sparse views. In this work, we investigate
common issues with existing spatial encodings and propose a simple yet highly
effective approach to modeling high-fidelity volumetric humans from sparse
views. One of the key ideas is to encode relative spatial 3D information via
sparse 3D keypoints. This approach is robust to the sparsity of viewpoints and
cross-dataset domain gap. Our approach outperforms state-of-the-art methods for
head reconstruction. On human body reconstruction for unseen subjects, we also
achieve performance comparable to prior work that uses a parametric human body
model and temporal feature aggregation. Our experiments show that a majority of
errors in prior work stem from an inappropriate choice of spatial encoding and
thus we suggest a new direction for high-fidelity image-based human modeling.
https://markomih.github.io/KeypointNeRF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TOCH: Spatio-Temporal Object-to-Hand Correspondence for Motion Refinement. (arXiv:2205.07982v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07982">
<div class="article-summary-box-inner">
<span><p>We present TOCH, a method for refining incorrect 3D hand-object interaction
sequences using a data prior. Existing hand trackers, especially those that
rely on very few cameras, often produce visually unrealistic results with
hand-object intersection or missing contacts. Although correcting such errors
requires reasoning about temporal aspects of interaction, most previous works
focus on static grasps and contacts. The core of our method are TOCH fields, a
novel spatio-temporal representation for modeling correspondences between hands
and objects during interaction. TOCH fields are a point-wise, object-centric
representation, which encode the hand position relative to the object.
Leveraging this novel representation, we learn a latent manifold of plausible
TOCH fields with a temporal denoising auto-encoder. Experiments demonstrate
that TOCH outperforms state-of-the-art 3D hand-object interaction models, which
are limited to static grasps and contacts. More importantly, our method
produces smooth interactions even before and after contact. Using a single
trained TOCH model, we quantitatively and qualitatively demonstrate its
usefulness for correcting erroneous sequences from off-the-shelf RGB/RGB-D
hand-object reconstruction methods and transferring grasps across objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrasting quadratic assignments for set-based representation learning. (arXiv:2205.15814v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15814">
<div class="article-summary-box-inner">
<span><p>The standard approach to contrastive learning is to maximize the agreement
between different views of the data. The views are ordered in pairs, such that
they are either positive, encoding different views of the same object, or
negative, corresponding to views of different objects. The supervisory signal
comes from maximizing the total similarity over positive pairs, while the
negative pairs are needed to avoid collapse. In this work, we note that the
approach of considering individual pairs cannot account for both intra-set and
inter-set similarities when the sets are formed from the views of the data. It
thus limits the information content of the supervisory signal available to
train representations. We propose to go beyond contrasting individual pairs of
objects by focusing on contrasting objects as sets. For this, we use
combinatorial quadratic assignment theory designed to evaluate set and graph
similarities and derive set-contrastive objective as a regularizer for
contrastive learning methods. We conduct experiments and demonstrate that our
method improves learned representations for the tasks of metric learning and
self-supervised classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. (arXiv:2206.04382v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04382">
<div class="article-summary-box-inner">
<span><p>We propose CLIP-Actor, a text-driven motion recommendation and neural mesh
stylization system for human mesh animation. CLIP-Actor animates a 3D human
mesh to conform to a text prompt by recommending a motion sequence and
optimizing mesh style attributes. We build a text-driven human motion
recommendation system by leveraging a large-scale human motion dataset with
language labels. Given a natural language prompt, CLIP-Actor suggests a
text-conforming human motion in a coarse-to-fine manner. Then, our novel
zero-shot neural style optimization detailizes and texturizes the recommended
mesh sequence to conform to the prompt in a temporally-consistent and
pose-agnostic manner. This is distinctive in that prior work fails to generate
plausible results when the pose of an artist-designed mesh does not conform to
the text from the beginning. We further propose the spatio-temporal view
augmentation and mask-weighted embedding attention, which stabilize the
optimization process by leveraging multi-frame human motion and rejecting
poorly rendered views. We demonstrate that CLIP-Actor produces plausible and
human-recognizable style 3D human mesh in motion with detailed geometry and
texture solely from a natural language prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Implicit Attention: Guided Attention by The Model Itself. (arXiv:2206.07434v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07434">
<div class="article-summary-box-inner">
<span><p>We propose Self-Supervised Implicit Attention (SSIA), a new approach that
adaptively guides deep neural network models to gain attention by exploiting
the properties of the models themselves. SSIA is a novel attention mechanism
that does not require any extra parameters, computation, or memory access costs
during inference, which is in contrast to existing attention mechanism. In
short, by considering attention weights as higher-level semantic information,
we reconsidered the implementation of existing attention mechanisms and further
propose generating supervisory signals from higher network layers to guide
lower network layers for parameter updates. We achieved this by building a
self-supervised learning task using the hierarchical features of the network
itself, which only works at the training stage. To verify the effectiveness of
SSIA, we performed a particular implementation (called an SSIA block) in
convolutional neural network models and validated it on several image
classification datasets. The experimental results show that an SSIA block can
significantly improve the model performance, even outperforms many popular
attention methods that require additional parameters and computation costs,
such as Squeeze-and-Excitation and Convolutional Block Attention Module. Our
implementation will be available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Segmentation of LiDAR Sequences: Dataset and Algorithm. (arXiv:2206.08194v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08194">
<div class="article-summary-box-inner">
<span><p>Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles.
However, most semantic datasets and algorithms used for LiDAR sequence
segmentation operate on $360^\circ$ frames, causing an acquisition latency
incompatible with real-time applications. To address this issue, we first
introduce HelixNet, a $10$ billion point dataset with fine-grained labels,
timestamps, and sensor rotation information necessary to accurately assess the
real-time readiness of segmentation algorithms. Second, we propose Helix4D, a
compact and efficient spatio-temporal transformer architecture specifically
designed for rotating LiDAR sequences. Helix4D operates on acquisition slices
corresponding to a fraction of a full sensor rotation, significantly reducing
the total latency. Helix4D reaches accuracy on par with the best segmentation
algorithms on HelixNet and SemanticKITTI with a reduction of over $5\times$ in
terms of latency and $50\times$ in model size. The code and data are available
at: https://romainloiseau.fr/helixnet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VectorMapNet: End-to-end Vectorized HD Map Learning. (arXiv:2206.08920v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08920">
<div class="article-summary-box-inner">
<span><p>Autonomous driving systems require a good understanding of surrounding
environments, including moving obstacles and static High-Definition (HD)
semantic map elements. Existing methods approach the semantic map problem by
offline manual annotation, which suffers from serious scalability issues.
Recent learning-based methods produce dense rasterized segmentation predictions
to construct maps. However, these predictions do not include instance
information of individual map elements and require heuristic post-processing,
that involves many hand-designed components, to obtain vectorized maps. To that
end, we introduce an end-to-end vectorized HD map learning pipeline, termed
VectorMapNet. VectorMapNet takes onboard sensor observations and predicts a
sparse set of polylines primitives in the bird's-eye view to model the geometry
of HD maps. This pipeline can explicitly model the spatial relation between map
elements and generate vectorized maps that are friendly to downstream
autonomous driving tasks without the need for post-processing. In our
experiments, VectorMapNet achieves strong HD map learning performance on
nuScenes dataset, surpassing previous state-of-the-art methods by 14.2 mAP.
Qualitatively, we also show that VectorMapNet is capable of generating
comprehensive maps and capturing more fine-grained details of road geometry. To
the best of our knowledge, VectorMapNet is the first work designed toward
end-to-end vectorized HD map learning problems. Our project website is
available at https://tsinghua-mars-lab.github.io/vectormapnet/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles. (arXiv:2206.10255v4 [eess.SY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10255">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) is among crucial applications in modern advanced
driver assistance systems (ADAS) and autonomous driving (AD) systems. The
global nearest neighbor (GNN) filter, as the earliest random vector-based
Bayesian tracking framework, has been adopted in most of state-of-the-arts
trackers and widely accepted in the automotive industry. With the development
of random finite set (RFS) theory, which facilitates a mathematically rigorous
treatment of the MOT problem, different variants of RFS-based Bayesian filters
have been developed. However, their usefulness in the real traffic for ADAS and
AD application is still open to doubt. In this paper, it is first demonstrated
that the latest RFS-based Bayesian tracking framework could be superior to
typical random vector-based Bayesian tracking framework like GNN, via a
systematic comparative study of both traditional random vector-based Bayesian
filters with rule-based heuristic track maintenance and RFS-based Bayesian
filters on the nuScenes validation dataset. Then, an RFS-based tracker, namely
Poisson multi-Bernoulli filter using the global nearest neighbor (GNN-PMB), is
proposed to LiDAR-based MOT tasks. This GNN-PMB tracker is simple to use but
can achieve competitive results on the nuScenes dataset. Specifically, the
proposed GNN-PMB tracker outperforms most of the state-of-the-art LiDAR-only
trackers and LiDAR and camera fusion-based trackers, ranking the 3rd among all
LiDAR-only trackers on nuScenes 3D tracking challenge leader board1 at the time
of submission. Our code is available at here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated GI tract segmentation using deep learning. (arXiv:2206.11048v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11048">
<div class="article-summary-box-inner">
<span><p>The job of Radiation oncologists is to deliver x-ray beams pointed toward the
tumor and at the same time avoid the stomach and intestines. With MR-Linacs
(magnetic resonance imaging and linear accelerator systems), oncologists can
visualize the position of the tumor and allow for precise dose according to
tumor cell presence which can vary from day to day. The current job of
outlining the position of the stomach and intestines to adjust the X-ray beams
direction for the dose delivery to the tumor while avoiding the organs. This is
a time-consuming and labor-intensive process that can easily prolong treatments
from 15 minutes to an hour a day unless deep learning methods can automate the
segmentation process. This paper discusses an automated segmentation process
using deep learning to make this process faster and allow more patients to get
effective treatment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Activity Localisation with Uncertainties in Temporal Boundary. (arXiv:2206.12923v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12923">
<div class="article-summary-box-inner">
<span><p>Current methods for video activity localisation over time assume implicitly
that activity temporal boundaries labelled for model training are determined
and precise. However, in unscripted natural videos, different activities mostly
transit smoothly, so that it is intrinsically ambiguous to determine in
labelling precisely when an activity starts and ends over time. Such
uncertainties in temporal labelling are currently ignored in model training,
resulting in learning mis-matched video-text correlation with poor
generalisation in test. In this work, we solve this problem by introducing
Elastic Moment Bounding (EMB) to accommodate flexible and adaptive activity
temporal boundaries towards modelling universally interpretable video-text
correlation with tolerance to underlying temporal uncertainties in pre-fixed
annotations. Specifically, we construct elastic boundaries adaptively by mining
and discovering frame-wise temporal endpoints that can maximise the alignment
between video segments and query sentences. To enable both more accurate
matching (segment content attention) and more robust localisation (segment
elastic boundaries), we optimise the selection of frame-wise endpoints subject
to segment-wise contents by a novel Guided Attention mechanism. Extensive
experiments on three video activity localisation benchmarks demonstrate
compellingly the EMB's advantages over existing methods without modelling
uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Lottery Ticket Hypothesis in Spiking Neural Networks. (arXiv:2207.01382v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01382">
<div class="article-summary-box-inner">
<span><p>Spiking Neural Networks (SNNs) have recently emerged as a new generation of
low-power deep neural networks, which is suitable to be implemented on
low-power mobile/edge devices. As such devices have limited memory storage,
neural pruning on SNNs has been widely explored in recent years. Most existing
SNN pruning works focus on shallow SNNs (2~6 layers), however, deeper SNNs (&gt;16
layers) are proposed by state-of-the-art SNN works, which is difficult to be
compatible with the current SNN pruning work. To scale up a pruning technique
towards deep SNNs, we investigate Lottery Ticket Hypothesis (LTH) which states
that dense networks contain smaller subnetworks (i.e., winning tickets) that
achieve comparable performance to the dense networks. Our studies on LTH reveal
that the winning tickets consistently exist in deep SNNs across various
datasets and architectures, providing up to 97% sparsity without huge
performance degradation. However, the iterative searching process of LTH brings
a huge training computational cost when combined with the multiple timesteps of
SNNs. To alleviate such heavy searching cost, we propose Early-Time (ET) ticket
where we find the important weight connectivity from a smaller number of
timesteps. The proposed ET ticket can be seamlessly combined with a common
pruning techniques for finding winning tickets, such as Iterative Magnitude
Pruning (IMP) and Early-Bird (EB) tickets. Our experiment results show that the
proposed ET ticket reduces search time by up to 38% compared to IMP or EB
methods. Code is available at Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease. (arXiv:2207.04574v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04574">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework for Alzheimer's disease (AD) detection using
brain MRIs. The framework starts with a data augmentation method called
Brain-Aware Replacements (BAR), which leverages a standard brain parcellation
to replace medically-relevant 3D brain regions in an anchor MRI from a randomly
picked MRI to create synthetic samples. Ground truth "hard" labels are also
linearly mixed depending on the replacement ratio in order to create "soft"
labels. BAR produces a great variety of realistic-looking synthetic MRIs with
higher local variability compared to other mix-based methods, such as CutMix.
On top of BAR, we propose using a soft-label-capable supervised contrastive
loss, aiming to learn the relative similarity of representations that reflect
how mixed are the synthetic MRIs using our soft labels. This way, we do not
fully exhaust the entropic capacity of our hard labels, since we only use them
to create soft labels and synthetic MRIs through BAR. We show that a model
pre-trained using our framework can be further fine-tuned with a cross-entropy
loss using the hard labels that were used to create the synthetic samples. We
validated the performance of our framework in a binary AD detection task
against both from-scratch supervised training and state-of-the-art
self-supervised training plus fine-tuning approaches. Then we evaluated BAR's
individual performance compared to another mix-based method CutMix by
integrating it within our framework. We show that our framework yields superior
results in both precision and recall for the AD detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Scale-Aware, Robust, and Generalizable Unsupervised Monocular Depth Estimation by Integrating IMU Motion Dynamics. (arXiv:2207.04680v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04680">
<div class="article-summary-box-inner">
<span><p>Unsupervised monocular depth and ego-motion estimation has drawn extensive
research attention in recent years. Although current methods have reached a
high up-to-scale accuracy, they usually fail to learn the true scale metric due
to the inherent scale ambiguity from training with monocular sequences. In this
work, we tackle this problem and propose DynaDepth, a novel scale-aware
framework that integrates information from vision and IMU motion dynamics.
Specifically, we first propose an IMU photometric loss and a cross-sensor
photometric consistency loss to provide dense supervision and absolute scales.
To fully exploit the complementary information from both sensors, we further
drive a differentiable camera-centric extended Kalman filter (EKF) to update
the IMU preintegrated motions when observing visual measurements. In addition,
the EKF formulation enables learning an ego-motion uncertainty measure, which
is non-trivial for unsupervised methods. By leveraging IMU during training,
DynaDepth not only learns an absolute scale, but also provides a better
generalization ability and robustness against vision degradation such as
illumination change and moving objects. We validate the effectiveness of
DynaDepth by conducting extensive experiments and simulations on the KITTI and
Make3D datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Graph Transformer for Video Question Answering. (arXiv:2207.05342v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05342">
<div class="article-summary-box-inner">
<span><p>This paper proposes a Video Graph Transformer (VGT) model for Video Quetion
Answering (VideoQA). VGT's uniqueness are two-fold: 1) it designs a dynamic
graph transformer module which encodes video by explicitly capturing the visual
objects, their relations, and dynamics for complex spatio-temporal reasoning;
and 2) it exploits disentangled video and text Transformers for relevance
comparison between the video and text to perform QA, instead of entangled
cross-modal Transformer for answer classification. Vision-text communication is
done by additional cross-modal interaction modules. With more reasonable video
encoding and QA solution, we show that VGT can achieve much better performances
on VideoQA tasks that challenge dynamic relation reasoning than prior arts in
the pretraining-free scenario. Its performances even surpass those models that
are pretrained with millions of external data. We further show that VGT can
also benefit a lot from self-supervised cross-modal pretraining, yet with
orders of magnitude smaller data. These results clearly demonstrate the
effectiveness and superiority of VGT, and reveal its potential for more
data-efficient pretraining. With comprehensive analyses and some heuristic
observations, we hope that VGT can promote VQA research beyond coarse
recognition/description towards fine-grained relation reasoning in realistic
videos. Our code is available at https://github.com/sail-sg/VGT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image and Model Transformation with Secret Key for Vision Transformer. (arXiv:2207.05366v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05366">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a combined use of transformed images and vision
transformer (ViT) models transformed with a secret key. We show for the first
time that models trained with plain images can be directly transformed to
models trained with encrypted images on the basis of the ViT architecture, and
the performance of the transformed models is the same as models trained with
plain images when using test images encrypted with the key. In addition, the
proposed scheme does not require any specially prepared data for training
models or network modification, so it also allows us to easily update the
secret key. In an experiment, the effectiveness of the proposed scheme is
evaluated in terms of performance degradation and model protection performance
in an image classification task on the CIFAR-10 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Lightweight Super-Resolution with Dual Regression Learning. (arXiv:2207.07929v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07929">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have exhibited remarkable performance in image
super-resolution (SR) tasks by learning a mapping from low-resolution (LR)
images to high-resolution (HR) images. However, the SR problem is typically an
ill-posed problem and existing methods would come with several limitations.
First, the possible mapping space of SR can be extremely large since there may
exist many different HR images that can be downsampled to the same LR image. As
a result, it is hard to directly learn a promising SR mapping from such a large
space. Second, it is often inevitable to develop very large models with
extremely high computational cost to yield promising SR performance. In
practice, one can use model compression techniques to obtain compact models by
reducing model redundancy. Nevertheless, it is hard for existing model
compression methods to accurately identify the redundant components due to the
extremely large SR mapping space. To alleviate the first challenge, we propose
a dual regression learning scheme to reduce the space of possible SR mappings.
Specifically, in addition to the mapping from LR to HR images, we learn an
additional dual regression mapping to estimate the downsampling kernel and
reconstruct LR images. In this way, the dual mapping acts as a constraint to
reduce the space of possible mappings. To address the second challenge, we
propose a lightweight dual regression compression method to reduce model
redundancy in both layer-level and channel-level based on channel pruning.
Specifically, we first develop a channel number search method that minimizes
the dual regression loss to determine the redundancy of each layer. Given the
searched channel numbers, we further exploit the dual regression manner to
evaluate the importance of channels and prune the redundant ones. Extensive
experiments show the effectiveness of our method in obtaining accurate and
efficient SR models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding The Semidefinite Relaxations of Truncated Least-Squares in Robust Rotation Search. (arXiv:2207.08350v2 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08350">
<div class="article-summary-box-inner">
<span><p>The rotation search problem aims to find a 3D rotation that best aligns a
given number of point pairs. To induce robustness against outliers for rotation
search, prior work considers truncated least-squares (TLS), which is a
non-convex optimization problem, and its semidefinite relaxation (SDR) as a
tractable alternative. Whether this SDR is theoretically tight in the presence
of noise, outliers, or both has remained largely unexplored. We derive
conditions that characterize the tightness of this SDR, showing that the
tightness depends on the noise level, the truncation parameters of TLS, and the
outlier distribution (random or clustered). In particular, we give a short
proof for the tightness in the noiseless and outlier-free case, as opposed to
the lengthy analysis of prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latency-Aware Collaborative Perception. (arXiv:2207.08560v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08560">
<div class="article-summary-box-inner">
<span><p>Collaborative perception has recently shown great potential to improve
perception capabilities over single-agent perception. Existing collaborative
perception methods usually consider an ideal communication environment.
However, in practice, the communication system inevitably suffers from latency
issues, causing potential performance degradation and high risks in
safety-critical applications, such as autonomous driving. To mitigate the
effect caused by the inevitable latency, from a machine learning perspective,
we present the first latency-aware collaborative perception system, which
actively adapts asynchronous perceptual features from multiple agents to the
same time stamp, promoting the robustness and effectiveness of collaboration.
To achieve such a feature-level synchronization, we propose a novel latency
compensation module, called SyncNet, which leverages feature-attention
symbiotic estimation and time modulation techniques. Experiments results show
that the proposed latency aware collaborative perception system with SyncNet
can outperforms the state-of-the-art collaborative perception method by 15.6%
in the communication latency scenario and keep collaborative perception being
superior to single agent perception under severe latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-Aware Reference Synthesis for Multi-View Image Super-Resolution. (arXiv:2207.08601v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08601">
<div class="article-summary-box-inner">
<span><p>Recent multi-view multimedia applications struggle between high-resolution
(HR) visual experience and storage or bandwidth constraints. Therefore, this
paper proposes a Multi-View Image Super-Resolution (MVISR) task. It aims to
increase the resolution of multi-view images captured from the same scene. One
solution is to apply image or video super-resolution (SR) methods to
reconstruct HR results from the low-resolution (LR) input view. However, these
methods cannot handle large-angle transformations between views and leverage
information in all multi-view images. To address these problems, we propose the
MVSRnet, which uses geometry information to extract sharp details from all LR
multi-view to support the SR of the LR input view. Specifically, the proposed
Geometry-Aware Reference Synthesis module in MVSRnet uses geometry information
and all multi-view LR images to synthesize pixel-aligned HR reference images.
Then, the proposed Dynamic High-Frequency Search network fully exploits the
high-frequency textural details in reference images for SR. Extensive
experiments on several benchmarks show that our method significantly improves
over the state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Partition Implicit with Surface Codes for 3D Representation. (arXiv:2207.08631v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08631">
<div class="article-summary-box-inner">
<span><p>Deep implicit functions have shown remarkable shape modeling ability in
various 3D computer vision tasks. One drawback is that it is hard for them to
represent a 3D shape as multiple parts. Current solutions learn various
primitives and blend the primitives directly in the spatial space, which still
struggle to approximate the 3D shape accurately. To resolve this problem, we
introduce a novel implicit representation to represent a single 3D shape as a
set of parts in the latent space, towards both highly accurate and plausibly
interpretable shape modeling. Our insight here is that both the part learning
and the part blending can be conducted much easier in the latent space than in
the spatial space. We name our method Latent Partition Implicit (LPI), because
of its ability of casting the global shape modeling into multiple local part
modeling, which partitions the global shape unity. LPI represents a shape as
Signed Distance Functions (SDFs) using surface codes. Each surface code is a
latent code representing a part whose center is on the surface, which enables
us to flexibly employ intrinsic attributes of shapes or additional surface
properties. Eventually, LPI can reconstruct both the shape and the parts on the
shape, both of which are plausible meshes. LPI is a multi-level representation,
which can partition a shape into different numbers of parts after training. LPI
can be learned without ground truth signed distances, point normals or any
supervision for part partition. LPI outperforms the latest methods under the
widely used benchmarks in terms of reconstruction accuracy and modeling
interpretability. Our code, data and models are available at
https://github.com/chenchao15/LPI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Action Affinity and Continuity for Semi-supervised Temporal Action Segmentation. (arXiv:2207.08653v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08653">
<div class="article-summary-box-inner">
<span><p>We present a semi-supervised learning approach to the temporal action
segmentation task. The goal of the task is to temporally detect and segment
actions in long, untrimmed procedural videos, where only a small set of videos
are densely labelled, and a large collection of videos are unlabelled. To this
end, we propose two novel loss functions for the unlabelled data: an action
affinity loss and an action continuity loss. The action affinity loss guides
the unlabelled samples learning by imposing the action priors induced from the
labelled set. Action continuity loss enforces the temporal continuity of
actions, which also provides frame-wise classification supervision. In
addition, we propose an Adaptive Boundary Smoothing (ABS) approach to build
coarser action boundaries for more robust and reliable learning. The proposed
loss functions and ABS were evaluated on three benchmarks. Results show that
they significantly improved action segmentation performance with a low amount
(5% and 10%) of labelled data and achieved comparable results to full
supervision with 50% labelled data. Furthermore, ABS succeeded in boosting
performance when integrated into fully-supervised learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video. (arXiv:2207.08920v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08920">
<div class="article-summary-box-inner">
<span><p>Introduction: Hand function is a central determinant of independence after
stroke. Measuring hand use in the home environment is necessary to evaluate the
impact of new interventions, and calls for novel wearable technologies.
Egocentric video can capture hand-object interactions in context, as well as
show how more-affected hands are used during bilateral tasks (for stabilization
or manipulation). Automated methods are required to extract this information.
Objective: To use artificial intelligence-based computer vision to classify
hand use and hand role from egocentric videos recorded at home after stroke.
Methods: Twenty-one stroke survivors participated in the study. A random forest
classifier, a SlowFast neural network, and the Hand Object Detector neural
network were applied to identify hand use and hand role at home.
Leave-One-Subject-Out-Cross-Validation (LOSOCV) was used to evaluate the
performance of the three models. Between-group differences of the models were
calculated based on the Mathews correlation coefficient (MCC). Results: For
hand use detection, the Hand Object Detector had significantly higher
performance than the other models. The macro average MCCs using this model in
the LOSOCV were 0.50 +- 0.23 for the more-affected hands and 0.58 +- 0.18 for
the less-affected hands. Hand role classification had macro average MCCs in the
LOSOCV that were close to zero for all models. Conclusion: Using egocentric
video to capture the hand use of stroke survivors at home is feasible. Pose
estimation to track finger movements may be beneficial to classifying hand
roles in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Interactive Object Segmentation Through a Singulation-and-Grasping Approach. (arXiv:2207.09314v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09314">
<div class="article-summary-box-inner">
<span><p>Instance segmentation with unseen objects is a challenging problem in
unstructured environments. To solve this problem, we propose a robot learning
approach to actively interact with novel objects and collect each object's
training label for further fine-tuning to improve the segmentation model
performance, while avoiding the time-consuming process of manually labeling a
dataset. The Singulation-and-Grasping (SaG) policy is trained through
end-to-end reinforcement learning. Given a cluttered pile of objects, our
approach chooses pushing and grasping motions to break the clutter and conducts
object-agnostic grasping for which the SaG policy takes as input the visual
observations and imperfect segmentation. We decompose the problem into three
subtasks: (1) the object singulation subtask aims to separate the objects from
each other, which creates more space that alleviates the difficulty of (2) the
collision-free grasping subtask; (3) the mask generation subtask to obtain the
self-labeled ground truth masks by using an optical flow-based binary
classifier and motion cue post-processing for transfer learning. Our system
achieves 70% singulation success rate in simulated cluttered scenes. The
interactive segmentation of our system achieves 87.8%, 73.9%, and 69.3% average
precision for toy blocks, YCB objects in simulation and real-world novel
objects, respectively, which outperforms several baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoserNet: Refining Relative Camera Poses Exploiting Object Detections. (arXiv:2207.09445v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09445">
<div class="article-summary-box-inner">
<span><p>The estimation of the camera poses associated with a set of images commonly
relies on feature matches between the images. In contrast, we are the first to
address this challenge by using objectness regions to guide the pose estimation
problem rather than explicit semantic object detections. We propose Pose
Refiner Network (PoserNet) a light-weight Graph Neural Network to refine the
approximate pair-wise relative camera poses. PoserNet exploits associations
between the objectness regions - concisely expressed as bounding boxes - across
multiple views to globally refine sparsely connected view graphs. We evaluate
on the 7-Scenes dataset across varied sizes of graphs and show how this process
can be beneficial to optimisation-based Motion Averaging algorithms improving
the median error on the rotation by 62 degrees with respect to the initial
estimates obtained based on bounding boxes. Code and data are available at
https://github.com/IIT-PAVIS/PoserNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images. (arXiv:2207.09508v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09508">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the results of the HSE-NN team in the 4th
competition on Affective Behavior Analysis in-the-wild (ABAW). The novel
multi-task EfficientNet model is trained for simultaneous recognition of facial
expressions and prediction of valence and arousal on static photos. The
resulting MT-EmotiEffNet extracts visual features that are fed into simple
feed-forward neural networks in the multi-task learning challenge. We obtain
performance measure 1.3 on the validation set, which is significantly greater
when compared to either performance of baseline (0.3) or existing models that
are trained only on the s-Aff-Wild2 database. In the learning from synthetic
data challenge, the quality of the original synthetic training set is increased
by using the super-resolution techniques, such as Real-ESRGAN. Next, the
MT-EmotiEffNet is fine-tuned on the new training set. The final prediction is a
simple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our
average validation F1 score is 18% greater than the baseline convolutional
neural network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation of 3D Dental Images Using Deep Learning. (arXiv:2207.09582v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09582">
<div class="article-summary-box-inner">
<span><p>3D image segmentation is a recent and crucial step in many medical analysis
and recognition schemes. In fact, it represents a relevant research subject and
a fundamental challenge due to its importance and influence. This paper
provides a multi-phase Deep Learning-based system that hybridizes various
efficient methods in order to get the best 3D segmentation output. First, to
reduce the amount of data and accelerate the processing time, the application
of Decimate compression technique is suggested and justified. We then use a CNN
model to segment dental images into fifteen separated classes. In the end, a
special KNN-based transformation is applied for the purpose of removing
isolated meshes and of correcting dental forms. Experimentations demonstrate
the precision and the robustness of the selected framework applied to 3D dental
images within a private clinical benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective Phase Angle Model for Polarimetric 3D Reconstruction. (arXiv:2207.09629v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09629">
<div class="article-summary-box-inner">
<span><p>Current polarimetric 3D reconstruction methods, including those in the
well-established shape from polarization literature, are all developed under
the orthographic projection assumption. In the case of a large field of view,
however, this assumption does not hold and may result in significant
reconstruction errors in methods that make this assumption. To address this
problem, we present the perspective phase angle (PPA) model that is applicable
to perspective cameras. Compared with the orthographic model, the proposed PPA
model accurately describes the relationship between polarization phase angle
and surface normal under perspective projection. In addition, the PPA model
makes it possible to estimate surface normals from only one single-view phase
angle map and does not suffer from the so-called $\pi$-ambiguity problem.
Experiments on real data show that the PPA model is more accurate for surface
normal estimation with a perspective camera than the orthographic model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers. (arXiv:2207.09662v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09662">
<div class="article-summary-box-inner">
<span><p>Temporal action localization (TAL) is a task of identifying a set of actions
in a video, which involves localizing the start and end frames and classifying
each action instance. Existing methods have addressed this task by using
predefined anchor windows or heuristic bottom-up boundary-matching strategies,
which are major bottlenecks in inference time. Additionally, the main challenge
is the inability to capture long-range actions due to a lack of global
contextual information. In this paper, we present a novel anchor-free
framework, referred to as HTNet, which predicts a set of &lt;start time, end time,
class&gt; triplets from a video based on a Transformer architecture. After the
prediction of coarse boundaries, we refine it through a background feature
sampling (BFS) module and hierarchical Transformers, which enables our model to
aggregate global contextual information and effectively exploit the inherent
semantic relationships in a video. We demonstrate how our method localizes
accurate action instances and achieves state-of-the-art performance on two TAL
benchmark datasets: THUMOS14 and ActivityNet 1.3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERA: Expert Retrieval and Assembly for Early Action Prediction. (arXiv:2207.09675v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09675">
<div class="article-summary-box-inner">
<span><p>Early action prediction aims to successfully predict the class label of an
action before it is completely performed. This is a challenging task because
the beginning stages of different actions can be very similar, with only minor
subtle differences for discrimination. In this paper, we propose a novel Expert
Retrieval and Assembly (ERA) module that retrieves and assembles a set of
experts most specialized at using discriminative subtle differences, to
distinguish an input sample from other highly similar samples. To encourage our
model to effectively use subtle differences for early action prediction, we
push experts to discriminate exclusively between samples that are highly
similar, forcing these experts to learn to use subtle differences that exist
between those samples. Additionally, we design an effective Expert Learning
Rate Optimization method that balances the experts' optimization and leads to
better performance. We evaluate our ERA module on four public action datasets
and achieve state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing. (arXiv:2207.09812v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09812">
<div class="article-summary-box-inner">
<span><p>Machine learning is transforming the video editing industry. Recent advances
in computer vision have leveled-up video editing tasks such as intelligent
reframing, rotoscoping, color grading, or applying digital makeups. However,
most of the solutions have focused on video manipulation and VFX. This work
introduces the Anatomy of Video Editing, a dataset, and benchmark, to foster
research in AI-assisted video editing. Our benchmark suite focuses on video
editing tasks, beyond visual effects, such as automatic footage organization
and assisted video assembling. To enable research on these fronts, we annotate
more than 1.5M tags, with relevant concepts to cinematography, from 196176
shots sampled from movie scenes. We establish competitive baseline methods and
detailed analyses for each of the tasks. We hope our work sparks innovative
research towards underexplored areas of AI-assisted video editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09933">
<div class="article-summary-box-inner">
<span><p>In clinical procedures of angioplasty (i.e., open clogged coronary arteries),
devices such as balloons and stents need to be placed and expanded in arteries
under the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,
the resulting images are often noisy. To check the correct placement of these
devices, typically multiple motion-compensated frames are averaged to enhance
the view. Therefore, device tracking is a necessary procedure for this purpose.
Even though angioplasty devices are designed to have radiopaque markers for the
ease of tracking, current methods struggle to deliver satisfactory results due
to the small marker size and complex scenes in angioplasty. In this paper, we
propose an end-to-end deep learning framework for single stent tracking, which
consists of three hierarchical modules: U-Net based landmark detection, ResNet
based stent proposal and feature extraction, and graph convolutional neural
network (GCN) based stent tracking that temporally aggregates both spatial
information and appearance features. The experiments show that our method
performs significantly better in detection compared with the state-of-the-art
point-based tracking models. In addition, its fast inference speed satisfies
clinical requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Secrets of Event-Based Optical Flow. (arXiv:2207.10022v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10022">
<div class="article-summary-box-inner">
<span><p>Event cameras respond to scene dynamics and offer advantages to estimate
motion. Following recent image-based deep-learning achievements, optical flow
estimation methods for event cameras have rushed to combine those image-based
methods with event data. However, it requires several adaptations (data
conversion, loss function, etc.) as they have very different properties. We
develop a principled method to extend the Contrast Maximization framework to
estimate optical flow from events alone. We investigate key elements: how to
design the objective function to prevent overfitting, how to warp events to
deal better with occlusions, and how to improve convergence with multi-scale
raw events. With these key elements, our method ranks first among unsupervised
methods on the MVSEC benchmark, and is competitive on the DSEC benchmark.
Moreover, our method allows us to expose the issues of the ground truth flow in
those benchmarks, and produces remarkable results when it is transferred to
unsupervised learning settings. Our code is available at
https://github.com/tub-rip/event_based_optical_flow
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks. (arXiv:2207.10025v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10025">
<div class="article-summary-box-inner">
<span><p>Facial expression in-the-wild is essential for various interactive computing
domains. Especially, "Learning from Synthetic Data" (LSD) is an important topic
in the facial expression recognition task. In this paper, we propose a
multi-task learning-based facial expression recognition approach which consists
of emotion and appearance learning branches that can share all face
information, and present preliminary results for the LSD challenge introduced
in the 4th affective behavior analysis in-the-wild (ABAW) competition. Our
method achieved the mean F1 score of 0.71.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Densely Constrained Depth Estimator for Monocular 3D Object Detection. (arXiv:2207.10047v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10047">
<div class="article-summary-box-inner">
<span><p>Estimating accurate 3D locations of objects from monocular images is a
challenging problem because of lacking depth. Previous work shows that
utilizing the object's keypoint projection constraints to estimate multiple
depth candidates boosts the detection performance. However, the existing
methods can only utilize vertical edges as projection constraints for depth
estimation. So these methods only use a small number of projection constraints
and produce insufficient depth candidates, leading to inaccurate depth
estimation. In this paper, we propose a method that utilizes dense projection
constraints from edges of any direction. In this way, we employ much more
projection constraints and produce considerable depth candidates. Besides, we
present a graph matching weighting module to merge the depth candidates. The
proposed method DCD (Densely Constrained Detector) achieves state-of-the-art
performance on the KITTI and WOD benchmarks. Code is released at
https://github.com/BraveGroup/DCD.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-22 23:08:14.863035486 UTC">2022-07-22 23:08:14 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>