<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-04T01:30:00Z">07-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiViz: An Analysis Benchmark for Visualizing and Understanding Multimodal Models. (arXiv:2207.00056v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00056">
<div class="article-summary-box-inner">
<span><p>The promise of multimodal models for real-world applications has inspired
research in visualizing and understanding their internal mechanics with the end
goal of empowering stakeholders to visualize model behavior, perform model
debugging, and promote trust in machine learning models. However, modern
multimodal models are typically black-box neural networks, which makes it
challenging to understand their internal mechanics. How can we visualize the
internal modeling of multimodal interactions in these models? Our paper aims to
fill this gap by proposing MultiViz, a method for analyzing the behavior of
multimodal models by scaffolding the problem of interpretability into 4 stages:
(1) unimodal importance: how each modality contributes towards downstream
modeling and prediction, (2) cross-modal interactions: how different modalities
relate with each other, (3) multimodal representations: how unimodal and
cross-modal interactions are represented in decision-level features, and (4)
multimodal prediction: how decision-level features are composed to make a
prediction. MultiViz is designed to operate on diverse modalities, models,
tasks, and research areas. Through experiments on 8 trained models across 6
real-world tasks, we show that the complementary stages in MultiViz together
enable users to (1) simulate model predictions, (2) assign interpretable
concepts to features, (3) perform error analysis on model misclassifications,
and (4) use insights from error analysis to debug models. MultiViz is publicly
available, will be regularly updated with new interpretation tools and metrics,
and welcomes inputs from the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Human-Agent Communication via the Information Bottleneck Principle. (arXiv:2207.00088v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00088">
<div class="article-summary-box-inner">
<span><p>Emergent communication research often focuses on optimizing task-specific
utility as a driver for communication. However, human languages appear to
evolve under pressure to efficiently compress meanings into communication
signals by optimizing the Information Bottleneck tradeoff between
informativeness and complexity. In this work, we study how trading off these
three factors -- utility, informativeness, and complexity -- shapes emergent
communication, including compared to human communication. To this end, we
propose Vector-Quantized Variational Information Bottleneck (VQ-VIB), a method
for training neural agents to compress inputs into discrete signals embedded in
a continuous space. We train agents via VQ-VIB and compare their performance to
previously proposed neural architectures in grounded environments and in a
Lewis reference game. Across all neural architectures and settings, taking into
account communicative informativeness benefits communication convergence rates,
and penalizing communicative complexity leads to human-like lexicon sizes while
maintaining high utility. Additionally, we find that VQ-VIB outperforms other
discrete communication methods. This work demonstrates how fundamental
principles that are believed to characterize human language evolution may
inform emergent communication in artificial agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language model compression with weighted low-rank factorization. (arXiv:2207.00112v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00112">
<div class="article-summary-box-inner">
<span><p>Factorizing a large matrix into small matrices is a popular strategy for
model compression. Singular value decomposition (SVD) plays a vital role in
this compression strategy, approximating a learned matrix with fewer
parameters. However, SVD minimizes the squared error toward reconstructing the
original matrix without gauging the importance of the parameters, potentially
giving a larger reconstruction error for those who affect the task accuracy
more. In other words, the optimization objective of SVD is not aligned with the
trained model's task accuracy. We analyze this previously unexplored problem,
make observations, and address it by introducing Fisher information to weigh
the importance of parameters affecting the model prediction. This idea leads to
our method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from
our approach do not result in smaller reconstruction errors, we find that our
resulting task accuracy is much closer to the original model's performance. We
perform analysis with the transformer-based language models, showing our
weighted SVD largely alleviates the mismatched optimization objectives and can
maintain model performance with a higher compression rate. Our method can
directly compress a task-specific model while achieving better performance than
other compact model strategies requiring expensive model pre-training.
Moreover, the evaluation of compressing an already compact model shows our
method can further reduce 9% to 30% parameters with an insignificant impact on
task accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Understanding-Oriented Robust Machine Reading Comprehension Model. (arXiv:2207.00187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00187">
<div class="article-summary-box-inner">
<span><p>Although existing machine reading comprehension models are making rapid
progress on many datasets, they are far from robust. In this paper, we propose
an understanding-oriented machine reading comprehension model to address three
kinds of robustness issues, which are over sensitivity, over stability and
generalization. Specifically, we first use a natural language inference module
to help the model understand the accurate semantic meanings of input questions
so as to address the issues of over sensitivity and over stability. Then in the
machine reading comprehension module, we propose a memory-guided multi-head
attention method that can further well understand the semantic meanings of
input questions and passages. Third, we propose a multilanguage learning
mechanism to address the issue of generalization. Finally, these modules are
integrated with a multi-task learning based method. We evaluate our model on
three benchmark datasets that are designed to measure models robustness,
including DuReader (robust) and two SQuAD-related datasets. Extensive
experiments show that our model can well address the mentioned three kinds of
robustness issues. And it achieves much better results than the compared
state-of-the-art models on all these datasets under different evaluation
metrics, even under some extreme and unfair evaluations. The source code of our
work is available at: https://github.com/neukg/RobustMRC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. (arXiv:2207.00220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00220">
<div class="article-summary-box-inner">
<span><p>One concern with the rise of large language models lies with their potential
for significant harm, particularly from pretraining on biased, obscene,
copyrighted, and private information. Emerging ethical approaches have
attempted to filter pretraining material, but such approaches have been ad hoc
and failed to take into account context. We offer an approach to filtering
grounded in law, which has directly addressed the tradeoffs in filtering
material. First, we gather and make available the Pile of Law, a 256GB (and
growing) dataset of open-source English-language legal and administrative data,
covering court opinions, contracts, administrative rules, and legislative
records. Pretraining on the Pile of Law may potentially help with legal tasks
that have the promise to improve access to justice. Second, we distill the
legal norms that governments have developed to constrain the inclusion of toxic
or private content into actionable lessons for researchers and discuss how our
dataset reflects these norms. Third, we show how the Pile of Law offers
researchers the opportunity to learn such filtering rules directly from the
data, providing an exciting new research direction in model-based processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00221">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pretraining (VLP) models have recently successfully
facilitated many cross-modal downstream tasks. Most existing works evaluated
their systems by comparing the fine-tuned downstream task performance. However,
only average downstream task accuracy provides little information about the
pros and cons of each VLP method, let alone provides insights on how the
community can improve the systems in the future. Inspired by the CheckList for
testing natural language processing, we introduce VL-CheckList, a novel
framework to understand the capabilities of VLP models. The proposed method
divides the image-texting ability of a VLP model into three categories:
objects, attributes, and relations, and uses a novel taxonomy to further break
down these three aspects. We conduct comprehensive studies to analyze seven
recently popular VLP models via the proposed framework. Results confirm the
effectiveness of the proposed method by revealing fine-grained differences
among the compared models that were not visible from downstream task-only
evaluation. Further results show promising research direction in building
better VLP models. Data and Code: https://github.com/om-ai-lab/VL-CheckList
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-features based Semantic Augmentation Networks for Named Entity Recognition in Threat Intelligence. (arXiv:2207.00232v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00232">
<div class="article-summary-box-inner">
<span><p>Extracting cybersecurity entities such as attackers and vulnerabilities from
unstructured network texts is an important part of security analysis. However,
the sparsity of intelligence data resulted from the higher frequency variations
and the randomness of cybersecurity entity names makes it difficult for current
methods to perform well in extracting security-related concepts and entities.
To this end, we propose a semantic augmentation method which incorporates
different linguistic features to enrich the representation of input tokens to
detect and classify the cybersecurity names over unstructured text. In
particular, we encode and aggregate the constituent feature, morphological
feature and part of speech feature for each input token to improve the
robustness of the method. More than that, a token gets augmented semantic
information from its most similar K words in cybersecurity domain corpus where
an attentive module is leveraged to weigh differences of the words, and from
contextual clues based on a large-scale general field corpus. We have conducted
experiments on the cybersecurity datasets DNRTI and MalwareTextDB, and the
results demonstrate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affordance Extraction with an External Knowledge Database for Text-Based Simulated Environments. (arXiv:2207.00265v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00265">
<div class="article-summary-box-inner">
<span><p>Text-based simulated environments have proven to be a valid testbed for
machine learning approaches. The process of affordance extraction can be used
to generate possible actions for interaction within such an environment. In
this paper the capabilities and challenges for utilizing external knowledge
databases (in particular ConceptNet) in the process of affordance extraction
are studied. An algorithm for automated affordance extraction is introduced and
evaluated on the Interactive Fiction (IF) platforms TextWorld and Jericho. For
this purpose, the collected affordances are translated into text commands for
IF agents. To probe the quality of the automated evaluation process, an
additional human baseline study is conducted. The paper illustrates that,
despite some challenges, external databases can in principle be used for
affordance extraction. The paper concludes with recommendations for further
modification and improvement of the process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vers la compr\'ehension automatique de la parole bout-en-bout \`a moindre effort. (arXiv:2207.00349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00349">
<div class="article-summary-box-inner">
<span><p>Recent advances in spoken language understanding benefited from
Self-Supervised models trained on large speech corpora. For French, the
LeBenchmark project has made such models available and has led to impressive
progress on several tasks including spoken language understanding. These
advances have a non-negligible cost in terms of computation time and energy
consumption. In this paper, we compare several learning strategies aiming at
reducing such cost while keeping competitive performances. The experiments are
performed on the MEDIA corpus, and show that it is possible to reduce the
learning cost while maintaining state-of-the-art performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Low-Cost End-to-End Spoken Language Understanding. (arXiv:2207.00352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00352">
<div class="article-summary-box-inner">
<span><p>Recent advances in spoken language understanding benefited from
Self-Supervised models trained on large speech corpora. For French, the
LeBenchmark project has made such models available and has led to impressive
progress on several tasks including spoken language understanding. These
advances have a non-negligible cost in terms of computation time and energy
consumption. In this paper, we compare several learning strategies trying to
reduce such cost while keeping competitive performance. At the same time we
propose an extensive analysis where we measure the cost of our models in terms
of training time and electric energy consumption, hopefully promoting a
comprehensive evaluation procedure. The experiments are performed on the FSC
and MEDIA corpora, and show that it is possible to reduce the learning cost
while maintaining state-of-the-art performance and using SSL models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Generation with a Question-Answering Blueprint. (arXiv:2207.00397v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00397">
<div class="article-summary-box-inner">
<span><p>The ability to convey relevant and faithful information is critical for many
tasks in conditional generation and yet remains elusive for neural seq-to-seq
models whose outputs often reveal hallucinations and fail to correctly cover
important details. In this work, we advocate planning as a useful intermediate
representation for rendering conditional generation less opaque and more
grounded. Our work proposes a new conceptualization of text plans as a sequence
of question-answer (QA) pairs. We enhance existing datasets (e.g., for
summarization) with a QA blueprint operating as a proxy for both content
selection (i.e.,~what to say) and planning (i.e.,~in what order). We obtain
blueprints automatically by exploiting state-of-the-art question generation
technology and convert input-output pairs into input-blueprint-output tuples.
We develop Transformer-based models, each varying in how they incorporate the
blueprint in the generated output (e.g., as a global plan or iteratively).
Evaluation across metrics and datasets demonstrates that blueprint models are
more factual than alternatives which do not resort to planning and allow
tighter control of the generation output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swiss German Speech to Text system evaluation. (arXiv:2207.00412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00412">
<div class="article-summary-box-inner">
<span><p>We present an in-depth evaluation of four commercially available
Speech-to-Text (STT) systems for Swiss German. The systems are anonymized and
referred to as system a-d in this report. We compare the four systems to our
STT model, referred to as FHNW from hereon after, and provide details on how we
trained our model. To evaluate the models, we use two STT datasets from
different domains. The Swiss Parliament Corpus (SPC) test set and a private
dataset in the news domain with an even distribution across seven dialect
regions. We provide a detailed error analysis to detect the three systems'
strengths and weaknesses. This analysis is limited by the characteristics of
the two test sets. Our model scored the highest bilingual evaluation understudy
(BLEU) on both datasets. On the SPC test set, we obtain a BLEU score of 0.607,
whereas the best commercial system reaches a BLEU score of 0.509. On our
private test set, we obtain a BLEU score of 0.722 and the best commercial
system a BLEU score of 0.568.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How trial-to-trial learning shapes mappings in the mental lexicon: Modelling Lexical Decision with Linear Discriminative Learning. (arXiv:2207.00430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00430">
<div class="article-summary-box-inner">
<span><p>Priming and antipriming can be modelled with error-driven learning (Marsolek,
2008), by assuming that the learning of the prime influences processing of the
target stimulus. This implies that participants are continuously learning in
priming studies, and predicts that they are also learning in each trial of
other psycholinguistic experiments. This study investigates whether
trial-to-trial learning can be detected in lexical decision experiments. We
used the Discriminative Lexicon Model (DLM; Baayen et al., 2019), a model of
the mental lexicon with meaning representations from distributional semantics,
which models incremental learning with the Widrow-Hoff rule. We used data from
the British Lexicon Project (BLP; Keuleers et al., 2012) and simulated the
lexical decision experiment with the DLM on a trial-by-trial basis for each
subject individually. Then, reaction times for words and nonwords were
predicted with Generalised Additive Models, using measures derived from the DLM
simulations as predictors. Models were developed with the data of two subjects
and tested on all other subjects. We extracted measures from two simulations
for each subject (one with learning updates between trials and one without),
and used them as input to two GAMs. Learning-based models showed better model
fit than the non-learning ones for the majority of subjects. Our measures also
provided insights into lexical processing and enabled us to explore individual
differences with Linear Mixed Models. This demonstrates the potential of the
DLM to model behavioural data and leads to the conclusion that trial-to-trial
learning can indeed be detected in psycholinguistic experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning of Multi-Domain Dialog Policies Via Action Embeddings. (arXiv:2207.00468v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00468">
<div class="article-summary-box-inner">
<span><p>Learning task-oriented dialog policies via reinforcement learning typically
requires large amounts of interaction with users, which in practice renders
such methods unusable for real-world applications. In order to reduce the data
requirements, we propose to leverage data from across different dialog domains,
thereby reducing the amount of data required from each given domain. In
particular, we propose to learn domain-agnostic action embeddings, which
capture general-purpose structure that informs the system how to act given the
current dialog context, and are then specialized to a specific domain. We show
how this approach is capable of learning with significantly less interaction
with users, with a reduction of 35% in the number of dialogs required to learn,
and to a higher level of proficiency than training separate policies for each
domain on a set of simulated domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Effects of Hyperparameters on Knowledge Graph Embedding Quality. (arXiv:2207.00473v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00473">
<div class="article-summary-box-inner">
<span><p>Embedding knowledge graphs into low-dimensional spaces is a popular method
for applying approaches, such as link prediction or node classification, to
these databases. This embedding process is very costly in terms of both
computational time and space. Part of the reason for this is the optimisation
of hyperparameters, which involves repeatedly sampling, by random, guided, or
brute-force selection, from a large hyperparameter space and testing the
resulting embeddings for their quality. However, not all hyperparameters in
this search space will be equally important. In fact, with prior knowledge of
the relative importance of the hyperparameters, some could be eliminated from
the search altogether without significantly impacting the overall quality of
the outputted embeddings. To this end, we ran a Sobol sensitivity analysis to
evaluate the effects of tuning different hyperparameters on the variance of
embedding quality. This was achieved by performing thousands of embedding
trials, each time measuring the quality of embeddings produced by different
hyperparameter configurations. We regressed the embedding quality on those
hyperparameter configurations, using this model to generate Sobol sensitivity
indices for each of the hyperparameters. By evaluating the correlation between
Sobol indices, we find substantial variability in the hyperparameter
sensitivities between knowledge graphs with differing dataset characteristics
as the probable cause of these inconsistencies. As an additional contribution
of this work we identify several relations in the UMLS knowledge graph that may
cause data leakage via inverse relations, and derive and present UMLS-43, a
leakage-robust variant of that graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panning for gold: Lessons learned from the platform-agnostic automated detection of political content in textual data. (arXiv:2207.00489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00489">
<div class="article-summary-box-inner">
<span><p>The growing availability of data about online information behaviour enables
new possibilities for political communication research. However, the volume and
variety of these data makes them difficult to analyse and prompts the need for
developing automated content approaches relying on a broad range of natural
language processing techniques (e.g. machine learning- or neural network-based
ones). In this paper, we discuss how these techniques can be used to detect
political content across different platforms. Using three validation datasets,
which include a variety of political and non-political textual documents from
online platforms, we systematically compare the performance of three groups of
detection techniques relying on dictionaries, supervised machine learning, or
neural networks. We also examine the impact of different modes of data
preprocessing (e.g. stemming and stopword removal) on the low-cost
implementations of these techniques using a large set (n = 66) of detection
models. Our results show the limited impact of preprocessing on model
performance, with the best results for less noisy data being achieved by neural
network- and machine-learning-based models, in contrast to the more robust
performance of dictionary-based models on noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reduce Indonesian Vocabularies with an Indonesian Sub-word Separator. (arXiv:2207.00552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00552">
<div class="article-summary-box-inner">
<span><p>Indonesian is an agglutinative language since it has a compounding process of
word-formation. Therefore, the translation model of this language requires a
mechanism that is even lower than the word level, referred to as the sub-word
level. This compounding process leads to a rare word problem since the number
of vocabulary explodes. We propose a strategy to address the unique word
problem of the neural machine translation (NMT) system, which uses Indonesian
as a pair language. Our approach uses a rule-based method to transform a word
into its roots and accompanied affixes to retain its meaning and context. Using
a rule-based algorithm has more advantages: it does not require corpus data but
only applies the standard Indonesian rules. Our experiments confirm that this
method is practical. It reduces the number of vocabulary significantly up to
57\%, and on the English to Indonesian translation, this strategy provides an
improvement of up to 5 BLEU points over a similar NMT system that does not use
this technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Learning. (arXiv:2207.00555v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00555">
<div class="article-summary-box-inner">
<span><p>Large-scale speech self-supervised learning (SSL) has emerged to the main
field of speech processing, however, the problem of computational cost arising
from its vast size makes a high entry barrier to academia. In addition,
existing distillation techniques of speech SSL models compress the model by
reducing layers, which induces performance degradation in linguistic pattern
recognition tasks such as phoneme recognition (PR). In this paper, we propose
FitHuBERT, which makes thinner in dimension throughout almost all model
components and deeper in layer compared to prior speech SSL distillation works.
Moreover, we employ a time-reduction layer to speed up inference time and
propose a method of hint-based distillation for less performance degradation.
Our method reduces the model to 23.8% in size and 35.9% in inference time
compared to HuBERT. Also, we achieve 12.1% word error rate and 13.3% phoneme
error rate on the SUPERB benchmark which is superior than prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is neural language acquisition similar to natural? A chronological probing study. (arXiv:2207.00560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00560">
<div class="article-summary-box-inner">
<span><p>The probing methodology allows one to obtain a partial representation of
linguistic phenomena stored in the inner layers of the neural network, using
external classifiers and statistical analysis. Pre-trained transformer-based
language models are widely used both for natural language understanding (NLU)
and natural language generation (NLG) tasks making them most commonly used for
downstream applications. However, little analysis was carried out, whether the
models were pre-trained enough or contained knowledge correlated with
linguistic theory. We are presenting the chronological probing study of
transformer English models such as MultiBERT and T5. We sequentially compare
the information about the language learned by the models in the process of
training on corpora. The results show that 1) linguistic information is
acquired in the early stages of training 2) both language models demonstrate
capabilities to capture various features from various levels of language,
including morphology, syntax, and even discourse, while they also can
inconsistently fail on tasks that are perceived as easy. We also introduce the
open-source framework for chronological probing research, compatible with other
transformer-based models.
https://github.com/EkaterinaVoloshina/chronological_probing
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Document Keyphrase Extraction: Dataset, Baselines and Review. (arXiv:2110.01073v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01073">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction has been extensively researched within the
single-document setting, with an abundance of methods, datasets and
applications. In contrast, multi-document keyphrase extraction has been
infrequently studied, despite its utility for describing sets of documents, and
its use in summarization. Moreover, no prior dataset exists for multi-document
keyphrase extraction, hindering the progress of the task. Recent advances in
multi-text processing make the task an even more appealing challenge to pursue.
To stimulate this pursuit, we present here the first dataset for the task,
MK-DUC-01, which can serve as a new benchmark, and test multiple keyphrase
extraction baselines on our data. In addition, we provide a brief, yet
comprehensive, literature review of the task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepA2: A Modular Framework for Deep Argument Analysis with Pretrained Neural Text2Text Language Models. (arXiv:2110.01509v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01509">
<div class="article-summary-box-inner">
<span><p>In this paper, we present and implement a multi-dimensional, modular
framework for performing deep argument analysis (DeepA2) using current
pre-trained language models (PTLMs). ArgumentAnalyst -- a T5 model (Raffel et
al. 2020) set up and trained within DeepA2 -- reconstructs argumentative texts,
which advance an informal argumentation, as valid arguments: It inserts, e.g.,
missing premises and conclusions, formalizes inferences, and coherently links
the logical reconstruction to the source text. We create a synthetic corpus for
deep argument analysis, and evaluate ArgumentAnalyst on this new dataset as
well as on existing data, specifically EntailmentBank (Dalvi et al. 2021). Our
empirical findings vindicate the overall framework and highlight the advantages
of a modular design, in particular its ability to emulate established
heuristics (such as hermeneutic cycles), to explore the model's uncertainty, to
cope with the plurality of correct solutions (underdetermination), and to
exploit higher-order evidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Waits for No One! Analysis and Challenges of Temporal Misalignment. (arXiv:2111.07408v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07408">
<div class="article-summary-box-inner">
<span><p>When an NLP model is trained on text data from one time period and tested or
deployed on data from another, the resulting temporal misalignment can degrade
end-task performance. In this work, we establish a suite of eight diverse tasks
across different domains (social media, science papers, news, and reviews) and
periods of time (spanning five years or more) to quantify the effects of
temporal misalignment. Our study is focused on the ubiquitous setting where a
pretrained model is optionally adapted through continued domain-specific
pretraining, followed by task-specific finetuning. We establish a suite of
tasks across multiple domains to study temporal misalignment in modern NLP
systems. We find stronger effects of temporal misalignment on task performance
than have been previously reported. We also find that, while temporal
adaptation through continued pretraining can help, these gains are small
compared to task-specific finetuning on data from the target time period. Our
findings motivate continued research to improve temporal robustness of NLP
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Degendering Resumes for Algorithmic Resume Screening. (arXiv:2112.08910v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08910">
<div class="article-summary-box-inner">
<span><p>We investigate whether it is feasible to remove gendered information from
resumes to mitigate potential bias in algorithmic resume screening. Using a
corpus of 709k resumes from IT firms, we first train a series of models to
classify the self-reported gender of the applicant, thereby measuring the
extent and nature of gendered information encoded in resumes. We then conduct a
series of gender obfuscation experiments, where we iteratively remove gendered
information from resumes. Finally, we train a resume screening algorithm and
investigate the trade-off between gender obfuscation and screening algorithm
performance. Results show: (1) There is a significant amount of gendered
information in resumes. (2) Lexicon-based gender obfuscation method (i.e.
removing tokens that are predictive of gender) can reduce the amount of
gendered information to a large extent. However, after a certain point, the
performance of the resume screening algorithm starts suffering. (3)
General-purpose gender debiasing methods for NLP models such as removing gender
subspace from embeddings are not effective in obfuscating gender.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization. (arXiv:2202.00443v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00443">
<div class="article-summary-box-inner">
<span><p>We present a novel benchmark and associated evaluation metrics for assessing
the performance of text anonymization methods. Text anonymization, defined as
the task of editing a text document to prevent the disclosure of personal
information, currently suffers from a shortage of privacy-oriented annotated
text resources, making it difficult to properly evaluate the level of privacy
protection offered by various anonymization methods. This paper presents TAB
(Text Anonymization Benchmark), a new, open-source annotated corpus developed
to address this shortage. The corpus comprises 1,268 English-language court
cases from the European Court of Human Rights (ECHR) enriched with
comprehensive annotations about the personal information appearing in each
document, including their semantic category, identifier type, confidential
attributes, and co-reference relations. Compared to previous work, the TAB
corpus is designed to go beyond traditional de-identification (which is limited
to the detection of predefined semantic categories), and explicitly marks which
text spans ought to be masked in order to conceal the identity of the person to
be protected. Along with presenting the corpus and its annotation layers, we
also propose a set of evaluation metrics that are specifically tailored towards
measuring the performance of text anonymization, both in terms of privacy
protection and utility preservation. We illustrate the use of the benchmark and
the proposed metrics by assessing the empirical performance of several baseline
text anonymization models. The full corpus along with its privacy-oriented
annotation guidelines, evaluation scripts and baseline models are available on:
https://github.com/NorskRegnesentral/text-anonymisation-benchmark
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15081">
<div class="article-summary-box-inner">
<span><p>We present a method for visually-grounded spoken term discovery. After
training either a HuBERT or wav2vec2.0 model to associate spoken captions with
natural images, we show that powerful word segmentation and clustering
capability emerges within the model's self-attention heads. Our experiments
reveal that this ability is not present to nearly the same extent in the base
HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a
crucial component of the word discovery capability we observe. We also evaluate
our method on the Buckeye word segmentation and ZeroSpeech spoken term
discovery tasks, where we outperform all currently published methods on several
metrics. Code and model weights are available at
https://github.com/jasonppy/word-discovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings. (arXiv:2203.16834v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16834">
<div class="article-summary-box-inner">
<span><p>In this paper, we conduct a comparative study on speaker-attributed automatic
speech recognition (SA-ASR) in the multi-party meeting scenario, a topic with
increasing attention in meeting rich transcription. Specifically, three
approaches are evaluated in this study. The first approach, FD-SOT, consists of
a frame-level diarization model to identify speakers and a multi-talker ASR to
recognize utterances. The speaker-attributed transcriptions are obtained by
aligning the diarization results and recognized hypotheses. However, such an
alignment strategy may suffer from erroneous timestamps due to the modular
independence, severely hindering the model performance. Therefore, we propose
the second approach, WD-SOT, to address alignment errors by introducing a
word-level diarization model, which can get rid of such timestamp alignment
dependency. To further mitigate the alignment issues, we propose the third
approach, TS-ASR, which trains a target-speaker separation module and an ASR
module jointly. By comparing various strategies for each SA-ASR approach,
experimental results on a real meeting scenario corpus, AliMeeting, reveal that
the WD-SOT approach achieves 10.7% relative reduction on averaged
speaker-dependent character error rate (SD-CER), compared with the FD-SOT
approach. In addition, the TS-ASR approach also outperforms the FD-SOT approach
and brings 16.5% relative average SD-CER reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building an ASR Error Robust Spoken Virtual Patient System in a Highly Class-Imbalanced Scenario Without Speech Data. (arXiv:2204.05183v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05183">
<div class="article-summary-box-inner">
<span><p>A Virtual Patient (VP) is a powerful tool for training medical students to
take patient histories, where responding to a diverse set of spoken questions
is essential to simulate natural conversations with a student. The performance
of such a Spoken Language Understanding system (SLU) can be adversely affected
by both the presence of Automatic Speech Recognition (ASR) errors in the test
data and a high degree of class imbalance in the SLU training data. While these
two issues have been addressed separately in prior work, we develop a novel
two-step training methodology that tackles both these issues effectively in a
single dialog agent. As it is difficult to collect spoken data from users
without a functioning SLU system, our method does not rely on spoken data for
training, rather we use an ASR error predictor to "speechify" the text data.
Our method shows significant improvements over strong baselines on the VP
intent classification task at various word error rate settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in End-to-End Speech-to-Intent Systems. (arXiv:2204.05188v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05188">
<div class="article-summary-box-inner">
<span><p>Recent advances in End-to-End (E2E) Spoken Language Understanding (SLU) have
been primarily due to effective pretraining of speech representations. One such
pretraining paradigm is the distillation of semantic knowledge from
state-of-the-art text-based models like BERT to speech encoder neural networks.
This work is a step towards doing the same in a much more efficient and
fine-grained manner where we align speech embeddings and BERT embeddings on a
token-by-token basis. We introduce a simple yet novel technique that uses a
cross-modal attention mechanism to extract token-level contextual embeddings
from a speech encoder such that these can be directly compared and aligned with
BERT based contextual embeddings. This alignment is performed using a novel
tokenwise contrastive loss. Fine-tuning such a pretrained model to perform
intent recognition using speech directly yields state-of-the-art performance on
two widely used SLU datasets. Our model improves further when fine-tuned with
additional regularization using SpecAugment especially when speech is noisy,
giving an absolute improvement as high as 8% over previous results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Document-Level Relation Extraction. (arXiv:2205.02048v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02048">
<div class="article-summary-box-inner">
<span><p>We present FREDo, a few-shot document-level relation extraction (FSDLRE)
benchmark. As opposed to existing benchmarks which are built on sentence-level
relation extraction corpora, we argue that document-level corpora provide more
realism, particularly regarding none-of-the-above (NOTA) distributions.
Therefore, we propose a set of FSDLRE tasks and construct a benchmark based on
two existing supervised learning data sets, DocRED and sciERC. We adapt the
state-of-the-art sentence-level method MNAV to the document-level and develop
it further for improved domain adaptation. We find FSDLRE to be a challenging
setting with interesting new characteristics such as the ability to sample NOTA
instances from the support set. The data, code, and trained models are
available online (https://github.com/nicpopovic/FREDo).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Case for a Single Model that can Both Generate Continuations and Fill in the Blank. (arXiv:2206.04812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04812">
<div class="article-summary-box-inner">
<span><p>The task of inserting text into a specified position in a passage, known as
fill in the blank (FitB), is useful for a variety of applications where writers
interact with a natural language generation (NLG) system to craft text. While
previous work has tackled this problem with models trained specifically to do
the fill-in-the-blank task, a more useful model is one that can effectively
perform _both_ FitB and continuation. In this work, we evaluate the feasibility
of using a single model to do both tasks. We show that models pre-trained with
a FitB-style objective are capable of both tasks, while models pre-trained for
continuation are not. Finally, we show how FitB models can be easily finetuned
to allow for fine-grained control over the length and word choice of the
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Heuristics for AI-Generated Language Are Flawed. (arXiv:2206.07271v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07271">
<div class="article-summary-box-inner">
<span><p>Human communication is increasingly intermixed with language generated by AI.
Across chat, email, and social media, AI systems produce smart replies,
autocompletes, and translations. AI-generated language is often not identified
as such but poses as human language, raising concerns about novel forms of
deception and manipulation. Here, we study how humans discern whether one of
the most personal and consequential forms of language - a self-presentation -
was generated by AI. In six experiments, participants (N = 4,600) tried to
detect self-presentations generated by state-of-the-art language models. Across
professional, hospitality, and dating settings, we find that humans are unable
to detect AI-generated self-presentations. Our findings show that human
judgments of AI-generated language are handicapped by intuitive but flawed
heuristics such as associating first-person pronouns, spontaneous wording, or
family topics with humanity. We demonstrate that these heuristics make human
judgment of generated language predictable and manipulable, allowing AI systems
to produce language perceived as more human than human. We discuss solutions,
such as AI accents, to reduce the deceptive potential of generated language,
limiting the subversion of human intuition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Knowledge Selection for Grounded Dialogues via Document Semantic Graphs. (arXiv:2206.07296v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07296">
<div class="article-summary-box-inner">
<span><p>Providing conversation models with background knowledge has been shown to
make open-domain dialogues more informative and engaging. Existing models treat
knowledge selection as a sentence ranking or classification problem where each
sentence is handled individually, ignoring the internal semantic connection
among sentences in the background document. In this work, we propose to
automatically convert the background knowledge documents into document semantic
graphs and then perform knowledge selection over such graphs. Our document
semantic graphs preserve sentence-level information through the use of sentence
nodes and provide concept connections between sentences. We jointly apply
multi-task learning for sentence-level and concept-level knowledge selection
and show that it improves sentence-level selection. Our experiments show that
our semantic graph-based knowledge selection improves over sentence selection
baselines for both the knowledge selection task and the end-to-end response
generation task on HollE and improves generalization on unseen topics in WoW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Impact of Noises in Crowd-Sourced Data for Speech Translation. (arXiv:2206.13756v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13756">
<div class="article-summary-box-inner">
<span><p>Training speech translation (ST) models requires large and high-quality
datasets. MuST-C is one of the most widely used ST benchmark datasets. It
contains around 400 hours of speech-transcript-translation data for each of the
eight translation directions. This dataset passes several quality-control
filters during creation. However, we find that MuST-C still suffers from three
major quality issues: audio-text misalignment, inaccurate translation, and
unnecessary speaker's name. What are the impacts of these data quality issues
for model development and evaluation? In this paper, we propose an automatic
method to fix or filter the above quality issues, using English-German (En-De)
translation as an example. Our experiments show that ST models perform better
on clean test sets, and the rank of proposed models remains consistent across
different test sets. Besides, simply removing misaligned data points from the
training set does not lead to a better ST model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is it possible not to cheat on the Turing Test: Exploring the potential and challenges for true natural language 'understanding' by computers. (arXiv:2206.14672v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14672">
<div class="article-summary-box-inner">
<span><p>Recent hype surrounding the increasing sophistication of language processing
models has renewed optimism regarding machines achieving a human-like command
of natural language. The area of natural language understanding in artificial
intelligence claims to have been making great strides in this area, however,
the lack of conceptual clarity in how 'understanding' is used in this and other
disciplines have made it difficult to discern how close we actually are. A
comprehensive, interdisciplinary overview of current approaches and remaining
challenges is yet to be carried out. Beyond linguistic knowledge, this requires
considering our species-specific capabilities to categorize, memorize, label
and communicate our (sufficiently similar) embodied and situated experiences.
Moreover, gauging the practical constraints requires critically analyzing the
technical capabilities of current models, as well as deeper philosophical
reflection on theoretical possibilities and limitations. In this paper, I unite
all of these perspectives -- the philosophical, cognitive-linguistic, and
technical -- to unpack the challenges involved in reaching true (human-like)
language understanding. By unpacking the theoretical assumptions inherent in
current approaches, I hope to illustrate how far we actually are from achieving
this goal, if indeed it is the goal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Quantitative Reasoning Problems with Language Models. (arXiv:2206.14858v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14858">
<div class="article-summary-box-inner">
<span><p>Language models have achieved remarkable performance on a wide range of tasks
that require natural language understanding. Nevertheless, state-of-the-art
models have generally struggled with tasks that require quantitative reasoning,
such as solving mathematics, science, and engineering problems at the college
level. To help close this gap, we introduce Minerva, a large language model
pretrained on general natural language data and further trained on technical
content. The model achieves state-of-the-art performance on technical
benchmarks without the use of external tools. We also evaluate our model on
over two hundred undergraduate-level problems in physics, biology, chemistry,
economics, and other sciences that require quantitative reasoning, and find
that the model can correctly answer nearly a third of them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">esCorpius: A Massive Spanish Crawling Corpus. (arXiv:2206.15147v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15147">
<div class="article-summary-box-inner">
<span><p>In the recent years, transformer-based models have lead to significant
advances in language modelling for natural language processing. However, they
require a vast amount of data to be (pre-)trained and there is a lack of
corpora in languages other than English. Recently, several initiatives have
presented multilingual datasets obtained from automatic web crawling. However,
the results in Spanish present important shortcomings, as they are either too
small in comparison with other languages, or present a low quality derived from
sub-optimal cleaning and deduplication. In this paper, we introduce esCorpius,
a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is
the most extensive corpus in Spanish with this level of quality in the
extraction, purification and deduplication of web textual content. Our data
curation process involves a novel highly parallel cleaning pipeline and
encompasses a series of deduplication mechanisms that together ensure the
integrity of both document and paragraph boundaries. Additionally, we maintain
both the source web page URL and the WARC shard origin URL in order to complain
with EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license
and is available on HuggingFace.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiEarth 2022 -- The Champion Solution for Image-to-Image Translation Challenge via Generation Models. (arXiv:2207.00001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00001">
<div class="article-summary-box-inner">
<span><p>The MultiEarth 2022 Image-to-Image Translation challenge provides a
well-constrained test bed for generating the corresponding RGB Sentinel-2
imagery with the given Sentinel-1 VV &amp; VH imagery. In this challenge, we
designed various generation models and found the SPADE [1] and pix2pixHD [2]
models could perform our best results. In our self-evaluation, the SPADE-2
model with L1-loss can achieve 0.02194 MAE score and 31.092 PSNR dB. In our
final submission, the best model can achieve 0.02795 MAE score ranked No.1 on
the leader board.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-stage Framework with Mean Subspace Computation and Recursive Feedback for Online Unsupervised Domain Adaptation. (arXiv:2207.00003v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00003">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the Online Unsupervised Domain Adaptation (OUDA)
problem and propose a novel multi-stage framework to solve real-world
situations when the target data are unlabeled and arriving online sequentially
in batches. To project the data from the source and the target domains to a
common subspace and manipulate the projected data in real-time, our proposed
framework institutes a novel method, called an Incremental Computation of
Mean-Subspace (ICMS) technique, which computes an approximation of mean-target
subspace on a Grassmann manifold and is proven to be a close approximate to the
Karcher mean. Furthermore, the transformation matrix computed from the
mean-target subspace is applied to the next target data in the
recursive-feedback stage, aligning the target data closer to the source domain.
The computation of transformation matrix and the prediction of next-target
subspace leverage the performance of the recursive-feedback stage by
considering the cumulative temporal dependency among the flow of the target
subspace on the Grassmann manifold. The labels of the transformed target data
are predicted by the pre-trained source classifier, then the classifier is
updated by the transformed data and predicted labels. Extensive experiments on
six datasets were conducted to investigate in depth the effect and contribution
of each stage in our proposed framework and its performance over previous
approaches in terms of classification accuracy and computational speed. In
addition, the experiments on traditional manifold-based learning models and
neural-network-based learning models demonstrated the applicability of our
proposed framework for various types of learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correction Algorithm of Sampling Effect and Its Application. (arXiv:2207.00004v1 [astro-ph.IM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00004">
<div class="article-summary-box-inner">
<span><p>The sampling effect of the imaging acquisition device is long considered to
be a modulation process of the input signal, introducing additional error into
the signal acquisition process. This paper proposes a correction algorithm for
the modulation process that solves the sampling effect with high accuracy. We
examine the algorithm with perfect continuous Gaussian images and selected
digitized images, which indicate an accuracy increase of 106 for Gaussian
images, 102 at 15 times of Shannon interpolation for digitized images, and 105
at 101 times of Shannon interpolation for digitized images. The accuracy limit
of the Gaussian image comes from the truncation error, while the accuracy limit
of the digitized images comes from their finite resolution, which can be
improved by increasing the time of Shannon interpolation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class Impression for Data-free Incremental Learning. (arXiv:2207.00005v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00005">
<div class="article-summary-box-inner">
<span><p>Standard deep learning-based classification approaches require collecting all
samples from all classes in advance and are trained offline. This paradigm may
not be practical in real-world clinical applications, where new classes are
incrementally introduced through the addition of new data. Class incremental
learning is a strategy allowing learning from such data. However, a major
challenge is catastrophic forgetting, i.e., performance degradation on previous
classes when adapting a trained model to new data. Prior methodologies to
alleviate this challenge save a portion of training data require perpetual
storage of such data that may introduce privacy issues. Here, we propose a
novel data-free class incremental learning framework that first synthesizes
data from the model trained on previous classes to generate a \ours.
Subsequently, it updates the model by combining the synthesized data with new
class data. Furthermore, we incorporate a cosine normalized Cross-entropy loss
to mitigate the adverse effects of the imbalance, a margin loss to increase
separation among previous classes and new ones, and an intra-domain contrastive
loss to generalize the model trained on the synthesized data to real data. We
compare our proposed framework with state-of-the-art methods in class
incremental learning, where we demonstrate improvement in accuracy for the
classification of 11,062 echocardiography cine series of patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaserMix for Semi-Supervised LiDAR Semantic Segmentation. (arXiv:2207.00026v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00026">
<div class="article-summary-box-inner">
<span><p>Densely annotating LiDAR point clouds is costly, which restrains the
scalability of fully-supervised learning methods. In this work, we study the
underexplored semi-supervised learning (SSL) in LiDAR segmentation. Our core
idea is to leverage the strong spatial cues of LiDAR point clouds to better
exploit unlabeled data. We propose LaserMix to mix laser beams from different
LiDAR scans, and then encourage the model to make consistent and confident
predictions before and after mixing. Our framework has three appealing
properties: 1) Generic: LaserMix is agnostic to LiDAR representations (e.g.,
range view and voxel), and hence our SSL framework can be universally applied.
2) Statistically grounded: We provide a detailed analysis to theoretically
explain the applicability of the proposed framework. 3) Effective:
Comprehensive experimental analysis on popular LiDAR segmentation datasets
(nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and
superiority. Notably, we achieve competitive results over fully-supervised
counterparts with 2x to 5x fewer labels and improve the supervised-only
baseline significantly by 10.8% on average. We hope this concise yet
high-performing framework could facilitate future research in semi-supervised
LiDAR segmentation. Code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Image Synthesis via Diffusion Models. (arXiv:2207.00050v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00050">
<div class="article-summary-box-inner">
<span><p>Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable
success in various image generation tasks compared with Generative Adversarial
Nets (GANs). Recent work on semantic image synthesis mainly follows the
\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality
or diversity of generated images. In this paper, we propose a novel framework
based on DDPM for semantic image synthesis. Unlike previous conditional
diffusion model directly feeds the semantic layout and noisy image as input to
a U-Net structure, which may not fully leverage the information in the input
semantic mask, our framework processes semantic layout and noisy image
differently. It feeds noisy image to the encoder of the U-Net structure while
the semantic layout to the decoder by multi-layer spatially-adaptive
normalization operators. To further improve the generation quality and semantic
interpretability in semantic image synthesis, we introduce the classifier-free
guidance sampling strategy, which acknowledge the scores of an unconditional
model for sampling process. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our proposed method, achieving
state-of-the-art performance in terms of fidelity~(FID) and diversity~(LPIPS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00052">
<div class="article-summary-box-inner">
<span><p>A powerful paradigm for sensorimotor control is to predict actions from
observations directly. Training such an end-to-end system allows
representations that are useful for the downstream tasks to emerge
automatically. In visual navigation, an agent can learn to navigate without any
manual designs by correlating how its views change with the actions being
taken. However, the lack of inductive bias makes this system data-inefficient
and impractical in scenarios like search and rescue, where interacting with the
environment to collect data is costly. We hypothesize a sufficient
representation of the current view and the goal view for a navigation policy
can be learned by predicting the location and size of a crop of the current
view that corresponds to the goal. We further show that training such random
crop prediction in a self-supervised fashion purely on random noise images
transfers well to natural home images. The learned representation can then be
bootstrapped to learn a navigation policy efficiently with little interaction
data. Code is available at https://github.com/yanweiw/noise2ptz.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiViz: An Analysis Benchmark for Visualizing and Understanding Multimodal Models. (arXiv:2207.00056v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00056">
<div class="article-summary-box-inner">
<span><p>The promise of multimodal models for real-world applications has inspired
research in visualizing and understanding their internal mechanics with the end
goal of empowering stakeholders to visualize model behavior, perform model
debugging, and promote trust in machine learning models. However, modern
multimodal models are typically black-box neural networks, which makes it
challenging to understand their internal mechanics. How can we visualize the
internal modeling of multimodal interactions in these models? Our paper aims to
fill this gap by proposing MultiViz, a method for analyzing the behavior of
multimodal models by scaffolding the problem of interpretability into 4 stages:
(1) unimodal importance: how each modality contributes towards downstream
modeling and prediction, (2) cross-modal interactions: how different modalities
relate with each other, (3) multimodal representations: how unimodal and
cross-modal interactions are represented in decision-level features, and (4)
multimodal prediction: how decision-level features are composed to make a
prediction. MultiViz is designed to operate on diverse modalities, models,
tasks, and research areas. Through experiments on 8 trained models across 6
real-world tasks, we show that the complementary stages in MultiViz together
enable users to (1) simulate model predictions, (2) assign interpretable
concepts to features, (3) perform error analysis on model misclassifications,
and (4) use insights from error analysis to debug models. MultiViz is publicly
available, will be regularly updated with new interpretation tools and metrics,
and welcomes inputs from the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2207.00067v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00067">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) adapts a model trained on one domain to
a novel domain using only unlabeled data. So many studies have been conducted,
especially for semantic segmentation due to its high annotation cost. The
existing studies stick to the basic assumption that no labeled sample is
available for the new domain. However, this assumption has several issues.
First, it is pretty unrealistic, considering the standard practice of ML to
confirm the model's performance before its deployment; the confirmation needs
labeled data. Second, any UDA method will have a few hyper-parameters, needing
a certain amount of labeled data. To rectify this misalignment with reality, we
rethink UDA from a data-centric point of view. Specifically, we start with the
assumption that we do have access to a minimum level of labeled data. Then, we
ask how many labeled samples are necessary for finding satisfactory
hyper-parameters of existing UDA methods. How well does it work if we use the
same data to train the model, e.g., finetuning? We conduct experiments to
answer these questions with popular scenarios, {GTA5,
SYNTHIA}$\rightarrow$Cityscapes. Our findings are as follows: i) for some UDA
methods, good hyper-parameters can be found with only a few labeled samples
(i.e., images), e.g., five, but this does not apply to others, and ii)
finetuning outperforms most existing UDA methods with only ten labeled images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Periodic Systolic Dataflow for Lowering Latency and Power Dissipation of Convolutional Neural Network Accelerators. (arXiv:2207.00068v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00068">
<div class="article-summary-box-inner">
<span><p>This paper introduces the sparse periodic systolic (SPS) dataflow, which
advances the state-of-the-art hardware accelerator for supporting lightweight
neural networks. Specifically, the SPS dataflow enables a novel hardware design
approach unlocked by an emergent pruning scheme, periodic pattern-based
sparsity (PPS). By exploiting the regularity of PPS, our sparsity-aware
compiler optimally reorders the weights and uses a simple indexing unit in
hardware to create matches between the weights and activations. Through the
compiler-hardware codesign, SPS dataflow enjoys higher degrees of parallelism
while being free of the high indexing overhead and without model accuracy loss.
Evaluated on popular benchmarks such as VGG and ResNet, the SPS dataflow and
accompanying neural network compiler outperform prior work in convolutional
neural network (CNN) accelerator designs targeting FPGA devices. Against other
sparsity-supporting weight storage formats, SPS results in 4.49x energy
efficiency gain while lowering storage requirements by 3.67x for total weight
storage (non-pruned weights plus indexing) and 22,044x for indexing memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stain-free, rapid, and quantitative viral plaque assay using deep learning and holography. (arXiv:2207.00089v1 [physics.ins-det])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00089">
<div class="article-summary-box-inner">
<span><p>Plaque assay is the gold standard method for quantifying the concentration of
replication-competent lytic virions. Expediting and automating viral plaque
assays will significantly benefit clinical diagnosis, vaccine development, and
the production of recombinant proteins or antiviral agents. Here, we present a
rapid and stain-free quantitative viral plaque assay using lensfree holographic
imaging and deep learning. This cost-effective, compact, and automated device
significantly reduces the incubation time needed for traditional plaque assays
while preserving their advantages over other virus quantification methods. This
device captures ~0.32 Giga-pixel/hour phase information of the objects per test
well, covering an area of ~30x30 mm^2, in a label-free manner, eliminating
staining entirely. We demonstrated the success of this computational method
using Vero E6 cells and vesicular stomatitis virus. Using a neural network,
this stain-free device automatically detected the first cell lysing events due
to the viral replication as early as 5 hours after the incubation, and achieved
&gt;90% detection rate for the plaque-forming units (PFUs) with 100% specificity
in &lt;20 hours, providing major time savings compared to the traditional plaque
assays that take ~48 hours or more. This data-driven plaque assay also offers
the capability of quantifying the infected area of the cell monolayer,
performing automated counting and quantification of PFUs and virus-infected
areas over a 10-fold larger dynamic range of virus concentration than standard
viral plaque assays. This compact, low-cost, automated PFU quantification
device can be broadly used in virology research, vaccine development, and
clinical applications
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology. (arXiv:2207.00095v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00095">
<div class="article-summary-box-inner">
<span><p>Current approaches for classification of whole slide images (WSI) in digital
pathology predominantly utilize a two-stage learning pipeline. The first stage
identifies areas of interest (e.g. tumor tissue), while the second stage
processes cropped tiles from these areas in a supervised fashion. During
inference, a large number of tiles are combined into a unified prediction for
the entire slide. A major drawback of such approaches is the requirement for
task-specific auxiliary labels which are not acquired in clinical routine. We
propose a novel learning pipeline for WSI classification that is trainable
end-to-end and does not require any auxiliary annotations. We apply our
approach to predict molecular alterations for a number of different use-cases,
including detection of microsatellite instability in colorectal tumors and
prediction of specific mutations for colon, lung, and breast cancer cases from
The Cancer Genome Atlas. Results reach AUC scores of up to 94% and are shown to
be competitive with state of the art two-stage pipelines. We believe our
approach can facilitate future research in digital pathology and contribute to
solve a large range of problems around the prediction of cancer phenotypes,
hopefully enabling personalized therapies for more patients in future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation. (arXiv:2207.00106v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00106">
<div class="article-summary-box-inner">
<span><p>Parkinson's disease (PD) is a neurological disorder that has a variety of
observable motor-related symptoms such as slow movement, tremor, muscular
rigidity, and impaired posture. PD is typically diagnosed by evaluating the
severity of motor impairments according to scoring systems such as the Movement
Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS).
Automated severity prediction using video recordings of individuals provides a
promising route for non-intrusive monitoring of motor impairments. However, the
limited size of PD gait data hinders model ability and clinical potential.
Because of this clinical data scarcity and inspired by the recent advances in
self-supervised large-scale language models like GPT-3, we use human motion
forecasting as an effective self-supervised pre-training task for the
estimation of motor impairment severity. We introduce GaitForeMer, Gait
Forecasting and impairment estimation transforMer, which is first pre-trained
on public datasets to forecast gait movements and then applied to clinical data
to predict MDS-UPDRS gait impairment severity. Our method outperforms previous
approaches that rely solely on clinical data by a large margin, achieving an F1
score of 0.76, precision of 0.79, and recall of 0.75. Using GaitForeMer, we
show how public human movement data repositories can assist clinical use cases
through learning universal motion representations. The code is available at
https://github.com/markendo/GaitForeMer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches. (arXiv:2207.00113v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00113">
<div class="article-summary-box-inner">
<span><p>Surgical captioning plays an important role in surgical instruction
prediction and report generation. However, the majority of captioning models
still rely on the heavy computational object detector or feature extractor to
extract regional features. In addition, the detection model requires additional
bounding box annotation which is costly and needs skilled annotators. These
lead to inference delay and limit the captioning model to deploy in real-time
robotic surgery. For this purpose, we design an end-to-end detector and feature
extractor-free captioning model by utilizing the patch-based shifted window
technique. We propose Shifted Window-Based Multi-Layer Perceptrons Transformer
Captioning model (SwinMLP-TranCAP) with faster inference speed and less
computation. SwinMLP-TranCAP replaces the multi-head attention module with
window-based multi-head MLP. Such deployments primarily focus on image
understanding tasks, but very few works investigate the caption generation
task. SwinMLP-TranCAP is also extended into a video version for video
captioning tasks using 3D patches and windows. Compared with previous
detector-based or feature extractor-based models, our models greatly simplify
the architecture design while maintaining performance on two surgical datasets.
The code is publicly available at
https://github.com/XuMengyaAmy/SwinMLP_TranCAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProSelfLC: Progressive Self Label Correction Towards A Low-Temperature Entropy State. (arXiv:2207.00118v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00118">
<div class="article-summary-box-inner">
<span><p>To train robust deep neural networks (DNNs), we systematically study several
target modification approaches, which include output regularisation, self and
non-self label correction (LC). Three key issues are discovered: (1) Self LC is
the most appealing as it exploits its own knowledge and requires no extra
models. However, how to automatically decide the trust degree of a learner as
training goes is not well answered in the literature. (2) Some methods penalise
while the others reward low-entropy predictions, prompting us to ask which one
is better. (3) Using the standard training setting, a trained network is of low
confidence when severe noise exists, making it hard to leverage its
high-entropy self knowledge.
</p>
<p>To resolve the issue (1), taking two well-accepted propositions--deep neural
networks learn meaningful patterns before fitting noise and minimum entropy
regularisation principle--we propose a novel end-to-end method named ProSelfLC,
which is designed according to learning time and entropy. Specifically, given a
data point, we progressively increase trust in its predicted label distribution
versus its annotated one if a model has been trained for enough time and the
prediction is of low entropy (high confidence). For the issue (2), according to
ProSelfLC, we empirically prove that it is better to redefine a meaningful
low-entropy status and optimise the learner toward it. This serves as a defence
of entropy minimisation. To address the issue (3), we decrease the entropy of
self knowledge using a low temperature before exploiting it to correct labels,
so that the revised labels redefine a low-entropy target state.
</p>
<p>We demonstrate the effectiveness of ProSelfLC through extensive experiments
in both clean and noisy settings, and on both image and protein datasets.
Furthermore, our source code is available at
https://github.com/XinshaoAmosWang/ProSelfLC-AT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Dataset and A Baseline Model for Breast Lesion Detection in Ultrasound Videos. (arXiv:2207.00141v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00141">
<div class="article-summary-box-inner">
<span><p>Breast lesion detection in ultrasound is critical for breast cancer
diagnosis. Existing methods mainly rely on individual 2D ultrasound images or
combine unlabeled video and labeled 2D images to train models for breast lesion
detection. In this paper, we first collect and annotate an ultrasound video
dataset (188 videos) for breast lesion detection. Moreover, we propose a
clip-level and video-level feature aggregated network (CVA-Net) for addressing
breast lesion detection in ultrasound videos by aggregating video-level lesion
classification features and clip-level temporal features. The clip-level
temporal features encode local temporal information of ordered video frames and
global temporal information of shuffled video frames. In our CVA-Net, an
inter-video fusion module is devised to fuse local features from original video
frames and global features from shuffled video frames, and an intra-video
fusion module is devised to learn the temporal information among adjacent video
frames. Moreover, we learn video-level features to classify the breast lesions
of the original video as benign or malignant lesions to further enhance the
final breast lesion detection performance in ultrasound videos. Experimental
results on our annotated dataset demonstrate that our CVA-Net clearly
outperforms state-of-the-art methods. The corresponding code and dataset are
publicly available at \url{https://github.com/jhl-Det/CVA-Net}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChrSNet: Chromosome Straightening using Self-attention Guided Networks. (arXiv:2207.00147v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00147">
<div class="article-summary-box-inner">
<span><p>Karyotyping is an important procedure to assess the possible existence of
chromosomal abnormalities. However, because of the non-rigid nature,
chromosomes are usually heavily curved in microscopic images and such deformed
shapes hinder the chromosome analysis for cytogeneticists. In this paper, we
present a self-attention guided framework to erase the curvature of
chromosomes. The proposed framework extracts spatial information and local
textures to preserve banding patterns in a regression module. With
complementary information from the bent chromosome, a refinement module is
designed to further improve fine details. In addition, we propose two dedicated
geometric constraints to maintain the length and restore the distortion of
chromosomes. To train our framework, we create a synthetic dataset where curved
chromosomes are generated from the real-world straight chromosomes by
grid-deformation. Quantitative and qualitative experiments are conducted on
synthetic and real-world data. Experimental results show that our proposed
method can effectively straighten bent chromosomes while keeping banding
details and length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Usable Region Estimate for Assessing Practical Usability of Medical Image Segmentation Models. (arXiv:2207.00156v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00156">
<div class="article-summary-box-inner">
<span><p>We aim to quantitatively measure the practical usability of medical image
segmentation models: to what extent, how often, and on which samples a model's
predictions can be used/trusted. We first propose a measure,
Correctness-Confidence Rank Correlation (CCRC), to capture how predictions'
confidence estimates correlate with their correctness scores in rank. A model
with a high value of CCRC means its prediction confidences reliably suggest
which samples' predictions are more likely to be correct. Since CCRC does not
capture the actual prediction correctness, it alone is insufficient to indicate
whether a prediction model is both accurate and reliable to use in practice.
Therefore, we further propose another method, Usable Region Estimate (URE),
which simultaneously quantifies predictions' correctness and reliability of
confidence assessments in one estimate. URE provides concrete information on to
what extent a model's predictions are usable. In addition, the sizes of usable
regions (UR) can be utilized to compare models: A model with a larger UR can be
taken as a more usable and hence better model. Experiments on six datasets
validate that the proposed evaluation methods perform well, providing a
concrete and concise measure for the practical usability of medical image
segmentation models. Code is made available at
https://github.com/yizhezhang2000/ure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Disease Classification Performance and Explainability of Deep Learning Models in Radiology with Heatmap Generators. (arXiv:2207.00157v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00157">
<div class="article-summary-box-inner">
<span><p>As deep learning is widely used in the radiology field, the explainability of
such models is increasingly becoming essential to gain clinicians' trust when
using the models for diagnosis. In this research, three experiment sets were
conducted with a U-Net architecture to improve the classification performance
while enhancing the heatmaps corresponding to the model's focus through
incorporating heatmap generators during training. All of the experiments used
the dataset that contained chest radiographs, associated labels from one of the
three conditions ("normal", "congestive heart failure (CHF)", and "pneumonia"),
and numerical information regarding a radiologist's eye-gaze coordinates on the
images. The paper (A. Karargyris and Moradi, 2021) that introduced this dataset
developed a U-Net model, which was treated as the baseline model for this
research, to show how the eye-gaze data can be used in multi-modal training for
explainability improvement. To compare the classification performances, the 95%
confidence intervals (CI) of the area under the receiver operating
characteristic curve (AUC) were measured. The best method achieved an AUC of
0.913 (CI: 0.860-0.966). The greatest improvements were for the "pneumonia" and
"CHF" classes, which the baseline model struggled most to classify, resulting
in AUCs of 0.859 (CI: 0.732-0.957) and 0.962 (CI: 0.933-0.989), respectively.
The proposed method's decoder was also able to produce probability masks that
highlight the determining image parts in model classifications, similarly as
the radiologist's eye-gaze data. Hence, this work showed that incorporating
heatmap generators and eye-gaze information into training can simultaneously
improve disease classification and provide explainable visuals that align well
with how the radiologist viewed the chest radiographs when making diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Presentation Attack using DCGAN and Deep CNN. (arXiv:2207.00161v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00161">
<div class="article-summary-box-inner">
<span><p>Biometric based authentication is currently playing an essential role over
conventional authentication system; however, the risk of presentation attacks
subsequently rising. Our research aims at identifying the areas where
presentation attack can be prevented even though adequate biometric image
samples of users are limited. Our work focusses on generating photorealistic
synthetic images from the real image sets by implementing Deep Convolution
Generative Adversarial Net (DCGAN). We have implemented the temporal and
spatial augmentation during the fake image generation. Our work detects the
presentation attacks on facial and iris images using our deep CNN, inspired by
VGGNet [1]. We applied the deep neural net techniques on three different
biometric image datasets, namely MICHE I [2], VISOB [3], and UBIPr [4]. The
datasets, used in this research, contain images that are captured both in
controlled and uncontrolled environment along with different resolutions and
sizes. We obtained the best test accuracy of 97% on UBI-Pr [4] Iris datasets.
For MICHE-I [2] and VISOB [3] datasets, we achieved the test accuracies of 95%
and 96% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Optical Coding Design in Computational Imaging. (arXiv:2207.00164v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00164">
<div class="article-summary-box-inner">
<span><p>Computational optical imaging (COI) systems leverage optical coding elements
(CE) in their setups to encode a high-dimensional scene in a single or multiple
snapshots and decode it by using computational algorithms. The performance of
COI systems highly depends on the design of its main components: the CE pattern
and the computational method used to perform a given task. Conventional
approaches rely on random patterns or analytical designs to set the
distribution of the CE. However, the available data and algorithm capabilities
of deep neural networks (DNNs) have opened a new horizon in CE data-driven
designs that jointly consider the optical encoder and computational decoder.
Specifically, by modeling the COI measurements through a fully differentiable
image formation model that considers the physics-based propagation of light and
its interaction with the CEs, the parameters that define the CE and the
computational decoder can be optimized in an end-to-end (E2E) manner. Moreover,
by optimizing just CEs in the same framework, inference tasks can be performed
from pure optics. This work surveys the recent advances on CE data-driven
design and provides guidelines on how to parametrize different optical elements
to include them in the E2E framework. Since the E2E framework can handle
different inference applications by changing the loss function and the DNN, we
present low-level tasks such as spectral imaging reconstruction or high-level
tasks such as pose estimation with privacy preserving enhanced by using optimal
task-based optical architectures. Finally, we illustrate classification and 3D
object recognition applications performed at the speed of the light using
all-optics DNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An adaptive bi-objective optimization algorithm for the satellite image data downlink scheduling problem considering request split. (arXiv:2207.00168v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00168">
<div class="article-summary-box-inner">
<span><p>The satellite image data downlink scheduling problem (SIDSP) is well studied
in literature for traditional satellites. With recent developments in satellite
technology, SIDSP for modern satellites became more complicated, adding new
dimensions of complexities and additional opportunities for the effective use
of the satellite. In this paper, we introduce the dynamic two-phase satellite
image data downlink scheduling problem (D-SIDSP) which combines two interlinked
operations of image data segmentation and image data downlink, in a dynamic
way, and thereby offering additional modelling flexibility and renewed
capabilities. D-SIDSP is formulated as a bi-objective problem of optimizing the
image data transmission rate and the service-balance degree. Harnessing the
power of an adaptive large neighborhood search algorithm (ALNS) with a
nondominated sorting genetic algorithm II (NSGA-II), an adaptive bi-objective
memetic algorithm, ALNS+NSGA-II, is developed to solve D-SIDSP. Results of
extensive computational experiments carried out using benchmark instances are
also presented. Our experimental results disclose that the algorithm
ALNS+NSGA-II is a viable alternative to solve D-SIDSP more efficiently and
demonstrates superior outcomes based on various performance metrics. The paper
also offers new benchmark instances for D-SIDSP that can be used in future
research works on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction. (arXiv:2207.00170v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00170">
<div class="article-summary-box-inner">
<span><p>This technical report presents an effective method for motion prediction in
autonomous driving. We develop a Transformer-based method for input encoding
and trajectory prediction. Besides, we propose the Temporal Flow Header to
enhance the trajectory encoding. In the end, an efficient K-means ensemble
method is used. Using our Transformer network and ensemble method, we win the
first place of Argoverse 2 Motion Forecasting Challenge with the
state-of-the-art brier-minFDE score of 1.90.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turbo: Opportunistic Enhancement for Edge Video Analytics. (arXiv:2207.00172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00172">
<div class="article-summary-box-inner">
<span><p>Edge computing is being widely used for video analytics. To alleviate the
inherent tension between accuracy and cost, various video analytics pipelines
have been proposed to optimize the usage of GPU on edge nodes. Nonetheless, we
find that GPU compute resources provisioned for edge nodes are commonly
under-utilized due to video content variations, subsampling and filtering at
different places of a pipeline. As opposed to model and pipeline optimization,
in this work, we study the problem of opportunistic data enhancement using the
non-deterministic and fragmented idle GPU resources. In specific, we propose a
task-specific discrimination and enhancement module and a model-aware
adversarial training mechanism, providing a way to identify and transform
low-quality images that are specific to a video pipeline in an accurate and
efficient manner. A multi-exit model structure and a resource-aware scheduler
is further developed to make online enhancement decisions and fine-grained
inference execution under latency and GPU resource constraints. Experiments
across multiple video analytics pipelines and datasets reveal that by
judiciously allocating a small amount of idle resources on frames that tend to
yield greater marginal benefits from enhancement, our system boosts DNN object
detection accuracy by $7.3-11.3\%$ without incurring any latency costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end cell recognition by point annotation. (arXiv:2207.00176v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00176">
<div class="article-summary-box-inner">
<span><p>Reliable quantitative analysis of immunohistochemical staining images
requires accurate and robust cell detection and classification. Recent
weakly-supervised methods usually estimate probability density maps for cell
recognition. However, in dense cell scenarios, their performance can be limited
by pre- and post-processing as it is impossible to find a universal parameter
setting. In this paper, we introduce an end-to-end framework that applies
direct regression and classification for preset anchor points. Specifically, we
propose a pyramidal feature aggregation strategy to combine low-level features
and high-level semantics simultaneously, which provides accurate cell
recognition for our purely point-based model. In addition, an optimized cost
function is designed to adapt our multi-task learning framework by matching
ground truth and predicted points. The experimental results demonstrate the
superior accuracy and efficiency of the proposed method, which reveals the high
potentiality in assisting pathologist assessments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Motion Network for Freehand 3D Ultrasound Reconstruction. (arXiv:2207.00177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00177">
<div class="article-summary-box-inner">
<span><p>Freehand 3D ultrasound (US) has important clinical value due to its low cost
and unrestricted field of view. Recently deep learning algorithms have removed
its dependence on bulky and expensive external positioning devices. However,
improving reconstruction accuracy is still hampered by difficult elevational
displacement estimation and large cumulative drift. In this context, we propose
a novel deep motion network (MoNet) that integrates images and a lightweight
sensor known as the inertial measurement unit (IMU) from a velocity perspective
to alleviate the obstacles mentioned above. Our contribution is two-fold.
First, we introduce IMU acceleration for the first time to estimate elevational
displacements outside the plane. We propose a temporal and multi-branch
structure to mine the valuable information of low signal-to-noise ratio (SNR)
acceleration. Second, we propose a multi-modal online self-supervised strategy
that leverages IMU information as weak labels for adaptive optimization to
reduce drift errors and further ameliorate the impacts of acceleration noise.
Experiments show that our proposed method achieves the superior reconstruction
performance, exceeding state-of-the-art methods across the board.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Monocular Disparity Estimation for Single-View Reconstruction. (arXiv:2207.00182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00182">
<div class="article-summary-box-inner">
<span><p>We present a fine-tuning method to improve the appearance of 3D geometries
reconstructed from single images. We leverage advances in monocular depth
estimation to obtain disparity maps and present a novel approach to
transforming 2D normalized disparity maps into 3D point clouds by solving an
optimization on the relevant camera parameters, After creating a 3D point cloud
from disparity, we introduce a method to combine the new point cloud with
existing information to form a more faithful and detailed final geometry. We
demonstrate the efficacy of our approach with multiple experiments on both
synthetic and real images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMFN: Multi-Modal-Fusion-Net for End-to-End Driving. (arXiv:2207.00186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00186">
<div class="article-summary-box-inner">
<span><p>Inspired by the fact that humans use diverse sensory organs to perceive the
world, sensors with different modalities are deployed in end-to-end driving to
obtain the global context of the 3D scene. In previous works, camera and LiDAR
inputs are fused through transformers for better driving performance. These
inputs are normally further interpreted as high-level map information to assist
navigation tasks. Nevertheless, extracting useful information from the complex
map input is challenging, for redundant information may mislead the agent and
negatively affect driving performance. We propose a novel approach to
efficiently extract features from vectorized High-Definition (HD) maps and
utilize them in the end-to-end driving tasks. In addition, we design a new
expert to further enhance the model performance by considering multi-road
rules. Experimental results prove that both of the proposed improvements enable
our agent to achieve superior performance compared with other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Query-Key Pairwise Interactions in Vision Transformers. (arXiv:2207.00188v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00188">
<div class="article-summary-box-inner">
<span><p>Vision Transformers have achieved state-of-the-art performance in many visual
tasks. Due to the quadratic computational and memory complexities of
self-attention, recent works either apply attention only to low-resolution
inputs or restrict the receptive field to a small local region. To overcome
these limitations, we propose key-only attention, which excludes query-key
pairwise interactions and uses a compute-efficient saliency-gate to obtain
attention weights, modeling local-global interactions in all stages. Key-only
attention has linear computational and memory complexities w.r.t input size. We
use alternate layout to hybridize convolution and attention layers instead of
grafting which is suggested by previous works, so that all stages can benefit
from both spatial attentions and convolutions. We leverage these improvements
to develop a new self-attention model family, LinGlos, which reach
state-of-the-art accuracies on the parameter-limited setting of ImageNet
classification benchmark, and outperform baselines significantly in downstream
tasks, e.g., COCO object detection and ADE20K semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data generation using simulation technology to improve perception mechanism of autonomous vehicles. (arXiv:2207.00191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00191">
<div class="article-summary-box-inner">
<span><p>Recent advancements in computer graphics technology allow more realistic
ren-dering of car driving environments. They have enabled self-driving car
simulators such as DeepGTA-V and CARLA (Car Learning to Act) to generate large
amounts of synthetic data that can complement the existing real-world dataset
in training autonomous car perception. Furthermore, since self-driving car
simulators allow full control of the environment, they can generate dangerous
driving scenarios that the real-world dataset lacks such as bad weather and
accident scenarios. In this paper, we will demonstrate the effectiveness of
combining data gathered from the real world with data generated in the
simulated world to train perception systems on object detection and
localization task. We will also propose a multi-level deep learning perception
framework that aims to emulate a human learning experience in which a series of
tasks from the simple to more difficult ones are learned in a certain domain.
The autonomous car perceptron can learn from easy-to-drive scenarios to more
challenging ones customized by simulation software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reading and Writing: Discriminative and Generative Modeling for Self-Supervised Text Recognition. (arXiv:2207.00193v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00193">
<div class="article-summary-box-inner">
<span><p>Existing text recognition methods usually need large-scale training data.
Most of them rely on synthetic training data due to the lack of annotated real
images. However, there is a domain gap between the synthetic data and real
data, which limits the performance of the text recognition models. Recent
self-supervised text recognition methods attempted to utilize unlabeled real
images by introducing contrastive learning, which mainly learns the
discrimination of the text images. Inspired by the observation that humans
learn to recognize the texts through both reading and writing, we propose to
learn discrimination and generation by integrating contrastive learning and
masked image modeling in our self-supervised method. The contrastive learning
branch is adopted to learn the discrimination of text images, which imitates
the reading behavior of humans. Meanwhile, masked image modeling is firstly
introduced for text recognition to learn the context generation of the text
images, which is similar to the writing behavior. The experimental results show
that our method outperforms previous self-supervised text recognition methods
by 10.2%-20.2% on irregular scene text recognition datasets. Moreover, our
proposed text recognizer exceeds previous state-of-the-art text recognition
methods by averagely 5.3% on 11 benchmarks, with similar model size. We also
demonstrate that our pre-trained model can be easily applied to other
text-related tasks with obvious performance gain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Studying the impact of magnitude pruning on contrastive learning methods. (arXiv:2207.00200v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00200">
<div class="article-summary-box-inner">
<span><p>We study the impact of different pruning techniques on the representation
learned by deep neural networks trained with contrastive loss functions. Our
work finds that at high sparsity levels, contrastive learning results in a
higher number of misclassified examples relative to models trained with
traditional cross-entropy loss. To understand this pronounced difference, we
use metrics such as the number of PIEs (Hooker et al., 2019), Q-Score (Kalibhat
et al., 2022), and PD-Score (Baldock et al., 2021) to measure the impact of
pruning on the learned representation quality. Our analysis suggests the
schedule of the pruning method implementation matters. We find that the
negative impact of sparsity on the quality of the learned representation is the
highest when pruning is introduced early on in the training phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce. (arXiv:2207.00208v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00208">
<div class="article-summary-box-inner">
<span><p>Understanding vision and language representations of product content is vital
for search and recommendation applications in e-commerce. As a backbone for
online shopping platforms and inspired by the recent success in representation
learning research, we propose a contrastive learning framework that aligns
language and visual models using unlabeled raw product text and images. We
present techniques we used to train large-scale representation learning models
and share solutions that address domain-specific challenges. We study the
performance using our pre-trained model as backbones for diverse downstream
tasks, including category classification, attribute extraction, product
matching, product clustering, and adult product recognition. Experimental
results show that our proposed method outperforms the baseline in each
downstream task regarding both single modality and multiple modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Parameterization for Dynamic Human Head Editing. (arXiv:2207.00210v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00210">
<div class="article-summary-box-inner">
<span><p>Implicit radiance functions emerged as a powerful scene representation for
reconstructing and rendering photo-realistic views of a 3D scene. These
representations, however, suffer from poor editability. On the other hand,
explicit representations such as polygonal meshes allow easy editing but are
not as suitable for reconstructing accurate details in dynamic human heads,
such as fine facial features, hair, teeth, and eyes. In this work, we present
Neural Parameterization (NeP), a hybrid representation that provides the
advantages of both implicit and explicit methods. NeP is capable of
photo-realistic rendering while allowing fine-grained editing of the scene
geometry and appearance. We first disentangle the geometry and appearance by
parameterizing the 3D geometry into 2D texture space. We enable geometric
editability by introducing an explicit linear deformation blending layer. The
deformation is controlled by a set of sparse key points, which can be
explicitly and intuitively displaced to edit the geometry. For appearance, we
develop a hybrid 2D texture consisting of an explicit texture map for easy
editing and implicit view and time-dependent residuals to model temporal and
view variations. We compare our method to several reconstruction and editing
baselines. The results show that the NeP achieves almost the same level of
rendering accuracy while maintaining high editability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polarized Color Image Denoising using Pocoformer. (arXiv:2207.00215v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00215">
<div class="article-summary-box-inner">
<span><p>Polarized color photography provides both visual textures and object
surficial information in one single snapshot. However, the use of the
directional polarizing filter array causes extremely lower photon count and SNR
compared to conventional color imaging. Thus, the feature essentially leads to
unpleasant noisy images and destroys polarization analysis performance. It is a
challenge for traditional image processing pipelines owing to the fact that the
physical constraints exerted implicitly in the channels are excessively
complicated. To address this issue, we propose a learning-based approach to
simultaneously restore clean signals and precise polarization information. A
real-world polarized color image dataset of paired raw short-exposed noisy and
long-exposed reference images are captured to support the learning-based
pipeline. Moreover, we embrace the development of vision Transformer and
propose a hybrid transformer model for the Polarized Color image denoising,
namely PoCoformer, for a better restoration performance. Abundant experiments
demonstrate the effectiveness of proposed method and key factors that affect
results are analyzed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00221">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pretraining (VLP) models have recently successfully
facilitated many cross-modal downstream tasks. Most existing works evaluated
their systems by comparing the fine-tuned downstream task performance. However,
only average downstream task accuracy provides little information about the
pros and cons of each VLP method, let alone provides insights on how the
community can improve the systems in the future. Inspired by the CheckList for
testing natural language processing, we introduce VL-CheckList, a novel
framework to understand the capabilities of VLP models. The proposed method
divides the image-texting ability of a VLP model into three categories:
objects, attributes, and relations, and uses a novel taxonomy to further break
down these three aspects. We conduct comprehensive studies to analyze seven
recently popular VLP models via the proposed framework. Results confirm the
effectiveness of the proposed method by revealing fine-grained differences
among the compared models that were not visible from downstream task-only
evaluation. Further results show promising research direction in building
better VLP models. Data and Code: https://github.com/om-ai-lab/VL-CheckList
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keeping Less is More: Point Sparsification for Visual SLAM. (arXiv:2207.00225v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00225">
<div class="article-summary-box-inner">
<span><p>When adapting Simultaneous Mapping and Localization (SLAM) to real-world
applications, such as autonomous vehicles, drones, and augmented reality
devices, its memory footprint and computing cost are the two main factors
limiting the performance and the range of applications. In sparse feature based
SLAM algorithms, one efficient way for this problem is to limit the map point
size by selecting the points potentially useful for local and global bundle
adjustment (BA). This study proposes an efficient graph optimization for
sparsifying map points in SLAM systems. Specifically, we formulate a maximum
pose-visibility and maximum spatial diversity problem as a minimum-cost
maximum-flow graph optimization problem. The proposed method works as an
additional step in existing SLAM systems, so it can be used in both
conventional or learning based SLAM systems. By extensive experimental
evaluations we demonstrate the proposed method achieves even more accurate
camera poses with approximately 1/3 of the map points and 1/2 of the
computation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Transformer Meets CutMix for Improved Accuracy, Communication Efficiency, and Data Privacy in Split Learning. (arXiv:2207.00234v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00234">
<div class="article-summary-box-inner">
<span><p>This article seeks for a distributed learning solution for the visual
transformer (ViT) architectures. Compared to convolutional neural network (CNN)
architectures, ViTs often have larger model sizes, and are computationally
expensive, making federated learning (FL) ill-suited. Split learning (SL) can
detour this problem by splitting a model and communicating the hidden
representations at the split-layer, also known as smashed data.
Notwithstanding, the smashed data of ViT are as large as and as similar as the
input data, negating the communication efficiency of SL while violating data
privacy. To resolve these issues, we propose a new form of CutSmashed data by
randomly punching and compressing the original smashed data. Leveraging this,
we develop a novel SL framework for ViT, coined CutMixSL, communicating
CutSmashed data. CutMixSL not only reduces communication costs and privacy
leakage, but also inherently involves the CutMix data augmentation, improving
accuracy and scalability. Simulations corroborate that CutMixSL outperforms
baselines such as parallelized SL and SplitFed that integrates FL with SL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance. (arXiv:2207.00251v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00251">
<div class="article-summary-box-inner">
<span><p>Although deep learning algorithms have been intensively developed for
computer-aided tuberculosis diagnosis (CTD), they mainly depend on carefully
annotated datasets, leading to much time and resource consumption. Weakly
supervised learning (WSL), which leverages coarse-grained labels to accomplish
fine-grained tasks, has the potential to solve this problem. In this paper, we
first propose a new large-scale tuberculosis (TB) chest X-ray dataset, namely
the tuberculosis chest X-ray attribute dataset (TBX-Att), and then establish an
attribute-assisted weakly-supervised framework to classify and localize TB by
leveraging the attribute information to overcome the insufficiency of
supervision in WSL scenarios. Specifically, first, the TBX-Att dataset contains
2000 X-ray images with seven kinds of attributes for TB relational reasoning,
which are annotated by experienced radiologists. It also includes the public
TBX11K dataset with 11200 X-ray images to facilitate weakly supervised
detection. Second, we exploit a multi-scale feature interaction model for TB
area classification and detection with attribute relational reasoning. The
proposed model is evaluated on the TBX-Att dataset and will serve as a solid
baseline for future research. The code and data will be available at
https://github.com/GangmingZhao/tb-attribute-weak-localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trajectory Forecasting on Temporal Graphs. (arXiv:2207.00255v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00255">
<div class="article-summary-box-inner">
<span><p>Predicting future locations of agents in the scene is an important problem in
self-driving. In recent years, there has been a significant progress in
representing the scene and the agents in it. The interactions of agents with
the scene and with each other are typically modeled with a Graph Neural
Network. However, the graph structure is mostly static and fails to represent
the temporal changes in highly dynamic scenes. In this work, we propose a
temporal graph representation to better capture the dynamics in traffic scenes.
We complement our representation with two types of memory modules; one focusing
on the agent of interest and the other on the entire scene. This allows us to
learn temporally-aware representations that can achieve good results even with
simple regression of multiple futures. When combined with goal-conditioned
prediction, we show better results that can reach the state-of-the-art
performance on the Argoverse benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised High-Resolution Portrait Gaze Correction and Animation. (arXiv:2207.00256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00256">
<div class="article-summary-box-inner">
<span><p>This paper proposes a gaze correction and animation method for
high-resolution, unconstrained portrait images, which can be trained without
the gaze angle and the head pose annotations. Common gaze-correction methods
usually require annotating training data with precise gaze, and head pose
information. Solving this problem using an unsupervised method remains an open
problem, especially for high-resolution face images in the wild, which are not
easy to annotate with gaze and head pose labels. To address this issue, we
first create two new portrait datasets: CelebGaze and high-resolution
CelebHQGaze. Second, we formulate the gaze correction task as an image
inpainting problem, addressed using a Gaze Correction Module (GCM) and a Gaze
Animation Module (GAM). Moreover, we propose an unsupervised training strategy,
i.e., Synthesis-As-Training, to learn the correlation between the eye region
features and the gaze angle. As a result, we can use the learned latent space
for gaze animation with semantic interpolation in this space. Moreover, to
alleviate both the memory and the computational costs in the training and the
inference stage, we propose a Coarse-to-Fine Module (CFM) integrated with GCM
and GAM. Extensive experiments validate the effectiveness of our method for
both the gaze correction and the gaze animation tasks in both low and
high-resolution face datasets in the wild and demonstrate the superiority of
our method with respect to the state of the arts. Code is available at
https://github.com/zhangqianhui/GazeAnimationV2
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Covid-19 detection using transfer learning approach from computed temography images. (arXiv:2207.00259v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00259">
<div class="article-summary-box-inner">
<span><p>Our main goal in this study is to propose a transfer learning based method
for COVID-19 detection from Computed Tomography (CT) images. The transfer
learning model used for the task is a pretrained Xception model. Both model
architecture and pre-trained weights on ImageNet were used. The resulting
modified model was trained with 128 batch size and 224x224, 3 channeled input
images, converted from original 512x512, grayscale images. The dataset used is
a the COV19-CT-DB. Labels in the dataset include COVID-19 cases and
Non-COVID-19 cases for COVID-1919 detection. Firstly, a accuracy and loss on
the validation partition of the dataset as well as precision recall and macro
F1 score were used to measure the performance of the proposed method. The
resulting Macro F1 score on the validation set exceeded the baseline model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Two-view 6D Object Pose Estimation: A Comparative Study on Fusion Strategy. (arXiv:2207.00260v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00260">
<div class="article-summary-box-inner">
<span><p>Current RGB-based 6D object pose estimation methods have achieved noticeable
performance on datasets and real world applications. However, predicting 6D
pose from single 2D image features is susceptible to disturbance from changing
of environment and textureless or resemblant object surfaces. Hence, RGB-based
methods generally achieve less competitive results than RGBD-based methods,
which deploy both image features and 3D structure features. To narrow down this
performance gap, this paper proposes a framework for 6D object pose estimation
that learns implicit 3D information from 2 RGB images. Combining the learned 3D
information and 2D image features, we establish more stable correspondence
between the scene and the object models. To seek for the methods best utilizing
3D information from RGB inputs, we conduct an investigation on three different
approaches, including Early- Fusion, Mid-Fusion, and Late-Fusion. We ascertain
the Mid- Fusion approach is the best approach to restore the most precise 3D
keypoints useful for object pose estimation. The experiments show that our
method outperforms state-of-the-art RGB-based methods, and achieves comparable
results with RGBD-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelet leader based formalism to compute multifractal features for classifying lung nodules in X-ray images. (arXiv:2207.00262v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00262">
<div class="article-summary-box-inner">
<span><p>This paper presents and validates a novel lung nodule classification
algorithm that uses multifractal features found in X-ray images. The proposed
method includes a pre-processing step where two enhancement techniques are
applied: histogram equalization and a combination of wavelet decomposition and
morphological operations. As a novelty, multifractal features using wavelet
leader based formalism are used with Support Vector Machine classifier; other
classical texture features were also included. Best results were obtained when
using multifractal features in combination with classical texture features,
with a maximum ROC AUC of 75\%. The results show improvements when using data
augmentation technique, and parameter optimization. The proposed method proved
to be more efficient and accurate than Modulus Maxima Wavelet Formalism in both
computational cost and accuracy when compared in a similar experimental set up.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. (arXiv:2207.00278v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00278">
<div class="article-summary-box-inner">
<span><p>Due to its powerful feature learning capability and high efficiency, deep
hashing has achieved great success in large-scale image retrieval. Meanwhile,
extensive works have demonstrated that deep neural networks (DNNs) are
susceptible to adversarial examples, and exploring adversarial attack against
deep hashing has attracted many research efforts. Nevertheless, backdoor
attack, another famous threat to DNNs, has not been studied for deep hashing
yet. Although various backdoor attacks have been proposed in the field of image
classification, existing approaches failed to realize a truly imperceptive
backdoor attack that enjoys invisible triggers and clean label setting
simultaneously, and they also cannot meet the intrinsic demand of image
retrieval backdoor.
</p>
<p>In this paper, we propose BadHash, the first generative-based imperceptible
backdoor attack against deep hashing, which can effectively generate invisible
and input-specific poisoned images with clean label. Specifically, we first
propose a new conditional generative adversarial network (cGAN) pipeline to
effectively generate poisoned samples. For any given benign image, it seeks to
generate a natural-looking poisoned counterpart with a unique invisible
trigger. In order to improve the attack effectiveness, we introduce a
label-based contrastive learning network LabCLN to exploit the semantic
characteristics of different labels, which are subsequently used for confusing
and misleading the target model to learn the embedded trigger. We finally
explore the mechanism of backdoor attacks on image retrieval in the hash space.
Extensive experiments on multiple benchmark datasets verify that BadHash can
generate imperceptible poisoned samples with strong attack ability and
transferability over state-of-the-art deep hashing schemes. Primary Subject
Area: [Engagement] Multimedia Search and Recommendation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">(Un)likelihood Training for Interpretable Embedding. (arXiv:2207.00282v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00282">
<div class="article-summary-box-inner">
<span><p>Cross-modal representation learning has become a new normal for bridging the
semantic gap between text and visual data. Learning modality agnostic
representations in a continuous latent space, however, is often treated as a
black-box data-driven training process. It is well-known that the effectiveness
of representation learning depends heavily on the quality and scale of training
data. For video representation learning, having a complete set of labels that
annotate the full spectrum of video content for training is highly difficult if
not impossible. These issues, black-box training and dataset bias, make
representation learning practically challenging to be deployed for video
understanding due to unexplainable and unpredictable results. In this paper, we
propose two novel training objectives, likelihood and unlikelihood functions,
to unroll semantics behind embeddings while addressing the label sparsity
problem in training. The likelihood training aims to interpret semantics of
embeddings beyond training labels, while the unlikelihood training leverages
prior knowledge for regularization to ensure semantically coherent
interpretation. With both training objectives, a new encoder-decoder network,
which learns interpretable cross-modal representation, is proposed for ad-hoc
video search. Extensive experiments on TRECVid and MSR-VTT datasets show the
proposed network outperforms several state-of-the-art retrieval models with a
statistically significant performance margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DALG: Deep Attentive Local and Global Modeling for Image Retrieval. (arXiv:2207.00287v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00287">
<div class="article-summary-box-inner">
<span><p>Deeply learned representations have achieved superior image retrieval
performance in a retrieve-then-rerank manner. Recent state-of-the-art single
stage model, which heuristically fuses local and global features, achieves
promising trade-off between efficiency and effectiveness. However, we notice
that efficiency of existing solutions is still restricted because of their
multi-scale inference paradigm. In this paper, we follow the single stage art
and obtain further complexity-effectiveness balance by successfully getting rid
of multi-scale testing. To achieve this goal, we abandon the widely-used
convolution network giving its limitation in exploring diverse visual patterns,
and resort to fully attention based framework for robust representation
learning motivated by the success of Transformer. Besides applying Transformer
for global feature extraction, we devise a local branch composed of
window-based multi-head attention and spatial attention to fully exploit local
image patterns. Furthermore, we propose to combine the hierarchical local and
global features via a cross-attention module, instead of using heuristically
fusion as previous art does. With our Deep Attentive Local and Global modeling
framework (DALG), extensive experimental results show that efficiency can be
significantly improved while maintaining competitive results with the state of
the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to segment from object sizes. (arXiv:2207.00289v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00289">
<div class="article-summary-box-inner">
<span><p>Deep learning has proved particularly useful for semantic segmentation, a
fundamental image analysis task. However, the standard deep learning methods
need many training images with ground-truth pixel-wise annotations, which are
usually laborious to obtain and, in some cases (e.g., medical images), require
domain expertise. Therefore, instead of pixel-wise annotations, we focus on
image annotations that are significantly easier to acquire but still
informative, namely the size of foreground objects. We define the object size
as the maximum distance between a foreground pixel and the background. We
propose an algorithm for training a deep segmentation network from a dataset of
a few pixel-wise annotated images and many images with known object sizes. The
algorithm minimizes a discrete (non-differentiable) loss function defined over
the object sizes by sampling the gradient and then using the standard
back-propagation algorithm. We study the performance of our approach in terms
of training time and generalization error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Graph Matching Algorithms in Computer Vision. (arXiv:2207.00291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00291">
<div class="article-summary-box-inner">
<span><p>The graph matching optimization problem is an essential component for many
tasks in computer vision, such as bringing two deformable objects in
correspondence. Naturally, a wide range of applicable algorithms have been
proposed in the last decades. Since a common standard benchmark has not been
developed, their performance claims are often hard to verify as evaluation on
differing problem instances and criteria make the results incomparable. To
address these shortcomings, we present a comparative study of graph matching
algorithms. We create a uniform benchmark where we collect and categorize a
large set of existing and publicly available computer vision graph matching
problems in a common format. At the same time we collect and categorize the
most popular open-source implementations of graph matching algorithms. Their
performance is evaluated in a way that is in line with the best practices for
comparing optimization algorithms. The study is designed to be reproducible and
extensible to serve as a valuable resource in the future.
</p>
<p>Our study provides three notable insights:
</p>
<p>1.) popular problem instances are exactly solvable in substantially less than
1 second and, therefore, are insufficient for future empirical evaluations;
</p>
<p>2.) the most popular baseline methods are highly inferior to the best
available methods;
</p>
<p>3.) despite the NP-hardness of the problem, instances coming from vision
applications are often solvable in a few seconds even for graphs with more than
500 vertices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offset equivariant networks and their applications. (arXiv:2207.00292v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00292">
<div class="article-summary-box-inner">
<span><p>In this paper we present a framework for the design and implementation of
offset equivariant networks, that is, neural networks that preserve in their
output uniform increments in the input. In a suitable color space this kind of
networks achieves equivariance with respect to the photometric transformations
that characterize changes in the lighting conditions. We verified the framework
on three different problems: image recognition, illuminant estimation, and
image inpainting. Our experiments show that the performance of offset
equivariant networks are comparable to those in the state of the art on regular
data. Differently from conventional networks, however, equivariant networks do
behave consistently well when the color of the illuminant changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopicFM: Robust and Interpretable Feature Matching with Topic-assisted. (arXiv:2207.00328v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00328">
<div class="article-summary-box-inner">
<span><p>Finding correspondences across images is an important task in many visual
applications. Recent state-of-the-art methods focus on end-to-end
learning-based architectures designed in a coarse-to-fine manner. They use a
very deep CNN or multi-block Transformer to learn robust representation, which
requires high computation power. Moreover, these methods learn features without
reasoning about objects, shapes inside images, thus lacks of interpretability.
In this paper, we propose an architecture for image matching which is
efficient, robust, and interpretable. More specifically, we introduce a novel
feature matching module called TopicFM which can roughly organize same spatial
structure across images into a topic and then augment the features inside each
topic for accurate matching. To infer topics, we first learn global embedding
of topics and then use a latent-variable model to detect-then-assign the image
structures into topics. Our method can only perform matching in co-visibility
regions to reduce computations. Extensive experiments in both outdoor and
indoor datasets show that our method outperforms the recent methods in terms of
matching performance and computational efficiency. The code is available at
https://github.com/TruongKhang/TopicFM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Literature on Hand GESTURE Recognition using Graph based methods. (arXiv:2207.00329v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00329">
<div class="article-summary-box-inner">
<span><p>Skeleton based recognition systems are gaining popularity and machine
learning models focusing on points or joints in a skeleton have proved to be
computationally effective and application in many areas like Robotics. It is
easy to track points and thereby preserving spatial and temporal information,
which plays an important role in abstracting the required information,
classification becomes an easy task. In this paper, we aim to study these
points but using a cloud mechanism, where we define a cloud as collection of
points. However, when we add temporal information, it may not be possible to
retrieve the coordinates of a point in each frame and hence instead of focusing
on a single point, we can use k-neighbors to retrieve the state of the point
under discussion. Our focus is to gather such information using weight sharing
but making sure that when we try to retrieve the information from neighbors, we
do not carry noise with it. LSTM which has capability of long-term modelling
and can carry both temporal and spatial information. In this article we tried
to summarise graph based gesture recognition method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Correlation Loss for Regression. (arXiv:2207.00347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00347">
<div class="article-summary-box-inner">
<span><p>Regression learning is classic and fundamental for medical image analysis. It
provides the continuous mapping for many critical applications, like the
attribute estimation, object detection, segmentation and non-rigid
registration. However, previous studies mainly took the case-wise criteria,
like the mean square errors, as the optimization objectives. They ignored the
very important population-wise correlation criterion, which is exactly the
final evaluation metric in many tasks. In this work, we propose to revisit the
classic regression tasks with novel investigations on directly optimizing the
fine-grained correlation losses. We mainly explore two complementary
correlation indexes as learnable losses: Pearson linear correlation (PLC) and
Spearman rank correlation (SRC). The contributions of this paper are two folds.
First, for the PLC on global level, we propose a strategy to make it robust
against the outliers and regularize the key distribution factors. These efforts
significantly stabilize the learning and magnify the efficacy of PLC. Second,
for the SRC on local level, we propose a coarse-to-fine scheme to ease the
learning of the exact ranking order among samples. Specifically, we convert the
learning for the ranking of samples into the learning of similarity
relationships among samples. We extensively validate our method on two typical
ultrasound image regression tasks, including the image quality assessment and
bio-metric measurement. Experiments prove that, with the fine-grained guidance
in directly optimizing the correlation, the regression performances are
significantly improved. Our proposed correlation losses are general and can be
extended to more important applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting the Mean Teacher for keypoint-based lung registration under geometric domain shifts. (arXiv:2207.00371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00371">
<div class="article-summary-box-inner">
<span><p>Recent deep learning-based methods for medical image registration achieve
results that are competitive with conventional optimization algorithms at
reduced run times. However, deep neural networks generally require plenty of
labeled training data and are vulnerable to domain shifts between training and
test data. While typical intensity shifts can be mitigated by keypoint-based
registration, these methods still suffer from geometric domain shifts, for
instance, due to different fields of view. As a remedy, in this work, we
present a novel approach to geometric domain adaptation for image registration,
adapting a model from a labeled source to an unlabeled target domain. We build
on a keypoint-based registration model, combining graph convolutions for
geometric feature learning with loopy belief optimization, and propose to
reduce the domain shift through self-ensembling. To this end, we embed the
model into the Mean Teacher paradigm. We extend the Mean Teacher to this
context by 1) adapting the stochastic augmentation scheme and 2) combining
learned feature extraction with differentiable optimization. This enables us to
guide the learning process in the unlabeled target domain by enforcing
consistent predictions of the learning student and the temporally averaged
teacher model. We evaluate the method for exhale-to-inhale lung CT registration
under two challenging adaptation scenarios (DIR-Lab 4D CT to COPD, COPD to
Learn2Reg). Our method consistently improves on the baseline model by 50%/47%
while even matching the accuracy of models trained on target data. Source code
is available at
https://github.com/multimodallearning/registration-da-mean-teacher.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReLER@ZJU-Alibaba Submission to the Ego4D Natural Language Queries Challenge 2022. (arXiv:2207.00383v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00383">
<div class="article-summary-box-inner">
<span><p>In this report, we present the ReLER@ZJU-Alibaba submission to the Ego4D
Natural Language Queries (NLQ) Challenge in CVPR 2022. Given a video clip and a
text query, the goal of this challenge is to locate a temporal moment of the
video clip where the answer to the query can be obtained. To tackle this task,
we propose a multi-scale cross-modal transformer and a video frame-level
contrastive loss to fully uncover the correlation between language queries and
video clips. Besides, we propose two data augmentation strategies to increase
the diversity of training samples. The experimental results demonstrate the
effectiveness of our method. The final submission ranked first on the
leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WNet: A data-driven dual-domain denoising model for sparse-view computed tomography with a trainable reconstruction layer. (arXiv:2207.00400v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00400">
<div class="article-summary-box-inner">
<span><p>Deep learning based solutions are being succesfully implemented for a wide
variety of applications. Most notably, clinical use-cases have gained an
increased interest and have been the main driver behind some of the
cutting-edge data-driven algorithms proposed in the last years. For
applications like sparse-view tomographic reconstructions, where the amount of
measurement data is small in order to keep acquisition times short and
radiation dose low, reduction of the streaking artifacts has prompted the
development of data-driven denoising algorithms with the main goal of obtaining
diagnostically viable images with only a subset of a full-scan data. We propose
WNet, a data-driven dual-domain denoising model which contains a trainable
reconstruction layer for sparse-view artifact denoising. Two encoder-decoder
networks perform denoising in both sinogram- and reconstruction-domain
simultaneously, while a third layer implementing the Filtered Backprojection
algorithm is sandwiched between the first two and takes care of the
reconstruction operation. We investigate the performance of the network on
sparse-view chest CT scans, and we highlight the added benefit of having a
trainable reconstruction layer over the more conventional fixed ones. We train
and test our network on two clinically relevant datasets and we compare the
obtained results with three different types of sparse-view CT denoising and
reconstruction algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Intraluminal Navigation of a Soft Robot using Deep-Learning-based Visual Servoing. (arXiv:2207.00401v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00401">
<div class="article-summary-box-inner">
<span><p>Navigation inside luminal organs is an arduous task that requires
non-intuitive coordination between the movement of the operator's hand and the
information obtained from the endoscopic video. The development of tools to
automate certain tasks could alleviate the physical and mental load of doctors
during interventions, allowing them to focus on diagnosis and decision-making
tasks. In this paper, we present a synergic solution for intraluminal
navigation consisting of a 3D printed endoscopic soft robot that can move
safely inside luminal structures. Visual servoing, based on Convolutional
Neural Networks (CNNs) is used to achieve the autonomous navigation task. The
CNN is trained with phantoms and in-vivo data to segment the lumen, and a
model-less approach is presented to control the movement in constrained
environments. The proposed robot is validated in anatomical phantoms in
different path configurations. We analyze the movement of the robot using
different metrics such as task completion time, smoothness, error in the
steady-state, and mean and maximum error. We show that our method is suitable
to navigate safely in hollow environments and conditions which are different
than the ones the network was originally trained on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for Videos: A Survey. (arXiv:2207.00419v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00419">
<div class="article-summary-box-inner">
<span><p>The remarkable success of deep learning in various domains relies on the
availability of large-scale annotated datasets. However, the use of
human-generated annotations leads to models with biased learning, poor domain
generalization, and poor robustness. Obtaining annotations is also expensive
and requires great effort, which is especially challenging for videos. As an
alternative, self-supervised learning provides a way for representation
learning which does not require annotations and has shown promise in both image
and video domains. Different from the image domain, learning video
representations are more challenging due to the temporal dimension, bringing in
motion and other environmental dynamics. This also provides opportunities for
exclusive ideas which can advance self-supervised learning in the video and
multimodal domain. In this survey, we provide a review of existing approaches
on self-supervised learning focusing on the video domain. We summarize these
methods into three different categories based on their learning objectives:
pre-text tasks, generative modeling, and contrastive learning. These approaches
also differ in terms of the modality which are being used: video, video-audio,
video-text, and video-audio-text. We further introduce the commonly used
datasets, downstream evaluation tasks, insights into the limitations of
existing works, and the potential future directions in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Showcases: Generating Multi-Modal Explanations for Recommendations. (arXiv:2207.00422v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00422">
<div class="article-summary-box-inner">
<span><p>Existing explanation models generate only text for recommendations but still
struggle to produce diverse contents. In this paper, to further enrich
explanations, we propose a new task named personalized showcases, in which we
provide both textual and visual information to explain our recommendations.
Specifically, we first select a personalized image set that is the most
relevant to a user's interest toward a recommended item. Then, natural language
explanations are generated accordingly given our selected images. For this new
task, we collect a large-scale dataset from Google Local (i.e.,~maps) and
construct a high-quality subset for generating multi-modal explanations. We
propose a personalized multi-modal framework which can generate diverse and
visually-aligned explanations via contrastive learning. Experiments show that
our framework benefits from different modalities as inputs, and is able to
produce more diverse and expressive explanations compared to previous methods
on a variety of evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stain Isolation-based Guidance for Improved Stain Translation. (arXiv:2207.00431v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00431">
<div class="article-summary-box-inner">
<span><p>Unsupervised and unpaired domain translation using generative adversarial
neural networks, and more precisely CycleGAN, is state of the art for the stain
translation of histopathology images. It often, however, suffers from the
presence of cycle-consistent but non structure-preserving errors. We propose an
alternative approach to the set of methods which, relying on segmentation
consistency, enable the preservation of pathology structures. Focusing on
immunohistochemistry (IHC) and multiplexed immunofluorescence (mIF), we
introduce a simple yet effective guidance scheme as a loss function that
leverages the consistency of stain translation with stain isolation.
Qualitative and quantitative experiments show the ability of the proposed
approach to improve translation between the two domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PROTOtypical Logic Tensor Networks (PROTO-LTN) for Zero Shot Learning. (arXiv:2207.00433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00433">
<div class="article-summary-box-inner">
<span><p>Semantic image interpretation can vastly benefit from approaches that combine
sub-symbolic distributed representation learning with the capability to reason
at a higher level of abstraction. Logic Tensor Networks (LTNs) are a class of
neuro-symbolic systems based on a differentiable, first-order logic grounded
into a deep neural network. LTNs replace the classical concept of training set
with a knowledge base of fuzzy logical axioms. By defining a set of
differentiable operators to approximate the role of connectives, predicates,
functions and quantifiers, a loss function is automatically specified so that
LTNs can learn to satisfy the knowledge base. We focus here on the subsumption
or \texttt{isOfClass} predicate, which is fundamental to encode most semantic
image interpretation tasks. Unlike conventional LTNs, which rely on a separate
predicate for each class (e.g., dog, cat), each with its own set of learnable
weights, we propose a common \texttt{isOfClass} predicate, whose level of truth
is a function of the distance between an object embedding and the corresponding
class prototype. The PROTOtypical Logic Tensor Networks (PROTO-LTN) extend the
current formulation by grounding abstract concepts as parametrized class
prototypes in a high-dimensional embedding space, while reducing the number of
parameters required to ground the knowledge base. We show how this architecture
can be effectively trained in the few and zero-shot learning scenarios.
Experiments on Generalized Zero Shot Learning benchmarks validate the proposed
implementation as a competitive alternative to traditional embedding-based
approaches. The proposed formulation opens up new opportunities in zero shot
learning settings, as the LTN formalism allows to integrate background
knowledge in the form of logical axioms to compensate for the lack of labelled
examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dissecting Self-Supervised Learning Methods for Surgical Computer Vision. (arXiv:2207.00449v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00449">
<div class="article-summary-box-inner">
<span><p>The field of surgical computer vision has undergone considerable
breakthroughs in recent years with the rising popularity of deep neural
network-based methods. However, standard fully-supervised approaches for
training such models require vast amounts of annotated data, imposing a
prohibitively high cost; especially in the clinical domain. Self-Supervised
Learning (SSL) methods, which have begun to gain traction in the general
computer vision community, represent a potential solution to these annotation
costs, allowing to learn useful representations from only unlabeled data.
Still, the effectiveness of SSL methods in more complex and impactful domains,
such as medicine and surgery, remains limited and unexplored. In this work, we
address this critical need by investigating four state-of-the-art SSL methods
(MoCo v2, SimCLR, DINO, SwAV) in the context of surgical computer vision. We
present an extensive analysis of the performance of these methods on the
Cholec80 dataset for two fundamental and popular tasks in surgical context
understanding, phase recognition and tool presence detection. We examine their
parameterization, then their behavior with respect to training data quantities
in semi-supervised settings. Correct transfer of these methods to surgery, as
described and conducted in this work, leads to substantial performance gains
over generic uses of SSL - up to 7% on phase recognition and 20% on tool
presence detection - as well as state-of-the-art semi-supervised phase
recognition approaches by up to 14%. The code will be made available at
https://github.com/CAMMA-public/SelfSupSurg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SD-LayerNet: Semi-supervised retinal layer segmentation in OCT using disentangled representation with anatomical priors. (arXiv:2207.00458v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00458">
<div class="article-summary-box-inner">
<span><p>Optical coherence tomography (OCT) is a non-invasive 3D modality widely used
in ophthalmology for imaging the retina. Achieving automated, anatomically
coherent retinal layer segmentation on OCT is important for the detection and
monitoring of different retinal diseases, like Age-related Macular Disease
(AMD) or Diabetic Retinopathy. However, the majority of state-of-the-art layer
segmentation methods are based on purely supervised deep-learning, requiring a
large amount of pixel-level annotated data that is expensive and hard to
obtain. With this in mind, we introduce a semi-supervised paradigm into the
retinal layer segmentation task that makes use of the information present in
large-scale unlabeled datasets as well as anatomical priors. In particular, a
novel fully differentiable approach is used for converting surface position
regression into a pixel-wise structured segmentation, allowing to use both 1D
surface and 2D layer representations in a coupled fashion to train the model.
In particular, these 2D segmentations are used as anatomical factors that,
together with learned style factors, compose disentangled representations used
for reconstructing the input image. In parallel, we propose a set of anatomical
priors to improve network training when a limited amount of labeled data is
available. We demonstrate on the real-world dataset of scans with intermediate
and wet-AMD that our method outperforms state-of-the-art when using our full
training set, but more importantly largely exceeds state-of-the-art when it is
trained with a fraction of the labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the solution space of linear inverse problems with GAN latent geometry. (arXiv:2207.00460v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00460">
<div class="article-summary-box-inner">
<span><p>Inverse problems consist in reconstructing signals from incomplete sets of
measurements and their performance is highly dependent on the quality of the
prior knowledge encoded via regularization. While traditional approaches focus
on obtaining a unique solution, an emerging trend considers exploring multiple
feasibile solutions. In this paper, we propose a method to generate multiple
reconstructions that fit both the measurements and a data-driven prior learned
by a generative adversarial network. In particular, we show that, starting from
an initial solution, it is possible to find directions in the latent space of
the generative model that are null to the forward operator, and thus keep
consistency with the measurements, while inducing significant perceptual
change. Our exploration approach allows to generate multiple solutions to the
inverse problem an order of magnitude faster than existing approaches; we show
results on image super-resolution and inpainting problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised High-fidelity Ultrasound Video Synthesis with Feature Decoupling. (arXiv:2207.00474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00474">
<div class="article-summary-box-inner">
<span><p>Ultrasound (US) is widely used for its advantages of real-time imaging,
radiation-free and portability. In clinical practice, analysis and diagnosis
often rely on US sequences rather than a single image to obtain dynamic
anatomical information. This is challenging for novices to learn because
practicing with adequate videos from patients is clinically unpractical. In
this paper, we propose a novel framework to synthesize high-fidelity US videos.
Specifically, the synthesis videos are generated by animating source content
images based on the motion of given driving videos. Our highlights are
three-fold. First, leveraging the advantages of self- and fully-supervised
learning, our proposed system is trained in weakly-supervised manner for
keypoint detection. These keypoints then provide vital information for handling
complex high dynamic motions in US videos. Second, we decouple content and
texture learning using the dual decoders to effectively reduce the model
learning difficulty. Last, we adopt the adversarial training strategy with GAN
losses for further improving the sharpness of the generated videos, narrowing
the gap between real and synthesis videos. We validate our method on a large
in-house pelvic dataset with high dynamic motion. Extensive evaluation metrics
and user study prove the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agent with Tangent-based Formulation and Anatomical Perception for Standard Plane Localization in 3D Ultrasound. (arXiv:2207.00475v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00475">
<div class="article-summary-box-inner">
<span><p>Standard plane (SP) localization is essential in routine clinical ultrasound
(US) diagnosis. Compared to 2D US, 3D US can acquire multiple view planes in
one scan and provide complete anatomy with the addition of coronal plane.
However, manually navigating SPs in 3D US is laborious and biased due to the
orientation variability and huge search space. In this study, we introduce a
novel reinforcement learning (RL) framework for automatic SP localization in 3D
US. Our contribution is three-fold. First, we formulate SP localization in 3D
US as a tangent-point-based problem in RL to restructure the action space and
significantly reduce the search space. Second, we design an auxiliary task
learning strategy to enhance the model's ability to recognize subtle
differences crossing Non-SPs and SPs in plane search. Finally, we propose a
spatial-anatomical reward to effectively guide learning trajectories by
exploiting spatial and anatomical information simultaneously. We explore the
efficacy of our approach on localizing four SPs on uterus and fetal brain
datasets. The experiments indicate that our approach achieves a high
localization accuracy as well as robust performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Reflective Learning for Robust Medical Image Segmentation. (arXiv:2207.00476v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00476">
<div class="article-summary-box-inner">
<span><p>Deep segmentation models often face the failure risks when the testing image
presents unseen distributions. Improving model robustness against these risks
is crucial for the large-scale clinical application of deep models. In this
study, inspired by human learning cycle, we propose a novel online reflective
learning framework (RefSeg) to improve segmentation robustness. Based on the
reflection-on-action conception, our RefSeg firstly drives the deep model to
take action to obtain semantic segmentation. Then, RefSeg triggers the model to
reflect itself. Because making deep models realize their segmentation failures
during testing is challenging, RefSeg synthesizes a realistic proxy image from
the semantic mask to help deep models build intuitive and effective
reflections. This proxy translates and emphasizes the segmentation flaws. By
maximizing the structural similarity between the raw input and the proxy, the
reflection-on-action loop is closed with segmentation robustness improved.
RefSeg runs in the testing phase and is general for segmentation models.
Extensive validation on three medical image segmentation tasks with a public
cardiac MR dataset and two in-house large ultrasound datasets show that our
RefSeg remarkably improves model robustness and reports state-of-the-art
performance over strong competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-based Conflict Detection within Crowds based on High-Resolution Human Pose Estimation for Smart and Safe Airport. (arXiv:2207.00477v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00477">
<div class="article-summary-box-inner">
<span><p>Future airports are becoming more complex and congested with the increasing
number of travellers. While the airports are more likely to become hotspots for
potential conflicts to break out which can cause serious delays to flights and
several safety issues. An intelligent algorithm which renders security
surveillance more effective in detecting conflicts would bring many benefits to
the passengers in terms of their safety, finance, and travelling efficiency.
This paper details the development of a machine learning model to classify
conflicting behaviour in a crowd. HRNet is used to segment the images and then
two approaches are taken to classify the poses of people in the frame via
multiple classifiers. Among them, it was found that the support vector machine
(SVM) achieved the most performant achieving precision of 94.37%. Where the
model falls short is against ambiguous behaviour such as a hug or losing track
of a subject in the frame. The resulting model has potential for deployment
within an airport if improvements are made to cope with the vast number of
potential passengers in view as well as training against further ambiguous
behaviours which will arise in an airport setting. In turn, will provide the
capability to enhance security surveillance and improve airport safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Diagnostic Tool for Thyroid Cancer Classification using Multi-view Ultrasound. (arXiv:2207.00496v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00496">
<div class="article-summary-box-inner">
<span><p>Over the past decades, the incidence of thyroid cancer has been increasing
globally. Accurate and early diagnosis allows timely treatment and helps to
avoid over-diagnosis. Clinically, a nodule is commonly evaluated from both
transverse and longitudinal views using thyroid ultrasound. However, the
appearance of the thyroid gland and lesions can vary dramatically across
individuals. Identifying key diagnostic information from both views requires
specialized expertise. Furthermore, finding an optimal way to integrate
multi-view information also relies on the experience of clinicians and adds
further difficulty to accurate diagnosis. To address these, we propose a
personalized diagnostic tool that can customize its decision-making process for
different patients. It consists of a multi-view classification module for
feature extraction and a personalized weighting allocation network that
generates optimal weighting for different views. It is also equipped with a
self-supervised view-aware contrastive loss to further improve the model
robustness towards different patient groups. Experimental results show that the
proposed framework can better utilize multi-view information and outperform the
competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MotionMixer: MLP-based 3D Human Body Pose Forecasting. (arXiv:2207.00499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00499">
<div class="article-summary-box-inner">
<span><p>In this work, we present MotionMixer, an efficient 3D human body pose
forecasting model based solely on multi-layer perceptrons (MLPs). MotionMixer
learns the spatial-temporal 3D body pose dependencies by sequentially mixing
both modalities. Given a stacked sequence of 3D body poses, a spatial-MLP
extracts fine grained spatial dependencies of the body joints. The interaction
of the body joints over time is then modelled by a temporal MLP. The
spatial-temporal mixed features are finally aggregated and decoded to obtain
the future motion. To calibrate the influence of each time step in the pose
sequence, we make use of squeeze-and-excitation (SE) blocks. We evaluate our
approach on Human3.6M, AMASS, and 3DPW datasets using the standard evaluation
protocols. For all evaluations, we demonstrate state-of-the-art performance,
while having a model with a smaller number of parameters. Our code is available
at: https://github.com/MotionMLP/MotionMixer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Cross-Domain Feature Extraction for Single Blood Cell Image Classification. (arXiv:2207.00501v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00501">
<div class="article-summary-box-inner">
<span><p>Diagnosing hematological malignancies requires identification and
classification of white blood cells in peripheral blood smears. Domain shifts
caused by different lab procedures, staining, illumination, and microscope
settings hamper the re-usability of recently developed machine learning methods
on data collected from different sites. Here, we propose a cross-domain adapted
autoencoder to extract features in an unsupervised manner on three different
datasets of single white blood cells scanned from peripheral blood smears. The
autoencoder is based on an R-CNN architecture allowing it to focus on the
relevant white blood cell and eliminate artifacts in the image. To evaluate the
quality of the extracted features we use a simple random forest to classify
single cells. We show that thanks to the rich features extracted by the
autoencoder trained on only one of the datasets, the random forest classifier
performs satisfactorily on the unseen datasets, and outperforms published
oracle networks in the cross-domain task. Our results suggest the possibility
of employing this unsupervised approach in more complicated diagnosis and
prognosis tasks without the need to add expensive expert labels to unseen data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Far Can I Go ? : A Self-Supervised Approach for Deterministic Video Depth Forecasting. (arXiv:2207.00506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00506">
<div class="article-summary-box-inner">
<span><p>In this paper we present a novel self-supervised method to anticipate the
depth estimate for a future, unobserved real-world urban scene. This work is
the first to explore self-supervised learning for estimation of monocular depth
of future unobserved frames of a video. Existing works rely on a large number
of annotated samples to generate the probabilistic prediction of depth for
unseen frames. However, this makes it unrealistic due to its requirement for
large amount of annotated depth samples of video. In addition, the
probabilistic nature of the case, where one past can have multiple future
outcomes often leads to incorrect depth estimates. Unlike previous methods, we
model the depth estimation of the unobserved frame as a view-synthesis problem,
which treats the depth estimate of the unseen video frame as an auxiliary task
while synthesizing back the views using learned pose. This approach is not only
cost effective - we do not use any ground truth depth for training (hence
practical) but also deterministic (a sequence of past frames map to an
immediate future). To address this task we first develop a novel depth
forecasting network DeFNet which estimates depth of unobserved future by
forecasting latent features. Second, we develop a channel-attention based pose
estimation network that estimates the pose of the unobserved frame. Using this
learned pose, estimated depth map is reconstructed back into the image domain,
thus forming a self-supervised solution. Our proposed approach shows
significant improvements in Abs Rel metric compared to state-of-the-art
alternatives on both short and mid-term forecasting setting, benchmarked on
KITTI and Cityscapes. Code is available at
https://github.com/sauradip/depthForecasting
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ray-Space Motion Compensation for Lenslet Plenoptic Video Coding. (arXiv:2207.00522v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00522">
<div class="article-summary-box-inner">
<span><p>Plenoptic images and videos bearing rich information demand a tremendous
amount of data storage and high transmission cost. While there has been much
study on plenoptic image coding, investigations into plenoptic video coding
have been very limited. We investigate the motion compensation for plenoptic
video coding from a slightly different perspective by looking at the problem in
the ray-space domain instead of in the conventional pixel domain. Here, we
develop a novel motion compensation scheme for lenslet video under two
sub-cases of ray-space motion, that is, integer ray-space motion and fractional
ray-space motion. The proposed new scheme of light field motion-compensated
prediction is designed such that it can be easily integrated into well-known
video coding techniques such as HEVC. Experimental results compared to relevant
existing methods have shown remarkable compression efficiency with an average
gain of 19.63% and a peak gain of 29.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Autoencoders for Self-Supervised Learning on Automotive Point Clouds. (arXiv:2207.00531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00531">
<div class="article-summary-box-inner">
<span><p>Masked autoencoding has become a successful pre-training paradigm for
Transformer models for text, images, and recently, point clouds. Raw automotive
datasets are a suitable candidate for self-supervised pre-training as they
generally are cheap to collect compared to annotations for tasks like 3D object
detection (OD). However, development of masked autoencoders for point clouds
has focused solely on synthetic and indoor data. Consequently, existing methods
have tailored their representations and models toward point clouds which are
small, dense and have homogeneous point density. In this work, we study masked
autoencoding for point clouds in an automotive setting, which are sparse and
for which the point density can vary drastically among objects in the same
scene. To this end, we propose Voxel-MAE, a simple masked autoencoding
pre-training scheme designed for voxel representations. We pre-train the
backbone of a Transformer-based 3D object detector to reconstruct masked voxels
and to distinguish between empty and non-empty voxels. Our method improves the
3D OD performance by 1.75 mAP points and 1.05 NDS on the challenging nuScenes
dataset. Compared to existing self-supervised methods for automotive data,
Voxel-MAE displays up to $2\times$ performance increase. Further, we show that
by pre-training with Voxel-MAE, we require only 40% of the annotated data to
outperform a randomly initialized equivalent. Code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming Image Generation from Scene Graphs. (arXiv:2207.00545v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00545">
<div class="article-summary-box-inner">
<span><p>Generating images from semantic visual knowledge is a challenging task, that
can be useful to condition the synthesis process in complex, subtle, and
unambiguous ways, compared to alternatives such as class labels or text
descriptions. Although generative methods conditioned by semantic
representations exist, they do not provide a way to control the generation
process aside from the specification of constraints between objects. As an
example, the possibility to iteratively generate or modify images by manually
adding specific items is a desired property that, to our knowledge, has not
been fully investigated in the literature. In this work we propose a
transformer-based approach conditioned by scene graphs that, conversely to
recent transformer-based methods, also employs a decoder to autoregressively
compose images, making the synthesis process more effective and controllable.
The proposed architecture is composed by three modules: 1) a graph
convolutional network, to encode the relationships of the input graph; 2) an
encoder-decoder transformer, which autoregressively composes the output image;
3) an auto-encoder, employed to generate representations used as input/output
of each generation step by the transformer. Results obtained on CIFAR10 and
MNIST images show that our model is able to satisfy semantic constraints
defined by a scene graph and to model relations between visual objects in the
scene by taking into account a user-provided partial rendering of the desired
target.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How can spherical CNNs benefit ML-based diffusion MRI parameter estimation?. (arXiv:2207.00572v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00572">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates spherical convolutional neural networks (S-CNN) offer
distinct advantages over conventional fully-connected networks (FCN) at
estimating scalar parameters of tissue microstructure from diffusion MRI
(dMRI). Such microstructure parameters are valuable for identifying pathology
and quantifying its extent. However, current clinical practice commonly
acquires dMRI data consisting of only 6 diffusion weighted images (DWIs),
limiting the accuracy and precision of estimated microstructure indices.
Machine learning (ML) has been proposed to address this challenge. However,
existing ML-based methods are not robust to differing dMRI gradient sampling
schemes, nor are they rotation equivariant. Lack of robustness to sampling
schemes requires a new network to be trained for each scheme, complicating the
analysis of data from multiple sources. A possible consequence of the lack of
rotational equivariance is that the training dataset must contain a diverse
range of microstucture orientations. Here, we show spherical CNNs represent a
compelling alternative that is robust to new sampling schemes as well as
offering rotational equivariance. We show the latter can be leveraged to
decrease the number of training datapoints required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video + CLIP Baseline for Ego4D Long-term Action Anticipation. (arXiv:2207.00579v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00579">
<div class="article-summary-box-inner">
<span><p>In this report, we introduce our adaptation of image-text models for
long-term action anticipation. Our Video + CLIP framework makes use of a
large-scale pre-trained paired image-text model: CLIP and a video encoder
Slowfast network. The CLIP embedding provides fine-grained understanding of
objects relevant for an action whereas the slowfast network is responsible for
modeling temporal information within a video clip of few frames. We show that
the features obtained from both encoders are complementary to each other, thus
outperforming the baseline on Ego4D for the task of long-term action
anticipation. Our code is available at
github.com/srijandas07/clip_baseline_LTA_Ego4d.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kernelized Similarity Learning and Embedding for Dynamic Texture Synthesis. (arXiv:1911.04254v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.04254">
<div class="article-summary-box-inner">
<span><p>Dynamic texture (DT) exhibits statistical stationarity in the spatial domain
and stochastic repetitiveness in the temporal dimension, indicating that
different frames of DT possess a high similarity correlation that is critical
prior knowledge. However, existing methods cannot effectively learn a promising
synthesis model for high-dimensional DT from a small number of training data.
In this paper, we propose a novel DT synthesis method, which makes full use of
similarity prior knowledge to address this issue. Our method bases on the
proposed kernel similarity embedding, which not only can mitigate the
high-dimensionality and small sample issues, but also has the advantage of
modeling nonlinear feature relationship. Specifically, we first raise two
hypotheses that are essential for DT model to generate new frames using
similarity correlation. Then, we integrate kernel learning and extreme learning
machine into a unified synthesis model to learn kernel similarity embedding for
representing DT. Extensive experiments on DT videos collected from the internet
and two benchmark datasets, i.e., Gatech Graphcut Textures and Dyntex,
demonstrate that the learned kernel similarity embedding can effectively
exhibit the discriminative representation for DT. Accordingly, our method is
capable of preserving the long-term temporal continuity of the synthesized DT
sequences with excellent sustainability and generalization. Meanwhile, it
effectively generates realistic DT videos with fast speed and low computation,
compared with the state-of-the-art methods. The code and more synthesis videos
are available at our project page
https://shiming-chen.github.io/Similarity-page/Similarit.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topology-Aware Network Pruning using Multi-stage Graph Embedding and Reinforcement Learning. (arXiv:2102.03214v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03214">
<div class="article-summary-box-inner">
<span><p>Model compression is an essential technique for deploying deep neural
networks (DNNs) on power and memory-constrained resources. However, existing
model-compression methods often rely on human expertise and focus on
parameters' local importance, ignoring the rich topology information within
DNNs. In this paper, we propose a novel multi-stage graph embedding technique
based on graph neural networks (GNNs) to identify DNN topologies and use
reinforcement learning (RL) to find a suitable compression policy. We performed
resource-constrained (i.e., FLOPs) channel pruning and compared our approach
with state-of-the-art model compression methods. We evaluated our method on
various models from typical to mobile-friendly networks, such as ResNet family,
VGG-16, MobileNet-v1/v2, and ShuffleNet. Results show that our method can
achieve higher compression ratios with a minimal fine-tuning cost yet yields
outstanding and competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M4Depth: Monocular depth estimation for autonomous vehicles in unseen environments. (arXiv:2105.09847v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09847">
<div class="article-summary-box-inner">
<span><p>Estimating the distance to objects is crucial for autonomous vehicles when
using depth sensors is not possible. In this case, the distance has to be
estimated from on-board mounted RGB cameras, which is a complex task especially
in environments such as natural outdoor landscapes. In this paper, we present a
new method named M4Depth for depth estimation. First, we establish a bijective
relationship between depth and the visual disparity of two consecutive frames
and show how to exploit it to perform motion-invariant pixel-wise depth
estimation. Then, we detail M4Depth which is based on a pyramidal convolutional
neural network architecture where each level refines an input disparity map
estimate by using two customized cost volumes. We use these cost volumes to
leverage the visual spatio-temporal constraints imposed by motion and to make
the network robust for varied scenes. We benchmarked our approach both in test
and generalization modes on public datasets featuring synthetic camera
trajectories recorded in a wide variety of outdoor scenes. Results show that
our network outperforms the state of the art on these datasets, while also
performing well on a standard depth estimation benchmark. The code of our
method is publicly available at https://github.com/michael-fonder/M4Depth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSN: Multi-Style Network for Trajectory Prediction. (arXiv:2107.00932v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00932">
<div class="article-summary-box-inner">
<span><p>Trajectory prediction aims to forecast agents' possible future locations
considering their observations along with the video context. It is strongly
needed for many autonomous platforms like tracking, detection, robot
navigation, and self-driving cars. Whether it is agents' internal personality
factors, interactive behaviors with the neighborhood, or the influence of
surroundings, all of them might represent impacts on agents' future plannings.
However, many previous methods model and predict agents' behaviors with the
same strategy or feature distribution, making them challenging to give
predictions with sufficient style differences. This manuscript proposes the
Multi-Style Network (MSN), which utilizes style proposal and stylized
prediction two sub-networks, to give agents multi-style predictions in a novel
categorical way adaptively. The proposed network contains a series of style
channels, and each channel is bound to a unique and specific behavior style. In
detail, we use agents' end-point plannings and their interaction context as the
basis for the behavior classification, so as to adaptively learn multiple
diverse behavior styles through these channels. Then, we assume that the target
agents will plan their future behaviors according to each of these categorized
styles, thus utilizing different style channels to give potential predictions
with significant style differences in parallel. Experiments show that MSN
outperforms current state-of-the-art methods up to 10\% quantitatively on two
widely used datasets, and presents better multi-style characteristics
qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Branch with Attention Network for Hand-Based Person Recognition. (arXiv:2108.02234v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02234">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel hand-based person recognition method for
the purpose of criminal investigations since the hand image is often the only
available information in cases of serious crime such as sexual abuse. Our
proposed method, Multi-Branch with Attention Network (MBA-Net), incorporates
both channel and spatial attention modules in branches in addition to a global
(without attention) branch to capture global structural information for
discriminative feature learning. The attention modules focus on the relevant
features of the hand image while suppressing the irrelevant backgrounds. In
order to overcome the weakness of the attention mechanisms, equivariant to
pixel shuffling, we integrate relative positional encodings into the spatial
attention module to capture the spatial positions of pixels. Extensive
evaluations on two large multi-ethnic and publicly available hand datasets
demonstrate that our proposed method achieves state-of-the-art performance,
surpassing the existing hand-based identification methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Caption Generation on Scenes with Seen and Unseen Object Categories. (arXiv:2108.06165v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06165">
<div class="article-summary-box-inner">
<span><p>Image caption generation is one of the most challenging problems at the
intersection of vision and language domains. In this work, we propose a
realistic captioning task where the input scenes may incorporate visual objects
with no corresponding visual or textual training examples. For this problem, we
propose a detection-driven approach that consists of a single-stage generalized
zero-shot detection model to recognize and localize instances of both seen and
unseen classes, and a template-based captioning model that transforms
detections into sentences. To improve the generalized zero-shot detection
model, which provides essential information for captioning, we define effective
class representations in terms of class-to-class semantic similarities, and
leverage their special structure to construct an effective unseen/seen class
confidence score calibration mechanism. We also propose a novel evaluation
metric that provides additional insights for the captioning outputs by
separately measuring the visual and non-visual contents of generated sentences.
Our experiments highlight the importance of studying captioning in the proposed
zero-shot setting, and verify the effectiveness of the proposed
detection-driven zero-shot captioning approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Volumetric Transformer for Accurate 3D Tumor Segmentation. (arXiv:2111.13300v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13300">
<div class="article-summary-box-inner">
<span><p>We propose a Transformer architecture for volumetric segmentation, a
challenging task that requires keeping a complex balance in encoding local and
global spatial cues, and preserving information along all axes of the volume.
Encoder of the proposed design benefits from self-attention mechanism to
simultaneously encode local and global cues, while the decoder employs a
parallel self and cross attention formulation to capture fine details for
boundary refinement. Empirically, we show that the proposed design choices
result in a computationally efficient model, with competitive and promising
results on the Medical Segmentation Decathlon (MSD) brain tumor segmentation
(BraTS) Task. We further show that the representations learned by our model are
robust against data corruptions.
\href{https://github.com/himashi92/VT-UNet}{Our code implementation is publicly
available}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Semi-Supervised Learning for Video Action Detection. (arXiv:2203.04251v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04251">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on semi-supervised learning for video action detection
which utilizes both labeled as well as unlabeled data. We propose a simple
end-to-end consistency based approach which effectively utilizes the unlabeled
data. Video action detection requires both, action class prediction as well as
a spatio-temporal localization of actions. Therefore, we investigate two types
of constraints, classification consistency, and spatio-temporal consistency.
The presence of predominant background and static regions in a video makes it
challenging to utilize spatio-temporal consistency for action detection. To
address this, we propose two novel regularization constraints for
spatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness.
Both these aspects exploit the temporal continuity of action in videos and are
found to be effective for utilizing unlabeled videos for action detection. We
demonstrate the effectiveness of the proposed approach on two different action
detection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show
the effectiveness of the proposed approach for video object segmentation on the
Youtube-VOS which demonstrates its generalization capability The proposed
approach achieves competitive performance by using merely 20% of annotations on
UCF101-24 when compared with recent fully supervised methods. On UCF101-24, it
improves the score by +8.9% and +11% at 0.5 f-mAP and v-mAP respectively,
compared to supervised approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Quality Assessment for Magnetic Resonance Imaging. (arXiv:2203.07809v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07809">
<div class="article-summary-box-inner">
<span><p>Image quality assessment (IQA) algorithms aim to reproduce the human's
perception of the image quality. The growing popularity of image enhancement,
generation, and recovery models instigated the development of many methods to
assess their performance. However, most IQA solutions are designed to predict
image quality in the general domain, with the applicability to specific areas,
such as medical imaging, remaining questionable. Moreover, the selection of
these IQA metrics for a specific task typically involves intentionally induced
distortions, such as manually added noise or artificial blurring; yet, the
chosen metrics are then used to judge the output of real-life computer vision
models. In this work, we aspire to fill these gaps by carrying out the most
extensive IQA evaluation study for Magnetic Resonance Imaging (MRI) to date
(14,700 subjective scores). We use outputs of neural network models trained to
solve problems relevant to MRI, including image reconstruction in the scan
acceleration, motion correction, and denoising. Our emphasis is on reflecting
the radiologist's perception of the reconstructed images, gauging the most
diagnostically influential criteria for the quality of MRI scans:
signal-to-noise ratio, contrast-to-noise ratio, and the presence of artifacts.
Seven trained radiologists assess these distorted images, with their verdicts
then correlated with 35 different image quality metrics (full-reference,
no-reference, and distribution-based metrics considered). The top performers --
DISTS, HaarPSI, VSI, and FID-VGG16 -- are found to be efficient across three
proposed quality criteria, for all considered anatomies and the target tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Reasoning Meets Visual Representation Learning: A Prospective Study. (arXiv:2204.12037v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12037">
<div class="article-summary-box-inner">
<span><p>Visual representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions and ignore the essential causal relations behind
the multi-modal knowledge, which lacks an unified guidance and analysis about
why modern visual representation learning methods are easily collapse into data
bias and have limited generalization and cognitive abilities. Inspired by the
strong inference ability of human-level agents, recent years have therefore
witnessed great effort in developing causal reasoning paradigms to realize
robust representation and model learning with good cognitive ability. In this
paper, we conduct a comprehensive review of existing causal reasoning methods
for visual representation learning, covering fundamental theories, models, and
datasets. The limitations of current methods and datasets are also discussed.
Moreover, we propose some prospective challenges, opportunities, and future
research directions for benchmarking causal reasoning algorithms in visual
representation learning. This paper aims to provide a comprehensive overview of
this emerging field, attract attention, encourage discussions, bring to the
forefront the urgency of developing novel causal reasoning methods, publicly
available benchmarks, and consensus-building standards for reliable visual
representation learning and related real-world applications more efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations. (arXiv:2205.06779v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06779">
<div class="article-summary-box-inner">
<span><p>Recently, weakly-supervised image segmentation using weak annotations like
scribbles has gained great attention, since such annotations are much easier to
obtain compared to time-consuming and label-intensive labeling at the
pixel/voxel level. However, because scribbles lack structure information of
region of interest (ROI), existing scribble-based methods suffer from poor
boundary localization. Furthermore, most current methods are designed for 2D
image segmentation, which do not fully leverage the volumetric information if
directly applied to image slices. In this paper, we propose a scribble-based
volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image
segmentation and improves boundary prediction. To achieve this, we augment a
2.5D attention UNet with a proposed label propagation module to extend semantic
information from scribbles and a combination of static and active boundary
prediction to learn ROI's boundary and regularize its shape. Extensive
experiments on three public datasets demonstrate Scribble2D5 significantly
outperforms current scribble-based methods and approaches the performance of
fully-supervised ones. Our code is available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints. (arXiv:2205.10636v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10636">
<div class="article-summary-box-inner">
<span><p>Structured representations such as keypoints are widely used in pose
transfer, conditional image generation, animation, and 3D reconstruction.
However, their supervised learning requires expensive annotation for each
target domain. We propose a self-supervised method that learns to disentangle
object structure from the appearance with a graph of 2D keypoints linked by
straight edges. Both the keypoint location and their pairwise edge weights are
learned, given only a collection of images depicting the same object class. The
graph is interpretable, for example, AutoLink recovers the human skeleton
topology when applied to images showing people. Our key ingredients are i) an
encoder that predicts keypoint locations in an input image, ii) a shared graph
as a latent variable that links the same pairs of keypoints in every image,
iii) an intermediate edge map that combines the latent graph edge weights and
keypoint locations in a soft, differentiable manner, and iv) an inpainting
objective on randomly masked images. Although simpler, AutoLink outperforms
existing self-supervised methods on the established keypoint and pose
estimation benchmarks and paves the way for structure-conditioned generative
models on more diverse datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask2Hand: Learning to Predict the 3D Hand Pose and Shape from Shadow. (arXiv:2205.15553v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15553">
<div class="article-summary-box-inner">
<span><p>We present a self-trainable method, Mask2Hand, which learns to solve the
challenging task of predicting 3D hand pose and shape from a 2D binary mask of
hand silhouette/shadow without additional manually-annotated data. Given the
intrinsic camera parameters and the parametric hand model in the camera space,
we adopt the differentiable rendering technique to project 3D estimations onto
the 2D binary silhouette space. By applying a tailored combination of losses
between the rendered silhouette and the input binary mask, we are able to
integrate the self-guidance mechanism into our end-to-end optimization process
for constraining global mesh registration and hand pose estimation. The
experiments show that our method, which takes a single binary mask as the
input, can achieve comparable prediction accuracy on both unaligned and aligned
settings as state-of-the-art methods that require RGB or depth inputs. Our code
is available at https://github.com/lijenchang/Mask2Hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BANet: Motion Forecasting with Boundary Aware Network. (arXiv:2206.07934v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07934">
<div class="article-summary-box-inner">
<span><p>We propose a motion forecasting model called BANet, which means
Boundary-Aware Network, and it is a variant of LaneGCN. We believe that it is
not enough to use only the lane centerline as input to obtain the embedding
features of the vector map nodes. The lane centerline can only provide the
topology of the lanes, and other elements of the vector map also contain rich
information. For example, the lane boundary can provide traffic rule constraint
information such as whether it is possible to change lanes which is very
important. Therefore, we achieved better performance by encoding more vector
map elements in the motion forecasting model.We report our results on the 2022
Argoverse2 Motion Forecasting challenge and rank 1st on the test leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning. (arXiv:2206.12596v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12596">
<div class="article-summary-box-inner">
<span><p>Deformable image registration is a crucial step in medical image analysis for
finding a non-linear spatial transformation between a pair of fixed and moving
images. Deep registration methods based on Convolutional Neural Networks (CNNs)
have been widely used as they can perform image registration in a fast and
end-to-end manner. However, these methods usually have limited performance for
image pairs with large deformations. Recently, iterative deep registration
methods have been used to alleviate this limitation, where the transformations
are iteratively learned in a coarse-to-fine manner. However, iterative methods
inevitably prolong the registration runtime, and tend to learn separate image
features for each iteration, which hinders the features from being leveraged to
facilitate the registration at later iterations. In this study, we propose a
Non-Iterative Coarse-to-finE registration Network (NICE-Net) for deformable
image registration. In the NICE-Net, we propose: (i) a Single-pass Deep
Cumulative Learning (SDCL) decoder that can cumulatively learn coarse-to-fine
transformations within a single pass (iteration) of the network, and (ii) a
Selectively-propagated Feature Learning (SFL) encoder that can learn common
image features for the whole coarse-to-fine registration process and
selectively propagate the features as needed. Extensive experiments on six
public datasets of 3D brain Magnetic Resonance Imaging (MRI) show that our
proposed NICE-Net can outperform state-of-the-art iterative deep registration
methods while only requiring similar runtime to non-iterative methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition. (arXiv:2206.13559v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13559">
<div class="article-summary-box-inner">
<span><p>Capitalizing on large pre-trained models for various downstream tasks of
interest have recently emerged with promising performance. Due to the
ever-growing model size, the standard full fine-tuning based task adaptation
strategy becomes prohibitively costly in terms of model training and storage.
This has led to a new research direction in parameter-efficient transfer
learning. However, existing attempts typically focus on downstream tasks from
the same modality (e.g., image understanding) of the pre-trained model. This
creates a limit because in some specific modalities, (e.g., video
understanding) such a strong pre-trained model with sufficient knowledge is
less or not available. In this work, we investigate such a novel cross-modality
transfer learning setting, namely parameter-efficient image-to-video transfer
learning. To solve this problem, we propose a new Spatio-Temporal Adapter
(ST-Adapter) for parameter-efficient fine-tuning per video task. With a
built-in spatio-temporal reasoning capability in a compact design, ST-Adapter
enables a pre-trained image model without temporal knowledge to reason about
dynamic video content at a small (~8%) per-task parameter cost, requiring
approximately 20 times fewer updated parameters compared to previous work.
Extensive experiments on video action recognition tasks show that our
ST-Adapter can match or even outperform the strong full fine-tuning strategy
and state-of-the-art video models, whilst enjoying the advantage of parameter
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTrGAN: Cycle Transformers GAN for Gait Transfer. (arXiv:2206.15248v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15248">
<div class="article-summary-box-inner">
<span><p>We attempt for the first time to address the problem of gait transfer. In
contrast to motion transfer, the objective here is not to imitate the source's
normal motions, but rather to transform the source's motion into a typical gait
pattern for the target. Using gait recognition models, we demonstrate that
existing techniques yield a discrepancy that can be easily detected. We
introduce a novel model, Cycle Transformers GAN (CTrGAN), that can successfully
generate the target's natural gait. CTrGAN's generators consist of a decoder
and encoder, both Transformers, where the attention is on the temporal domain
between complete images rather than the spatial domain between patches. While
recent Transformer studies in computer vision mainly focused on discriminative
tasks, we introduce an architecture that can be applied to synthesis tasks.
Using a widely-used gait recognition dataset, we demonstrate that our approach
is capable of producing over an order of magnitude more realistic personalized
gaits than existing methods, even when used with sources that were not
available during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolarFormer: Multi-camera 3D Object Detection with Polar Transformers. (arXiv:2206.15398v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15398">
<div class="article-summary-box-inner">
<span><p>3D object detection in autonomous driving aims to reason "what" and "where"
the objects of interest present in a 3D world. Following the conventional
wisdom of previous 2D object detection, existing methods often adopt the
canonical Cartesian coordinate system with perpendicular axis. However, we
conjugate that this does not fit the nature of the ego car's perspective, as
each onboard camera perceives the world in shape of wedge intrinsic to the
imaging geometry with radical (non-perpendicular) axis. Hence, in this paper we
advocate the exploitation of the Polar coordinate system and propose a new
Polar Transformer (PolarFormer) for more accurate 3D object detection in the
bird's-eye-view (BEV) taking as input only multi-camera 2D images.
Specifically, we design a cross attention based Polar detection head without
restriction to the shape of input structure to deal with irregular Polar grids.
For tackling the unconstrained object scale variations along Polar's distance
dimension, we further introduce a multi-scalePolar representation learning
strategy. As a result, our model can make best use of the Polar representation
rasterized via attending to the corresponding image observation in a
sequence-to-sequence fashion subject to the geometric constraints. Thorough
experiments on the nuScenes dataset demonstrate that our PolarFormer
outperforms significantly state-of-the-art 3D object detection alternatives, as
well as yielding competitive performance on BEV semantic segmentation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image features of a splashing drop on a solid surface extracted using a feedforward neural network. (arXiv:2201.09541v1 [physics.flu-dyn] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09541">
<div class="article-summary-box-inner">
<span><p>This article reports nonintuitive characteristic of a splashing drop on a
solid surface discovered through extracting image features using a feedforward
neural network (FNN). Ethanol of area-equivalent radius about 1.29 mm was
dropped from impact heights ranging from 4 cm to 60 cm (splashing threshold 20
cm) and impacted on a hydrophilic surface. The images captured when half of the
drop impacted the surface were labeled according to their outcome, splashing or
nonsplashing, and were used to train an FNN. A classification accuracy higher
than 96% was achieved. To extract the image features identified by the FNN for
classification, the weight matrix of the trained FNN for identifying splashing
drops was visualized. Remarkably, the visualization showed that the trained FNN
identified the contour height of the main body of the impacting drop as an
important characteristic differentiating between splashing and nonsplashing
drops, which has not been reported in previous studies. This feature was found
throughout the impact, even when one and three-quarters of the drop impacted
the surface. To confirm the importance of this image feature, the FNN was
retrained to classify using only the main body without checking for the
presence of ejected secondary droplets. The accuracy was still higher than 82%,
confirming that the contour height is an important feature distinguishing
splashing from nonsplashing drops. Several aspects of drop impact are analyzed
and discussed with the aim of identifying the possible mechanism underlying the
difference in contour height between splashing and nonsplashing drops.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-04 23:07:38.820625721 UTC">2022-07-04 23:07:38 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>