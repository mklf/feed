<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-22T01:30:00Z">10-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents. (arXiv:2110.10150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10150">
<div class="article-summary-box-inner">
<span><p>Text summarization is an essential task to help readers capture salient
information from documents, news, interviews, and meetings. However, most
state-of-the-art pretrained language models are unable to efficiently process
long text commonly seen in the summarization problem domain. In this paper, we
propose Summ^N, a simple, flexible, and effective multi-stage framework for
input texts that are longer than the maximum context lengths of typical
pretrained LMs. Summ^N first generates the coarse summary in multiple stages
and then produces the final fine-grained summary based on them. The framework
can process input text of arbitrary length by adjusting the number of stages
while keeping the LM context size fixed. Moreover, it can deal with both
documents and dialogues and can be used on top of any underlying backbone
abstractive summarization model. Our experiments demonstrate that Summ^N
significantly outperforms previous state-of-the-art methods by improving ROUGE
scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two
long TV series datasets from SummScreen, and a newly proposed long document
summarization dataset GovReport. Our data and code are available at
https://github.com/chatc/Summ-N.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenNI: Human-AI Collaboration for Data-Backed Text Generation. (arXiv:2110.10185v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10185">
<div class="article-summary-box-inner">
<span><p>Table2Text systems generate textual output based on structured data utilizing
machine learning. These systems are essential for fluent natural language
interfaces in tools such as virtual assistants; however, left to generate
freely these ML systems often produce misleading or unexpected outputs. GenNI
(Generation Negotiation Interface) is an interactive visual system for
high-level human-AI collaboration in producing descriptive text. The tool
utilizes a deep learning model designed with explicit control states. These
controls allow users to globally constrain model generations, without
sacrificing the representation power of the deep learning models. The visual
interface makes it possible for users to interact with AI systems following a
Refine-Forecast paradigm to ensure that the generation system acts in a manner
human users find suitable. We report multiple use cases on two experiments that
improve over uncontrolled generation approaches, while at the same time
providing fine-grained control. A demo and source code are available at
https://genni.vizhub.ai .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StructFormer: Learning Spatial Structure for Language-Guided Semantic Rearrangement of Novel Objects. (arXiv:2110.10189v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10189">
<div class="article-summary-box-inner">
<span><p>Geometric organization of objects into semantically meaningful arrangements
pervades the built world. As such, assistive robots operating in warehouses,
offices, and homes would greatly benefit from the ability to recognize and
rearrange objects into these semantically meaningful structures. To be useful,
these robots must contend with previously unseen objects and receive
instructions without significant programming. While previous works have
examined recognizing pairwise semantic relations and sequential manipulation to
change these simple relations none have shown the ability to arrange objects
into complex structures such as circles or table settings. To address this
problem we propose a novel transformer-based neural network, StructFormer,
which takes as input a partial-view point cloud of the current object
arrangement and a structured language command encoding the desired object
configuration. We show through rigorous experiments that StructFormer enables a
physical robot to rearrange novel objects into semantically meaningful
structures with multi-object relational constraints inferred from the language
command.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Medication Extraction: A Comparison of Recent Models in Supervised and Semi-supervised Learning Settings. (arXiv:2110.10213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10213">
<div class="article-summary-box-inner">
<span><p>Drug prescriptions are essential information that must be encoded in
electronic medical records. However, much of this information is hidden within
free-text reports. This is why the medication extraction task has emerged. To
date, most of the research effort has focused on small amount of data and has
only recently considered deep learning methods. In this paper, we present an
independent and comprehensive evaluation of state-of-the-art neural
architectures on the I2B2 medical prescription extraction task both in the
supervised and semi-supervised settings. The study shows the very competitive
performance of simple DNN models on the task as well as the high interest of
pre-trained models. Adapting the latter models on the I2B2 dataset enables to
push medication extraction performances above the state-of-the-art. Finally,
the study also confirms that semi-supervised techniques are promising to
leverage large amounts of unlabeled data in particular in low resource setting
when labeled data is too costly to acquire.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Domain Specific Language Models for Automatic Speech Recognition through Machine Translation. (arXiv:2110.10261v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10261">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems have been gaining popularity in
the recent years for their widespread usage in smart phones and speakers.
Building ASR systems for task-specific scenarios is subject to the availability
of utterances that adhere to the style of the task as well as the language in
question. In our work, we target such a scenario wherein task-specific text
data is available in a language that is different from the target language in
which an ASR Language Model (LM) is expected. We use Neural Machine Translation
(NMT) as an intermediate step to first obtain translations of the task-specific
text data. We then train LMs on the 1-best and N-best translations and study
ways to improve on such a baseline LM. We develop a procedure to derive word
confusion networks from NMT beam search graphs and evaluate LMs trained on
these confusion networks. With experiments on the WMT20 chat translation task
dataset, we demonstrate that NMT confusion networks can help to reduce the
perplexity of both n-gram and recurrent neural network LMs compared to those
trained only on N-best translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction. (arXiv:2110.10318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10318">
<div class="article-summary-box-inner">
<span><p>We evaluate a simple approach to improving zero-shot multilingual transfer of
mBERT on social media corpus by adding a pretraining task called translation
pair prediction (TPP), which predicts whether a pair of cross-lingual texts are
a valid translation. Our approach assumes access to translations (exact or
approximate) between source-target language pairs, where we fine-tune a model
on source language task data and evaluate the model in the target language. In
particular, we focus on language pairs where transfer learning is difficult for
mBERT: those where source and target languages are different in script,
vocabulary, and linguistic typology. We show improvements from TPP pretraining
over mBERT alone in zero-shot transfer from English to Hindi, Arabic, and
Japanese on two social media tasks: NER (a 37% average relative improvement in
F1 across target languages) and sentiment classification (12% relative
improvement in F1) on social media text, while also benchmarking on a
non-social media task of Universal Dependency POS tagging (6.7% relative
improvement in accuracy). Our results are promising given the lack of social
media bitext corpus. Our code can be found at:
https://github.com/twitter-research/multilingual-alignment-tpp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMSOC: An Approach for Socially Sensitive Pretraining. (arXiv:2110.10319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10319">
<div class="article-summary-box-inner">
<span><p>While large-scale pretrained language models have been shown to learn
effective linguistic representations for many NLP tasks, there remain many
real-world contextual aspects of language that current approaches do not
capture. For instance, consider a cloze-test "I enjoyed the ____ game this
weekend": the correct answer depends heavily on where the speaker is from, when
the utterance occurred, and the speaker's broader social milieu and
preferences. Although language depends heavily on the geographical, temporal,
and other social contexts of the speaker, these elements have not been
incorporated into modern transformer-based language models. We propose a simple
but effective approach to incorporate speaker social context into the learned
representations of large-scale language models. Our method first learns dense
representations of social contexts using graph representation learning
algorithms and then primes language model pretraining with these social context
representations. We evaluate our approach on geographically-sensitive
language-modeling tasks and show a substantial improvement (more than 100%
relative lift on MRR) compared to baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R$^3$Net:Relation-embedded Representation Reconstruction Network for Change Captioning. (arXiv:2110.10328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10328">
<div class="article-summary-box-inner">
<span><p>Change captioning is to use a natural language sentence to describe the
fine-grained disagreement between two similar images. Viewpoint change is the
most typical distractor in this task, because it changes the scale and location
of the objects and overwhelms the representation of real change. In this paper,
we propose a Relation-embedded Representation Reconstruction Network (R$^3$Net)
to explicitly distinguish the real change from the large amount of clutter and
irrelevant changes. Specifically, a relation-embedded module is first devised
to explore potential changed objects in the large amount of clutter. Then,
based on the semantic similarities of corresponding locations in the two
images, a representation reconstruction module (RRM) is designed to learn the
reconstruction representation and further model the difference representation.
Besides, we introduce a syntactic skeleton predictor (SSP) to enhance the
semantic interaction between change localization and caption generation.
Extensive experiments show that the proposed method achieves the
state-of-the-art results on two public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training. (arXiv:2110.10329v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10329">
<div class="article-summary-box-inner">
<span><p>Unsupervised pre-training is now the predominant approach for both text and
speech understanding. Self-attention models pre-trained on large amounts of
unannotated data have been hugely successful when fine-tuned on downstream
tasks from a variety of domains and languages. This paper takes the
universality of unsupervised language pre-training one step further, by
unifying speech and text pre-training within a single model. We build a single
encoder with the BERT objective on unlabeled text together with the w2v-BERT
objective on unlabeled speech. To further align our model representations
across modalities, we leverage alignment losses, specifically Translation
Language Modeling (TLM) and Speech Text Matching (STM) that make use of
supervised speech-text recognition data. We demonstrate that incorporating both
speech and text data during pre-training can significantly improve downstream
quality on CoVoST~2 speech translation, by around 1 BLEU compared to
single-modality pre-trained models, while retaining close to SotA performance
on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and
text-normalization, we observe evidence of capacity limitations and
interference between the two modalities, leading to degraded performance
compared to an equivalent text-only model, while still being competitive with
BERT. Through extensive empirical analysis we also demonstrate the importance
of the choice of objective function for speech pre-training, and the beneficial
effect of adding additional supervised signals on the quality of the learned
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">News-based Business Sentiment and its Properties as an Economic Index. (arXiv:2110.10340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10340">
<div class="article-summary-box-inner">
<span><p>This paper presents an approach to measuring business sentiment based on
textual data. Business sentiment has been measured by traditional surveys,
which are costly and time-consuming to conduct. To address the issues, we take
advantage of daily newspaper articles and adopt a self-attention-based model to
define a business sentiment index, named S-APIR, where outlier detection models
are investigated to properly handle various genres of news articles. Moreover,
we propose a simple approach to temporally analyzing how much any given event
contributed to the predicted business sentiment index. To demonstrate the
validity of the proposed approach, an extensive analysis is carried out on 12
years' worth of newspaper articles. The analysis shows that the S-APIR index is
strongly and positively correlated with established survey-based index (up to
correlation coefficient r=0.937) and that the outlier detection is effective
especially for a general newspaper. Also, S-APIR is compared with a variety of
economic indices, revealing the properties of S-APIR that it reflects the trend
of the macroeconomy as well as the economic outlook and sentiment of economic
agents. Moreover, to illustrate how S-APIR could benefit economists and
policymakers, several events are analyzed with respect to their impacts on
business sentiment over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Aspect-guided Explanation Generation for Explainable Recommendation. (arXiv:2110.10358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10358">
<div class="article-summary-box-inner">
<span><p>Explainable recommendation systems provide explanations for recommendation
results to improve their transparency and persuasiveness. The existing
explainable recommendation methods generate textual explanations without
explicitly considering the user's preferences on different aspects of the item.
In this paper, we propose a novel explanation generation framework, named
Hierarchical Aspect-guided explanation Generation (HAG), for explainable
recommendation. Specifically, HAG employs a review-based syntax graph to
provide a unified view of the user/item details. An aspect-guided graph pooling
operator is proposed to extract the aspect-relevant information from the
review-based syntax graphs to model the user's preferences on an item at the
aspect level. Then, a hierarchical explanation decoder is developed to generate
aspects and aspect-relevant explanations based on the attention mechanism. The
experimental results on three real datasets indicate that HAG outperforms
state-of-the-art explanation generation methods in both single-aspect and
multi-aspect explanation generation tasks, and also achieves comparable or even
better preference prediction accuracy than strong baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributionally Robust Classifiers in Sentiment Analysis. (arXiv:2110.10372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10372">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose sentiment classification models based on BERT
integrated with DRO (Distributionally Robust Classifiers) to improve model
performance on datasets with distributional shifts. We added 2-Layer Bi-LSTM,
projection layer (onto simplex or Lp ball), and linear layer on top of BERT to
achieve distributionally robustness. We considered one form of distributional
shift (from IMDb dataset to Rotten Tomatoes dataset). We have confirmed through
experiments that our DRO model does improve performance on our test set with
distributional shift from the training set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge distillation from language model to acoustic model: a hierarchical multi-task learning approach. (arXiv:2110.10429v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10429">
<div class="article-summary-box-inner">
<span><p>The remarkable performance of the pre-trained language model (LM) using
self-supervised learning has led to a major paradigm shift in the study of
natural language processing. In line with these changes, leveraging the
performance of speech recognition systems with massive deep learning-based LMs
is a major topic of speech recognition research. Among the various methods of
applying LMs to speech recognition systems, in this paper, we focus on a
cross-modal knowledge distillation method that transfers knowledge between two
types of deep neural networks with different modalities. We propose an acoustic
model structure with multiple auxiliary output layers for cross-modal
distillation and demonstrate that the proposed method effectively compensates
for the shortcomings of the existing label-interpolation-based distillation
method. In addition, we extend the proposed method to a hierarchical
distillation method using LMs trained in different units (senones, monophones,
and subwords) and reveal the effectiveness of the hierarchical distillation
method through an ablation study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discontinuous Grammar as a Foreign Language. (arXiv:2110.10431v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10431">
<div class="article-summary-box-inner">
<span><p>In order to achieve deep natural language understanding, syntactic
constituent parsing is a vital step, highly demanded by many artificial
intelligence systems to process both text and speech. One of the most recent
proposals is the use of standard sequence-to-sequence models to perform
constituent parsing as a machine translation task, instead of applying
task-specific parsers. While they show a competitive performance, these
text-to-parse transducers are still lagging behind classic techniques in terms
of accuracy, coverage and speed. To close the gap, we here extend the framework
of sequence-to-sequence models for constituent parsing, not only by providing a
more powerful neural architecture for improving their performance, but also by
enlarging their coverage to handle the most complex syntactic phenomena:
discontinuous structures. To that end, we design several novel linearizations
that can fully produce discontinuities and, for the first time, we test a
sequence-to-sequence model on the main discontinuous benchmarks, obtaining
competitive results on par with task-specific discontinuous constituent parsers
and achieving state-of-the-art scores on the (discontinuous) English Penn
Treebank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph informed Fake News Classification via Heterogeneous Representation Ensembles. (arXiv:2110.10457v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10457">
<div class="article-summary-box-inner">
<span><p>Increasing amounts of freely available data both in textual and relational
form offers exploration of richer document representations, potentially
improving the model performance and robustness. An emerging problem in the
modern era is fake news detection -- many easily available pieces of
information are not necessarily factually correct, and can lead to wrong
conclusions or are used for manipulation. In this work we explore how different
document representations, ranging from simple symbolic bag-of-words, to
contextual, neural language model-based ones can be used for efficient fake
news identification. One of the key contributions is a set of novel document
representation learning methods based solely on knowledge graphs, i.e.
extensive collections of (grounded) subject-predicate-object triplets. We
demonstrate that knowledge graph-based representations already achieve
competitive performance to conventionally accepted representation learners.
Furthermore, when combined with existing, contextual representations, knowledge
graph-based document representations can achieve state-of-the-art performance.
To our knowledge this is the first larger-scale evaluation of how knowledge
graph-based representations can be systematically incorporated into the process
of fake news classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Deep Learning Models in Natural Language Processing: A Review. (arXiv:2110.10470v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10470">
<div class="article-summary-box-inner">
<span><p>Neural network models have achieved state-of-the-art performances in a wide
range of natural language processing (NLP) tasks. However, a long-standing
criticism against neural network models is the lack of interpretability, which
not only reduces the reliability of neural NLP systems but also limits the
scope of their applications in areas where interpretability is essential (e.g.,
health care applications). In response, the increasing interest in interpreting
neural NLP models has spurred a diverse array of interpretation methods over
recent years. In this survey, we provide a comprehensive review of various
interpretation methods for neural models in NLP. We first stretch out a
high-level taxonomy for interpretation methods in NLP, i.e., training-based
approaches, test-based approaches, and hybrid approaches. Next, we describe
sub-categories in each category in detail, e.g., influence-function based
methods, KNN-based methods, attention-based models, saliency-based methods,
perturbation-based methods, etc. We point out deficiencies of current methods
and suggest some avenues for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Unsupervised Neural Machine Translation with Denoising Adapters. (arXiv:2110.10472v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10472">
<div class="article-summary-box-inner">
<span><p>We consider the problem of multilingual unsupervised machine translation,
translating to and from languages that only have monolingual data by using
auxiliary parallel language pairs. For this problem the standard procedure so
far to leverage the monolingual data is back-translation, which is
computationally costly and hard to tune.
</p>
<p>In this paper we propose instead to use denoising adapters, adapter layers
with a denoising objective, on top of pre-trained mBART-50. In addition to the
modularity and flexibility of such an approach we show that the resulting
translations are on-par with back-translating as measured by BLEU, and
furthermore it allows adding unseen languages incrementally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning in Multilingual NMT via Language-Specific Embeddings. (arXiv:2110.10478v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10478">
<div class="article-summary-box-inner">
<span><p>This paper proposes a technique for adding a new source or target language to
an existing multilingual NMT model without re-training it on the initial set of
languages. It consists in replacing the shared vocabulary with a small
language-specific vocabulary and fine-tuning the new embeddings on the new
language's parallel data. Some additional language-specific components may be
trained to improve performance (e.g., Transformer layers or adapter modules).
Because the parameters of the original model are not modified, its performance
on the initial languages does not degrade. We show on two sets of experiments
(small-scale on TED Talks, and large-scale on ParaCrawl) that this approach
performs as well or better as the more costly alternatives; and that it has
excellent zero-shot performance: training on English-centric data is enough to
translate between the new language and any of the initial languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SocialVisTUM: An Interactive Visualization Toolkit for Correlated Neural Topic Models on Social Media Opinion Mining. (arXiv:2110.10575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10575">
<div class="article-summary-box-inner">
<span><p>Recent research in opinion mining proposed word embedding-based topic
modeling methods that provide superior coherence compared to traditional topic
modeling. In this paper, we demonstrate how these methods can be used to
display correlated topic models on social media texts using SocialVisTUM, our
proposed interactive visualization toolkit. It displays a graph with topics as
nodes and their correlations as edges. Further details are displayed
interactively to support the exploration of large text collections, e.g.,
representative words and sentences of topics, topic and sentiment
distributions, hierarchical topic clustering, and customizable, predefined
topic labels. The toolkit optimizes automatically on custom data for optimal
coherence. We show a working instance of the toolkit on data crawled from
English social media discussions about organic food consumption. The
visualization confirms findings of a qualitative consumer research study.
SocialVisTUM and its training procedures are accessible online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the 2021 Key Point Analysis Shared Task. (arXiv:2110.10577v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10577">
<div class="article-summary-box-inner">
<span><p>We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point
analysis that we organized as a part of the 8th Workshop on Argument Mining
(ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the
results of the shared task. We expect the task and the findings reported in
this paper to be relevant for researchers working on text summarization and
argument mining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SILG: The Multi-environment Symbolic Interactive Language Grounding Benchmark. (arXiv:2110.10661v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10661">
<div class="article-summary-box-inner">
<span><p>Existing work in language grounding typically study single environments. How
do we build unified models that apply across multiple environments? We propose
the multi-environment Symbolic Interactive Language Grounding benchmark (SILG),
which unifies a collection of diverse grounded language learning environments
under a common interface. SILG consists of grid-world environments that require
generalization to new dynamics, entities, and partially observed worlds (RTFM,
Messenger, NetHack), as well as symbolic counterparts of visual worlds that
require interpreting rich natural language with respect to complex scenes
(ALFWorld, Touchdown). Together, these environments provide diverse grounding
challenges in richness of observation space, action space, language
specification, and plan complexity. In addition, we propose the first shared
model architecture for RL on these environments, and evaluate recent advances
such as egocentric local convolution, recurrent state-tracking, entity-centric
attention, and pretrained LM using SILG. Our shared architecture achieves
comparable performance to environment-specific architectures. Moreover, we find
that many recent modelling advances do not result in significant gains on
environments other than the one they were designed for. This highlights the
need for a multi-environment benchmark. Finally, the best models significantly
underperform humans on SILG, which suggests ample room for future work. We hope
SILG enables the community to quickly identify new methodologies for language
grounding that generalize to a diverse set of environments and their associated
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer. (arXiv:2110.10668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10668">
<div class="article-summary-box-inner">
<span><p>While the field of style transfer (ST) has been growing rapidly, it has been
hampered by a lack of standardized practices for automatic evaluation. In this
paper, we evaluate leading ST automatic metrics on the oft-researched task of
formality style transfer. Unlike previous evaluations, which focus solely on
English, we expand our focus to Brazilian-Portuguese, French, and Italian,
making this work the first multilingual evaluation of metrics in ST. We outline
best practices for automatic evaluation in (formality) style transfer and
identify several models that correlate well with human judgments and are robust
across languages. We hope that this work will help accelerate development in
ST, where human evaluation is often challenging to collect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Self-Explainable Stylish Image Captioning Framework via Multi-References. (arXiv:2110.10704v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10704">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to build a stylish image captioning model through a
Multi-style Multi modality mechanism (2M). We demonstrate that with 2M, we can
build an effective stylish captioner and that multi-references produced by the
model can also support explaining the model through identifying erroneous input
features on faulty examples. We show how this 2M mechanism can be used to build
stylish captioning models and show how these models can be utilized to provide
explanations of likely errors in the models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better than Average: Paired Evaluation of NLP Systems. (arXiv:2110.10746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10746">
<div class="article-summary-box-inner">
<span><p>Evaluation in NLP is usually done by comparing the scores of competing
systems independently averaged over a common set of test instances. In this
work, we question the use of averages for aggregating evaluation scores into a
final number used to decide which system is best, since the average, as well as
alternatives such as the median, ignores the pairing arising from the fact that
systems are evaluated on the same test instances. We illustrate the importance
of taking the instance-level pairing of evaluation scores into account and
demonstrate, both theoretically and empirically, the advantages of aggregation
methods based on pairwise comparisons, such as the Bradley-Terry (BT) model, a
mechanism based on the estimated probability that a given system scores better
than another on the test set. By re-evaluating 296 real NLP evaluation setups
across four tasks and 18 evaluation metrics, we show that the choice of
aggregation mechanism matters and yields different conclusions as to which
systems are state of the art in about 30% of the setups. To facilitate the
adoption of pairwise evaluation, we release a practical tool for performing the
full analysis of evaluation scores with the mean, median, BT, and two variants
of BT (Elo and TrueSkill), alongside functionality for appropriate statistical
testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation. (arXiv:2110.10774v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10774">
<div class="article-summary-box-inner">
<span><p>Generating texts in scientific papers requires not only capturing the content
contained within the given input but also frequently acquiring the external
information called \textit{context}. We push forward the scientific text
generation by proposing a new task, namely \textbf{context-aware text
generation} in the scientific domain, aiming at exploiting the contributions of
context in generated texts. To this end, we present a novel challenging
large-scale \textbf{Sci}entific Paper Dataset for Conte\textbf{X}t-Aware Text
\textbf{Gen}eration (SciXGen), consisting of well-annotated 205,304 papers with
full references to widely-used objects (e.g., tables, figures, algorithms) in a
paper. We comprehensively benchmark, using state-of-the-arts, the efficacy of
our newly constructed SciXGen dataset in generating description and paragraph.
Our dataset and benchmarks will be made publicly available to hopefully
facilitate the scientific text generation research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Document Representation Learning with Graph Attention Networks. (arXiv:2110.10778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10778">
<div class="article-summary-box-inner">
<span><p>Recent progress in pretrained Transformer-based language models has shown
great success in learning contextual representation of text. However, due to
the quadratic self-attention complexity, most of the pretrained Transformers
models can only handle relatively short text. It is still a challenge when it
comes to modeling very long documents. In this work, we propose to use a graph
attention network on top of the available pretrained Transformers model to
learn document embeddings. This graph attention network allows us to leverage
the high-level semantic structure of the document. In addition, based on our
graph document model, we design a simple contrastive learning strategy to
pretrain our models on a large amount of unlabeled corpus. Empirically, we
demonstrate the effectiveness of our approaches in document classification and
document retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Open Natural Language Processing Development Framework for EHR-based Clinical Research: A case demonstration using the National COVID Cohort Collaborative (N3C). (arXiv:2110.10780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10780">
<div class="article-summary-box-inner">
<span><p>While we pay attention to the latest advances in clinical natural language
processing (NLP), we can notice some resistance in the clinical and
translational research community to adopt NLP models due to limited
transparency, Interpretability and usability. Built upon our previous work, in
this study, we proposed an open natural language processing development
framework and evaluated it through the implementation of NLP algorithms for the
National COVID Cohort Collaborative (N3C). Based on the interests in
information extraction from COVID-19 related clinical notes, our work includes
1) an open data annotation process using COVID-19 signs and symptoms as the use
case, 2) a community-driven ruleset composing platform, and 3) a synthetic text
data generation workflow to generate texts for information extraction tasks
without involving human subjects. The generated corpora derived out of the
texts from multiple intuitions and gold standard annotation are tested on a
single institution's rule set has the performances in F1 score of 0.876, 0.706
and 0.694, respectively. The study as a consortium effort of the N3C NLP
subgroup demonstrates the feasibility of creating a federated NLP algorithm
development and benchmarking platform to enhance multi-institution clinical NLP
study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The R package sentometrics to compute, aggregate and predict with textual sentiment. (arXiv:2110.10817v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10817">
<div class="article-summary-box-inner">
<span><p>We provide a hands-on introduction to optimized textual sentiment indexation
using the R package sentometrics. Textual sentiment analysis is increasingly
used to unlock the potential information value of textual data. The
sentometrics package implements an intuitive framework to efficiently compute
sentiment scores of numerous texts, to aggregate the scores into multiple time
series, and to use these time series to predict other variables. The workflow
of the package is illustrated with a built-in corpus of news articles from two
major U.S. journals to forecast the CBOE Volatility Index.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Visuospatial, Linguistic and Commonsense Structure into Story Visualization. (arXiv:2110.10834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10834">
<div class="article-summary-box-inner">
<span><p>While much research has been done in text-to-image synthesis, little work has
been done to explore the usage of linguistic structure of the input text. Such
information is even more important for story visualization since its inputs
have an explicit narrative structure that needs to be translated into an image
sequence (or visual story). Prior work in this domain has shown that there is
ample room for improvement in the generated image sequence in terms of visual
quality, consistency and relevance. In this paper, we first explore the use of
constituency parse trees using a Transformer-based recurrent architecture for
encoding structured input. Second, we augment the structured input with
commonsense information and study the impact of this external knowledge on the
generation of visual story. Third, we also incorporate visual structure via
bounding boxes and dense captioning to provide feedback about the
characters/objects in generated images within a dual learning setup. We show
that off-the-shelf dense-captioning models trained on Visual Genome can improve
the spatial structure of images from a different target domain without needing
fine-tuning. We train the model end-to-end using intra-story contrastive loss
(between words and image sub-regions) and show significant improvements in
several metrics (and human evaluation) for multiple datasets. Finally, we
provide an analysis of the linguistic and visuo-spatial information. Code and
data: https://github.com/adymaharana/VLCStoryGan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint Model for Aspect-Category Sentiment Analysis with Shared Sentiment Prediction Layer. (arXiv:1908.11017v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.11017">
<div class="article-summary-box-inner">
<span><p>Aspect-category sentiment analysis (ACSA) aims to predict the aspect
categories mentioned in texts and their corresponding sentiment polarities.
Some joint models have been proposed to address this task. Given a text, these
joint models detect all the aspect categories mentioned in the text and predict
the sentiment polarities toward them at once. Although these joint models
obtain promising performances, they train separate parameters for each aspect
category and therefore suffer from data deficiency of some aspect categories.
To solve this problem, we propose a novel joint model which contains a shared
sentiment prediction layer. The shared sentiment prediction layer transfers
sentiment knowledge between aspect categories and alleviates the problem caused
by data deficiency. Experiments conducted on SemEval-2016 Datasets demonstrate
the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualSem: A High-quality Knowledge Graph for Vision and Language. (arXiv:2008.09150v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09150">
<div class="article-summary-box-inner">
<span><p>An exciting frontier in natural language understanding (NLU) and generation
(NLG) calls for (vision-and-) language models that can efficiently access
external structured knowledge repositories. However, many existing knowledge
bases only cover limited domains, or suffer from noisy data, and most of all
are typically hard to integrate into neural language pipelines. To fill this
gap, we release VisualSem: a high-quality knowledge graph (KG) which includes
nodes with multilingual glosses, multiple illustrative images, and visually
relevant relations. We also release a neural multi-modal retrieval model that
can use images or sentences as inputs and retrieves entities in the KG. This
multi-modal retrieval model can be integrated into any (neural network) model
pipeline. We encourage the research community to use VisualSem for data
augmentation and/or as a source of grounding, among other possible uses.
VisualSem as well as the multi-modal retrieval models are publicly available
and can be downloaded in this URL: https://github.com/iacercalixto/visualsem
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inducing Alignment Structure with Gated Graph Attention Networks for Sentence Matching. (arXiv:2010.07668v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07668">
<div class="article-summary-box-inner">
<span><p>Sentence matching is a fundamental task of natural language processing with
various applications. Most recent approaches adopt attention-based neural
models to build word- or phrase-level alignment between two sentences. However,
these models usually ignore the inherent structure within the sentences and
fail to consider various dependency relationships among text units. To address
these issues, this paper proposes a graph-based approach for sentence matching.
First, we represent a sentence pair as a graph with several carefully design
strategies. We then employ a novel gated graph attention network to encode the
constructed graph for sentence matching. Experimental results demonstrate that
our method substantially achieves state-of-the-art performance on two datasets
across tasks of natural language and paraphrase identification. Further
discussions show that our model can learn meaningful graph structure,
indicating its superiority on improved interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions. (arXiv:2010.10216v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.10216">
<div class="article-summary-box-inner">
<span><p>Popular dialog datasets such as MultiWOZ are created by providing crowd
workers an instruction, expressed in natural language, that describes the task
to be accomplished. Crowd workers play the role of a user and an agent to
generate dialogs to accomplish tasks involving booking restaurant tables,
calling a taxi etc. In this paper, we present a data creation strategy that
uses the pre-trained language model, GPT2, to simulate the interaction between
crowd workers by creating a user bot and an agent bot. We train the simulators
using a smaller percentage of actual crowd-generated conversations and their
corresponding instructions. We demonstrate that by using the simulated data, we
achieve significant improvements in low-resource settings on two publicly
available datasets - the MultiWOZ dataset and the Persona chat dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora. (arXiv:2010.14649v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14649">
<div class="article-summary-box-inner">
<span><p>We propose a new approach for learning contextualised cross-lingual word
embeddings based on a small parallel corpus (e.g. a few hundred sentence
pairs). Our method obtains word embeddings via an LSTM encoder-decoder model
that simultaneously translates and reconstructs an input sentence. Through
sharing model parameters among different languages, our model jointly trains
the word embeddings in a common cross-lingual space. We also propose to combine
word and subword embeddings to make use of orthographic similarities across
different languages. We base our experiments on real-world data from endangered
languages, namely Yongning Na, Shipibo-Konibo, and Griko. Our experiments on
bilingual lexicon induction and word alignment tasks show that our model
outperforms existing methods by a large margin for most language pairs. These
results demonstrate that, contrary to common belief, an encoder-decoder
translation model is beneficial for learning cross-lingual representations even
in extremely low-resource conditions. Furthermore, our model also works well on
high-resource conditions, achieving state-of-the-art performance on a
German-English word-alignment task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements. (arXiv:2101.06053v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06053">
<div class="article-summary-box-inner">
<span><p>Truly real-life data presents a strong, but exciting challenge for sentiment
and emotion research. The high variety of possible `in-the-wild' properties
makes large datasets such as these indispensable with respect to building
robust machine learning models. A sufficient quantity of data covering a deep
variety in the challenges of each modality to force the exploratory analysis of
the interplay of all modalities has not yet been made available in this
context. In this contribution, we present MuSe-CaR, a first of its kind
multimodal dataset. The data is publicly available as it recently served as the
testing bed for the 1st Multimodal Sentiment Analysis Challenge, and focused on
the tasks of emotion, emotion-target engagement, and trustworthiness
recognition by means of comprehensively integrating the audio-visual and
language modalities. Furthermore, we give a thorough overview of the dataset in
terms of collection and annotation, including annotation tiers not used in this
year's MuSe 2020. In addition, for one of the sub-challenges - predicting the
level of trustworthiness - no participant outperformed the baseline model, and
so we propose a simple, but highly efficient Multi-Head-Attention network that
exceeds using multimodal fusion the baseline by around 0.2 CCC (almost 50 %
improvement).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximating How Single Head Attention Learns. (arXiv:2103.07601v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07601">
<div class="article-summary-box-inner">
<span><p>Why do models often attend to salient words, and how does this evolve
throughout training? We approximate model training as a two stage process:
early on in training when the attention weights are uniform, the model learns
to translate individual input word `i` to `o` if they co-occur frequently.
Later, the model learns to attend to `i` while the correct output is $o$
because it knows `i` translates to `o`. To formalize, we define a model
property, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i`
translates to `o`), and claim that it drives the learning of the attention.
This claim is supported by the fact that before the attention mechanism is
learned, KTIW can be learned from word co-occurrence statistics, but not the
other way around. Particularly, we can construct a training distribution that
makes KTIW hard to learn, the learning of the attention fails, and the model
cannot even learn the simple task of copying the input words to the output. Our
approximation explains why models sometimes attend to salient words, and
inspires a toy example where a multi-head attention model can overcome the
above hard training distribution by improving learning dynamics rather than
expressiveness. We end by discussing the limitation of our approximation
framework and suggest future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models have a Moral Dimension. (arXiv:2103.11790v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11790">
<div class="article-summary-box-inner">
<span><p>Artificial writing is permeating our lives due to recent advances in
large-scale, transformer-based language models (LMs) such as BERT, its
variants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning
them for specific tasks, researchers have extended state of the art for many
NLP tasks and shown that they capture not only linguistic knowledge but also
retain general knowledge implicitly present in the data. Unfortunately, LMs
trained on unfiltered text corpora suffer from degenerated and biased
behaviour. While this is well established, we show that recent improvements of
LMs also store ethical and moral norms of the society and actually bring a
"moral direction" to surface. In this study, we show that these norms can be
captured geometrically by a direction, which can be computed, e.g., by a PCA,
in the embedding space, reflecting well the agreement of phrases to social
norms implicitly expressed in the training texts. Furthermore, this provides a
path for attenuating or even preventing toxic degeneration in LMs. Being able
to rate the (non-)normativity of arbitrary phrases without explicitly training
the LM for this task, we demonstrate the capabilities of the moral direction
for guiding (even other) LMs towards producing normative text and showcase it
on RealToxicityPrompts testbed, preventing the neural toxic degeneration in
GPT-2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting the Reproducibility of Social and Behavioral Science Papers Using Supervised Learning Models. (arXiv:2104.04580v2 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04580">
<div class="article-summary-box-inner">
<span><p>In recent years, significant effort has been invested verifying the
reproducibility and robustness of research claims in social and behavioral
sciences (SBS), much of which has involved resource-intensive replication
projects. In this paper, we investigate prediction of the reproducibility of
SBS papers using machine learning methods based on a set of features. We
propose a framework that extracts five types of features from scholarly work
that can be used to support assessments of reproducibility of published
research claims. Bibliometric features, venue features, and author features are
collected from public APIs or extracted using open source machine learning
libraries with customized parsers. Statistical features, such as p-values, are
extracted by recognizing patterns in the body text. Semantic features, such as
funding information, are obtained from public APIs or are extracted using
natural language processing models. We analyze pairwise correlations between
individual features and their importance for predicting a set of human-assessed
ground truth labels. In doing so, we identify a subset of 9 top features that
play relatively more important roles in predicting the reproducibility of SBS
papers in our corpus. Results are verified by comparing performances of 10
supervised predictive classifiers trained on different sets of features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v4 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08663">
<div class="article-summary-box-inner">
<span><p>Existing neural information retrieval (IR) models have often been studied in
homogeneous and narrow settings, which has considerably limited insights into
their out-of-distribution (OOD) generalization capabilities. To address this,
and to facilitate researchers to broadly evaluate the effectiveness of their
models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous
evaluation benchmark for information retrieval. We leverage a careful selection
of 18 publicly available datasets from diverse text retrieval tasks and domains
and evaluate 10 state-of-the-art retrieval systems including lexical, sparse,
dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our
results show BM25 is a robust baseline and re-ranking and
late-interaction-based models on average achieve the best zero-shot
performances, however, at high computational costs. In contrast, dense and
sparse-retrieval models are computationally more efficient but often
underperform other approaches, highlighting the considerable room for
improvement in their generalization capabilities. We hope this framework allows
us to better evaluate and understand existing retrieval systems, and
contributes to accelerating progress towards better robust and generalizable
systems in the future. BEIR is publicly available at
https://github.com/UKPLab/beir.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations. (arXiv:2104.08667v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08667">
<div class="article-summary-box-inner">
<span><p>Next generation task-oriented dialog systems need to understand
conversational contexts with their perceived surroundings, to effectively help
users in the real-world multimodal environment. Existing task-oriented dialog
datasets aimed towards virtual assistance fall short and do not situate the
dialog in the user's multimodal context. To overcome, we present a new dataset
for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which
includes 11K task-oriented user&lt;-&gt;assistant dialogs (117K utterances) in the
shopping domain, grounded in immersive and photo-realistic scenes.
</p>
<p>The dialogs are collected using a two-phase pipeline: (1) A novel multimodal
dialog simulator generates simulated dialog flows, with an emphasis on
diversity and richness of interactions, (2) Manual paraphrasing of the
generated utterances to collect diverse referring expressions. We provide an
in-depth analysis of the collected dataset, and describe in detail the four
main benchmark tasks we propose. Our baseline model, powered by the
state-of-the-art language model, shows promising results, and highlights new
challenges and directions for the community to study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Knowledge Graph-based World Models of Textual Environments. (arXiv:2106.09608v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09608">
<div class="article-summary-box-inner">
<span><p>World models improve a learning agent's ability to efficiently operate in
interactive and situated environments. This work focuses on the task of
building world models of text-based game environments. Text-based games, or
interactive narratives, are reinforcement learning environments in which agents
perceive and interact with the world using textual natural language. These
environments contain long, multi-step puzzles or quests woven through a world
that is filled with hundreds of characters, locations, and objects. Our world
model learns to simultaneously: (1) predict changes in the world caused by an
agent's actions when representing the world as a knowledge graph; and (2)
generate the set of contextually relevant natural language actions required to
operate in the world. We frame this task as a Set of Sequences generation
problem by exploiting the inherent structure of knowledge graphs and actions
and introduce both a transformer-based multi-task architecture and a loss
function to train it. A zero-shot ablation study on never-before-seen textual
worlds shows that our methodology significantly outperforms existing textual
world modeling techniques as well as the importance of each of our
contributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11483">
<div class="article-summary-box-inner">
<span><p>Recently, the development of pre-trained language models has brought natural
language processing (NLP) tasks to the new state-of-the-art. In this paper we
explore the efficiency of various pre-trained language models. We pre-train a
list of transformer-based models with the same amount of text and the same
training steps. The experimental results shows that the most improvement upon
the origin BERT is adding the RNN-layer to capture more contextual information
for short text understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuSe-Toolbox: The Multimodal Sentiment Analysis Continuous Annotation Fusion and Discrete Class Transformation Toolbox. (arXiv:2107.11757v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11757">
<div class="article-summary-box-inner">
<span><p>We introduce the MuSe-Toolbox - a Python-based open-source toolkit for
creating a variety of continuous and discrete emotion gold standards. In a
single framework, we unify a wide range of fusion methods and propose the novel
Rater Aligned Annotation Weighting (RAAW), which aligns the annotations in a
translation-invariant way before weighting and fusing them based on the
inter-rater agreements between the annotations. Furthermore, discrete
categories tend to be easier for humans to interpret than continuous signals.
With this in mind, the MuSe-Toolbox provides the functionality to run
exhaustive searches for meaningful class clusters in the continuous gold
standards. To our knowledge, this is the first toolkit that provides a wide
selection of state-of-the-art emotional gold standard methods and their
transformation to discrete classes. Experimental results indicate that
MuSe-Toolbox can provide promising and novel class formations which can be
better predicted than hard-coded classes boundaries with minimal human
intervention. The implementation (1) is out-of-the-box available with all
dependencies using a Docker container (2).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Answer Similarity for Evaluating Question Answering Models. (arXiv:2108.06130v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06130">
<div class="article-summary-box-inner">
<span><p>The evaluation of question answering models compares ground-truth annotations
with model predictions. However, as of today, this comparison is mostly
lexical-based and therefore misses out on answers that have no lexical overlap
but are still semantically similar, thus treating correct answers as false.
This underestimation of the true performance of models hinders user acceptance
in applications and complicates a fair comparison of different models.
Therefore, there is a need for an evaluation metric that is based on semantics
instead of pure string similarity. In this short paper, we present SAS, a
cross-encoder-based metric for the estimation of semantic answer similarity,
and compare it to seven existing metrics. To this end, we create an English and
a German three-way annotated evaluation dataset containing pairs of answers
along with human judgment of their semantic similarity, which we release along
with an implementation of the SAS metric and the experiments. We find that
semantic similarity metrics based on recent transformer models correlate much
better with human judgment than traditional lexical similarity metrics on our
two newly created datasets and one dataset from related work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v4 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08597">
<div class="article-summary-box-inner">
<span><p>Answering complex questions over knowledge bases (KB-QA) faces huge input
data with billions of facts, involving millions of entities and thousands of
predicates. For efficiency, QA systems first reduce the answer search space by
identifying a set of facts that is likely to contain all answers and relevant
cues. The most common technique or doing this is to apply named entity
disambiguation (NED) systems to the question, and retrieve KB facts for the
disambiguated entities. This work presents CLOCQ, an efficient method that
prunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses
a top-k query processor over score-ordered lists of KB items that combine
signals about lexical matching, relevance to the question, coherence among
candidate items, and connectivity in the KB graph. Experiments with two recent
QA benchmarks for complex questions demonstrate the superiority of CLOCQ over
state-of-the-art baselines with respect to answer presence, size of the search
space, and runtimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Poisson Stochastic Beam Search. (arXiv:2109.11034v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11034">
<div class="article-summary-box-inner">
<span><p>Beam search is the default decoding strategy for many sequence generation
tasks in NLP. The set of approximate K-best items returned by the algorithm is
a useful summary of the distribution for many applications; however, the
candidates typically exhibit high overlap and may give a highly biased estimate
for expectations under our model. These problems can be addressed by instead
using stochastic decoding strategies. In this work, we propose a new method for
turning beam search into a stochastic process: Conditional Poisson stochastic
beam search. Rather than taking the maximizing set at each iteration, we sample
K candidates without replacement according to the conditional Poisson sampling
design. We view this as a more natural alternative to Kool et. al. 2019's
stochastic beam search (SBS). Furthermore, we show how samples generated under
the CPSBS design can be used to build consistent estimators and sample diverse
sets from sequence models. In our experiments, we observe CPSBS produces lower
variance and more efficient estimators than SBS, even showing improvements in
high entropy settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. (arXiv:2109.12068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12068">
<div class="article-summary-box-inner">
<span><p>Transfer learning with a unified Transformer framework (T5) that converts all
language problems into a text-to-text format has recently been proposed as a
simple, yet effective, transfer learning approach. Although a multilingual
version of the T5 model (mT5) has been introduced, it is not clear how well it
can fare on non-English tasks involving diverse data. To investigate this
question, we apply mT5 on a language with a wide variety of dialects--Arabic.
For evaluation, we use an existing benchmark for Arabic language understanding
and introduce a new benchmark for Arabic language generation (ARGEN). We also
pre-train three powerful Arabic-specific text-to-text Transformer based models
and evaluate them on the two benchmarks. Our new models perform significantly
better than mT5 and exceed MARBERT, the current state-of-the-art Arabic
BERT-based model, on Arabic language understanding. The models also set new
SOTA on the generation benchmark. Our new models and are publicly released at
https://github.com/UBC-NLP/araT5 and ARGEN will be released through the same
repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSG@Dravidian-CodeMix-HASOC2021: Pretrained Transformers for Offensive Language Identification in Tanglish. (arXiv:2110.02852v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02852">
<div class="article-summary-box-inner">
<span><p>This paper describes the system submitted to Dravidian-Codemix-HASOC2021:
Hate Speech and Offensive Language Identification in Dravidian Languages
(Tamil-English and Malayalam-English). This task aims to identify offensive
content in code-mixed comments/posts in Dravidian Languages collected from
social media. Our approach utilizes pooling the last layers of pretrained
transformer multilingual BERT for this task which helped us achieve rank nine
on the leaderboard with a weighted average score of 0.61 for the Tamil-English
dataset in subtask B. After the task deadline, we sampled the dataset uniformly
and used the MuRIL pretrained model, which helped us achieve a weighted average
score of 0.67, the top score in the leaderboard. Furthermore, our approach to
utilizing the pretrained models helps reuse our models for the same task with a
different dataset. Our code and models are available in
https://github.com/seanbenhur/tanglish-offensive-language-identification
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">text2sdg: An open-source solution to monitoring sustainable development goals from text. (arXiv:2110.05856v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05856">
<div class="article-summary-box-inner">
<span><p>Monitoring progress on the United Nations Sustainable Development Goals
(SDGs) is important for both academic and non-academic organizations. Existing
approaches to monitoring SDGs have focused on specific data types, namely,
publications listed in proprietary research databases. We present the text2sdg
R package, a user-friendly, open-source package that detects SDGs in any kind
of text data using several different query systems from any text source. The
text2sdg package thereby facilitates the monitoring of SDGs for a wide array of
text sources and provides a much-needed basis for validating and improving
extant methods to detect SDGs from text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Model Supervised by Understanding Map. (arXiv:2110.06043v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06043">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization. (arXiv:2110.06388v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06388">
<div class="article-summary-box-inner">
<span><p>To capture the semantic graph structure from raw text, most existing
summarization approaches are built on GNNs with a pre-trained model. However,
these methods suffer from cumbersome procedures and inefficient computations
for long-text documents. To mitigate these issues, this paper proposes
HETFORMER, a Transformer-based pre-trained model with multi-granularity sparse
attentions for long-text extractive summarization. Specifically, we model
different types of semantic nodes in raw text as a potential heterogeneous
graph and directly learn heterogeneous relationships (edges) among nodes by
Transformer. Extensive experiments on both single- and multi-document
summarization tasks show that HETFORMER achieves state-of-the-art performance
in Rouge F1 while using less memory and fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hindsight: Posterior-guided training of retrievers for improved open-ended generation. (arXiv:2110.07752v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07752">
<div class="article-summary-box-inner">
<span><p>Many text generation systems benefit from using a retriever to retrieve
passages from a textual knowledge corpus (e.g., Wikipedia) which are then
provided as additional context to the generator. For open-ended generation
tasks (like generating informative utterances in conversations) many varied
passages may be equally relevant and we find that existing methods that jointly
train the retriever and generator underperform: the retriever may not find
relevant passages even amongst the top-10 and hence the generator may not learn
a preference to ground its generated output in them. We propose using an
additional guide retriever that is allowed to use the target output and "in
hindsight" retrieve relevant passages during training. We model the guide
retriever after the posterior distribution Q of passages given the input and
the target output and train it jointly with the standard retriever and the
generator by maximizing the evidence lower bound (ELBo) in expectation over Q.
For informative conversations from the Wizard of Wikipedia dataset, with
posterior-guided training, the retriever finds passages with higher relevance
in the top-10 (23% relative improvement), the generator's responses are more
grounded in the retrieved passage (19% relative improvement) and the end-to-end
system produces better overall output (6.4% relative improvement).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">n-stage Latent Dirichlet Allocation: A Novel Approach for LDA. (arXiv:2110.08591v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08591">
<div class="article-summary-box-inner">
<span><p>Nowadays, data analysis has become a problem as the amount of data is
constantly increasing. In order to overcome this problem in textual data, many
models and methods are used in natural language processing. The topic modeling
field is one of these methods. Topic modeling allows determining the semantic
structure of a text document. Latent Dirichlet Allocation (LDA) is the most
common method among topic modeling methods. In this article, the proposed
n-stage LDA method, which can enable the LDA method to be used more
effectively, is explained in detail. The positive effect of the method has been
demonstrated by the applied English and Turkish studies. Since the method
focuses on reducing the word count in the dictionary, it can be used
language-independently. You can access the open-source code of the method and
the example: https://github.com/anil1055/n-stage_LDA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A non-hierarchical attention network with modality dropout for textual response generation in multimodal dialogue systems. (arXiv:2110.09702v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09702">
<div class="article-summary-box-inner">
<span><p>Existing text- and image-based multimodal dialogue systems use the
traditional Hierarchical Recurrent Encoder-Decoder (HRED) framework, which has
an utterance-level encoder to model utterance representation and a
context-level encoder to model context representation. Although pioneer efforts
have shown promising performances, they still suffer from the following
challenges: (1) the interaction between textual features and visual features is
not fine-grained enough. (2) the context representation can not provide a
complete representation for the context. To address the issues mentioned above,
we propose a non-hierarchical attention network with modality dropout, which
abandons the HRED framework and utilizes attention modules to encode each
utterance and model the context representation. To evaluate our proposed model,
we conduct comprehensive experiments on a public multimodal dialogue dataset.
Automatic and human evaluation demonstrate that our proposed model outperforms
the existing methods and achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?. (arXiv:2105.09142v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09142">
<div class="article-summary-box-inner">
<span><p>The automatic detection of humor poses a grand challenge for natural language
processing. Transformer-based systems have recently achieved remarkable results
on this task, but they usually (1)~were evaluated in setups where serious vs
humorous texts came from entirely different sources, and (2)~focused on
benchmarking performance without providing insights into how the models work.
We make progress in both respects by training and analyzing transformer-based
humor recognition models on a recently introduced dataset consisting of minimal
pairs of aligned sentences, one serious, the other humorous. We find that,
although our aligned dataset is much harder than previous datasets,
transformer-based models recognize the humorous sentence in an aligned pair
with high accuracy (78%). In a careful error analysis, we characterize easy vs
hard instances. Finally, by analyzing attention weights, we obtain important
insights into the mechanisms by which transformers recognize humor. Most
remarkably, we find clear evidence that one single attention head learns to
recognize the words that make a test sentence humorous, even without access to
this information at training time.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Sim-NGF: FFT-Based Global Rigid Multimodal Alignment of Image Volumes using Normalized Gradient Fields. (arXiv:2110.10156v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10156">
<div class="article-summary-box-inner">
<span><p>Multimodal image alignment involves finding spatial correspondences between
volumes varying in appearance and structure. Automated alignment methods are
often based on local optimization that can be highly sensitive to their
initialization. We propose a global optimization method for rigid multimodal 3D
image alignment, based on a novel efficient algorithm for computing similarity
of normalized gradient fields (NGF) in the frequency domain. We validate the
method experimentally on a dataset comprised of 20 brain volumes acquired in
four modalities (T1w, Flair, CT, [18F] FDG PET), synthetically displaced with
known transformations. The proposed method exhibits excellent performance on
all six possible modality combinations, and outperforms all four reference
methods by a large margin. The method is fast; a 3.4Mvoxel global rigid
alignment requires approximately 40 seconds of computation, and the proposed
algorithm outperforms a direct algorithm for the same task by more than three
orders of magnitude. Open-source implementation is provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hand-Object Contact Prediction via Motion-Based Pseudo-Labeling and Guided Progressive Label Correction. (arXiv:2110.10174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10174">
<div class="article-summary-box-inner">
<span><p>Every hand-object interaction begins with contact. Despite predicting the
contact state between hands and objects is useful in understanding hand-object
interactions, prior methods on hand-object analysis have assumed that the
interacting hands and objects are known, and were not studied in detail. In
this study, we introduce a video-based method for predicting contact between a
hand and an object. Specifically, given a video and a pair of hand and object
tracks, we predict a binary contact state (contact or no-contact) for each
frame. However, annotating a large number of hand-object tracks and contact
labels is costly. To overcome the difficulty, we propose a semi-supervised
framework consisting of (i) automatic collection of training data with
motion-based pseudo-labels and (ii) guided progressive label correction (gPLC),
which corrects noisy pseudo-labels with a small amount of trusted data. We
validated our framework's effectiveness on a newly built benchmark dataset for
hand-object contact prediction and showed superior performance against existing
baseline methods. Code and data are available at
https://github.com/takumayagi/hand_object_contact_prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascaded Cross MLP-Mixer GANs for Cross-View Image Translation. (arXiv:2110.10183v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10183">
<div class="article-summary-box-inner">
<span><p>It is hard to generate an image at target view well for previous cross-view
image translation methods that directly adopt a simple encoder-decoder or U-Net
structure, especially for drastically different views and severe deformation
cases. To ease this problem, we propose a novel two-stage framework with a new
Cascaded Cross MLP-Mixer (CrossMLP) sub-network in the first stage and one
refined pixel-level loss in the second stage. In the first stage, the CrossMLP
sub-network learns the latent transformation cues between image code and
semantic map code via our novel CrossMLP blocks. Then the coarse results are
generated progressively under the guidance of those cues. Moreover, in the
second stage, we design a refined pixel-level loss that eases the noisy
semantic label problem with more reasonable regularization in a more compact
fashion for better optimization. Extensive experimental results on
Dayton~\cite{vo2016localizing} and CVUSA~\cite{workman2015wide} datasets show
that our method can generate significantly better results than state-of-the-art
methods. The source code and trained models are available at
https://github.com/Amazingren/CrossMLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StructFormer: Learning Spatial Structure for Language-Guided Semantic Rearrangement of Novel Objects. (arXiv:2110.10189v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10189">
<div class="article-summary-box-inner">
<span><p>Geometric organization of objects into semantically meaningful arrangements
pervades the built world. As such, assistive robots operating in warehouses,
offices, and homes would greatly benefit from the ability to recognize and
rearrange objects into these semantically meaningful structures. To be useful,
these robots must contend with previously unseen objects and receive
instructions without significant programming. While previous works have
examined recognizing pairwise semantic relations and sequential manipulation to
change these simple relations none have shown the ability to arrange objects
into complex structures such as circles or table settings. To address this
problem we propose a novel transformer-based neural network, StructFormer,
which takes as input a partial-view point cloud of the current object
arrangement and a structured language command encoding the desired object
configuration. We show through rigorous experiments that StructFormer enables a
physical robot to rearrange novel objects into semantically meaningful
structures with multi-object relational constraints inferred from the language
command.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoFi: Coarse-to-Fine ICP for LiDAR Localization in an Efficient Long-lasting Point Cloud Map. (arXiv:2110.10194v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10194">
<div class="article-summary-box-inner">
<span><p>LiDAR odometry and localization has attracted increasing research interest in
recent years. In the existing works, iterative closest point (ICP) is widely
used since it is precise and efficient. Due to its non-convexity and its local
iterative strategy, however, ICP-based method easily falls into local optima,
which in turn calls for a precise initialization. In this paper, we propose
CoFi, a Coarse-to-Fine ICP algorithm for LiDAR localization. Specifically, the
proposed algorithm down-samples the input point sets under multiple voxel
resolution, and gradually refines the transformation from the coarse point sets
to the fine-grained point sets. In addition, we propose a map based LiDAR
localization algorithm that extracts semantic feature points from the LiDAR
frames and apply CoFi to estimate the pose on an efficient point cloud map.
With the help of the Cylinder3D algorithm for LiDAR scan semantic segmentation,
the proposed CoFi localization algorithm demonstrates the state-of-the-art
performance on the KITTI odometry benchmark, with significant improvement over
the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Come Again? Re-Query in Referring Expression Comprehension. (arXiv:2110.10206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10206">
<div class="article-summary-box-inner">
<span><p>To build a shared perception of the world, humans rely on the ability to
resolve misunderstandings by requesting and accepting clarifications. However,
when evaluating visiolinguistic models, metrics such as accuracy enforce the
assumption that a decision must be made based on a single piece of evidence. In
this work, we relax this assumption for the task of referring expression
comprehension by allowing the model to request help when its confidence is low.
We consider two ways in which this help can be provided: multimodal re-query,
where the user is allowed to point or click to provide additional information
to the model, and rephrase re-query, where the user is only allowed to provide
another referring expression. We demonstrate the importance of re-query by
showing that providing the best referring expression for all objects can
increase accuracy by up to 21.9% and that this accuracy can be matched by
re-querying only 12% of initial referring expressions. We further evaluate
re-query functions for both multimodal and rephrase re-query across three
modern approaches and demonstrate combined replacement for rephrase re-query,
which improves average single-query performance by up to 6.5% and converges to
as close as 1.6% of the upper bound of single-query performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Equivariances and Partial Equivariances from Data. (arXiv:2110.10211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10211">
<div class="article-summary-box-inner">
<span><p>Group equivariant Convolutional Neural Networks (G-CNNs) constrain features
to respect the chosen symmetries, and lead to better generalization when these
symmetries appear in the data. However, if the chosen symmetries are not
present, group equivariant architectures lead to overly constrained models and
worse performance. Frequently, the distribution of the data can be better
represented by a subset of a group than by the group as a whole, e.g.,
rotations in $[-90^{\circ}, 90^{\circ}]$. In such cases, a model that respects
equivariance partially is better suited to represent the data. Moreover,
relevant symmetries may differ for low and high-level features, e.g., edge
orientations in a face, and face poses relative to the camera. As a result, the
optimal level of equivariance may differ per layer. In this work, we introduce
Partial G-CNNs: a family of equivariant networks able to learn partial and full
equivariances from data at every layer end-to-end. Partial G-CNNs retain full
equivariance whenever beneficial, e.g., for rotated MNIST, but are able to
restrict it whenever it becomes harmful, e.g., for 6~/~9 or natural image
classification. Partial G-CNNs perform on par with G-CNNs when full
equivariance is necessary, and outperform them otherwise. Our method is
applicable to discrete groups, continuous groups and combinations thereof.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks. (arXiv:2110.10217v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10217">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art methods of image classification using convolutional
neural networks are often constrained by both latency and power consumption.
This places a limit on the devices, particularly low-power edge devices, that
can employ these methods. Spiking neural networks (SNNs) are considered to be
the third generation of artificial neural networks which aim to address these
latency and power constraints by taking inspiration from biological neuronal
communication processes. Before data such as images can be input into an SNN,
however, they must be first encoded into spike trains. Herein, we propose a
method for encoding static images into temporal spike trains using edge
detection and an adaptive signal sampling method for use in SNNs. The edge
detection process consists of first performing Canny edge detection on the 2D
static images and then converting the edge detected images into two X and Y
signals using an image-to-signal conversion method. The adaptive signaling
approach consists of sampling the signals such that the signals maintain enough
detail and are sensitive to abrupt changes in the signal. Temporal encoding
mechanisms such as threshold-based representation (TBR) and step-forward (SF)
are then able to be used to convert the sampled signals into spike trains. We
use various error and indicator metrics to optimize and evaluate the efficiency
and precision of the proposed image encoding approach. Comparison results
between the original and reconstructed signals from spike trains generated
using edge-detection and adaptive temporal encoding mechanism exhibit 18x and
7x reduction in average root mean square error (RMSE) compared to the
conventional SF and TBR encoding, respectively, while used for encoding MNIST
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test time Adaptation through Perturbation Robustness. (arXiv:2110.10232v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10232">
<div class="article-summary-box-inner">
<span><p>Data samples generated by several real world processes are dynamic in nature
\textit{i.e.}, their characteristics vary with time. Thus it is not possible to
train and tackle all possible distributional shifts between training and
inference, using the host of transfer learning methods in literature. In this
paper, we tackle this problem of adapting to domain shift at inference time
\textit{i.e.}, we do not change the training process, but quickly adapt the
model at test-time to handle any domain shift. For this, we propose to enforce
consistency of predictions of data sampled in the vicinity of test sample on
the image manifold. On a host of test scenarios like dealing with corruptions
(CIFAR-10-C and CIFAR-100-C), and domain adaptation (VisDA-C), our method is at
par or significantly outperforms previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">1st Place Solution for the UVO Challenge on Image-based Open-World Segmentation 2021. (arXiv:2110.10239v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10239">
<div class="article-summary-box-inner">
<span><p>We describe our two-stage instance segmentation framework we use to compete
in the challenge. The first stage of our framework consists of an object
detector, which generates object proposals in the format of bounding boxes.
Then, the images and the detected bounding boxes are fed to the second stage,
where a segmentation network is applied to segment the objects in the bounding
boxes. We train all our networks in a class-agnostic way. Our approach achieves
the first place in the UVO 2021 Image-based Open-World Segmentation Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Automatic Change Detection Frame-work Based on Region Growing and Weighted Local Mutual Information: Analysis of Breast Tumor Response to Chemotherapy in Serial MR Images. (arXiv:2110.10242v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10242">
<div class="article-summary-box-inner">
<span><p>The automatic analysis of subtle changes between longitudinal MR images is an
important task as it is still a challenging issue in scope of the breast
medical image processing. In this paper we propose an effective automatic
change detection framework composed of two phases since previously used methods
have features with low distinctive power. First, in the preprocessing phase an
intensity normalization method is suggested based on Hierarchical Histogram
Matching (HHM) that is more robust to noise than previous methods. To eliminate
undesirable changes and extract the regions containing significant changes the
proposed Extraction Region of Changes (EROC) method is applied based on
intensity distribution and Hill-Climbing algorithm. Second, in the detection
phase a region growing-based approach is suggested to differentiate significant
changes from unreal ones. Due to using proposed Weighted Local Mutual
Information (WLMI) method to extract high level features and also utilizing the
principle of the local consistency of changes, the proposed approach enjoys
reasonable performance. The experimental results on both simulated and real
longitudinal Breast MR Images confirm the effectiveness of the proposed
framework. Also, this framework outperforms the human expert in some cases
which can detect many lesion evolutions that are missed by expert.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early- and in-season crop type mapping without current-year ground truth: generating labels from historical information via a topology-based approach. (arXiv:2110.10275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10275">
<div class="article-summary-box-inner">
<span><p>Land cover classification in remote sensing is often faced with the challenge
of limited ground truth. Incorporating historical information has the potential
to significantly lower the expensive cost associated with collecting ground
truth and, more importantly, enable early- and in-season mapping that is
helpful to many pre-harvest decisions. In this study, we propose a new approach
that can effectively transfer knowledge about the topology (i.e. relative
position) of different crop types in the spectral feature space (e.g. the
histogram of SWIR1 vs RDEG1 bands) to generate labels, thereby support crop
classification in a different year. Importantly, our approach does not attempt
to transfer classification decision boundaries that are susceptible to
inter-annual variations of weather and management, but relies on the more
robust and shift-invariant topology information. We tested this approach for
mapping corn/soybeans in the US Midwest and paddy rice/corn/soybeans in
Northeast China using Landsat-8 and Sentinel-2 data. Results show that our
approach automatically generates high-quality labels for crops in the target
year immediately after each image becomes available. Based on these generated
labels from our approach, the subsequent crop type mapping using a random
forest classifier reach the F1 score as high as 0.887 for corn as early as the
silking stage and 0.851 for soybean as early as the flowering stage and the
overall accuracy of 0.873 in Iowa. In Northeast China, F1 scores of paddy rice,
corn and soybeans and the overall accuracy can exceed 0.85 two and half months
ahead of harvest. Overall, these results highlight unique advantages of our
approach in transferring historical knowledge and maximizing the timeliness of
crop maps. Our approach supports a general paradigm shift towards learning
transferrable and generalizable knowledge to facilitate land cover
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Control of Artistic Styles in Image Generation. (arXiv:2110.10278v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10278">
<div class="article-summary-box-inner">
<span><p>Recent advances in generative models and adversarial training have enabled
artificially generating artworks in various artistic styles. It is highly
desirable to gain more control over the generated style in practice. However,
artistic styles are unlike object categories -- there are a continuous spectrum
of styles distinguished by subtle differences. Few works have been explored to
capture the continuous spectrum of styles and apply it to a style generation
task. In this paper, we propose to achieve this by embedding original artwork
examples into a continuous style space. The style vectors are fed to the
generator and discriminator to achieve fine-grained control. Our method can be
used with common generative adversarial networks (such as StyleGAN).
Experiments show that our method not only precisely controls the fine-grained
artistic style but also improves image quality over vanilla StyleGAN as
measured by FID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Coordinate Decoding for Keypoint Estimation Tasks. (arXiv:2110.10289v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10289">
<div class="article-summary-box-inner">
<span><p>A series of 2D (and 3D) keypoint estimation tasks are built upon heatmap
coordinate representation, i.e. a probability map that allows for learnable and
spatially aware encoding and decoding of keypoint coordinates on grids, even
allowing for sub-pixel coordinate accuracy. In this report, we aim to reproduce
the findings of DARK that investigated the 2D heatmap representation by
highlighting the importance of the encoding of the ground truth heatmap and the
decoding of the predicted heatmap to keypoint coordinates. The authors claim
that a) a more principled distribution-aware coordinate decoding method
overcomes the limitations of the standard techniques widely used in the
literature, and b), that the reconstruction of heatmaps from ground-truth
coordinates by generating accurate and continuous heatmap distributions lead to
unbiased model training, contrary to the standard coordinate encoding process
that quantizes the keypoint coordinates on the resolution of the input image
grid.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Rich Nearest Neighbor Representations from Self-supervised Ensembles. (arXiv:2110.10293v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10293">
<div class="article-summary-box-inner">
<span><p>Pretraining convolutional neural networks via self-supervision, and applying
them in transfer learning, is an incredibly fast-growing field that is rapidly
and iteratively improving performance across practically all image domains.
Meanwhile, model ensembling is one of the most universally applicable
techniques in supervised learning literature and practice, offering a simple
solution to reliably improve performance. But how to optimally combine
self-supervised models to maximize representation quality has largely remained
unaddressed. In this work, we provide a framework to perform self-supervised
model ensembling via a novel method of learning representations directly
through gradient descent at inference time. This technique improves
representation quality, as measured by k-nearest neighbors, both on the
in-domain dataset and in the transfer setting, with models transferable from
the former setting to the latter. Additionally, this direct learning of feature
through backpropagation improves representations from even a single model,
echoing the improvements found in self-distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Momentum Contrastive Autoencoder: Using Contrastive Learning for Latent Space Distribution Matching in WAE. (arXiv:2110.10303v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10303">
<div class="article-summary-box-inner">
<span><p>Wasserstein autoencoder (WAE) shows that matching two distributions is
equivalent to minimizing a simple autoencoder (AE) loss under the constraint
that the latent space of this AE matches a pre-specified prior distribution.
This latent space distribution matching is a core component of WAE, and a
challenging task. In this paper, we propose to use the contrastive learning
framework that has been shown to be effective for self-supervised
representation learning, as a means to resolve this problem. We do so by
exploiting the fact that contrastive learning objectives optimize the latent
space distribution to be uniform over the unit hyper-sphere, which can be
easily sampled from. We show that using the contrastive learning framework to
optimize the WAE loss achieves faster convergence and more stable optimization
compared with existing popular algorithms for WAE. This is also reflected in
the FID scores on CelebA and CIFAR-10 datasets, and the realistic generated
image quality on the CelebA-HQ dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Mean Shift for Representation Learning. (arXiv:2110.10309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10309">
<div class="article-summary-box-inner">
<span><p>We are interested in representation learning from labeled or unlabeled data.
Inspired by recent success of self-supervised learning (SSL), we develop a
non-contrastive representation learning method that can exploit additional
knowledge. This additional knowledge may come from annotated labels in the
supervised setting or an SSL model from another modality in the SSL setting.
Our main idea is to generalize the mean-shift algorithm by constraining the
search space of nearest neighbors, resulting in semantically purer
representations. Our method simply pulls the embedding of an instance closer to
its nearest neighbors in a search space that is constrained using the
additional knowledge. By leveraging this non-contrastive loss, we show that the
supervised ImageNet-1k pretraining with our method results in better transfer
performance as compared to the baselines. Further, we demonstrate that our
method is relatively robust to label noise. Finally, we show that it is
possible to use the noisy constraint across modalities to train self-supervised
video models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R$^3$Net:Relation-embedded Representation Reconstruction Network for Change Captioning. (arXiv:2110.10328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10328">
<div class="article-summary-box-inner">
<span><p>Change captioning is to use a natural language sentence to describe the
fine-grained disagreement between two similar images. Viewpoint change is the
most typical distractor in this task, because it changes the scale and location
of the objects and overwhelms the representation of real change. In this paper,
we propose a Relation-embedded Representation Reconstruction Network (R$^3$Net)
to explicitly distinguish the real change from the large amount of clutter and
irrelevant changes. Specifically, a relation-embedded module is first devised
to explore potential changed objects in the large amount of clutter. Then,
based on the semantic similarities of corresponding locations in the two
images, a representation reconstruction module (RRM) is designed to learn the
reconstruction representation and further model the difference representation.
Besides, we introduce a syntactic skeleton predictor (SSP) to enhance the
semantic interaction between change localization and caption generation.
Extensive experiments show that the proposed method achieves the
state-of-the-art results on two public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Intelligence-Based Detection, Classification and Prediction/Prognosis in PET Imaging: Towards Radiophenomics. (arXiv:2110.10332v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10332">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) techniques have significant potential to enable
effective, robust, and automated image phenotyping including identification of
subtle patterns. AI-based detection searches the image space to find the
regions of interest based on patterns and features. There is a spectrum of
tumor histologies from benign to malignant that can be identified by AI-based
classification approaches using image features. The extraction of minable
information from images gives way to the field of radiomics and can be explored
via explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics
analysis has the potential to be utilized as a noninvasive technique for the
accurate characterization of tumors to improve diagnosis and treatment
monitoring. This work reviews AI-based techniques, with a special focus on
oncological PET and PET/CT imaging, for different detection, classification,
and prediction/prognosis tasks. We also discuss needed efforts to enable the
translation of AI techniques to routine clinical workflows, and potential
improvements and complementary techniques such as the use of natural language
processing on electronic health records and neuro-symbolic AI techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Accurate and Reliable Iris Segmentation Using Uncertainty Learning. (arXiv:2110.10334v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10334">
<div class="article-summary-box-inner">
<span><p>As an upstream task of iris recognition, iris segmentation plays a vital role
in multiple subsequent tasks, including localization and matching. A slight
bias in iris segmentation often results in obvious performance degradation of
the iris recognition system. In the paper, we propose an Iris U-transformer
(IrisUsformer) for accurate and reliable iris segmentation. For better
accuracy, we elaborately design IrisUsformer by adopting position-sensitive
operation and re-packaging transformer block to raise the spatial perception
ability of the model. For better reliability, IrisUsformer utilizes an
auxiliary head to distinguishes the high- and low-uncertainty regions of
segmentation predictions and then adopts a weighting scheme to guide model
optimization. Experimental results on three publicly available databases
demonstrate that IrisUsformer achieves better segmentation accuracy using 35%
MACs of the SOTA IrisParseNet. More importantly, our method estimates the
uncertainty map corresponding to the segmentation prediction for subsequent
processing in iris recognition systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simpler Does It: Generating Semantic Labels with Objectness Guidance. (arXiv:2110.10335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10335">
<div class="article-summary-box-inner">
<span><p>Existing weakly or semi-supervised semantic segmentation methods utilize
image or box-level supervision to generate pseudo-labels for weakly labeled
images. However, due to the lack of strong supervision, the generated
pseudo-labels are often noisy near the object boundaries, which severely
impacts the network's ability to learn strong representations. To address this
problem, we present a novel framework that generates pseudo-labels for training
images, which are then used to train a segmentation model. To generate
pseudo-labels, we combine information from: (i) a class agnostic objectness
network that learns to recognize object-like regions, and (ii) either
image-level or bounding box annotations. We show the efficacy of our approach
by demonstrating how the objectness network can naturally be leveraged to
generate object-like regions for unseen categories. We then propose an
end-to-end multi-task learning strategy, that jointly learns to segment
semantics and objectness using the generated pseudo-labels. Extensive
experiments demonstrate the high quality of our generated pseudo-labels and
effectiveness of the proposed framework in a variety of domains. Our approach
achieves better or competitive performance compared to existing
weakly-supervised and semi-supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EBJR: Energy-Based Joint Reasoning for Adaptive Inference. (arXiv:2110.10343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10343">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep learning models have achieved significant performance
levels on various benchmarks. However, the excellent performance comes at a
cost of inefficient computational cost. Light-weight architectures, on the
other hand, achieve moderate accuracies, but at a much more desirable latency.
This paper presents a new method of jointly using the large accurate models
together with the small fast ones. To this end, we propose an Energy-Based
Joint Reasoning (EBJR) framework that adaptively distributes the samples
between shallow and deep models to achieve an accuracy close to the deep model,
but latency close to the shallow one. Our method is applicable to
out-of-the-box pre-trained models as it does not require an architecture change
nor re-training. Moreover, it is easy to use and deploy, especially for cloud
services. Through a comprehensive set of experiments on different down-stream
tasks, we show that our method outperforms strong state-of-the-art approaches
with a considerable margin. In addition, we propose specialized EBJR, an
extension of our method where we create a smaller specialized side model that
performs the target task only partially, but yields an even higher accuracy and
faster inference. We verify the strengths of our methods with both theoretical
and experimental evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GTM: Gray Temporal Model for Video Recognition. (arXiv:2110.10348v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10348">
<div class="article-summary-box-inner">
<span><p>Data input modality plays an important role in video action recognition.
Normally, there are three types of input: RGB, flow stream and compressed data.
In this paper, we proposed a new input modality: gray stream. Specifically,
taken the stacked consecutive 3 gray images as input, which is the same size of
RGB, can not only skip the conversion process from video decoding data to RGB,
but also improve the spatio-temporal modeling ability at zero computation and
zero parameters. Meanwhile, we proposed a 1D Identity Channel-wise
Spatio-temporal Convolution(1D-ICSC) which captures the temporal relationship
at channel-feature level within a controllable computation budget(by parameters
G &amp; R). Finally, we confirm its effectiveness and efficiency on several action
recognition benchmarks, such as Kinetics, Something-Something, HMDB-51 and
UCF-101, and achieve impressive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Gradient Scaling for Few-Shot Learning. (arXiv:2110.10353v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10353">
<div class="article-summary-box-inner">
<span><p>Model-agnostic meta-learning (MAML) is a well-known optimization-based
meta-learning algorithm that works well in various computer vision tasks, e.g.,
few-shot classification. MAML is to learn an initialization so that a model can
adapt to a new task in a few steps. However, since the gradient norm of a
classifier (head) is much bigger than those of backbone layers, the model
focuses on learning the decision boundary of the classifier with similar
representations. Furthermore, gradient norms of high-level layers are small
than those of the other layers. So, the backbone of MAML usually learns
task-generic features, which results in deteriorated adaptation performance in
the inner-loop. To resolve or mitigate this problem, we propose contextual
gradient scaling (CxGrad), which scales gradient norms of the backbone to
facilitate learning task-specific knowledge in the inner-loop. Since the
scaling factors are generated from task-conditioned parameters, gradient norms
of the backbone can be scaled in a task-wise fashion. Experimental results show
that CxGrad effectively encourages the backbone to learn task-specific
knowledge in the inner-loop and improves the performance of MAML up to a
significant margin in both same- and cross-domain few-shot classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Backdoor Attacks Against Point Cloud Classifiers. (arXiv:2110.10354v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10354">
<div class="article-summary-box-inner">
<span><p>Backdoor attacks (BA) are an emerging threat to deep neural network
classifiers. A classifier being attacked will predict to the attacker's target
class when a test sample from a source class is embedded with the backdoor
pattern (BP). Recently, the first BA against point cloud (PC) classifiers was
proposed, creating new threats to many important applications including
autonomous driving. Such PC BAs are not detectable by existing BA defenses due
to their special BP embedding mechanism. In this paper, we propose a
reverse-engineering defense that infers whether a PC classifier is backdoor
attacked, without access to its training set or to any clean classifiers for
reference. The effectiveness of our defense is demonstrated on the benchmark
ModeNet40 dataset for PCs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Multi-Person Mesh Recovery From Uncalibrated Multi-View Cameras. (arXiv:2110.10355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10355">
<div class="article-summary-box-inner">
<span><p>Dynamic multi-person mesh recovery has been a hot topic in 3D vision
recently. However, few works focus on the multi-person motion capture from
uncalibrated cameras, which mainly faces two challenges: the one is that
inter-person interactions and occlusions introduce inherent ambiguities for
both camera calibration and motion capture; The other is that a lack of dense
correspondences can be used to constrain sparse camera geometries in a dynamic
multi-person scene. Our key idea is incorporating motion prior knowledge into
simultaneous optimization of extrinsic camera parameters and human meshes from
noisy human semantics. First, we introduce a physics-geometry consistency to
reduce the low and high frequency noises of the detected human semantics. Then
a novel latent motion prior is proposed to simultaneously optimize extrinsic
camera parameters and coherent human motions from slightly noisy inputs.
Experimental results show that accurate camera parameters and human motions can
be obtained through one-stage optimization. The codes will be publicly
available at~\url{https://www.yangangwang.com}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NOD: Taking a Closer Look at Detection under Extreme Low-Light Conditions with Night Object Detection Dataset. (arXiv:2110.10364v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10364">
<div class="article-summary-box-inner">
<span><p>Recent work indicates that, besides being a challenge in producing
perceptually pleasing images, low light proves more difficult for machine
cognition than previously thought. In our work, we take a closer look at object
detection in low light. First, to support the development and evaluation of new
methods in this domain, we present a high-quality large-scale Night Object
Detection (NOD) dataset showing dynamic scenes captured on the streets at
night. Next, we directly link the lighting conditions to perceptual difficulty
and identify what makes low light problematic for machine cognition.
Accordingly, we provide instance-level annotation for a subset of the dataset
for an in-depth evaluation of future methods. We also present an analysis of
the baseline model performance to highlight opportunities for future research
and show that low light is a non-trivial problem that requires special
attention from the researchers. Further, to address the issues caused by low
light, we propose to incorporate an image enhancement module into the object
detection framework and two novel data augmentation techniques. Our image
enhancement module is trained under the guidance of the object detector to
learn image representation optimal for machine cognition rather than for the
human visual system. Finally, experimental results confirm that the proposed
method shows consistent improvement of the performance on low-light datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Repaint: Improving the Generalization of Down-Stream Visual Tasks by Generating Multiple Instances of Training Examples. (arXiv:2110.10366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10366">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) for visual tasks are believed to learn
both the low-level textures and high-level object attributes, throughout the
network depth. This paper further investigates the `texture bias' in CNNs. To
this end, we regenerate multiple instances of training examples from each
original image, through a process we call `repainting'. The repainted examples
preserve the shape and structure of the regions and objects within the scenes,
but diversify their texture and color. Our method can regenerate a same image
at different daylight, season, or weather conditions, can have colorization or
de-colorization effects, or even bring back some texture information from
blacked-out areas. The in-place repaint allows us to further use these
repainted examples for improving the generalization of CNNs. Through an
extensive set of experiments, we demonstrate the usefulness of the repainted
examples in training, for the tasks of image classification (ImageNet) and
object detection (COCO), over several state-of-the-art network architectures at
different capacities, and across different data availability regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning. (arXiv:2110.10368v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10368">
<div class="article-summary-box-inner">
<span><p>Existing semi-supervised learning (SSL) algorithms typically assume
class-balanced datasets, although the class distributions of many real-world
datasets are imbalanced. In general, classifiers trained on a class-imbalanced
dataset are biased toward the majority classes. This issue becomes more
problematic for SSL algorithms because they utilize the biased prediction of
unlabeled data for training. However, traditional class-imbalanced learning
techniques, which are designed for labeled data, cannot be readily combined
with SSL algorithms. We propose a scalable class-imbalanced SSL algorithm that
can effectively use unlabeled data, while mitigating class imbalance by
introducing an auxiliary balanced classifier (ABC) of a single layer, which is
attached to a representation layer of an existing SSL algorithm. The ABC is
trained with a class-balanced loss of a minibatch, while using high-quality
representations learned from all data points in the minibatch using the
backbone SSL algorithm to avoid overfitting and information loss.Moreover, we
use consistency regularization, a recent SSL technique for utilizing unlabeled
data in a modified way, to train the ABC to be balanced among the classes by
selecting unlabeled data with the same probability for each class. The proposed
algorithm achieves state-of-the-art performance in various class-imbalanced SSL
experiments using four benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Composition: Can Multiple Neural Networks Be Combined into a Single Network Using Only Unlabeled Data?. (arXiv:2110.10369v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10369">
<div class="article-summary-box-inner">
<span><p>The diversity of deep learning applications, datasets, and neural network
architectures necessitates a careful selection of the architecture and data
that match best to a target application. As an attempt to mitigate this
dilemma, this paper investigates the idea of combining multiple trained neural
networks using unlabeled data. In addition, combining multiple models into one
can speed up the inference, result in stronger, more capable models, and allows
us to select efficient device-friendly target network architectures. To this
end, the proposed method makes use of generation, filtering, and aggregation of
reliable pseudo-labels collected from unlabeled data. Our method supports using
an arbitrary number of input models with arbitrary architectures and
categories. Extensive performance evaluations demonstrated that our method is
very effective. For example, for the task of object detection and without using
any ground-truth labels, an EfficientDet-D0 trained on Pascal-VOC and an
EfficientDet-D1 trained on COCO, can be combined to a RetinaNet-ResNet50 model,
with a similar mAP as the supervised training. If fine-tuned in a
semi-supervised setting, the combined model achieves +18.6%, +12.6%, and +8.1%
mAP improvements over supervised training with 1%, 5%, and 10% of labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images. (arXiv:2110.10381v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10381">
<div class="article-summary-box-inner">
<span><p>Elbow fractures are one of the most common fracture types. Diagnoses on elbow
fractures often need the help of radiographic imaging to be read and analyzed
by a specialized radiologist with years of training. Thanks to the recent
advances of deep learning, a model that can classify and detect different types
of bone fractures needs only hours of training and has shown promising results.
However, most existing deep learning models are purely data-driven, lacking
incorporation of known domain knowledge from human experts. In this work, we
propose a novel deep learning method to diagnose elbow fracture from elbow
X-ray images by integrating domain-specific medical knowledge into a curriculum
learning framework. In our method, the training data are permutated by sampling
without replacement at the beginning of each training epoch. The sampling
probability of each training sample is guided by a scoring criterion
constructed based on clinically known knowledge from human experts, where the
scoring indicates the diagnosis difficultness of different elbow fracture
subtypes. We also propose an algorithm that updates the sampling probabilities
at each epoch, which is applicable to other sampling-based curriculum learning
frameworks. We design an experiment with 1865 elbow X-ray images for a
fracture/normal binary classification task and compare our proposed method to a
baseline method and a previous method using multiple metrics. Our results show
that the proposed method achieves the highest classification performance. Also,
our proposed probability update algorithm boosts the performance of the
previous method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Guided Multiview Deep Curriculum Learning for Elbow Fracture Classification. (arXiv:2110.10383v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10383">
<div class="article-summary-box-inner">
<span><p>Elbow fracture diagnosis often requires patients to take both frontal and
lateral views of elbow X-ray radiographs. In this paper, we propose a multiview
deep learning method for an elbow fracture subtype classification task. Our
strategy leverages transfer learning by first training two single-view models,
one for frontal view and the other for lateral view, and then transferring the
weights to the corresponding layers in the proposed multiview network
architecture. Meanwhile, quantitative medical knowledge was integrated into the
training process through a curriculum learning framework, which enables the
model to first learn from "easier" samples and then transition to "harder"
samples to reach better performance. In addition, our multiview network can
work both in a dual-view setting and with a single view as input. We evaluate
our method through extensive experiments on a classification task of elbow
fracture with a dataset of 1,964 images. Results show that our method
outperforms two related methods on bone fracture study in multiple settings,
and our technique is able to boost the performance of the compared methods. The
code is available at https://github.com/ljaiverson/multiview-curriculum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Data Repair Lead to Fair Models? Curating Contextually Fair Data To Reduce Model Bias. (arXiv:2110.10389v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10389">
<div class="article-summary-box-inner">
<span><p>Contextual information is a valuable cue for Deep Neural Networks (DNNs) to
learn better representations and improve accuracy. However, co-occurrence bias
in the training dataset may hamper a DNN model's generalizability to unseen
scenarios in the real world. For example, in COCO, many object categories have
a much higher co-occurrence with men compared to women, which can bias a DNN's
prediction in favor of men. Recent works have focused on task-specific training
strategies to handle bias in such scenarios, but fixing the available data is
often ignored. In this paper, we propose a novel and more generic solution to
address the contextual bias in the datasets by selecting a subset of the
samples, which is fair in terms of the co-occurrence with various classes for a
protected attribute. We introduce a data repair algorithm using the coefficient
of variation, which can curate fair and contextually balanced data for a
protected class(es). This helps in training a fair model irrespective of the
task, architecture or training methodology. Our proposed solution is simple,
effective, and can even be used in an active learning setting where the data
labels are not present or being generated incrementally. We demonstrate the
effectiveness of our algorithm for the task of object detection and multi-label
image classification across different datasets. Through a series of
experiments, we validate that curating contextually fair data helps make model
predictions fair by balancing the true positive rate for the protected class
across groups without compromising on the model's overall performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for HDR Imaging: State-of-the-Art and Future Trends. (arXiv:2110.10394v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10394">
<div class="article-summary-box-inner">
<span><p>High dynamic range (HDR) imaging is a technique that allows an extensive
dynamic range of exposures, which is important in image processing, computer
graphics, and computer vision. In recent years, there has been a significant
advancement in HDR imaging using deep learning (DL). This study conducts a
comprehensive and insightful survey and analysis of recent developments in deep
HDR imaging methodologies. We hierarchically and structurally group existing
deep HDR imaging methods into five categories based on (1) number/domain of
input exposures, (2) number of learning tasks, (3) novel sensor data, (4) novel
learning strategies, and (5) applications. Importantly, we provide a
constructive discussion on each category regarding its potential and
challenges. Moreover, we review some crucial aspects of deep HDR imaging, such
as datasets and evaluation metrics. Finally, we highlight some open problems
and point out future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DFaceFill: An Analysis-By-Synthesis Approach to Face Completion. (arXiv:2110.10395v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10395">
<div class="article-summary-box-inner">
<span><p>Existing face completion solutions are primarily driven by end-to-end models
that directly generate 2D completions of 2D masked faces. By having to
implicitly account for geometric and photometric variations in facial shape and
appearance, such approaches result in unrealistic completions, especially under
large variations in pose, shape, illumination and mask sizes. To alleviate
these limitations, we introduce 3DFaceFill, an analysis-by-synthesis approach
for face completion that explicitly considers the image formation process. It
comprises three components, (1) an encoder that disentangles the face into its
constituent 3D mesh, 3D pose, illumination and albedo factors, (2) an
autoencoder that inpaints the UV representation of facial albedo, and (3) a
renderer that resynthesizes the completed face. By operating on the UV
representation, 3DFaceFill affords the power of correspondence and allows us to
naturally enforce geometrical priors (e.g. facial symmetry) more effectively.
Quantitatively, 3DFaceFill improves the state-of-the-art by up to 4dB higher
PSNR and 25% better LPIPS for large masks. And, qualitatively, it leads to
demonstrably more photorealistic face completions over a range of masks and
occlusions while preserving consistency in global and component-wise shape,
pose, illumination and eye-gaze.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation. (arXiv:2110.10403v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10403">
<div class="article-summary-box-inner">
<span><p>Recent advances in transformer-based models have drawn attention to exploring
these techniques in medical image segmentation, especially in conjunction with
the U-Net model (or its variants), which has shown great success in medical
image segmentation, under both 2D and 3D settings. Current 2D based methods
either directly replace convolutional layers with pure transformers or consider
a transformer as an additional intermediate encoder between the encoder and
decoder of U-Net. However, these approaches only consider the attention
encoding within one single slice and do not utilize the axial-axis information
naturally provided by a 3D volume. In the 3D setting, convolution on volumetric
data and transformers both consume large GPU memory. One has to either
downsample the image or use cropped local patches to reduce GPU memory usage,
which limits its performance. In this paper, we propose Axial Fusion
Transformer UNet (AFTer-UNet), which takes both advantages of convolutional
layers' capability of extracting detailed features and transformers' strength
on long sequence modeling. It considers both intra-slice and inter-slice
long-range cues to guide the segmentation. Meanwhile, it has fewer parameters
and takes less GPU memory to train than the previous transformer-based models.
Extensive experiments on three multi-organ segmentation datasets demonstrate
that our method outperforms current state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARTS: Eliminating Inconsistency between Text Detection and Recognition with Auto-Rectification Text Spotter. (arXiv:2110.10405v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10405">
<div class="article-summary-box-inner">
<span><p>Recent approaches for end-to-end text spotting have achieved promising
results. However, most of the current spotters were plagued by the
inconsistency problem between text detection and recognition. In this work, we
introduce and prove the existence of the inconsistency problem and analyze it
from two aspects: (1) inconsistency of text recognition features between
training and testing, and (2) inconsistency of optimization targets between
text detection and recognition. To solve the aforementioned issues, we propose
a differentiable Auto-Rectification Module (ARM) together with a new training
strategy to enable propagating recognition loss back into detection branch, so
that our detection branch can be jointly optimized by detection and recognition
targets, which largely alleviates the inconsistency problem between text
detection and recognition. Based on these designs, we present a simple yet
robust end-to-end text spotting framework, termed Auto-Rectification Text
Spotter (ARTS), to detect and recognize arbitrarily-shaped text in natural
scenes. Extensive experiments demonstrate the superiority of our method. In
particular, our ARTS-S achieves 77.1% end-to-end text spotting F-measure on
Total-Text at a competitive speed of 10.5 FPS, which significantly outperforms
previous methods in both accuracy and inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth360: Monocular Depth Estimation using Learnable Axisymmetric Camera Model for Spherical Camera Image. (arXiv:2110.10415v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10415">
<div class="article-summary-box-inner">
<span><p>Self-supervised monocular depth estimation has been widely investigated to
estimate depth images and relative poses from RGB images. This framework is
attractive for researchers because the depth and pose networks can be trained
from just time sequence images without the need for the ground truth depth and
poses.
</p>
<p>In this work, we estimate the depth around a robot (360 degree view) using
time sequence spherical camera images, from a camera whose parameters are
unknown. We propose a learnable axisymmetric camera model which accepts
distorted spherical camera images with two fisheye camera images. In addition,
we trained our models with a photo-realistic simulator to generate ground truth
depth images to provide supervision. Moreover, we introduced loss functions to
provide floor constraints to reduce artifacts that can result from reflective
floor surfaces. We demonstrate the efficacy of our method using the spherical
camera images from the GO Stanford dataset and pinhole camera images from the
KITTI dataset to compare our method's performance with that of baseline method
in learning the camera parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A unifying framework for $n$-dimensional quasi-conformal mappings. (arXiv:2110.10437v1 [cs.CG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10437">
<div class="article-summary-box-inner">
<span><p>With the advancement of computer technology, there is a surge of interest in
effective mapping methods for objects in higher-dimensional spaces. To
establish a one-to-one correspondence between objects, higher-dimensional
quasi-conformal theory can be utilized for ensuring the bijectivity of the
mappings. In addition, it is often desirable for the mappings to satisfy
certain prescribed geometric constraints and possess low distortion in
conformality or volume. In this work, we develop a unifying framework for
computing $n$-dimensional quasi-conformal mappings. More specifically, we
propose a variational model that integrates quasi-conformal distortion,
volumetric distortion, landmark correspondence, intensity mismatch and volume
prior information to handle a large variety of deformation problems. We further
prove the existence of a minimizer for the proposed model and devise efficient
numerical methods to solve the optimization problem. We demonstrate the
effectiveness of the proposed framework using various experiments in two- and
three-dimensions, with applications to medical image registration, adaptive
remeshing and shape modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moir\'e Attack (MA): A New Potential Risk of Screen Photos. (arXiv:2110.10444v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10444">
<div class="article-summary-box-inner">
<span><p>Images, captured by a camera, play a critical role in training Deep Neural
Networks (DNNs). Usually, we assume the images acquired by cameras are
consistent with the ones perceived by human eyes. However, due to the different
physical mechanisms between human-vision and computer-vision systems, the final
perceived images could be very different in some cases, for example shooting on
digital monitors. In this paper, we find a special phenomenon in digital image
processing, the moir\'e effect, that could cause unnoticed security threats to
DNNs. Based on it, we propose a Moir\'e Attack (MA) that generates the
physical-world moir\'e pattern adding to the images by mimicking the shooting
process of digital devices. Extensive experiments demonstrate that our proposed
digital Moir\'e Attack (MA) is a perfect camouflage for attackers to tamper
with DNNs with a high success rate ($100.0\%$ for untargeted and $97.0\%$ for
targeted attack with the noise budget $\epsilon=4$), high transferability rate
across different models, and high robustness under various defenses.
Furthermore, MA owns great stealthiness because the moir\'e effect is
unavoidable due to the camera's inner physical structure, which therefore
hardly attracts the awareness of humans. Our code is available at
https://github.com/Dantong88/Moire_Attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Annotation Refinement for Object Detection. (arXiv:2110.10456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10456">
<div class="article-summary-box-inner">
<span><p>Supervised training of object detectors requires well-annotated large-scale
datasets, whose production is costly. Therefore, some efforts have been made to
obtain annotations in economical ways, such as cloud sourcing. However,
datasets obtained by these methods tend to contain noisy annotations such as
inaccurate bounding boxes and incorrect class labels. In this study, we propose
a new problem setting of training object detectors on datasets with entangled
noises of annotations of class labels and bounding boxes. Our proposed method
efficiently decouples the entangled noises, corrects the noisy annotations, and
subsequently trains the detector using the corrected annotations. We verified
the effectiveness of our proposed method and compared it with the baseline on
noisy datasets with different noise levels. The experimental results show that
our proposed method significantly outperforms the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Style Transfer. (arXiv:2110.10481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10481">
<div class="article-summary-box-inner">
<span><p>Currently, it is hard to compare and evaluate different style transfer
algorithms due to chaotic definitions of style and the absence of agreed
objective validation methods in the study of style transfer. In this paper, a
novel approach, the Unified Style Transfer (UST) model, is proposed. With the
introduction of a generative model for internal style representation, UST can
transfer images in two approaches, i.e., Domain-based and Image-based,
simultaneously. At the same time, a new philosophy based on the human sense of
art and style distributions for evaluating the transfer model is presented and
demonstrated, called Statistical Style Analysis. It provides a new path to
validate style transfer models' feasibility by validating the general
consistency between internal style representation and art facts. Besides, the
translation-invariance of AdaIN features is also discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of augmentation methods in classifying autism spectrum disorders from fMRI data with 3D convolutional neural networks. (arXiv:2110.10489v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10489">
<div class="article-summary-box-inner">
<span><p>Classifying subjects as healthy or diseased using neuroimaging data has
gained a lot of attention during the last 10 years. Here we apply deep learning
to derivatives from resting state fMRI data, and investigate how different 3D
augmentation techniques affect the test accuracy. Specifically, we use resting
state derivatives from 1,112 subjects in ABIDE preprocessed to train a 3D
convolutional neural network (CNN) to perform the classification. Our results
show that augmentation only provide minor improvements to the test accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Point Cloud Normal Estimation via Triplet Learning. (arXiv:2110.10494v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10494">
<div class="article-summary-box-inner">
<span><p>Normal estimation on 3D point clouds is a fundamental problem in 3D vision
and graphics. Current methods often show limited accuracy in predicting normals
at sharp features (e.g., edges and corners) and less robustness to noise. In
this paper, we propose a novel normal estimation method for point clouds. It
consists of two phases: (a) feature encoding which learns representations of
local patches, and (b) normal estimation that takes the learned representation
as input and regresses the normal vector. We are motivated that local patches
on isotropic and anisotropic surfaces have similar or distinct normals, and
that separable features or representations can be learned to facilitate normal
estimation. To realise this, we first construct triplets of local patches on 3D
point cloud data, and design a triplet network with a triplet loss for feature
encoding. We then design a simple network with several MLPs and a loss function
to regress the normal vector. Despite having a smaller network size compared to
most other methods, experimental results show that our method preserves sharp
features and achieves better normal estimation results on CAD-like shapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STALP: Style Transfer with Auxiliary Limited Pairing. (arXiv:2110.10501v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10501">
<div class="article-summary-box-inner">
<span><p>We present an approach to example-based stylization of images that uses a
single pair of a source image and its stylized counterpart. We demonstrate how
to train an image translation network that can perform real-time semantically
meaningful style transfer to a set of target images with similar content as the
source image. A key added value of our approach is that it considers also
consistency of target images during training. Although those have no stylized
counterparts, we constrain the translation to keep the statistics of neural
responses compatible with those extracted from the stylized source. In contrast
to concurrent techniques that use a similar input, our approach better
preserves important visual characteristics of the source style and can deliver
temporally stable results without the need to explicitly handle temporal
consistency. We demonstrate its practical utility on various applications
including video stylization, style transfer to panoramas, faces, and 3D models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Guided Depth Sensing. (arXiv:2110.10505v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10505">
<div class="article-summary-box-inner">
<span><p>Active depth sensors like structured light, lidar, and time-of-flight systems
sample the depth of the entire scene uniformly at a fixed scan rate. This leads
to limited spatio-temporal resolution where redundant static information is
over-sampled and precious motion information might be under-sampled. In this
paper, we present an efficient bio-inspired event-camera-driven depth
estimation algorithm. In our approach, we dynamically illuminate areas of
interest densely, depending on the scene activity detected by the event camera,
and sparsely illuminate areas in the field of view with no motion. The depth
estimation is achieved by an event-based structured light system consisting of
a laser point projector coupled with a second event-based sensor tuned to
detect the reflection of the laser from the scene. We show the feasibility of
our approach in a simulated autonomous driving scenario and real indoor
sequences using our prototype. We show that, in natural scenes like autonomous
driving and indoor environments, moving edges correspond to less than 10% of
the scene on average. Thus our setup requires the sensor to scan only 10% of
the scene, which could lead to almost 90% less power consumption by the
illumination source. While we present the evaluation and proof-of-concept for
an event-based structured-light system, the ideas presented here are applicable
for a wide range of depth-sensing modalities like LIDAR, time-of-flight, and
standard stereo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development and accuracy evaluation of Coded Phase-shift 3D scanner. (arXiv:2110.10520v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10520">
<div class="article-summary-box-inner">
<span><p>In this paper, we provide an overview of development of a structured light
3D-scanner based on combination of binary-coded patterns and sinusoidal
phase-shifted fringe patterns called Coded Phase-shift technique. Further, we
describe the experiments performed to evaluate measurement accuracy and
precision of the developed system. A study of this kind is expected to be
helpful in understanding the basic working of current structured-light 3D
scanners and the approaches followed for their performance assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Identifying Optical Signal Attacks on Autonomous Driving Systems. (arXiv:2110.10523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10523">
<div class="article-summary-box-inner">
<span><p>For autonomous driving, an essential task is to detect surrounding objects
accurately. To this end, most existing systems use optical devices, including
cameras and light detection and ranging (LiDAR) sensors, to collect environment
data in real time. In recent years, many researchers have developed advanced
machine learning models to detect surrounding objects. Nevertheless, the
aforementioned optical devices are vulnerable to optical signal attacks, which
could compromise the accuracy of object detection. To address this critical
issue, we propose a framework to detect and identify sensors that are under
attack. Specifically, we first develop a new technique to detect attacks on a
system that consists of three sensors. Our main idea is to: 1) use data from
three sensors to obtain two versions of depth maps (i.e., disparity) and 2)
detect attacks by analyzing the distribution of disparity errors. In our study,
we use real data sets and the state-of-the-art machine learning model to
evaluate our attack detection scheme and the results confirm the effectiveness
of our detection method. Based on the detection scheme, we further develop an
identification model that is capable of identifying up to n-2 attacked sensors
in a system with one LiDAR and n cameras. We prove the correctness of our
identification scheme and conduct experiments to show the accuracy of our
identification method. Finally, we investigate the overall sensitivity of our
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AniFormer: Data-driven 3D Animation with Transformer. (arXiv:2110.10533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10533">
<div class="article-summary-box-inner">
<span><p>We present a novel task, i.e., animating a target 3D object through the
motion of a raw driving sequence. In previous works, extra auxiliary
correlations between source and target meshes or intermedia factors are
inevitable to capture the motions in the driving sequences. Instead, we
introduce AniFormer, a novel Transformer-based architecture, that generates
animated 3D sequences by directly taking the raw driving sequences and
arbitrary same-type target meshes as inputs. Specifically, we customize the
Transformer architecture for 3D animation that generates mesh sequences by
integrating styles from target meshes and motions from the driving meshes.
Besides, instead of the conventional single regression head in the vanilla
Transformer, AniFormer generates multiple frames as outputs to preserve the
sequential consistency of the generated meshes. To achieve this, we carefully
design a pair of regression constraints, i.e., motion and appearance
constraints, that can provide strong regularization on the generated mesh
sequences. Our AniFormer achieves high-fidelity, realistic, temporally coherent
animated results and outperforms compared start-of-the-art methods on
benchmarks of diverse categories. Code is available:
https://github.com/mikecheninoulu/AniFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Model Generalization by Agreement of Learned Representations from Data Augmentation. (arXiv:2110.10536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10536">
<div class="article-summary-box-inner">
<span><p>Data augmentation reduces the generalization error by forcing a model to
learn invariant representations given different transformations of the input
image. In computer vision, on top of the standard image processing functions,
data augmentation techniques based on regional dropout such as CutOut, MixUp,
and CutMix and policy-based selection such as AutoAugment demonstrated
state-of-the-art (SOTA) results. With an increasing number of data augmentation
algorithms being proposed, the focus is always on optimizing the input-output
mapping while not realizing that there might be an untapped value in the
transformed images with the same label. We hypothesize that by forcing the
representations of two transformations to agree, we can further reduce the
model generalization error. We call our proposed method Agreement Maximization
or simply AgMax. With this simple constraint applied during training, empirical
results show that data augmentation algorithms can further improve the
classification accuracy of ResNet50 on ImageNet by up to 1.5%, WideResNet40-2
on CIFAR10 by up to 0.7%, WideResNet40-2 on CIFAR100 by up to 1.6%, and LeNet5
on Speech Commands Dataset by up to 1.4%. Experimental results further show
that unlike other regularization terms such as label smoothing, AgMax can take
advantage of the data augmentation to consistently improve model generalization
by a significant margin. On downstream tasks such as object detection and
segmentation on PascalVOC and COCO, AgMax pre-trained models outperforms other
data augmentation methods by as much as 1.0mAP (box) and 0.5mAP (mask). Code is
available at https://github.com/roatienza/agmax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning. (arXiv:2110.10538v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10538">
<div class="article-summary-box-inner">
<span><p>Access to 3D point cloud representations has been widely facilitated by LiDAR
sensors embedded in various mobile devices. This has led to an emerging need
for fast and accurate point cloud processing techniques. In this paper, we
revisit and dive deeper into PointNet++, one of the most influential yet
under-explored networks, and develop faster and more accurate variants of the
model. We first present a novel Separable Set Abstraction (SA) module that
disentangles the vanilla SA module used in PointNet++ into two separate
learning stages: (1) learning channel correlation and (2) learning spatial
correlation. The Separable SA module is significantly faster than the vanilla
version, yet it achieves comparable performance. We then introduce a new
Anisotropic Reduction function into our Separable SA module and propose an
Anisotropic Separable SA (ASSA) module that substantially increases the
network's accuracy. We later replace the vanilla SA modules in PointNet++ with
the proposed ASSA module, and denote the modified network as ASSANet. Extensive
experiments on point cloud classification, semantic segmentation, and part
segmentation show that ASSANet outperforms PointNet++ and other methods,
achieving much higher accuracy and faster speeds. In particular, ASSANet
outperforms PointNet++ by $7.4$ mIoU on S3DIS Area 5, while maintaining $1.6
\times $ faster inference speed on a single NVIDIA 2080Ti GPU. Our scaled
ASSANet variant achieves $66.8$ mIoU and outperforms KPConv, while being more
than $54 \times$ faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trash or Treasure? An Interactive Dual-Stream Strategy for Single Image Reflection Separation. (arXiv:2110.10546v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10546">
<div class="article-summary-box-inner">
<span><p>Single image reflection separation (SIRS), as a representative blind source
separation task, aims to recover two layers, $\textit{i.e.}$, transmission and
reflection, from one mixed observation, which is challenging due to the highly
ill-posed nature. Existing deep learning based solutions typically restore the
target layers individually, or with some concerns at the end of the output,
barely taking into account the interaction across the two streams/branches. In
order to utilize information more efficiently, this work presents a general yet
simple interactive strategy, namely $\textit{your trash is my treasure}$
(YTMT), for constructing dual-stream decomposition networks. To be specific, we
explicitly enforce the two streams to communicate with each other block-wisely.
Inspired by the additive property between the two components, the interactive
path can be easily built via transferring, instead of discarding, deactivated
information by the ReLU rectifier from one stream to the other. Both ablation
studies and experimental results on widely-used SIRS datasets are conducted to
demonstrate the efficacy of YTMT, and reveal its superiority over other
state-of-the-art alternatives. The implementation is quite simple and our code
is publicly available at
$\href{https://github.com/mingcv/YTMT-Strategy}{\textit{https://github.com/mingcv/YTMT-Strategy}}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Temporal Action Localization with Query Adaptive Transformer. (arXiv:2110.10552v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10552">
<div class="article-summary-box-inner">
<span><p>Existing temporal action localization (TAL) works rely on a large number of
training videos with exhaustive segment-level annotation, preventing them from
scaling to new classes. As a solution to this problem, few-shot TAL (FS-TAL)
aims to adapt a model to a new class represented by as few as a single video.
Exiting FS-TAL methods assume trimmed training videos for new classes. However,
this setting is not only unnatural actions are typically captured in untrimmed
videos, but also ignores background video segments containing vital contextual
cues for foreground action segmentation. In this work, we first propose a new
FS-TAL setting by proposing to use untrimmed training videos. Further, a novel
FS-TAL model is proposed which maximizes the knowledge transfer from training
classes whilst enabling the model to be dynamically adapted to both the new
class and each video of that class simultaneously. This is achieved by
introducing a query adaptive Transformer in the model. Extensive experiments on
two action localization benchmarks demonstrate that our method can outperform
all the state of the art alternatives significantly in both single-domain and
cross-domain scenarios. The source code can be found in
https://github.com/sauradip/fewshotQAT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Monocular Localization in Sparse HD Maps Leveraging Multi-Task Uncertainty Estimation. (arXiv:2110.10563v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10563">
<div class="article-summary-box-inner">
<span><p>Robust localization in dense urban scenarios using a low-cost sensor setup
and sparse HD maps is highly relevant for the current advances in autonomous
driving, but remains a challenging topic in research. We present a novel
monocular localization approach based on a sliding-window pose graph that
leverages predicted uncertainties for increased precision and robustness
against challenging scenarios and per frame failures. To this end, we propose
an efficient multi-task uncertainty-aware perception module, which covers
semantic segmentation, as well as bounding box detection, to enable the
localization of vehicles in sparse maps, containing only lane borders and
traffic lights. Further, we design differentiable cost maps that are directly
generated from the estimated uncertainties. This opens up the possibility to
minimize the reprojection loss of amorphous map elements in an association free
and uncertainty-aware manner. Extensive evaluation on the Lyft 5 dataset shows
that, despite the sparsity of the map, our approach enables robust and accurate
6D localization in challenging urban scenarios
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fingerprint recognition with embedded presentation attacks detection: are we ready?. (arXiv:2110.10567v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10567">
<div class="article-summary-box-inner">
<span><p>The diffusion of fingerprint verification systems for security applications
makes it urgent to investigate the embedding of software-based presentation
attack detection algorithms (PAD) into such systems. Companies and institutions
need to know whether such integration would make the system more "secure" and
whether the technology available is ready, and, if so, at what operational
working conditions. Despite significant improvements, especially by adopting
deep learning approaches to fingerprint PAD, current research did not state
much about their effectiveness when embedded in fingerprint verification
systems. We believe that the lack of works is explained by the lack of
instruments to investigate the problem, that is, modeling the cause-effect
relationships when two non-zero error-free systems work together. Accordingly,
this paper explores the fusion of PAD into verification systems by proposing a
novel investigation instrument: a performance simulator based on the
probabilistic modeling of the relationships among the Receiver Operating
Characteristics (ROC) of the two individual systems when PAD and verification
stages are implemented sequentially. As a matter of fact, this is the most
straightforward, flexible, and widespread approach. We carry out simulations on
the PAD algorithms' ROCs submitted to the most recent editions of LivDet
(2017-2019), the state-of-the-art NIST Bozorth3, and the top-level Veryfinger
12 matchers. Reported experiments explore significant scenarios to get the
conditions under which fingerprint matching with embedded PAD can improve,
rather than degrade, the overall personal verification performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inference Graphs for CNN Interpretation. (arXiv:2110.10568v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10568">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have achieved superior accuracy in many
visual related tasks. However, the inference process through intermediate
layers is opaque, making it difficult to interpret such networks or develop
trust in their operation. We propose to model the network hidden layers
activity using probabilistic models. The activity patterns in layers of
interest are modeled as Gaussian mixture models, and transition probabilities
between clusters in consecutive modeled layers are estimated. Based on
maximum-likelihood considerations, nodes and paths relevant for network
prediction are chosen, connected, and visualized as an inference graph. We show
that such graphs are useful for understanding the general inference process of
a class, as well as explaining decisions the network makes regarding specific
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Learning Framework for Diffeomorphic Image Registration based on Quasi-conformal Geometry. (arXiv:2110.10580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10580">
<div class="article-summary-box-inner">
<span><p>Image registration, the process of defining meaningful correspondences
between images, is essential for various image analysis tasks, especially
medical imaging. Numerous learning-based methods, notably convolutional neural
networks (CNNs), for deformable image registration proposed in recent years
have demonstrated the feasibility and superiority of deep learning techniques
for registration problems. Besides, compared to traditional algorithms'
optimization scheme of the objective function for each image pair,
learning-based algorithms are several orders of magnitude faster. However,
these data-driven methods without proper constraint on the deformation field
will easily lead to topological foldings.
</p>
<p>To tackle this problem, We propose the quasi-conformal registration network
(QCRegNet), an unsupervised learning framework, to obtain diffeomorphic 2D
image registrations with large deformations based on quasi-conformal (QC) map,
an orientation-preserving homeomorphism between two manifolds.
</p>
<p>The basic idea is to design a CNN mapping image pairs to deformation fields.
QCRegNet consists of the estimator network and the Beltrami solver network
(BSNet). The estimator network takes image pair as input and outputs the
Beltrami coefficient (BC). The BC, which captures conformal distortion of a QC
map and guarantees the bijectivity, will then be input to the BSNet, a
task-independent network which reconstructs the desired QC map.
</p>
<p>Furthermore, we reduce the number of network parameters and computational
complexity by utilizing Fourier approximation to compress BC. Experiments have
been carried out on different data such as underwater and medical images.
Registration results show that the registration accuracy is comparable to
state-of-the-art methods and diffeomorphism is to a great extent guaranteed
compared to other diffeomorphic registration algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look at What I'm Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos. (arXiv:2110.10596v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10596">
<div class="article-summary-box-inner">
<span><p>We introduce the task of spatially localizing narrated interactions in
videos. Key to our approach is the ability to learn to spatially localize
interactions with self-supervision on a large corpus of videos with
accompanying transcribed narrations. To achieve this goal, we propose a
multilayer cross-modal attention network that enables effective optimization of
a contrastive loss during training. We introduce a divided strategy that
alternates between computing inter- and intra-modal attention across the visual
and natural language modalities, which allows effective training via directly
contrasting the two modalities' representations. We demonstrate the
effectiveness of our approach by self-training on the HowTo100M instructional
video dataset and evaluating on a newly collected dataset of localized
described interactions in the YouCook2 dataset. We show that our approach
outperforms alternative baselines, including shallow co-attention and full
cross-modal attention. We also apply our approach to grounding phrases in
images with weak supervision on Flickr30K and show that stacking multiple
attention layers is effective and, when combined with a word-to-region loss,
achieves state of the art on recall-at-one and pointing hand accuracies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Instance Segmentation by Instance Flow Assembly. (arXiv:2110.10599v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10599">
<div class="article-summary-box-inner">
<span><p>Instance segmentation is a challenging task aiming at classifying and
segmenting all object instances of specific classes. While two-stage box-based
methods achieve top performances in the image domain, they cannot easily extend
their superiority into the video domain. This is because they usually deal with
features or images cropped from the detected bounding boxes without alignment,
failing to capture pixel-level temporal consistency. We embrace the observation
that bottom-up methods dealing with box-free features could offer accurate
spacial correlations across frames, which can be fully utilized for object and
pixel level tracking. We first propose our bottom-up framework equipped with a
temporal context fusion module to better encode inter-frame correlations.
Intra-frame cues for semantic segmentation and object localization are
simultaneously extracted and reconstructed by corresponding decoders after a
shared backbone. For efficient and robust tracking among instances, we
introduce an instance-level correspondence across adjacent frames, which is
represented by a center-to-center flow, termed as instance flow, to assemble
messy dense temporal correspondences. Experiments demonstrate that the proposed
method outperforms the state-of-the-art online methods (taking image-level
input) on the challenging Youtube-VIS dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Domain Adaptation for Semantic Segmentation. (arXiv:2110.10639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10639">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches for semantic segmentation rely primarily on
supervised learning approaches and require substantial efforts in producing
pixel-level annotations. Further, such approaches may perform poorly when
applied to unseen image domains. To cope with these limitations, both
unsupervised domain adaptation (UDA) with full source supervision but without
target supervision and semi-supervised learning (SSL) with partial supervision
have been proposed. While such methods are effective at aligning different
feature distributions, there is still a need to efficiently exploit unlabeled
data to address the performance gap with respect to fully-supervised methods.
In this paper we address semi-supervised domain adaptation (SSDA) for semantic
segmentation, where a large amount of labeled source data as well as a small
amount of labeled target data are available. We propose a novel and effective
two-step semi-supervised dual-domain adaptation (SSDDA) approach to address
both cross- and intra-domain gaps in semantic segmentation. The proposed
framework is comprised of two mixing modules. First, we conduct a cross-domain
adaptation via an image-level mixing strategy, which learns to align the
distribution shift of features between the source data and target data. Second,
intra-domain adaptation is achieved using a separate student-teacher network
which is built to generate category-level data augmentation by mixing unlabeled
target data in a way that respects predicted object boundaries. We demonstrate
that the proposed approach outperforms state-of-the-art methods on two common
synthetic-to-real semantic segmentation benchmarks. An extensive ablation study
is provided to further validate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSS-Net: Memory Efficient High Resolution Semantic Segmentation of 3D Medical Data. (arXiv:2110.10640v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10640">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) are the current state-of-the-art
meta-algorithm for volumetric segmentation of medical data, for example, to
localize COVID-19 infected tissue on computer tomography scans or the detection
of tumour volumes in magnetic resonance imaging. A key limitation of 3D CNNs on
voxelised data is that the memory consumption grows cubically with the training
data resolution. Occupancy networks (O-Nets) are an alternative for which the
data is represented continuously in a function space and 3D shapes are learned
as a continuous decision boundary. While O-Nets are significantly more memory
efficient than 3D CNNs, they are limited to simple shapes, are relatively slow
at inference, and have not yet been adapted for 3D semantic segmentation of
medical data. Here, we propose Occupancy Networks for Semantic Segmentation
(OSS-Nets) to accurately and memory-efficiently segment 3D medical data. We
build upon the original O-Net with modifications for increased expressiveness
leading to improved segmentation performance comparable to 3D CNNs, as well as
modifications for faster inference. We leverage local observations to represent
complex shapes and prior encoder predictions to expedite inference. We showcase
OSS-Net's performance on 3D brain tumour and liver segmentation against a
function space baseline (O-Net), a performance baseline (3D residual U-Net),
and an efficiency baseline (2D residual U-Net). OSS-Net yields segmentation
results similar to the performance baseline and superior to the function space
and efficiency baselines. In terms of memory efficiency, OSS-Net consumes
comparable amounts of memory as the function space baseline, somewhat more
memory than the efficiency baseline and significantly less than the performance
baseline. As such, OSS-Net enables memory-efficient and accurate 3D semantic
segmentation that can scale to high resolutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Different V1 Brain Model Variants to Improve Robustness to Image Corruptions in CNNs. (arXiv:2110.10645v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10645">
<div class="article-summary-box-inner">
<span><p>While some convolutional neural networks (CNNs) have surpassed human visual
abilities in object classification, they often struggle to recognize objects in
images corrupted with different types of common noise patterns, highlighting a
major limitation of this family of models. Recently, it has been shown that
simulating a primary visual cortex (V1) at the front of CNNs leads to small
improvements in robustness to these image perturbations. In this study, we
start with the observation that different variants of the V1 model show gains
for specific corruption types. We then build a new model using an ensembling
technique, which combines multiple individual models with different V1
front-end variants. The model ensemble leverages the strengths of each
individual model, leading to significant improvements in robustness across all
corruption categories and outperforming the base model by 38% on average.
Finally, we show that using distillation, it is possible to partially compress
the knowledge in the ensemble model into a single model with a V1 front-end.
While the ensembling and distillation techniques used here are hardly
biologically-plausible, the results presented here demonstrate that by
combining the specific strengths of different neuronal circuits in V1 it is
possible to improve the robustness of CNNs for a wide range of perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervision and Spatial-Sequential Attention Based Loss for Multi-Person Pose Estimation. (arXiv:2110.10734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10734">
<div class="article-summary-box-inner">
<span><p>Bottom-up based multi-person pose estimation approaches use heatmaps with
auxiliary predictions to estimate joint positions and belonging at one time.
Recently, various combinations between auxiliary predictions and heatmaps have
been proposed for higher performance, these predictions are supervised by the
corresponding L2 loss function directly. However, the lack of more explicit
supervision results in low features utilization and contradictions between
predictions in one model. To solve these problems, this paper proposes (i) a
new loss organization method which uses self-supervised heatmaps to reduce
prediction contradictions and spatial-sequential attention to enhance networks'
features extraction; (ii) a new combination of predictions composed by
heatmaps, Part Affinity Fields (PAFs) and our block-inside offsets to fix
pixel-level joints positions and further demonstrates the effectiveness of
proposed loss function. Experiments are conducted on the MS COCO keypoint
dataset and adopting OpenPose as the baseline model. Our method outperforms the
baseline overall. On the COCO verification dataset, the mAP of OpenPose trained
with our proposals outperforms the OpenPose baseline by over 5.5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class Incremental Online Streaming Learning. (arXiv:2110.10741v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10741">
<div class="article-summary-box-inner">
<span><p>A wide variety of methods have been developed to enable lifelong learning in
conventional deep neural networks. However, to succeed, these methods require a
`batch' of samples to be available and visited multiple times during training.
While this works well in a static setting, these methods continue to suffer in
a more realistic situation where data arrives in \emph{online streaming
manner}. We empirically demonstrate that the performance of current approaches
degrades if the input is obtained as a stream of data with the following
restrictions: $(i)$ each instance comes one at a time and can be seen only
once, and $(ii)$ the input data violates the i.i.d assumption, i.e., there can
be a class-based correlation. We propose a novel approach (CIOSL) for the
class-incremental learning in an \emph{online streaming setting} to address
these challenges. The proposed approach leverages implicit and explicit dual
weight regularization and experience replay. The implicit regularization is
leveraged via the knowledge distillation, while the explicit regularization
incorporates a novel approach for parameter regularization by learning the
joint distribution of the buffer replay and the current sample. Also, we
propose an efficient online memory replay and replacement buffer strategy that
significantly boosts the model's performance. Extensive experiments and
ablation on challenging datasets show the efficacy of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Real-world Image Super-resolution via Hardware-based Adaptive Degradation Models. (arXiv:2110.10755v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10755">
<div class="article-summary-box-inner">
<span><p>Most single image super-resolution (SR) methods are developed on synthetic
low-resolution (LR) and high-resolution (HR) image pairs, which are simulated
by a predetermined degradation operation, e.g., bicubic downsampling. However,
these methods only learn the inverse process of the predetermined operation, so
they fail to super resolve the real-world LR images; the true formulation
deviates from the predetermined operation. To address this problem, we propose
a novel supervised method to simulate an unknown degradation process with the
inclusion of the prior hardware knowledge of the imaging system. We design an
adaptive blurring layer (ABL) in the supervised learning framework to estimate
the target LR images. The hyperparameters of the ABL can be adjusted for
different imaging hardware. The experiments on the real-world datasets validate
that our degradation model can estimate LR images more accurately than the
predetermined degradation operation, as well as facilitate existing SR methods
to perform reconstructions on real-world LR images more accurately than the
conventional approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Closed-loop Feedback Registration for Consecutive Images of Moving Flexible Targets. (arXiv:2110.10772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10772">
<div class="article-summary-box-inner">
<span><p>Advancement of imaging techniques enables consecutive image sequences to be
acquired for quality monitoring of manufacturing production lines. Registration
for these image sequences is essential for in-line pattern inspection and
metrology, e.g., in the printing process of flexible electronics. However,
conventional image registration algorithms cannot produce accurate results when
the images contain many similar and deformable patterns in the manufacturing
process. Such a failure originates from a fact that the conventional algorithms
only use the spatial and pixel intensity information for registration.
Considering the nature of temporal continuity and consecution of the product
images, in this paper, we propose a closed-loop feedback registration algorithm
for matching and stitching the deformable printed patterns on a moving flexible
substrate. The algorithm leverages the temporal and spatial relationships of
the consecutive images and the continuity of the image sequence for fast,
accurate, and robust point matching. Our experimental results show that our
algorithm can find more matching point pairs with a lower root mean squared
error (RMSE) compared to other state-of-the-art algorithms while offering
significant improvements to running time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style Agnostic 3D Reconstruction via Adversarial Style Transfer. (arXiv:2110.10784v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10784">
<div class="article-summary-box-inner">
<span><p>Reconstructing the 3D geometry of an object from an image is a major
challenge in computer vision. Recently introduced differentiable renderers can
be leveraged to learn the 3D geometry of objects from 2D images, but those
approaches require additional supervision to enable the renderer to produce an
output that can be compared to the input image. This can be scene information
or constraints such as object silhouettes, uniform backgrounds, material,
texture, and lighting. In this paper, we propose an approach that enables a
differentiable rendering-based learning of 3D objects from images with
backgrounds without the need for silhouette supervision. Instead of trying to
render an image close to the input, we propose an adversarial style-transfer
and domain adaptation pipeline that allows to translate the input image domain
to the rendered image domain. This allows us to directly compare between a
translated image and the differentiable rendering of a 3D object reconstruction
in order to train the 3D object reconstruction network. We show that the
approach learns 3D geometry from images with backgrounds and provides a better
performance than constrained methods for single-view 3D object reconstruction
on this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DVIO: Depth aided visual inertial odometry for RGBD sensors. (arXiv:2110.10805v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10805">
<div class="article-summary-box-inner">
<span><p>In past few years we have observed an increase in the usage of RGBD sensors
in mobile devices. These sensors provide a good estimate of the depth map for
the camera frame, which can be used in numerous augmented reality applications.
This paper presents a new visual inertial odometry (VIO) system, which uses
measurements from a RGBD sensor and an inertial measurement unit (IMU) sensor
for estimating the motion state of the mobile device. The resulting system is
called the depth-aided VIO (DVIO) system. In this system we add the depth
measurement as part of the nonlinear optimization process. Specifically, we
propose methods to use the depth measurement using one-dimensional (1D) feature
parameterization as well as three-dimensional (3D) feature parameterization. In
addition, we propose to utilize the depth measurement for estimating time
offset between the unsynchronized IMU and the RGBD sensors. Last but not least,
we propose a novel block-based marginalization approach to speed up the
marginalization processes and maintain the real-time performance of the overall
system. Experimental results validate that the proposed DVIO system outperforms
the other state-of-the-art VIO systems in terms of trajectory accuracy as well
as processing time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Based Person Search with Limited Data. (arXiv:2110.10807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10807">
<div class="article-summary-box-inner">
<span><p>Text-based person search (TBPS) aims at retrieving a target person from an
image gallery with a descriptive text query. Solving such a fine-grained
cross-modal retrieval task is challenging, which is further hampered by the
lack of large-scale datasets. In this paper, we present a framework with two
novel components to handle the problems brought by limited data. Firstly, to
fully utilize the existing small-scale benchmarking datasets for more
discriminative feature learning, we introduce a cross-modal momentum
contrastive learning framework to enrich the training data for a given
mini-batch. Secondly, we propose to transfer knowledge learned from existing
coarse-grained large-scale datasets containing image-text pairs from
drastically different problem domains to compensate for the lack of TBPS
training data. A transfer learning method is designed so that useful
information can be transferred despite the large domain gap. Armed with these
components, our method achieves new state of the art on the CUHK-PEDES dataset
with significant improvements over the prior art in terms of Rank-1 and mAP.
Our code is available at https://github.com/BrandonHanx/TextReID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HALP: Hardware-Aware Latency Pruning. (arXiv:2110.10811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10811">
<div class="article-summary-box-inner">
<span><p>Structural pruning can simplify network architecture and improve inference
speed. We propose Hardware-Aware Latency Pruning (HALP) that formulates
structural pruning as a global resource allocation optimization problem, aiming
at maximizing the accuracy while constraining latency under a predefined
budget. For filter importance ranking, HALP leverages latency lookup table to
track latency reduction potential and global saliency score to gauge accuracy
drop. Both metrics can be evaluated very efficiently during pruning, allowing
us to reformulate global structural pruning under a reward maximization problem
given target constraint. This makes the problem solvable via our augmented
knapsack solver, enabling HALP to surpass prior work in pruning efficacy and
accuracy-efficiency trade-off. We examine HALP on both classification and
detection tasks, over varying networks, on ImageNet and VOC datasets. In
particular, for ResNet-50/-101 pruning on ImageNet, HALP improves network
throughput by $1.60\times$/$1.90\times$ with $+0.3\%$/$-0.2\%$ top-1 accuracy
changes, respectively. For SSD pruning on VOC, HALP improves throughput by
$1.94\times$ with only a $0.56$ mAP drop. HALP consistently outperforms prior
art, sometimes by large margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CXR-Net: An Encoder-Decoder-Encoder Multitask Deep Neural Network for Explainable and Accurate Diagnosis of COVID-19 pneumonia with Chest X-ray Images. (arXiv:2110.10813v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10813">
<div class="article-summary-box-inner">
<span><p>Accurate and rapid detection of COVID-19 pneumonia is crucial for optimal
patient treatment. Chest X-Ray (CXR) is the first line imaging test for
COVID-19 pneumonia diagnosis as it is fast, cheap and easily accessible.
Inspired by the success of deep learning (DL) in computer vision, many
DL-models have been proposed to detect COVID-19 pneumonia using CXR images.
Unfortunately, these deep classifiers lack the transparency in interpreting
findings, which may limit their applications in clinical practice. The existing
commonly used visual explanation methods are either too noisy or imprecise,
with low resolution, and hence are unsuitable for diagnostic purposes. In this
work, we propose a novel explainable deep learning framework (CXRNet) for
accurate COVID-19 pneumonia detection with an enhanced pixel-level visual
explanation from CXR images. The proposed framework is based on a new
Encoder-Decoder-Encoder multitask architecture, allowing for both disease
classification and visual explanation. The method has been evaluated on real
world CXR datasets from both public and private data sources, including:
healthy, bacterial pneumonia, viral pneumonia and COVID-19 pneumonia cases The
experimental results demonstrate that the proposed method can achieve a
satisfactory level of accuracy and provide fine-resolution classification
activation maps for visual explanation in lung disease detection. The Average
Accuracy, the Precision, Recall and F1-score of COVID-19 pneumonia reached
0.879, 0.985, 0.992 and 0.989, respectively. We have also found that using lung
segmented (CXR) images can help improve the performance of the model. The
proposed method can provide more detailed high resolution visual explanation
for the classification decision, compared to current state-of-the-art visual
explanation methods and has a great potential to be used in clinical practice
for COVID-19 pneumonia diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization. (arXiv:2110.10832v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10832">
<div class="article-summary-box-inner">
<span><p>In Domain Generalization (DG) settings, models trained on a given set of
training domains have notoriously chaotic performance on distribution shifted
test domains, and stochasticity in optimization (e.g. seed) plays a big role.
This makes deep learning models unreliable in real world settings. We first
show that a simple protocol for averaging model parameters along the
optimization path, starting early during training, both significantly boosts
domain generalization and diminishes the impact of stochasticity by improving
the rank correlation between the in-domain validation accuracy and out-domain
test accuracy, which is crucial for reliable model selection. Next, we show
that an ensemble of independently trained models also has a chaotic behavior in
the DG setting. Taking advantage of our observation, we show that instead of
ensembling unaveraged models, ensembling moving average models (EoA) from
different runs does increase stability and further boosts performance. On the
DomainBed benchmark, when using a ResNet-50 pre-trained on ImageNet, this
ensemble of averages achieves $88.6\%$ on PACS, $79.1\%$ on VLCS, $72.5\%$ on
OfficeHome, $52.3\%$ on TerraIncognita, and $47.4\%$ on DomainNet, an average
of $68.0\%$, beating ERM (w/o model averaging) by $\sim 4\%$. We also evaluate
a model that is pre-trained on a larger dataset, where we show EoA achieves an
average accuracy of $72.7\%$, beating its corresponding ERM baseline by $5\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-resolution rainfall-runoff modeling using graph neural network. (arXiv:2110.10833v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10833">
<div class="article-summary-box-inner">
<span><p>Time-series modeling has shown great promise in recent studies using the
latest deep learning algorithms such as LSTM (Long Short-Term Memory). These
studies primarily focused on watershed-scale rainfall-runoff modeling or
streamflow forecasting, but the majority of them only considered a single
watershed as a unit. Although this simplification is very effective, it does
not take into account spatial information, which could result in significant
errors in large watersheds. Several studies investigated the use of GNN (Graph
Neural Networks) for data integration by decomposing a large watershed into
multiple sub-watersheds, but each sub-watershed is still treated as a whole,
and the geoinformation contained within the watershed is not fully utilized. In
this paper, we propose the GNRRM (Graph Neural Rainfall-Runoff Model), a novel
deep learning model that makes full use of spatial information from
high-resolution precipitation data, including flow direction and geographic
information. When compared to baseline models, GNRRM has less over-fitting and
significantly improves model performance. Our findings support the importance
of hydrological data in deep learning-based rainfall-runoff modeling, and we
encourage researchers to include more domain knowledge in their models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Visuospatial, Linguistic and Commonsense Structure into Story Visualization. (arXiv:2110.10834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10834">
<div class="article-summary-box-inner">
<span><p>While much research has been done in text-to-image synthesis, little work has
been done to explore the usage of linguistic structure of the input text. Such
information is even more important for story visualization since its inputs
have an explicit narrative structure that needs to be translated into an image
sequence (or visual story). Prior work in this domain has shown that there is
ample room for improvement in the generated image sequence in terms of visual
quality, consistency and relevance. In this paper, we first explore the use of
constituency parse trees using a Transformer-based recurrent architecture for
encoding structured input. Second, we augment the structured input with
commonsense information and study the impact of this external knowledge on the
generation of visual story. Third, we also incorporate visual structure via
bounding boxes and dense captioning to provide feedback about the
characters/objects in generated images within a dual learning setup. We show
that off-the-shelf dense-captioning models trained on Visual Genome can improve
the spatial structure of images from a different target domain without needing
fine-tuning. We train the model end-to-end using intra-story contrastive loss
(between words and image sub-regions) and show significant improvements in
several metrics (and human evaluation) for multiple datasets. Finally, we
provide an analysis of the linguistic and visuo-spatial information. Code and
data: https://github.com/adymaharana/VLCStoryGan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving the L1 regularized least square problem via a box-constrained smooth minimization. (arXiv:1704.03443v3 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1704.03443">
<div class="article-summary-box-inner">
<span><p>In this paper, an equivalent smooth minimization for the L1 regularized least
square problem is proposed. The proposed problem is a convex box-constrained
smooth minimization which allows applying fast optimization methods to find its
solution. Further, it is investigated that the property "the dual of dual is
primal" holds for the L1 regularized least square problem. A solver for the
smooth problem is proposed, and its affinity to the proximal gradient is shown.
Finally, the experiments on L1 and total variation regularized problems are
performed, and the corresponding results are reported.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Deep Neural Network for Photo-realistic Image Super-Resolution. (arXiv:1903.02240v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1903.02240">
<div class="article-summary-box-inner">
<span><p>Recent progress in deep learning-based models has improved photo-realistic
(or perceptual) single-image super-resolution significantly. However, despite
their powerful performance, many methods are difficult to apply to real-world
applications because of the heavy computational requirements. To facilitate the
use of a deep model under such demands, we focus on keeping the network
efficient while maintaining its performance. In detail, we design an
architecture that implements a cascading mechanism on a residual network to
boost the performance with limited resources via multi-level feature fusion. In
addition, our proposed model adopts group convolution and recursive schemes in
order to achieve extreme efficiency. We further improve the perceptual quality
of the output by employing the adversarial learning paradigm and a multi-scale
discriminator approach. The performance of our method is investigated through
extensive internal experiments and benchmarks using various datasets. Our
results show that our models outperform the recent methods with similar
complexity, for both traditional pixel-based and perception-based tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.09147">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has surpassed traditional methods for
medical image classification. However, CNN is vulnerable to adversarial attacks
which may lead to disastrous consequences in medical applications. Although
adversarial noises are usually generated by attack algorithms,
white-noise-induced adversarial samples can exist, and therefore the threats
are real. In this study, we propose a novel training method, named IMA, to
improve the robust-ness of CNN against adversarial noises. During training, the
IMA method increases the margins of training samples in the input space, i.e.,
moving CNN decision boundaries far away from the training samples to improve
robustness. The IMA method is evaluated on publicly available datasets under
strong 100-PGD white-box adversarial attacks, and the results show that the
proposed method significantly improved CNN classification and segmentation
accuracy on noisy data while keeping a high accuracy on clean data. We hope our
approach may facilitate the development of robust applications in medical
field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Evaluating Weakly Supervised Action Segmentation Methods. (arXiv:2005.09743v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.09743">
<div class="article-summary-box-inner">
<span><p>Action segmentation is the task of temporally segmenting every frame of an
untrimmed video. Weakly supervised approaches to action segmentation,
especially from transcripts have been of considerable interest to the computer
vision community. In this work, we focus on two aspects of the use and
evaluation of weakly supervised action segmentation approaches that are often
overlooked: the performance variance over multiple training runs and the impact
of selecting feature extractors for this task. To tackle the first problem, we
train each method on the Breakfast dataset 5 times and provide average and
standard deviation of the results. Our experiments show that the standard
deviation over these repetitions is between 1 and 2.5% and significantly
affects the comparison between different approaches. Furthermore, our
investigation on feature extraction shows that, for the studied
weakly-supervised action segmentation methods, higher-level I3D features
perform worse than classical IDT features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Few-Shot Image Classification with Unlabelled Examples. (arXiv:2006.12245v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12245">
<div class="article-summary-box-inner">
<span><p>We develop a transductive meta-learning method that uses unlabelled instances
to improve few-shot image classification performance. Our approach combines a
regularized Mahalanobis-distance-based soft k-means clustering procedure with a
modified state of the art neural adaptive feature extractor to achieve improved
test-time classification accuracy using unlabelled data. We evaluate our method
on transductive few-shot learning tasks, in which the goal is to jointly
predict labels for query (test) examples given a set of support (training)
examples. We achieve state of the art performance on the Meta-Dataset,
mini-ImageNet and tiered-ImageNet benchmarks. All trained models and code have
been made publicly available at github.com/plai-group/simple-cnaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Nonnegative Tensor Factorization and Completion with Noisy Observations. (arXiv:2007.10626v3 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.10626">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the sparse nonnegative tensor factorization and
completion problem from partial and noisy observations for third-order tensors.
Because of sparsity and nonnegativity, the underlying tensor is decomposed into
the tensor-tensor product of one sparse nonnegative tensor and one nonnegative
tensor. We propose to minimize the sum of the maximum likelihood estimation for
the observations with nonnegativity constraints and the tensor $\ell_0$ norm
for the sparse factor. We show that the error bounds of the estimator of the
proposed model can be established under general noise observations. The
detailed error bounds under specific noise distributions including additive
Gaussian noise, additive Laplace noise, and Poisson observations can be
derived. Moreover, the minimax lower bounds are shown to be matched with the
established upper bounds up to a logarithmic factor of the sizes of the
underlying tensor. These theoretical results for tensors are better than those
obtained for matrices, and this illustrates the advantage of the use of
nonnegative sparse tensor models for completion and denoising. Numerical
experiments are provided to validate the superiority of the proposed
tensor-based method compared with the matrix-based approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WAN: Watermarking Attack Network. (arXiv:2008.06255v3 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.06255">
<div class="article-summary-box-inner">
<span><p>Multi-bit watermarking (MW) has been developed to improve robustness against
signal processing operations and geometric distortions. To this end, benchmark
tools that test robustness by applying simulated attacks on watermarked images
are available. However, limitations in these general attacks exist since they
cannot exploit specific characteristics of the targeted MW. In addition, these
attacks are usually devised without consideration of visual quality, which
rarely occurs in the real world. To address these limitations, we propose a
watermarking attack network (WAN), a fully trainable watermarking benchmark
tool that utilizes the weak points of the target MW and induces an inversion of
the watermark bit, thereby considerably reducing the watermark extractability.
To hinder the extraction of hidden information while ensuring high visual
quality, we utilize a residual dense blocks-based architecture specialized in
local and global feature learning. A novel watermarking attack loss is
introduced to break the MW systems. We empirically demonstrate that the WAN can
successfully fool various block-based MW systems. Moreover, we show that
existing MW methods can be improved with the help of the WAN as an add-on
module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualSem: A High-quality Knowledge Graph for Vision and Language. (arXiv:2008.09150v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09150">
<div class="article-summary-box-inner">
<span><p>An exciting frontier in natural language understanding (NLU) and generation
(NLG) calls for (vision-and-) language models that can efficiently access
external structured knowledge repositories. However, many existing knowledge
bases only cover limited domains, or suffer from noisy data, and most of all
are typically hard to integrate into neural language pipelines. To fill this
gap, we release VisualSem: a high-quality knowledge graph (KG) which includes
nodes with multilingual glosses, multiple illustrative images, and visually
relevant relations. We also release a neural multi-modal retrieval model that
can use images or sentences as inputs and retrieves entities in the KG. This
multi-modal retrieval model can be integrated into any (neural network) model
pipeline. We encourage the research community to use VisualSem for data
augmentation and/or as a source of grounding, among other possible uses.
VisualSem as well as the multi-modal retrieval models are publicly available
and can be downloaded in this URL: https://github.com/iacercalixto/visualsem
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAC: Semantic Attention Composition for Text-Conditioned Image Retrieval. (arXiv:2009.01485v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.01485">
<div class="article-summary-box-inner">
<span><p>The ability to efficiently search for images is essential for improving the
user experiences across various products. Incorporating user feedback, via
multi-modal inputs, to navigate visual search can help tailor retrieved results
to specific user queries. We focus on the task of text-conditioned image
retrieval that utilizes support text feedback alongside a reference image to
retrieve images that concurrently satisfy constraints imposed by both inputs.
The task is challenging since it requires learning composite image-text
features by incorporating multiple cross-granular semantic edits from text
feedback and then applying the same to visual features. To address this, we
propose a novel framework SAC which resolves the above in two major steps:
"where to see" (Semantic Feature Attention) and "how to change" (Semantic
Feature Modification). We systematically show how our architecture streamlines
the generation of text-aware image features by removing the need for various
modules required by other state-of-art techniques. We present extensive
quantitative, qualitative analysis, and ablation studies, to show that our
architecture SAC outperforms existing techniques by achieving state-of-the-art
performance on 3 benchmark datasets: FashionIQ, Shoes, and Birds-to-Words,
while supporting natural language feedback of varying lengths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PERF-Net: Pose Empowered RGB-Flow Net. (arXiv:2009.13087v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13087">
<div class="article-summary-box-inner">
<span><p>In recent years, many works in the video action recognition literature have
shown that two stream models (combining spatial and temporal input streams) are
necessary for achieving state of the art performance. In this paper we show the
benefits of including yet another stream based on human pose estimated from
each frame -- specifically by rendering pose on input RGB frames. At first
blush, this additional stream may seem redundant given that human pose is fully
determined by RGB pixel values -- however we show (perhaps surprisingly) that
this simple and flexible addition can provide complementary gains. Using this
insight, we then propose a new model, which we dub PERF-Net (short for Pose
Empowered RGB-Flow Net), which combines this new pose stream with the standard
RGB and flow based input streams via distillation techniques and show that our
model outperforms the state-of-the-art by a large margin in a number of human
action recognition datasets while not requiring flow or pose to be explicitly
computed at inference time. The proposed pose stream is also part of the winner
solution of the ActivityNet Kinetics Challenge 2020.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESAD: End-to-end Deep Semi-supervised Anomaly Detection. (arXiv:2012.04905v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04905">
<div class="article-summary-box-inner">
<span><p>This paper explores semi-supervised anomaly detection, a more practical
setting for anomaly detection where a small additional set of labeled samples
are provided. We propose a new KL-divergence based objective function for
semi-supervised anomaly detection, and show that two factors: the mutual
information between the data and latent representations, and the entropy of
latent representations, constitute an integral objective function for anomaly
detection. To resolve the contradiction in simultaneously optimizing the two
factors, we propose a novel encoder-decoder-encoder structure, with the first
encoder focusing on optimizing the mutual information and the second encoder
focusing on optimizing the entropy. The two encoders are enforced to share
similar encoding with a consistent constraint on their latent representations.
Extensive experiments have revealed that the proposed method significantly
outperforms several state-of-the-arts on multiple benchmark datasets, including
medical diagnosis and several classic anomaly detection benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Inter- and Intraframe Representations for Non-Lambertian Photometric Stereo. (arXiv:2012.13720v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13720">
<div class="article-summary-box-inner">
<span><p>Photometric stereo provides an important method for high-fidelity 3D
reconstruction based on multiple intensity images captured under different
illumination directions. In this paper, we present a complete framework,
including a multilight source illumination and acquisition hardware system and
a two-stage convolutional neural network (CNN) architecture, to construct
inter- and intraframe representations for accurate normal estimation of
non-Lambertian objects. We experimentally investigate numerous network design
alternatives for identifying the optimal scheme to deploy inter- and intraframe
feature extraction modules for the photometric stereo problem. Moreover, we
propose utilizing the easily obtained object mask to eliminate adverse
interference from invalid background regions in intraframe spatial
convolutions, thus effectively improving the accuracy of normal estimation for
surfaces made of dark materials or with cast shadows. Experimental results
demonstrate that the proposed masked two-stage photometric stereo CNN model
(MT-PS-CNN) performs favourably against state-of-the-art photometric stereo
techniques in terms of both accuracy and efficiency. In addition, the proposed
method is capable of predicting accurate and rich surface normal details for
non-Lambertian objects of complex geometry and performs stably given inputs
captured in both sparse and dense lighting distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporally Guided Articulated Hand Pose Tracking in Surgical Videos. (arXiv:2101.04281v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04281">
<div class="article-summary-box-inner">
<span><p>Articulated hand pose tracking is an under-explored problem that carries the
potential for use in an extensive number of applications, especially in the
medical domain. With a robust and accurate tracking system on in-vivo surgical
videos, the motion dynamics and movement patterns of the hands can be captured
and analyzed for many rich tasks. In this work, we propose a novel hand pose
estimation model, Res152-CondPose, which improves detection and tracking
accuracy by incorporating a hand pose prior into its pose prediction. We show
improvements over state-of-the-art methods which provide frame-wise independent
predictions, by following a temporally guided approach that effectively
leverages past predictions. Additionally, we collect the first dataset,
Surgical Hands, that provides multi-instance articulated hand pose annotations
for in-vivo videos. Our dataset contains 76 video clips from 28 publicly
available surgical videos and over 8.1k annotated hand pose instances. We
provide bounding boxes, articulated hand pose annotations, and tracking IDs to
enable multi-instance area-based and articulated tracking. When evaluated on
Surgical Hands, we show our method outperforms the state-of-the-art method
using mean Average Precision (mAP), to measure pose estimation accuracy, and
Multiple Object Tracking Accuracy (MOTA), to assess pose tracking performance.
Both the code and dataset are available at
https://github.com/MichiganCOG/Surgical_ Hands_RELEASE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kimera: from SLAM to Spatial Perception with 3D Dynamic Scene Graphs. (arXiv:2101.06894v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06894">
<div class="article-summary-box-inner">
<span><p>Humans are able to form a complex mental model of the environment they move
in. This mental model captures geometric and semantic aspects of the scene,
describes the environment at multiple levels of abstractions (e.g., objects,
rooms, buildings), includes static and dynamic entities and their relations
(e.g., a person is in a room at a given time). In contrast, current robots'
internal representations still provide a partial and fragmented understanding
of the environment, either in the form of a sparse or dense set of geometric
primitives (e.g., points, lines, planes, voxels) or as a collection of objects.
This paper attempts to reduce the gap between robot and human perception by
introducing a novel representation, a 3D Dynamic Scene Graph(DSG), that
seamlessly captures metric and semantic aspects of a dynamic environment. A DSG
is a layered graph where nodes represent spatial concepts at different levels
of abstraction, and edges represent spatio-temporal relations among nodes. Our
second contribution is Kimera, the first fully automatic method to build a DSG
from visual-inertial data. Kimera includes state-of-the-art techniques for
visual-inertial SLAM, metric-semantic 3D reconstruction, object localization,
human pose and shape estimation, and scene parsing. Our third contribution is a
comprehensive evaluation of Kimera in real-life datasets and photo-realistic
simulations, including a newly released dataset, uHumans2, which simulates a
collection of crowded indoor and outdoor scenes. Our evaluation shows that
Kimera achieves state-of-the-art performance in visual-inertial SLAM, estimates
an accurate 3D metric-semantic mesh model in real-time, and builds a DSG of a
complex indoor environment with tens of objects and humans in minutes. Our
final contribution shows how to use a DSG for real-time hierarchical semantic
path-planning. The core modules in Kimera are open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Too-Good-to-be-True Prior to Reduce Shortcut Reliance. (arXiv:2102.06406v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06406">
<div class="article-summary-box-inner">
<span><p>Despite their impressive performance in object recognition and other tasks
under standard testing conditions, deep networks often fail to generalize to
out-of-distribution (o.o.d.) samples. One cause for this shortcoming is that
modern architectures tend to rely on "shortcuts" - superficial features that
correlate with categories without capturing deeper invariants that hold across
contexts. Real-world concepts often possess a complex structure that can vary
superficially across contexts, which can make the most intuitive and promising
solutions in one context not generalize to others. One potential way to improve
o.o.d. generalization is to assume simple solutions are unlikely to be valid
across contexts and avoid them, which we refer to as the too-good-to-be-true
prior. A low-capacity network (LCN) with a shallow architecture should only be
able to learn surface relationships, including shortcuts. We find that LCNs can
serve as shortcut detectors. Furthermore, an LCN's predictions can be used in a
two-stage approach to encourage a high-capacity network (HCN) to rely on deeper
invariant features that should generalize broadly. In particular, items that
the LCN can master are downweighted when training the HCN. Using a modified
version of the CIFAR-10 dataset in which we introduced shortcuts, we found that
the two-stage LCN-HCN approach reduced reliance on shortcuts and facilitated
o.o.d. generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EllipsoidNet: Ellipsoid Representation for Point Cloud Classification and Segmentation. (arXiv:2103.02517v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02517">
<div class="article-summary-box-inner">
<span><p>Point cloud patterns are hard to learn because of the implicit local geometry
features among the orderless points. In recent years, point cloud
representation in 2D space has attracted increasing research interest since it
exposes the local geometry features in a 2D space. By projecting those points
to a 2D feature map, the relationship between points is inherited in the
context between pixels, which are further extracted by a 2D convolutional
neural network. However, existing 2D representing methods are either accuracy
limited or time-consuming. In this paper, we propose a novel 2D representation
method that projects a point cloud onto an ellipsoid surface space, where local
patterns are well exposed in ellipsoid-level and point-level. Additionally, a
novel convolutional neural network named EllipsoidNet is proposed to utilize
those features for point cloud classification and segmentation applications.
The proposed methods are evaluated in ModelNet40 and ShapeNet benchmarks, where
the advantages are clearly shown over existing 2D representation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Predict Vehicle Trajectories with Model-based Planning. (arXiv:2103.04027v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04027">
<div class="article-summary-box-inner">
<span><p>Predicting the future trajectories of on-road vehicles is critical for
autonomous driving. In this paper, we introduce a novel prediction framework
called PRIME, which stands for Prediction with Model-based Planning. Unlike
recent prediction works that utilize neural networks to model scene context and
produce unconstrained trajectories, PRIME is designed to generate accurate and
feasibility-guaranteed future trajectory predictions. PRIME guarantees the
trajectory feasibility by exploiting a model-based generator to produce future
trajectories under explicit constraints and enables accurate multimodal
prediction by utilizing a learning-based evaluator to select future
trajectories. We conduct experiments on the large-scale Argoverse Motion
Forecasting Benchmark, where PRIME outperforms the state-of-the-art methods in
prediction accuracy, feasibility, and robustness under imperfect tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy Labels. (arXiv:2103.13646v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13646">
<div class="article-summary-box-inner">
<span><p>The success of learning with noisy labels (LNL) methods relies heavily on the
success of a warm-up stage where standard supervised training is performed
using the full (noisy) training set. In this paper, we identify a "warm-up
obstacle": the inability of standard warm-up stages to train high quality
feature extractors and avert memorization of noisy labels. We propose "Contrast
to Divide" (C2D), a simple framework that solves this problem by pre-training
the feature extractor in a self-supervised fashion. Using self-supervised
pre-training boosts the performance of existing LNL approaches by drastically
reducing the warm-up stage's susceptibility to noise level, shortening its
duration, and improving extracted feature quality. C2D works out of the box
with existing methods and demonstrates markedly improved performance,
especially in the high noise regime, where we get a boost of more than 27% for
CIFAR-100 with 90% noise over the previous state of the art. In real-life noise
settings, C2D trained on mini-WebVision outperforms previous works both in
WebVision and ImageNet validation sets by 3% top-1 accuracy. We perform an
in-depth analysis of the framework, including investigating the performance of
different pre-training approaches and estimating the effective upper bound of
the LNL performance with semi-supervised learning. Code for reproducing our
experiments is available at https://github.com/ContrastToDivide/C2D
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds. (arXiv:2104.03437v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03437">
<div class="article-summary-box-inner">
<span><p>In this work, we tackle the problem of category-level online pose tracking of
objects from point cloud sequences. For the first time, we propose a unified
framework that can handle 9DoF pose tracking for novel rigid object instances
as well as per-part pose tracking for articulated objects from known
categories. Here the 9DoF pose, comprising 6D pose and 3D size, is equivalent
to a 3D amodal bounding box representation with free 6D pose. Given the depth
point cloud at the current frame and the estimated pose from the last frame,
our novel end-to-end pipeline learns to accurately update the pose. Our
pipeline is composed of three modules: 1) a pose canonicalization module that
normalizes the pose of the input depth point cloud; 2) RotationNet, a module
that directly regresses small interframe delta rotations; and 3) CoordinateNet,
a module that predicts the normalized coordinates and segmentation, enabling
analytical computation of the 3D size and translation. Leveraging the small
pose regime in the pose-canonicalized point clouds, our method integrates the
best of both worlds by combining dense coordinate prediction and direct
rotation regression, thus yielding an end-to-end differentiable pipeline
optimized for 9DoF pose accuracy (without using non-differentiable RANSAC). Our
extensive experiments demonstrate that our method achieves new state-of-the-art
performance on category-level rigid object pose (NOCS-REAL275) and articulated
object pose benchmarks (SAPIEN, BMVC) at the fastest FPS ~12.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advanced Deep Networks for 3D Mitochondria Instance Segmentation. (arXiv:2104.07961v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07961">
<div class="article-summary-box-inner">
<span><p>Mitochondria instance segmentation from electron microscopy (EM) images has
seen notable progress since the introduction of deep learning methods. In this
paper, we propose two advanced deep networks, named Res-UNet-R and Res-UNet-H,
for 3D mitochondria instance segmentation from Rat and Human samples.
Specifically, we design a simple yet effective anisotropic convolution block
and deploy a multi-scale training strategy, which together boost the
segmentation performance. Moreover, we enhance the generalizability of the
trained models on the test set by adding a denoising operation as
pre-processing. In the Large-scale 3D Mitochondria Instance Segmentation
Challenge at ISBI 2021, our method ranks the 1st place. Code is available at
https://github.com/Limingxing00/MitoEM2021-Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08760">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (especially contrastive learning) has attracted
great interest due to its huge potential in learning discriminative
representations in an unsupervised manner. Despite the acknowledged successes,
existing contrastive learning methods suffer from very low learning efficiency,
e.g., taking about ten times more training epochs than supervised learning for
comparable recognition accuracy. In this paper, we reveal two contradictory
phenomena in contrastive learning that we call under-clustering and
over-clustering problems, which are major obstacles to learning efficiency.
Under-clustering means that the model cannot efficiently learn to discover the
dissimilarity between inter-class samples when the negative sample pairs for
contrastive learning are insufficient to differentiate all the actual object
classes. Over-clustering implies that the model cannot efficiently learn
features from excessive negative sample pairs, forcing the model to
over-cluster samples of the same actual classes into different clusters. To
simultaneously overcome these two problems, we propose a novel self-supervised
learning framework using a truncated triplet loss. Precisely, we employ a
triplet loss tending to maximize the relative distance between the positive
pair and negative pairs to address the under-clustering problem; and we
construct the negative pair by selecting a negative sample deputy from all
negative samples to avoid the over-clustering problem, guaranteed by the
Bernoulli Distribution model. We extensively evaluate our framework in several
large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results
demonstrate our model's superiority (e.g., the learning efficiency) over the
latest state-of-the-art methods by a clear margin. Codes available at:
https://github.com/wanggrun/triplet .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarioNette: Self-Supervised Sprite Learning. (arXiv:2104.14553v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14553">
<div class="article-summary-box-inner">
<span><p>Artists and video game designers often construct 2D animations using
libraries of sprites -- textured patches of objects and characters. We propose
a deep learning approach that decomposes sprite-based video animations into a
disentangled representation of recurring graphic elements in a self-supervised
manner. By jointly learning a dictionary of possibly transparent patches and
training a network that places them onto a canvas, we deconstruct sprite-based
content into a sparse, consistent, and explicit representation that can be
easily used in downstream tasks, like editing or analysis. Our framework offers
a promising approach for discovering recurring visual patterns in image
collections without supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Semantic Photo Geolocation. (arXiv:2104.14995v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14995">
<div class="article-summary-box-inner">
<span><p>Planet-scale photo geolocalization is the complex task of estimating the
location depicted in an image solely based on its visual content. Due to the
success of convolutional neural networks (CNNs), current approaches achieve
super-human performance. However, previous work has exclusively focused on
optimizing geolocalization accuracy. Due to the black-box property of deep
learning systems, their predictions are difficult to validate for humans.
State-of-the-art methods treat the task as a classification problem, where the
choice of the classes, that is the partitioning of the world map, is crucial
for the performance. In this paper, we present two contributions to improve the
interpretability of a geolocalization model: (1) We propose a novel semantic
partitioning method which intuitively leads to an improved understanding of the
predictions, while achieving state-of-the-art results for geolocational
accuracy on benchmark test sets; (2) We introduce a metric to assess the
importance of semantic visual concepts for a certain prediction to provide
additional interpretable information, which allows for a large-scale analysis
of already trained models. Source code and dataset are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Perceptual Distortion Reduction Framework: Towards Generating Adversarial Examples with High Perceptual Quality and Attack Success Rate. (arXiv:2105.00278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00278">
<div class="article-summary-box-inner">
<span><p>Most of the adversarial attack methods suffer from large perceptual
distortions such as visible artifacts, when the attack strength is relatively
high. These perceptual distortions contain a certain portion which contributes
less to the attack success rate. This portion of distortions, which is induced
by unnecessary modifications and lack of proper perceptual distortion
constraint, is the target of the proposed framework. In this paper, we propose
a perceptual distortion reduction framework to tackle this problem from two
perspectives. Firstly, we propose a perceptual distortion constraint and add it
into the objective function to jointly optimize the perceptual distortions and
attack success rate. Secondly, we propose an adaptive penalty factor $\lambda$
to balance the discrepancies between different samples. Since SGD and
Momentum-SGD cannot optimize our complex non-convex problem, we exploit Adam in
optimization. Extensive experiments have verified the superiority of our
proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robotic Approach towards Quantifying Epipelagic Bound Plastic Using Deep Visual Models. (arXiv:2105.01882v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01882">
<div class="article-summary-box-inner">
<span><p>The quantification of positively buoyant marine plastic debris is critical to
understanding how plastic litter accumulates across the world's oceans and is
also crucial to identifying hotspots for targeted cleanup efforts. Currently,
the most common method to quantify marine plastic is using manta trawls for
manual sampling. However, this method is cost-intensive and requires human
labor. This study removes the need for manual sampling by using an autonomous
method using neural networks and computer vision models, which trained on
images captured from various layers of the ocean column to perform real-time
plastic quantification. The best performing model has a Mean Average Precision
of 85% and an F1-Score of 0.89 while maintaining near real-time processing
speeds ~2 ms/img.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RandCrowns: A Quantitative Metric for Imprecisely Labeled Tree Crown Delineation. (arXiv:2105.02186v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02186">
<div class="article-summary-box-inner">
<span><p>Supervised methods for object delineation in remote sensing require labeled
ground-truth data. Gathering sufficient high quality ground-truth data is
difficult, especially when targets are of irregular shape or difficult to
distinguish from background or neighboring objects. Tree crown delineation
provides key information from remote sensing images for forestry, ecology, and
management. However, tree crowns in remote sensing imagery are often difficult
to label and annotate due to irregular shape, overlapping canopies, shadowing,
and indistinct edges. There are also multiple approaches to annotation in this
field (e.g., rectangular boxes vs. convex polygons) that further contribute to
annotation imprecision. However, current evaluation methods do not account for
this uncertainty in annotations, and quantitative metrics for evaluation can
vary across multiple annotators. In this paper, we address these limitations by
developing an adaptation of the Rand index for weakly-labeled crown delineation
that we call RandCrowns. Our new RandCrowns evaluation metric provides a method
to appropriately evaluate delineated tree crowns while taking into account
imprecision in the ground-truth delineations. The RandCrowns metric
reformulates the Rand index by adjusting the areas over which each term of the
index is computed to account for uncertain and imprecise object delineation
labels. Quantitative comparisons to the commonly used intersection over union
method shows a decrease in the variance generated by differences among multiple
annotators. Combined with qualitative examples, our results suggest that the
RandCrowns metric is more robust for scoring target delineations in the
presence of uncertainty and imprecision in annotations that are inherent to
tree crown delineation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Aided Saliency Maps Improve Generalization of Deep Learning. (arXiv:2105.03492v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03492">
<div class="article-summary-box-inner">
<span><p>Deep learning has driven remarkable accuracy increases in many computer
vision problems. One ongoing challenge is how to achieve the greatest accuracy
in cases where training data is limited. A second ongoing challenge is that
trained models oftentimes do not generalize well even to new data that is
subjectively similar to the training set. We address these challenges in a
novel way, with the first-ever (to our knowledge) exploration of encoding human
judgement about salient regions of images into the training data. We compare
the accuracy and generalization of a state-of-the-art deep learning algorithm
for a difficult problem in biometric presentation attack detection when trained
on (a) original images with typical data augmentations, and (b) the same
original images transformed to encode human judgement about salient image
regions. The latter approach results in models that achieve higher accuracy and
better generalization, decreasing the error of the LivDet-Iris 2020 winner from
29.78% to 16.37%, and achieving impressive generalization in a
leave-one-attack-type-out evaluation scenario. This work opens a new area of
study for how to embed human intelligence into training strategies for deep
learning to achieve high accuracy and generalization in cases of limited
training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Truly shift-equivariant convolutional neural networks with adaptive polyphase upsampling. (arXiv:2105.04040v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04040">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks lack shift equivariance due to the presence of
downsampling layers. In image classification, adaptive polyphase downsampling
(APS-D) was recently proposed to make CNNs perfectly shift invariant. However,
in networks used for image reconstruction tasks, it can not by itself restore
shift equivariance. We address this problem by proposing adaptive polyphase
upsampling (APS-U), a non-linear extension of conventional upsampling, which
allows CNNs to exhibit perfect shift equivariance. With MRI and CT
reconstruction experiments, we show that networks containing APS-D/U layers
exhibit state of the art equivariance performance without sacrificing on image
reconstruction quality. In addition, unlike prior methods like data
augmentation and anti-aliasing, the gains in equivariance obtained from APS-D/U
also extend to images outside the training distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HPNet: Deep Primitive Segmentation Using Hybrid Representations. (arXiv:2105.10620v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10620">
<div class="article-summary-box-inner">
<span><p>This paper introduces HPNet, a novel deep-learning approach for segmenting a
3D shape represented as a point cloud into primitive patches. The key to deep
primitive segmentation is learning a feature representation that can separate
points of different primitives. Unlike utilizing a single feature
representation, HPNet leverages hybrid representations that combine one learned
semantic descriptor, two spectral descriptors derived from predicted geometric
parameters, as well as an adjacency matrix that encodes sharp edges. Moreover,
instead of merely concatenating the descriptors, HPNet optimally combines
hybrid representations by learning combination weights. This weighting module
builds on the entropy of input features. The output primitive segmentation is
obtained from a mean-shift clustering module. Experimental results on benchmark
datasets ANSI and ABCParts show that HPNet leads to significant performance
gains from baseline approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention. (arXiv:2105.13495v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13495">
<div class="article-summary-box-inner">
<span><p>Functional connectivity (FC) between regions of the brain can be assessed by
the degree of temporal correlation measured with functional neuroimaging
modalities. Based on the fact that these connectivities build a network,
graph-based approaches for analyzing the brain connectome have provided
insights into the functions of the human brain. The development of graph neural
networks (GNNs) capable of learning representation from graph structured data
has led to increased interest in learning the graph representation of the brain
connectome. Although recent attempts to apply GNN to the FC network have shown
promising results, there is still a common limitation that they usually do not
incorporate the dynamic characteristics of the FC network which fluctuates over
time. In addition, a few studies that have attempted to use dynamic FC as an
input for the GNN reported a reduction in performance compared to static FC
methods, and did not provide temporal explainability. Here, we propose STAGIN,
a method for learning dynamic graph representation of the brain connectome with
spatio-temporal attention. Specifically, a temporal sequence of brain graphs is
input to the STAGIN to obtain the dynamic graph representation, while novel
READOUT functions and the Transformer encoder provide spatial and temporal
explainability with attention, respectively. Experiments on the HCP-Rest and
the HCP-Task datasets demonstrate exceptional performance of our proposed
method. Analysis of the spatio-temporal attention also provide concurrent
interpretation with the neuroscientific knowledge, which further validates our
method. Code is available at https://github.com/egyptdj/stagin
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Segmentation via Cycle-Consistent Transformer. (arXiv:2106.02320v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02320">
<div class="article-summary-box-inner">
<span><p>Few-shot segmentation aims to train a segmentation model that can fast adapt
to novel classes with few exemplars. The conventional training paradigm is to
learn to make predictions on query images conditioned on the features from
support images. Previous methods only utilized the semantic-level prototypes of
support images as the conditional information. These methods cannot utilize all
pixel-wise support information for the query predictions, which is however
critical for the segmentation task. In this paper, we focus on utilizing
pixel-wise relationships between support and target images to facilitate the
few-shot semantic segmentation task. We design a novel Cycle-Consistent
Transformer (CyCTR) module to aggregate pixel-wise support features into query
ones. CyCTR performs cross-attention between features from different images,
i.e. support and query images. We observe that there may exist unexpected
irrelevant pixel-level support features. Directly performing cross-attention
may aggregate these features from support to query and bias the query features.
Thus, we propose using a novel cycle-consistent attention mechanism to filter
out possible harmful support features and encourage query features to attend to
the most informative pixels from support images. Experiments on all few-shot
segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable
improvement compared to previous state-of-the-art methods. Specifically, on
Pascal-$5^i$ and COCO-$20^i$ datasets, we achieve 66.6% and 45.6% mIoU for
5-shot segmentation, outperforming previous state-of-the-art by 4.6% and 7.1%
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning of Domain Invariant Features for Depth Estimation. (arXiv:2106.02594v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02594">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of unsupervised synthetic-to-real domain adaptation for
single image depth estimation. An essential building block of single image
depth estimation is an encoder-decoder task network that takes RGB images as
input and produces depth maps as output. In this paper, we propose a novel
training strategy to force the task network to learn domain invariant
representations in a selfsupervised manner. Specifically, we extend
self-supervised learning from traditional representation learning, which works
on images from a single domain, to domain invariant representation learning,
which works on images from two different domains by utilizing an image-to-image
translation network. Firstly, we use an image-to-image translation network to
transfer domain-specific styles between synthetic and real domains. This style
transfer operation allows us to obtain similar images from the different
domains. Secondly, we jointly train our task network and Siamese network with
the same images from the different domains to obtain domain invariance for the
task network. Finally, we fine-tune the task network using labeled synthetic
and unlabeled realworld data. Our training strategy yields improved
generalization capability in the real-world domain. We carry out an extensive
evaluation on two popular datasets for depth estimation, KITTI and Make3D. The
results demonstrate that our proposed method outperforms the state-of-the-art
on all metrics, e.g. by 14.7% on Sq Rel on KITTI. The source code and model
weights will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations. (arXiv:2106.05967v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05967">
<div class="article-summary-box-inner">
<span><p>Contrastive self-supervised learning has outperformed supervised pretraining
on many downstream tasks like segmentation and object detection. However,
current methods are still primarily applied to curated datasets like ImageNet.
In this paper, we first study how biases in the dataset affect existing
methods. Our results show that current contrastive approaches work surprisingly
well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed
and (iii) general versus domain-specific datasets. Second, given the generality
of the approach, we try to realize further gains with minor modifications. We
show that learning additional invariances -- through the use of multi-scale
cropping, stronger augmentations and nearest neighbors -- improves the
representations. Finally, we observe that MoCo learns spatially structured
representations when trained with a multi-crop strategy. The representations
can be used for semantic segment retrieval and video instance segmentation
without finetuning. Moreover, the results are on par with specialized models.
We hope this work will serve as a useful study for other researchers. The code
and models are available at
https://github.com/wvangansbeke/Revisiting-Contrastive-SSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Real than Real: A Study on Human Visual Perception of Synthetic Faces. (arXiv:2106.07226v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07226">
<div class="article-summary-box-inner">
<span><p>Deep fakes became extremely popular in the last years, also thanks to their
increasing realism. Therefore, there is the need to measures human's ability to
distinguish between real and synthetic face images when confronted with
cutting-edge creation technologies. We describe the design and results of a
perceptual experiment we have conducted, where a wide and diverse group of
volunteers has been exposed to synthetic face images produced by
state-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN,
StyleGAN2). The experiment outcomes reveal how strongly we should call into
question our human ability to discriminate real faces from synthetic ones
generated through modern AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised GANs with Label Augmentation. (arXiv:2106.08601v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08601">
<div class="article-summary-box-inner">
<span><p>Recently, transformation-based self-supervised learning has been applied to
generative adversarial networks (GANs) to mitigate catastrophic forgetting in
the discriminator by introducing stationary learning environments. However, the
separate self-supervised tasks in existing self-supervised GANs cause a goal
inconsistent with generative modeling due to the fact that their
self-supervised classifiers are agnostic to the generator distribution. To
address this problem, we propose a novel self-supervised GAN that unifies the
GAN task with the self-supervised task by augmenting the GAN labels (real or
fake) via self-supervision of data transformation. Specifically, the original
discriminator and self-supervised classifier are unified into a label-augmented
discriminator that predicts the augmented labels to be aware of the generator
distribution and the data distribution under every transformation, and then
provide the discrepancy between them to optimize the generator. Theoretically,
we prove that the optimal generator converges to replicate the real data
distribution under mild assumptions. Empirically, we show that the proposed
method significantly outperforms previous self-supervised and data augmentation
GANs on both generative modeling and representation learning across various
benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network. (arXiv:2106.09996v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09996">
<div class="article-summary-box-inner">
<span><p>Training a Convolutional Neural Network (CNN) to be robust against rotation
has mostly been done with data augmentation. In this paper, another progressive
vision of research direction is highlighted to encourage less dependence on
data augmentation by achieving structural rotational invariance of a network.
The deep equivariance-bridged SO(2) invariant network is proposed to echo such
vision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network
(SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the
graph representation of an image to acquire rotationally equivariant
representation, as GCN is more suitable for constructing deeper network than
spectral graph convolution-based approaches. Then, invariant representation is
eventually obtained with Global Average Pooling (GAP), a permutation-invariant
operation suitable for aggregating high-dimensional representations, over the
equivariant set of vertices retrieved from SWN-GCN. Our method achieves the
state-of-the-art image classification performance on rotated MNIST and CIFAR-10
images, where the models are trained with a non-augmented dataset only.
Quantitative validations over invariance of the representations also
demonstrate strong invariance of deep representations of SWN-GCN over
rotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting. (arXiv:2106.10137v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10137">
<div class="article-summary-box-inner">
<span><p>Instance-level contrastive learning techniques, which rely on data
augmentation and a contrastive loss function, have found great success in the
domain of visual representation learning. They are not suitable for exploiting
the rich dynamical structure of video however, as operations are done on many
augmented instances. In this paper we propose "Video Cross-Stream Prototypical
Contrasting", a novel method which predicts consistent prototype assignments
from both RGB and optical flow views, operating on sets of samples.
Specifically, we alternate the optimization process; while optimizing one of
the streams, all views are mapped to one set of stream prototype vectors. Each
of the assignments is predicted with all views except the one matching the
prediction, pushing representations closer to their assigned prototypes. As a
result, more efficient video embeddings with ingrained motion information are
learned, without the explicit need for optical flow computation during
inference. We obtain state-of-the-art results on nearest-neighbour video
retrieval and action recognition, outperforming previous best by +3.2% on
UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and
+15.1% on HMDB51 using the R(2+1)D backbone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast whole-slide cartography in colon cancer histology using superpixels and CNN classification. (arXiv:2106.15893v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15893">
<div class="article-summary-box-inner">
<span><p>Automatic outlining of different tissue types in digitized histological
specimen provides a basis for follow-up analyses and can potentially guide
subsequent medical decisions. The immense size of whole-slide-images (WSI),
however, poses a challenge in terms of computation time. In this regard, the
analysis of non-overlapping patches outperforms pixelwise segmentation
approaches, but still leaves room for optimization. Furthermore, the division
into patches, regardless of the biological structures they contain, is a
drawback due to the loss of local dependencies. We propose to subdivide the WSI
into coherent regions prior to classification by grouping visually similar
adjacent pixels into superpixels. Afterwards, only a random subset of patches
per superpixel is classified and patch labels are combined into a superpixel
label. We propose a metric for identifying superpixels with an uncertain
classification and evaluate two medical applications, namely tumor area and
invasive margin estimation and tumor composition analysis. The algorithm has
been developed on 159 hand-annotated WSIs of colon resections and its
performance is compared to an analysis without prior segmentation. The
algorithm shows an average speed-up of 41% and an increase in accuracy from
93.8% to 95.7%. By assigning a rejection label to uncertain superpixels, we
further increase the accuracy by 0.4%. Whilst tumor area estimation shows high
concordance to the annotated area, the analysis of tumor composition highlights
limitations of our approach. By combining superpixel segmentation and patch
classification, we designed a fast and accurate framework for whole-slide
cartography that is AI-model agnostic and provides the basis for various
medical endpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overhead-MNIST: Machine Learning Baselines for Image Classification. (arXiv:2107.00436v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00436">
<div class="article-summary-box-inner">
<span><p>Twenty-three machine learning algorithms were trained then scored to
establish baseline comparison metrics and to select an image classification
algorithm worthy of embedding into mission-critical satellite imaging systems.
The Overhead-MNIST dataset is a collection of satellite images similar in style
to the ubiquitous MNIST hand-written digits found in the machine learning
literature. The CatBoost classifier, Light Gradient Boosting Machine, and
Extreme Gradient Boosting models produced the highest accuracies, Areas Under
the Curve (AUC), and F1 scores in a PyCaret general comparison. Separate
evaluations showed that a deep convolutional architecture was the most
promising. We present results for the overall best performing algorithm as a
baseline for edge deployability and future performance improvement: a
convolutional neural network (CNN) scoring 0.965 categorical accuracy on unseen
test data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distance-based Hyperspherical Classification for Multi-source Open-Set Domain Adaptation. (arXiv:2107.02067v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02067">
<div class="article-summary-box-inner">
<span><p>Vision systems trained in closed-world scenarios fail when presented with new
environmental conditions, new data distributions, and novel classes at
deployment time. How to move towards open-world learning is a long-standing
research question. The existing solutions mainly focus on specific aspects of
the problem (single domain Open-Set, multi-domain Closed-Set), or propose
complex strategies which combine several losses and manually tuned
hyperparameters. In this work, we tackle multi-source Open-Set domain
adaptation by introducing HyMOS: a straightforward model that exploits the
power of contrastive learning and the properties of its hyperspherical feature
space to correctly predict known labels on the target, while rejecting samples
belonging to any unknown class. HyMOS includes style transfer among the
instance transformations of contrastive learning to get domain invariance while
avoiding the risk of negative-transfer. A self-paced threshold is defined on
the basis of the observed data distribution and updates online during training,
allowing to handle the known-unknown separation. We validate our method over
three challenging datasets. The obtained results show that HyMOS outperforms
several competitors, defining the new state-of-the-art. Our code is available
at https://github.com/silvia1993/HyMOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03996">
<div class="article-summary-box-inner">
<span><p>We propose to address quadrupedal locomotion tasks using Reinforcement
Learning (RL) with a Transformer-based model that learns to combine
proprioceptive information and high-dimensional depth sensor inputs. While
learning-based locomotion has made great advances using RL, most methods still
rely on domain randomization for training blind agents that generalize to
challenging terrains. Our key insight is that proprioceptive states only offer
contact measurements for immediate reaction, whereas an agent equipped with
visual sensory observations can learn to proactively maneuver environments with
obstacles and uneven terrain by anticipating changes in the environment many
steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL
method that leverages both proprioceptive states and visual observations for
locomotion control. We evaluate our method in challenging simulated
environments with different obstacles and uneven terrain. We transfer our
learned policy from simulation to a real robot by running it indoor and
in-the-wild with unseen obstacles and terrain. Our method not only
significantly improves over baselines, but also achieves far better
generalization performance, especially when transferred to the real robot. Our
project page with videos is at https://rchalyang.github.io/LocoTransformer/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional GANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10060">
<div class="article-summary-box-inner">
<span><p>Conditional generative models aim to learn the underlying joint distribution
of data and labels, and thus realize conditional generation. Among them,
auxiliary classifier generative adversarial networks (AC-GAN) have been widely
used, but suffer from the problem of low intra-class diversity on generated
samples. In this paper, we point out that the fundamental reason is that the
classifier of AC-GAN is generator-agnostic, and therefore cannot provide
informative guidance to the generator to approximate the target distribution,
resulting in minimization of conditional entropy that decreases the intra-class
diversity. Motivated by this observation, we propose a novel conditional GAN
with auxiliary \textit{discriminative} classifier (ADC-GAN) to resolve the
problem of AC-GAN. Specifically, the proposed auxiliary \textit{discriminative}
classifier becomes generator-aware by recognizing the labels of the real data
and the generated data \textit{discriminatively}. Our theoretical analysis
reveals that the generator can faithfully replicate the target distribution
even without the original discriminator, making the proposed ADC-GAN robust to
the hyper-parameter and stable on the training process. Extensive experimental
results on synthetic and real-world datasets demonstrate the superiority of
ADC-GAN on conditional generative modeling compared with competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos. (arXiv:2107.11629v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11629">
<div class="article-summary-box-inner">
<span><p>Exploring to what humans pay attention in dynamic panoramic scenes is useful
for many fundamental applications, including augmented reality (AR) in retail,
AR-powered recruitment, and visual language navigation. With this goal in mind,
we propose PV-SOD, a new task that aims to segment salient objects from
panoramic videos. In contrast to existing fixation-/object-level saliency
detection tasks, we focus on audio-induced salient object detection (SOD),
where the salient objects are labeled with the guidance of audio-induced eye
movements. To support this task, we collect the first large-scale dataset,
named ASOD60K, which contains 4K-resolution video frames annotated with a
six-level hierarchy, thus distinguishing itself with richness, diversity and
quality. Specifically, each sequence is marked with both its super-/sub-class,
with objects of each sub-class being further annotated with human eye
fixations, bounding boxes, object-/instance-level masks, and associated
attributes (e.g., geometrical distortion). These coarse-to-fine annotations
enable detailed analysis for PV-SOD modelling, e.g., determining the major
challenges for existing SOD models, and predicting scanpaths to study the
long-term eye fixation behaviors of humans. We systematically benchmark 11
representative approaches on ASOD60K and derive several interesting findings.
We hope this study could serve as a good starting point for advancing SOD
research towards panoramic videos. The dataset and benchmark will be made
publicly available at https://github.com/PanoAsh/ASOD60K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Adversarially Blur Visual Object Tracking. (arXiv:2107.12085v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12085">
<div class="article-summary-box-inner">
<span><p>Motion blur caused by the moving of the object or camera during the exposure
can be a key challenge for visual object tracking, affecting tracking accuracy
significantly. In this work, we explore the robustness of visual object
trackers against motion blur from a new angle, i.e., adversarial blur attack
(ABA). Our main objective is to online transfer input frames to their natural
motion-blurred counterparts while misleading the state-of-the-art trackers
during the tracking process. To this end, we first design the motion blur
synthesizing method for visual tracking based on the generation principle of
motion blur, considering the motion information and the light accumulation
process. With this synthetic method, we propose optimization-based ABA (OP-ABA)
by iteratively optimizing an adversarial objective function against the
tracking w.r.t. the motion and light accumulation parameters. The OP-ABA is
able to produce natural adversarial examples but the iteration can cause heavy
time cost, making it unsuitable for attacking real-time trackers. To alleviate
this issue, we further propose one-step ABA (OS-ABA) where we design and train
a joint adversarial motion and accumulation predictive network (JAMANet) with
the guidance of OP-ABA, which is able to efficiently estimate the adversarial
motion and accumulation parameters in a one-step way. The experiments on four
popular datasets (e.g., OTB100, VOT2018, UAV123, and LaSOT) demonstrate that
our methods are able to cause significant accuracy drops on four
state-of-the-art trackers with high transferability. Please find the source
code at \url{https://github.com/tsingqguo/ABA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Semantic Segmentation with Superpixel-Mix. (arXiv:2108.00968v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00968">
<div class="article-summary-box-inner">
<span><p>Along with predictive performance and runtime speed, reliability is a key
requirement for real-world semantic segmentation. Reliability encompasses
robustness, predictive uncertainty and reduced bias. To improve reliability, we
introduce Superpixel-mix, a new superpixel-based data augmentation method with
teacher-student consistency training. Unlike other mixing-based augmentation
techniques, mixing superpixels between images is aware of object boundaries,
while yielding consistent gains in segmentation accuracy. Our proposed
technique achieves state-of-the-art results in semi-supervised semantic
segmentation on the Cityscapes dataset. Moreover, Superpixel-mix improves the
reliability of semantic segmentation by reducing network uncertainty and bias,
as confirmed by competitive results under strong distributions shift (adverse
weather, image corruptions) and when facing out-of-distribution data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning for Pose Estimation of Illustrated Characters. (arXiv:2108.01819v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01819">
<div class="article-summary-box-inner">
<span><p>Human pose information is a critical component in many downstream image
processing tasks, such as activity recognition and motion tracking. Likewise, a
pose estimator for the illustrated character domain would provide a valuable
prior for assistive content creation tasks, such as reference pose retrieval
and automatic character animation. But while modern data-driven techniques have
substantially improved pose estimation performance on natural images, little
work has been done for illustrations. In our work, we bridge this domain gap by
efficiently transfer-learning from both domain-specific and task-specific
source models. Additionally, we upgrade and expand an existing illustrated pose
estimation dataset, and introduce two new datasets for classification and
segmentation subtasks. We then apply the resultant state-of-the-art character
pose estimator to solve the novel task of pose-guided illustration retrieval.
All data, models, and code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution. (arXiv:2108.03541v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03541">
<div class="article-summary-box-inner">
<span><p>Images tell powerful stories but cannot always be trusted. Matching images
back to trusted sources (attribution) enables users to make a more informed
judgment of the images they encounter online. We propose a robust image hashing
algorithm to perform such matching. Our hash is sensitive to manipulation of
subtle, salient visual details that can substantially change the story told by
an image. Yet the hash is invariant to benign transformations (changes in
quality, codecs, sizes, shapes, etc.) experienced by images during online
redistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph
Attention for Image Attribution Network); a robust image hashing model inspired
by recent successes of Transformers in the visual domain. OSCAR-Net constructs
a scene graph representation that attends to fine-grained changes of every
object's visual appearance and their spatial relationships. The network is
trained via contrastive learning on a dataset of original and manipulated
images yielding a state of the art image hash for content fingerprinting that
scales to millions of images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">White blood cell subtype detection and classification. (arXiv:2108.04614v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04614">
<div class="article-summary-box-inner">
<span><p>Machine learning has endless applications in the health care industry. White
blood cell classification is one of the interesting and promising area of
research. The classification of the white blood cells plays an important part
in the medical diagnosis. In practise white blood cell classification is
performed by the haematologist by taking a small smear of blood and careful
examination under the microscope. The current procedures to identify the white
blood cell subtype is more time taking and error-prone. The computer aided
detection and diagnosis of the white blood cells tend to avoid the human error
and reduce the time taken to classify the white blood cells. In the recent
years several deep learning approaches have been developed in the context of
classification of the white blood cells that are able to identify but are
unable to localize the positions of white blood cells in the blood cell image.
Following this, the present research proposes to utilize YOLOv3 object
detection technique to localize and classify the white blood cells with
bounding boxes. With exhaustive experimental analysis, the proposed work is
found to detect the white blood cell with 99.2% accuracy and classify with 90%
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled GAN-Based Creature Synthesis via a Challenging Game Art Dataset -- Addressing the Noise-Latent Trade-Off. (arXiv:2108.08922v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08922">
<div class="article-summary-box-inner">
<span><p>The state-of-the-art StyleGAN2 network supports powerful methods to create
and edit art, including generating random images, finding images "like" some
query, and modifying content or style. Further, recent advancements enable
training with small datasets. We apply these methods to synthesize card art, by
training on a novel Yu-Gi-Oh dataset. While noise inputs to StyleGAN2 are
essential for good synthesis, we find that coarse-scale noise interferes with
latent variables on this dataset because both control long-scale image effects.
We observe over-aggressive variation in art with changes in noise and weak
content control via latent variable edits. Here, we demonstrate that training a
modified StyleGAN2, where coarse-scale noise is suppressed, removes these
unwanted effects. We obtain a superior FID; changes in noise result in local
exploration of style; and identity control is markedly improved. These results
and analysis lead towards a GAN-assisted art synthesis tool for digital artists
of all skill levels, which can be used in film, games, or any creative industry
for artistic ideation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">External Knowledge enabled Text Visual Question Answering. (arXiv:2108.09717v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09717">
<div class="article-summary-box-inner">
<span><p>The open-ended question answering task of Text-VQA requires reading and
reasoning about local, often previously unseen, scene-text content of an image
to generate answers. In this work, we propose the generalized use of external
knowledge to augment our understanding of the said scene-text. We design a
framework to extract, validate, and reason with knowledge using a standard
multimodal transformer for vision language understanding tasks. Through
empirical evidence and qualitative results, we demonstrate how external
knowledge can highlight instance-only cues and thus help deal with training
data bias, improve answer entity type correctness, and detect multiword named
entities. We generate results comparable to the state-of-the-art on two
publicly available datasets, under the constraints of similar upstream OCR
systems and training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Object Detection by Label Assignment Distillation. (arXiv:2108.10520v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10520">
<div class="article-summary-box-inner">
<span><p>Label assignment in object detection aims to assign targets, foreground or
background, to sampled regions in an image. Unlike labeling for image
classification, this problem is not well defined due to the object's bounding
box. In this paper, we investigate the problem from a perspective of
distillation, hence we call Label Assignment Distillation (LAD). Our initial
motivation is very simple, we use a teacher network to generate labels for the
student. This can be achieved in two ways: either using the teacher's
prediction as the direct targets (soft label), or through the hard labels
dynamically assigned by the teacher (LAD). Our experiments reveal that: (i) LAD
is more effective than soft-label, but they are complementary. (ii) Using LAD,
a smaller teacher can also improve a larger student significantly, while
soft-label can't. We then introduce Co-learning LAD, in which two networks
simultaneously learn from scratch and the role of teacher and student are
dynamically interchanged. Using PAA-ResNet50 as a teacher, our LAD techniques
can improve detectors PAA-ResNet101 and PAA-ResNeXt101 to $46 \rm AP$ and
$47.5\rm AP$ on the COCO test-dev set. With a stronger teacher PAA-SwinB, we
improve the students PAA-ResNet50 to $43.7\rm AP$ by only 1x schedule training
and standard setting, and PAA-ResNet101 to $47.9\rm AP$, significantly
surpassing the current methods. Our source code and checkpoints are released at
https://git.io/JrDZo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-RAFT: Dense Optical Flow from Event Cameras. (arXiv:2108.10552v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10552">
<div class="article-summary-box-inner">
<span><p>We propose to incorporate feature correlation and sequential processing into
dense optical flow estimation from event cameras. Modern frame-based optical
flow methods heavily rely on matching costs computed from feature correlation.
In contrast, there exists no optical flow method for event cameras that
explicitly computes matching costs. Instead, learning-based approaches using
events usually resort to the U-Net architecture to estimate optical flow
sparsely. Our key finding is that the introduction of correlation features
significantly improves results compared to previous methods that solely rely on
convolution layers. Compared to the state-of-the-art, our proposed approach
computes dense optical flow and reduces the end-point error by 23% on MVSEC.
Furthermore, we show that all existing optical flow methods developed so far
for event cameras have been evaluated on datasets with very small displacement
fields with a maximum flow magnitude of 10 pixels. Based on this observation,
we introduce a new real-world dataset that exhibits displacement fields with
magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed
approach reduces the end-point error on this dataset by 66%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physical Adversarial Attacks on an Aerial Imagery Object Detector. (arXiv:2108.11765v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11765">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have become essential for processing the vast
amounts of aerial imagery collected using earth-observing satellite platforms.
However, DNNs are vulnerable towards adversarial examples, and it is expected
that this weakness also plagues DNNs for aerial imagery. In this work, we
demonstrate one of the first efforts on physical adversarial attacks on aerial
imagery, whereby adversarial patches were optimised, fabricated and installed
on or near target objects (cars) to significantly reduce the efficacy of an
object detector applied on overhead images. Physical adversarial attacks on
aerial images, particularly those captured from satellite platforms, are
challenged by atmospheric factors (lighting, weather, seasons) and the distance
between the observer and target. To investigate the effects of these
challenges, we devised novel experiments and metrics to evaluate the efficacy
of physical adversarial attacks against object detectors in aerial scenes. Our
results indicate the palpable threat posed by physical adversarial attacks
towards DNNs for processing satellite imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Compression for Resource-Constrained Edge Computing Systems. (arXiv:2108.11898v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11898">
<div class="article-summary-box-inner">
<span><p>There has been much interest in deploying deep learning algorithms on
low-powered devices, including smartphones, drones, and medical sensors.
However, full-scale deep neural networks are often too resource-intensive in
terms of energy and storage. As a result, the bulk part of the machine learning
operation is therefore often carried out on an edge server, where the data is
compressed and transmitted. However, compressing data (such as images) leads to
transmitting information irrelevant to the supervised task. Another popular
approach is to split the deep network between the device and the server while
compressing intermediate features. To date, however, such split computing
strategies have barely outperformed the aforementioned naive data compression
baselines due to their inefficient approaches to feature compression. This
paper adopts ideas from knowledge distillation and neural image compression to
compress intermediate feature representations more efficiently. Our supervised
compression approach uses a teacher model and a student model with a stochastic
bottleneck and learnable prior for entropy coding (Entropic Student). We
compare our approach to various neural image and feature compression baselines
in three vision tasks and found that it achieves better supervised
rate-distortion performance while maintaining smaller end-to-end latency. We
furthermore show that the learned feature representations can be tuned to serve
multiple downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02639">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection and lossless compression constitute two
problems that can be solved by the training of probabilistic models on a first
dataset with subsequent likelihood evaluation on a second dataset, where data
distributions differ. By defining the generalization of probabilistic models in
terms of likelihood we show that, in the case of image models, the OOD
generalization ability is dominated by local features. This motivates our
proposal of a Local Autoregressive model that exclusively models local image
features towards improving OOD performance. We apply the proposed model to OOD
detection tasks and achieve state-of-the-art unsupervised OOD detection
performance without the introduction of additional data. Additionally, we
employ our model to build a new lossless image compressor: NeLLoC (Neural Local
Lossless Compressor) and report state-of-the-art compression rates and model
size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting. (arXiv:2109.06061v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06061">
<div class="article-summary-box-inner">
<span><p>In this work, we address the problem of jointly estimating albedo, normals,
depth and 3D spatially-varying lighting from a single image. Most existing
methods formulate the task as image-to-image translation, ignoring the 3D
properties of the scene. However, indoor scenes contain complex 3D light
transport where a 2D representation is insufficient. In this paper, we propose
a unified, learning-based inverse rendering framework that formulates 3D
spatially-varying lighting. Inspired by classic volume rendering techniques, we
propose a novel Volumetric Spherical Gaussian representation for lighting,
which parameterizes the exitant radiance of the 3D scene surfaces on a voxel
grid. We design a physics based differentiable renderer that utilizes our 3D
lighting representation, and formulates the energy-conserving image formation
process that enables joint training of all intrinsic properties with the
re-rendering constraint. Our model ensures physically correct predictions and
avoids the need for ground-truth HDR lighting which is not easily accessible.
Experiments show that our method outperforms prior works both quantitatively
and qualitatively, and is capable of producing photorealistic results for AR
applications such as virtual object insertion even for highly specular objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Discrete Wavelet Pooling (LDW-Pooling) For Convolutional Networks. (arXiv:2109.06638v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06638">
<div class="article-summary-box-inner">
<span><p>Pooling is a simple but essential layer in modern deep CNN architectures for
feature aggregation and extraction. Typical CNN design focuses on the conv
layers and activation functions, while leaving the pooling layers with fewer
options. We introduce the Learning Discrete Wavelet Pooling (LDW-Pooling) that
can be applied universally to replace standard pooling operations to better
extract features with improved accuracy and efficiency. Motivated from the
wavelet theory, we adopt the low-pass (L) and high-pass (H) filters
horizontally and vertically for pooling on a 2D feature map. Feature signals
are decomposed into four (LL, LH, HL, HH) subbands to retain features better
and avoid information dropping. The wavelet transform ensures features after
pooling can be fully preserved and recovered. We next adopt an energy-based
attention learning to fine-select crucial and representative features.
LDW-Pooling is effective and efficient when compared with other
state-of-the-art pooling techniques such as WaveletPooling and LiftPooling.
Extensive experimental validation shows that LDW-Pooling can be applied to a
wide range of standard CNN architectures and consistently outperform standard
(max, mean, mixed, and stochastic) pooling operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">F-CAM: Full Resolution Class Activation Maps via Guided Parametric Upscaling. (arXiv:2109.07069v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07069">
<div class="article-summary-box-inner">
<span><p>Class Activation Mapping (CAM) methods have recently gained much attention
for weakly-supervised object localization (WSOL) tasks. They allow for CNN
visualization and interpretation without training on fully annotated image
datasets. CAM methods are typically integrated within off-the-shelf CNN
backbones, such as ResNet50. Due to convolution and pooling operations, these
backbones yield low resolution CAMs with a down-scaling factor of up to 32,
contributing to inaccurate localizations. Interpolation is required to restore
full size CAMs, yet it does not consider the statistical properties of objects,
such as color and texture, leading to activations with inconsistent boundaries,
and inaccurate localizations. As an alternative, we introduce a generic method
for parametric upscaling of CAMs that allows constructing accurate full
resolution CAMs (F-CAMs). In particular, we propose a trainable decoding
architecture that can be connected to any CNN classifier to produce highly
accurate CAM localizations. Given an original low resolution CAM, foreground
and background pixels are randomly sampled to fine-tune the decoder. Additional
priors such as image statistics and size constraints are also considered to
expand and refine object boundaries. Extensive experiments, over three CNN
backbones and six WSOL baselines on the CUB-200-2011 and OpenImages datasets,
indicate that our F-CAM method yields a significant improvement in CAM
localization accuracy. F-CAM performance is competitive with state-of-art WSOL
methods, yet it requires fewer computations during inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03825">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are known to be vulnerable to adversarial
attacks. A range of defense methods have been proposed to train adversarially
robust DNNs, among which adversarial training has demonstrated promising
results. However, despite preliminary understandings developed for adversarial
training, it is still not clear, from the architectural perspective, what
configurations can lead to more robust DNNs. In this paper, we address this gap
via a comprehensive investigation on the impact of network width and depth on
the robustness of adversarially trained DNNs. Specifically, we make the
following key observations: 1) more parameters (higher model capacity) does not
necessarily help adversarial robustness; 2) reducing capacity at the last stage
(the last group of blocks) of the network can actually improve adversarial
robustness; and 3) under the same parameter budget, there exists an optimal
architectural configuration for adversarial robustness. We also provide a
theoretical analysis explaning why such network configuration can help
robustness. These architectural insights can help design adversarially robust
DNNs. Code is available at \url{https://github.com/HanxunH/RobustWRN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CyTran: Cycle-Consistent Transformers for Non-Contrast to Contrast CT Translation. (arXiv:2110.06400v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06400">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach to translate unpaired contrast computed
tomography (CT) scans to non-contrast CT scans and the other way around.
Solving this task has two important applications: (i) to automatically generate
contrast CT scans for patients for whom injecting contrast substance is not an
option, and (ii) to enhance alignment between contrast and non-contrast CT by
reducing the differences induced by the contrast substance before registration.
Our approach is based on cycle-consistent generative adversarial convolutional
transformers, for short, CyTran. Our neural model can be trained on unpaired
images, due to the integration of a cycle-consistency loss. To deal with
high-resolution images, we design a hybrid architecture based on convolutional
and multi-head attention layers. In addition, we introduce a novel data set,
Coltea-Lung-CT-100W, containing 3D triphasic lung CT scans (with a total of
37,290 images) collected from 100 female patients. Each scan contains three
phases (non-contrast, early portal venous, and late arterial), allowing us to
perform experiments to compare our novel approach with state-of-the-art methods
for image style transfer. Our empirical results show that CyTran outperforms
all competing methods. Moreover, we show that CyTran can be employed as a
preliminary step to improve a state-of-the-art medical image alignment method.
We release our novel model and data set as open source at:
https://github.com/ristea/cycle-transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comprehensive review of Binary Neural Network. (arXiv:2110.06804v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06804">
<div class="article-summary-box-inner">
<span><p>Binary Neural Network (BNN) method is an extreme application of convolutional
neural network (CNN) parameter quantization. As opposed to the original CNN
methods which employed floating-point computation with full-precision weights
and activations, BBN uses 1-bit activations and weights. With BBNs, a
significant amount of storage, network complexity and energy consumption can be
reduced, and neural networks can be implemented more efficiently in embedded
applications. Unfortunately, binarization causes severe information loss. A gap
still exists between full-precision CNN models and their binarized
counterparts. The recent developments in BNN have led to a lot of algorithms
and solutions that have helped address this issue. This article provides a full
overview of recent developments in BNN. The present paper focuses exclusively
on 1-bit activations and weights networks, as opposed to previous surveys in
which low-bit works are mixed in. In this paper, we conduct a complete
investigation of BNN's development from their predecessors to the latest BNN
algorithms and techniques, presenting a broad design pipeline, and discussing
each module's variants. Along the way, this paper examines BNN (a) purpose:
their early successes and challenges; (b) BNN optimization: selected
representative works that contain key optimization techniques; (c) deployment:
open-source frameworks for BNN modeling and development; (d) terminal:
efficient computing architectures and devices for BNN and (e) applications:
diverse applications with BNN. Moreover, this paper discusses potential
directions and future research opportunities for the latest BNN algorithms and
techniques, presents a broad design pipeline, and discusses each module's
variants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media. (arXiv:2110.07235v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07235">
<div class="article-summary-box-inner">
<span><p>We introduce HUMAN4D, a large and multimodal 4D dataset that contains a
variety of human activities simultaneously captured by a professional
marker-based MoCap, a volumetric capture and an audio recording system. By
capturing 2 female and $2$ male professional actors performing various
full-body movements and expressions, HUMAN4D provides a diverse set of motions
and poses encountered as part of single- and multi-person daily, physical and
social activities (jumping, dancing, etc.), along with multi-RGBD (mRGBD),
volumetric and audio data. Despite the existence of multi-view color datasets
captured with the use of hardware (HW) synchronization, to the best of our
knowledge, HUMAN4D is the first and only public resource that provides
volumetric depth maps with high synchronization precision due to the use of
intra- and inter-sensor HW-SYNC. Moreover, a spatio-temporally aligned scanned
and rigged 3D character complements HUMAN4D to enable joint research on
time-varying and high-quality dynamic meshes. We provide evaluation baselines
by benchmarking HUMAN4D with state-of-the-art human pose estimation and 3D
compression methods. For the former, we apply 2D and 3D pose estimation
algorithms both on single- and multi-view data cues. For the latter, we
benchmark open-source 3D codecs on volumetric data respecting online volumetric
video encoding and steady bit-rates. Furthermore, qualitative and quantitative
visual comparison between mesh-based volumetric data reconstructed in different
qualities showcases the available options with respect to 4D representations.
HUMAN4D is introduced to the computer vision and graphics research communities
to enable joint research on spatio-temporally aligned pose, volumetric, mRGBD
and audio data cues. The dataset and its code are available
https://tofis.github.io/myurls/human4d.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Aggregation Network for Fast MR Imaging. (arXiv:2110.08080v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08080">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance (MR) imaging is a commonly used scanning technique for
disease detection, diagnosis and treatment monitoring. Although it is able to
produce detailed images of organs and tissues with better contrast, it suffers
from a long acquisition time, which makes the image quality vulnerable to say
motion artifacts. Recently, many approaches have been developed to reconstruct
full-sampled images from partially observed measurements in order to accelerate
MR imaging. However, most of these efforts focus on reconstruction over a
single modality or simple fusion of multiple modalities, neglecting the
discovery of correlation knowledge at different feature level. In this work, we
propose a novel Multi-modal Aggregation Network, named MANet, which is capable
of discovering complementary representations from a fully sampled auxiliary
modality, with which to hierarchically guide the reconstruction of a given
target modality. In our MANet, the representations from the fully sampled
auxiliary and undersampled target modalities are learned independently through
a specific network. Then, a guided attention module is introduced in each
convolutional stage to selectively aggregate multi-modal features for better
reconstruction, yielding comprehensive, multi-scale, multi-modal feature
fusion. Moreover, our MANet follows a hybrid domain learning framework, which
allows it to simultaneously recover the frequency signal in the $k$-space
domain as well as restore the image details from the image domain. Extensive
experiments demonstrate the superiority of the proposed approach over
state-of-the-art MR image reconstruction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based detection of intravenous contrast in computed tomography scans. (arXiv:2110.08424v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08424">
<div class="article-summary-box-inner">
<span><p>Purpose: Identifying intravenous (IV) contrast use within CT scans is a key
component of data curation for model development and testing. Currently, IV
contrast is poorly documented in imaging metadata and necessitates manual
correction and annotation by clinician experts, presenting a major barrier to
imaging analyses and algorithm deployment. We sought to develop and validate a
convolutional neural network (CNN)-based deep learning (DL) platform to
identify IV contrast within CT scans. Methods: For model development and
evaluation, we used independent datasets of CT scans of head, neck (HN) and
lung cancer patients, totaling 133,480 axial 2D scan slices from 1,979 CT scans
manually annotated for contrast presence by clinical experts. Five different DL
models were adopted and trained in HN training datasets for slice-level
contrast detection. Model performances were evaluated on a hold-out set and on
an independent validation set from another institution. DL models was then
fine-tuned on chest CT data and externally validated on a separate chest CT
dataset. Results: Initial DICOM metadata tags for IV contrast were missing or
erroneous in 1,496 scans (75.6%). The EfficientNetB4-based model showed the
best overall detection performance. For HN scans, AUC was 0.996 in the internal
validation set (n = 216) and 1.0 in the external validation set (n = 595). The
fine-tuned model on chest CTs yielded an AUC: 1.0 for the internal validation
set (n = 53), and AUC: 0.980 for the external validation set (n = 402).
Conclusion: The DL model could accurately detect IV contrast in both HN and
chest CT scans with near-perfect performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08733">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches have shown promising results in remote sensing high
spatial resolution (HSR) land-cover mapping. However, urban and rural scenes
can show completely different geographical landscapes, and the inadequate
generalizability of these algorithms hinders city-level or national-level
mapping. Most of the existing HSR land-cover datasets mainly promote the
research of learning semantic representation, thereby ignoring the model
transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive
semantic segmentation (LoveDA) dataset to advance semantic and transferable
learning. The LoveDA dataset contains 5987 HSR images with 166768 annotated
objects from three different cities. Compared to the existing datasets, the
LoveDA dataset encompasses two domains (urban and rural), which brings
considerable challenges due to the: 1) multi-scale objects; 2) complex
background samples; and 3) inconsistent class distributions. The LoveDA dataset
is suitable for both land-cover semantic segmentation and unsupervised domain
adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on
eleven semantic segmentation methods and eight UDA methods. Some exploratory
studies including multi-scale architectures and strategies, additional
background supervision, and pseudo-label analysis were also carried out to
address these challenges. The code and data are available at
https://github.com/Junjue-Wang/LoveDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Effect of Selfie Beautification Filters on Face Detection and Recognition. (arXiv:2110.08934v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08934">
<div class="article-summary-box-inner">
<span><p>Beautification and augmented reality filters are very popular in applications
that use selfie images captured with smartphones or personal devices. However,
they can distort or modify biometric features, severely affecting the
capability of recognizing individuals' identity or even detecting the face.
Accordingly, we address the effect of such filters on the accuracy of automated
face detection and recognition. The social media image filters studied either
modify the image contrast or illumination or occlude parts of the face with for
example artificial glasses or animal noses. We observe that the effect of some
of these filters is harmful both to face detection and identity recognition,
specially if they obfuscate the eye or (to a lesser extent) the nose. To
counteract such effect, we develop a method to reconstruct the applied
manipulation with a modified version of the U-NET segmentation network. This is
observed to contribute to a better face detection and recognition accuracy.
From a recognition perspective, we employ distance measures and trained machine
learning algorithms applied to features extracted using a ResNet-34 network
trained to recognize faces. We also evaluate if incorporating filtered images
to the training set of machine learning approaches are beneficial for identity
recognition. Our results show good recognition when filters do not occlude
important landmarks, specially the eyes (identification accuracy &gt;99%, EER&lt;2%).
The combined effect of the proposed approaches also allow to mitigate the
effect produced by filters that occlude parts of the face, achieving an
identification accuracy of &gt;92% with the majority of perturbations evaluated,
and an EER &lt;8%. Although there is room for improvement, when neither U-NET
reconstruction nor training with filtered images is applied, the accuracy with
filters that severely occlude the eye is &lt;72% (identification) and &gt;12% (EER)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asymmetric Modality Translation For Face Presentation Attack Detection. (arXiv:2110.09108v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09108">
<div class="article-summary-box-inner">
<span><p>Face presentation attack detection (PAD) is an essential measure to protect
face recognition systems from being spoofed by malicious users and has
attracted great attention from both academia and industry. Although most of the
existing methods can achieve desired performance to some extent, the
generalization issue of face presentation attack detection under cross-domain
settings (e.g., the setting of unseen attacks and varying illumination) remains
to be solved. In this paper, we propose a novel framework based on asymmetric
modality translation for face presentation attack detection in bi-modality
scenarios. Under the framework, we establish connections between two modality
images of genuine faces. Specifically, a novel modality fusion scheme is
presented that the image of one modality is translated to the other one through
an asymmetric modality translator, then fused with its corresponding paired
image. The fusion result is fed as the input to a discriminator for inference.
The training of the translator is supervised by an asymmetric modality
translation loss. Besides, an illumination normalization module based on
Pattern of Local Gravitational Force (PLGF) representation is used to reduce
the impact of illumination variation. We conduct extensive experiments on three
public datasets, which validate that our method is effective in detecting
various types of attacks and achieves state-of-the-art performance under
different evaluation protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mesh Convolutional Autoencoder for Semi-Regular Meshes of Different Sizes. (arXiv:2110.09401v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09401">
<div class="article-summary-box-inner">
<span><p>The analysis of deforming 3D surface meshes is accelerated by autoencoders
since the low-dimensional embeddings can be used to visualize underlying
dynamics. But, state-of-the-art mesh convolutional autoencoders require a fixed
connectivity of all input meshes handled by the autoencoder. This is due to
either the use of spectral convolutional layers or mesh dependent pooling
operations. Therefore, the types of datasets that one can study are limited and
the learned knowledge cannot be transferred to other datasets that exhibit
similar behavior. To address this, we transform the discretization of the
surfaces to semi-regular meshes that have a locally regular connectivity and
whose meshing is hierarchical. This allows us to apply the same spatial
convolutional filters to the local neighborhoods and to define a pooling
operator that can be applied to every semi-regular mesh. We apply the same mesh
autoencoder to different datasets and our reconstruction error is more than 50%
lower than the error from state-of-the-art models, which have to be trained for
every mesh separately. Additionally, we visualize the underlying dynamics of
unseen mesh sequences with an autoencoder trained on different classes of
meshes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HRFormer: High-Resolution Transformer for Dense Prediction. (arXiv:2110.09408v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09408">
<div class="article-summary-box-inner">
<span><p>We present a High-Resolution Transformer (HRFormer) that learns
high-resolution representations for dense prediction tasks, in contrast to the
original Vision Transformer that produces low-resolution representations and
has high memory and computational cost. We take advantage of the
multi-resolution parallel design introduced in high-resolution convolutional
networks (HRNet), along with local-window self-attention that performs
self-attention over small non-overlapping image windows, for improving the
memory and computation efficiency. In addition, we introduce a convolution into
the FFN to exchange information across the disconnected image windows. We
demonstrate the effectiveness of the High-Resolution Transformer on both human
pose estimation and semantic segmentation tasks, e.g., HRFormer outperforms
Swin transformer by $1.3$ AP on COCO pose estimation with $50\%$ fewer
parameters and $30\%$ fewer FLOPs. Code is available at:
https://github.com/HRNet/HRFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Monocular Depth Estimation with Internal Feature Fusion. (arXiv:2110.09482v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09482">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning for depth estimation uses geometry in image
sequences for supervision and shows promising results. Like many computer
vision tasks, depth network performance is determined by the capability to
learn accurate spatial and semantic representations from images. Therefore, it
is natural to exploit semantic segmentation networks for depth estimation. In
this work, based on a well-developed semantic segmentation network HRNet, we
propose a novel depth estimation networkDIFFNet, which can make use of semantic
information in down and upsampling procedures. By applying feature fusion and
an attention mechanism, our proposed method outperforms the state-of-the-art
monocular depth estimation methods on the KITTI benchmark. Our method also
demonstrates greater potential on higher resolution training data. We propose
an additional extended evaluation strategy by establishing a test set of
challenging cases, empirically derived from the standard benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Variability Augmented Sparse Unmixing of Hyperspectral Images. (arXiv:2110.09744v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09744">
<div class="article-summary-box-inner">
<span><p>Spectral unmixing (SU) expresses the mixed pixels existed in hyperspectral
images as the product of endmember and abundance, which has been widely used in
hyperspectral imagery analysis. However, the influence of light, acquisition
conditions and the inherent properties of materials, results in that the
identified endmembers can vary spectrally within a given image (construed as
spectral variability). To address this issue, recent methods usually use a
priori obtained spectral library to represent multiple characteristic spectra
of the same object, but few of them extracted the spectral variability
explicitly. In this paper, a spectral variability augmented sparse unmixing
model (SVASU) is proposed, in which the spectral variability is extracted for
the first time. The variable spectra are divided into two parts of intrinsic
spectrum and spectral variability for spectral reconstruction, and modeled
synchronously in the SU model adding the regular terms restricting the sparsity
of abundance and the generalization of the variability coefficient. It is noted
that the spectral variability library and the intrinsic spectral library are
all constructed from the In-situ observed image. Experimental results over both
synthetic and real-world data sets demonstrate that the augmented decomposition
by spectral variability significantly improves the unmixing performance than
the decomposition only by spectral library, as well as compared to
state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry. (arXiv:2110.09772v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09772">
<div class="article-summary-box-inner">
<span><p>This work studies learning from a synergy process of 3D Morphable Models
(3DMM) and 3D facial landmarks to predict complete 3D facial geometry,
including 3D alignment, face orientation, and 3D face modeling. Our synergy
process leverages a representation cycle for 3DMM parameters and 3D landmarks.
3D landmarks can be extracted and refined from face meshes built by 3DMM
parameters. We next reverse the representation direction and show that
predicting 3DMM parameters from sparse 3D landmarks improves the information
flow. Together we create a synergy process that utilizes the relation between
3D landmarks and 3DMM parameters, and they collaboratively contribute to better
performance. We extensively validate our contribution on full tasks of facial
geometry prediction and show our superior and robust performance on these tasks
for various scenarios. Particularly, we adopt only simple and widely-used
network operations to attain fast and accurate facial geometry prediction.
Codes and data: https://choyingw.github.io/works/SynergyNet/
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-24 23:02:13.621968045 UTC">2021-10-24 23:02:13 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.4</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>