<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-08T01:30:00Z">04-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems. (arXiv:2204.03021v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03021">
<div class="article-summary-box-inner">
<span><p>Conversational agents have come increasingly closer to human competence in
open-domain dialogue settings; however, such models can reflect insensitive,
hurtful, or entirely incoherent viewpoints that erode a user's trust in the
moral integrity of the system. Moral deviations are difficult to mitigate
because moral judgments are not universal, and there may be multiple competing
judgments that apply to a situation simultaneously. In this work, we introduce
a new resource, not to authoritatively resolve moral ambiguities, but instead
to facilitate systematic understanding of the intuitions, values and moral
judgments reflected in the utterances of dialogue systems. The Moral Integrity
Corpus, MIC, is such a resource, which captures the moral assumptions of 38k
prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects
a particular moral conviction that can explain why a chatbot's reply may appear
acceptable or problematic. We further organize RoTs with a set of 9 moral and
social attributes and benchmark performance for attribute classification. Most
importantly, we show that current neural language models can automatically
generate new RoTs that reasonably describe previously unseen interactions, but
they still struggle with certain scenarios. Our findings suggest that MIC will
be a useful resource for understanding and language models' implicit moral
assumptions and flexibly benchmarking the integrity of conversational agents.
To download the data, see https://github.com/GT-SALT/mic
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment. (arXiv:2204.03025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03025">
<div class="article-summary-box-inner">
<span><p>Most research on question answering focuses on the pre-deployment stage;
i.e., building an accurate model for deployment. In this paper, we ask the
question: Can we improve QA systems further \emph{post-}deployment based on
user interactions? We focus on two kinds of improvements: 1) improving the QA
system's performance itself, and 2) providing the model with the ability to
explain the correctness or incorrectness of an answer. We collect a
retrieval-based QA dataset, FeedbackQA, which contains interactive feedback
from users. We collect this dataset by deploying a base QA system to
crowdworkers who then engage with the system and provide feedback on the
quality of its answers. The feedback contains both structured ratings and
unstructured natural language explanations. We train a neural model with this
feedback data that can generate explanations and re-score answer candidates. We
show that feedback data not only improves the accuracy of the deployed QA
system but also other stronger non-deployed systems. The generated explanations
also help users make informed decisions about the correctness of answers.
Project page: https://mcgill-nlp.github.io/feedbackqa/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VALUE: Understanding Dialect Disparity in NLU. (arXiv:2204.03031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03031">
<div class="article-summary-box-inner">
<span><p>English Natural Language Understanding (NLU) systems have achieved great
performances and even outperformed humans on benchmarks like GLUE and
SuperGLUE. However, these benchmarks contain only textbook Standard American
English (SAE). Other dialects have been largely overlooked in the NLP
community. This leads to biased and inequitable NLU systems that serve only a
sub-population of speakers. To understand disparities in current models and to
facilitate more dialect-competent NLU systems, we introduce the VernAcular
Language Understanding Evaluation (VALUE) benchmark, a challenging variant of
GLUE that we created with a set of lexical and morphosyntactic transformation
rules. In this initial release (V.1), we construct rules for 11 features of
African American Vernacular English (AAVE), and we recruit fluent AAVE speakers
to validate each feature transformation via linguistic acceptability judgments
in a participatory design manner. Experiments show that these new dialectal
features can lead to a drop in model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding. (arXiv:2204.03035v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03035">
<div class="article-summary-box-inner">
<span><p>Applying methods in natural language processing on electronic health records
(EHR) data is a growing field. Existing corpus and annotation focus on modeling
textual features and relation prediction. However, there is a paucity of
annotated corpus built to model clinical diagnostic thinking, a process
involving text understanding, domain knowledge abstraction and reasoning. This
work introduces a hierarchical annotation schema with three stages to address
clinical text understanding, clinical reasoning, and summarization. We created
an annotated corpus based on an extensive collection of publicly available
daily progress notes, a type of EHR documentation that is collected in time
series in a problem-oriented format. The conventional format for a progress
note follows a Subjective, Objective, Assessment and Plan heading (SOAP). We
also define a new suite of tasks, Progress Note Understanding, with three tasks
utilizing the three annotation stages. The novel suite of tasks was designed to
train and evaluate future NLP models for clinical text understanding, clinical
knowledge representation, inference, and summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis. (arXiv:2204.03040v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03040">
<div class="article-summary-box-inner">
<span><p>In this work, we present the SOMOS dataset, the first large-scale mean
opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS)
samples. It can be employed to train automatic MOS prediction systems focused
on the assessment of modern synthesizers, and can stimulate advancements in
acoustic model evaluation. It consists of 20K synthetic utterances of the LJ
Speech voice, a public domain speech dataset which is a common benchmark for
building neural acoustic models and vocoders. Utterances are generated from 200
TTS systems including vanilla neural acoustic models as well as models which
allow prosodic variations. An LPCNet vocoder is used for all systems, so that
the samples' variation depends only on the acoustic models. The synthesized
utterances provide balanced and adequate domain and length coverage. We collect
MOS naturalness evaluations on 3 English Amazon Mechanical Turk locales and
share practices leading to reliable crowdsourced annotations for this task.
Baseline results of state-of-the-art MOS prediction models on the SOMOS dataset
are presented, while we show the challenges that such models face when assigned
to evaluate synthetic utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing finetuned models for better pretraining. (arXiv:2204.03044v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03044">
<div class="article-summary-box-inner">
<span><p>Pretrained models are the standard starting point for training. This approach
consistently outperforms the use of a random initialization. However,
pretraining is a costly endeavour that few can undertake.
</p>
<p>In this paper, we create better base models at hardly any cost, by fusing
multiple existing fine tuned models into one. Specifically, we fuse by
averaging the weights of these models. We show that the fused model results
surpass the pretrained model ones. We also show that fusing is often better
than intertraining.
</p>
<p>We find that fusing is less dependent on the target task. Furthermore, weight
decay nullifies intertraining effects but not those of fusing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abusive and Threatening Language Detection in Urdu using Supervised Machine Learning and Feature Combinations. (arXiv:2204.03062v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03062">
<div class="article-summary-box-inner">
<span><p>This paper presents the system descriptions submitted at the FIRE Shared Task
2021 on Urdu's Abusive and Threatening Language Detection Task. This challenge
aims at automatically identifying abusive and threatening tweets written in
Urdu. Our submitted results were selected for the third recognition at the
competition. This paper reports a non-exhaustive list of experiments that
allowed us to reach the submitted results. Moreover, after the result
declaration of the competition, we managed to attain even better results than
the submitted results. Our models achieved 0.8318 F1 score on Task A (Abusive
Language Detection for Urdu Tweets) and 0.4931 F1 score on Task B (Threatening
Language Detection for Urdu Tweets). Results show that Support Vector Machines
with stopwords removed, lemmatization applied, and features vector created by
the combinations of word n-grams for n=1,2,3 produced the best results for Task
A. For Task B, Support Vector Machines with stopwords removed, lemmatization
not applied, feature vector created from a pre-trained Urdu Word2Vec (on word
unigrams and bigrams), and making the dataset balanced using oversampling
technique produced the best results. The code is made available for
reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 2021 Urdu Fake News Detection Task using Supervised Machine Learning and Feature Combinations. (arXiv:2204.03064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03064">
<div class="article-summary-box-inner">
<span><p>This paper presents the system description submitted at the FIRE Shared Task:
"The 2021 Fake News Detection in the Urdu Language". This challenge aims at
automatically identifying Fake news written in Urdu. Our submitted results
ranked fifth in the competition. However, after the result declaration of the
competition, we managed to attain even better results than the submitted
results. The best F1 Macro score achieved by one of our models is 0.6674,
higher than the second-best score in the competition. The result is achieved on
Support Vector Machines (polynomial kernel degree 1) with stopwords removed,
lemmatization applied, and selecting the 20K best features out of 1.557 million
features in total (which were produced by Word n-grams n=1,2,3,4 and Char
n-grams n=2,3,4,5,6). The code is made available for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ByT5 model for massively multilingual grapheme-to-phoneme conversion. (arXiv:2204.03067v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03067">
<div class="article-summary-box-inner">
<span><p>In this study, we tackle massively multilingual grapheme-to-phoneme
conversion through implementing G2P models based on ByT5. We have curated a G2P
dataset from various sources that covers around 100 languages and trained
large-scale multilingual G2P models based on ByT5. We found that ByT5 operating
on byte-level inputs significantly outperformed the token-based mT5 model in
terms of multilingual G2P. Pairwise comparison with monolingual models in these
languages suggests that multilingual ByT5 models generally lower the phone
error rate by jointly learning from a variety of languages. The pretrained
model can further benefit low resource G2P through zero-shot prediction on
unseen languages or provides pretrained weights for finetuning, which helps the
model converge to a lower phone error rate than randomly initialized weights.
To facilitate future research on multilingual G2P, we make available our code
and pretrained multilingual G2P models at:
https://github.com/lingjzhu/CharsiuG2P.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Urdu Morphology, Orthography and Lexicon Extraction. (arXiv:2204.03071v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03071">
<div class="article-summary-box-inner">
<span><p>Urdu is a challenging language because of, first, its Perso-Arabic script and
second, its morphological system having inherent grammatical forms and
vocabulary of Arabic, Persian and the native languages of South Asia. This
paper describes an implementation of the Urdu language as a software API, and
we deal with orthography, morphology and the extraction of the lexicon. The
morphology is implemented in a toolkit called Functional Morphology (Forsberg &amp;
Ranta, 2004), which is based on the idea of dealing grammars as software
libraries. Therefore this implementation could be reused in applications such
as intelligent search of keywords, language training and infrastructure for
syntax. We also present an implementation of a small part of Urdu syntax to
demonstrate this reusability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Infused Decoding. (arXiv:2204.03084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03084">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs) have been shown to memorize a substantial
amount of knowledge from the pre-training corpora; however, they are still
limited in recalling factually correct knowledge given a certain context.
Hence, they tend to suffer from counterfactual or hallucinatory generation when
used in knowledge-intensive natural language generation (NLG) tasks. Recent
remedies to this problem focus on modifying either the pre-training or task
fine-tuning objectives to incorporate knowledge, which normally require
additional costly training or architecture modification of LMs for practical
applications. We present Knowledge Infused Decoding (KID) -- a novel decoding
algorithm for generative LMs, which dynamically infuses external knowledge into
each step of the LM decoding. Specifically, we maintain a local knowledge
memory based on the current context, interacting with a dynamically created
external knowledge trie, and continuously update the local memory as a
knowledge-aware constraint to guide decoding via reinforcement learning. On six
diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART)
armed with KID outperform many task-optimized state-of-the-art models, and show
particularly strong performance in few-shot scenarios over seven related
knowledge-infusion techniques. Human evaluation confirms KID's ability to
generate more relevant and factual language for the input context when compared
with multiple baselines. Finally, KID also alleviates exposure bias and
provides stable generation quality when generating longer sequences. Code for
KID is available at https://github.com/microsoft/KID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis. (arXiv:2204.03117v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03117">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to align aspects and corresponding sentiments for
aspect-specific sentiment polarity inference. It is challenging because a
sentence may contain multiple aspects or complicated (e.g., conditional,
coordinating, or adversative) relations. Recently, exploiting dependency syntax
information with graph neural networks has been the most popular trend. Despite
its success, methods that heavily rely on the dependency tree pose challenges
in accurately modeling the alignment of the aspects and their words indicative
of sentiment, since the dependency tree may provide noisy signals of unrelated
associations (e.g., the "conj" relation between "great" and "dreadful" in
Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax
aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully
exploits the syntax information (e.g., phrase segmentation and hierarchical
structure) of the constituent tree of a sentence to model the sentiment-aware
context of every single aspect (called intra-context) and the sentiment
relations across aspects (called inter-context) for learning. Experiments on
four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the
state-of-the-art methods consistently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. (arXiv:2204.03162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03162">
<div class="article-summary-box-inner">
<span><p>We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech recognition. (arXiv:2204.03178v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03178">
<div class="article-summary-box-inner">
<span><p>Recently, Conformer based CTC/AED model has become a mainstream architecture
for ASR. In this paper, based on our prior work, we identify and integrate
several approaches to achieve further improvements for ASR tasks, which we
denote as multi-loss, multi-path and multi-level, summarized as "3M" model.
Specifically, multi-loss refers to the joint CTC/AED loss and multi-path
denotes the Mixture-of-Experts(MoE) architecture which can effectively increase
the model capacity without remarkably increasing computation cost. Multi-level
means that we introduce auxiliary loss at multiple level of a deep model to
help training. We evaluate our proposed method on the public WenetSpeech
dataset and experimental results show that the proposed method provides
12.2%-17.6% relative CER improvement over the baseline model trained by Wenet
toolkit. On our large scale dataset of 150k hours corpus, the 3M model has also
shown obvious superiority over the baseline Conformer model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint Learning Approach for Semi-supervised Neural Topic Modeling. (arXiv:2204.03208v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03208">
<div class="article-summary-box-inner">
<span><p>Topic models are some of the most popular ways to represent textual data in
an interpret-able manner. Recently, advances in deep generative models,
specifically auto-encoding variational Bayes (AEVB), have led to the
introduction of unsupervised neural topic models, which leverage deep
generative models as opposed to traditional statistics-based topic models. We
extend upon these neural topic models by introducing the Label-Indexed Neural
Topic Model (LI-NTM), which is, to the extent of our knowledge, the first
effective upstream semi-supervised neural topic model. We find that LI-NTM
outperforms existing neural topic models in document reconstruction benchmarks,
with the most notable results in low labeled data regimes and for data-sets
with informative labels; furthermore, our jointly learned classifier
outperforms baseline classifiers in ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Attention through Gradient-Based Learned Runtime Pruning. (arXiv:2204.03227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03227">
<div class="article-summary-box-inner">
<span><p>Self-attention is a key enabler of state-of-art accuracy for various
transformer-based Natural Language Processing models. This attention mechanism
calculates a correlation score for each word with respect to the other words in
a sentence. Commonly, only a small subset of words highly correlates with the
word under attention, which is only determined at runtime. As such, a
significant amount of computation is inconsequential due to low attention
scores and can potentially be pruned. The main challenge is finding the
threshold for the scores below which subsequent computation will be
inconsequential. Although such a threshold is discrete, this paper formulates
its search through a soft differentiable regularizer integrated into the loss
function of the training. This formulation piggy backs on the back-propagation
training to analytically co-optimize the threshold and the weights
simultaneously, striking a formally optimal balance between accuracy and
computation pruning. To best utilize this mathematical innovation, we devise a
bit-serial architecture, dubbed LeOPArd, for transformer language models with
bit-level early termination microarchitectural mechanism. We evaluate our
design across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision
transformer models. Post-layout results show that, on average, LeOPArd yields
1.9x and 3.9x speedup and energy reduction, respectively, while keeping the
average accuracy virtually intact (&lt;0.2% degradation)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators. (arXiv:2204.03243v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03243">
<div class="article-summary-box-inner">
<span><p>We present a new framework AMOS that pretrains text encoders with an
Adversarial learning curriculum via a Mixture Of Signals from multiple
auxiliary generators. Following ELECTRA-style pretraining, the main encoder is
trained as a discriminator to detect replaced tokens generated by auxiliary
masked language models (MLMs). Different from ELECTRA which trains one MLM as
the generator, we jointly train multiple MLMs of different sizes to provide
training signals at various levels of difficulty. To push the discriminator to
learn better with challenging replaced tokens, we learn mixture weights over
the auxiliary MLMs' outputs to maximize the discriminator loss by
backpropagating the gradient from the discriminator via Gumbel-Softmax. For
better pretraining efficiency, we propose a way to assemble multiple MLMs into
one unified auxiliary model. AMOS outperforms ELECTRA and recent
state-of-the-art pretrained models by about 1 point on the GLUE benchmark for
BERT base-sized models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings. (arXiv:2204.03251v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03251">
<div class="article-summary-box-inner">
<span><p>Language resources such as wordnets remain indispensable tools for different
natural language tasks and applications. However, for low-resource languages
such as Filipino, existing wordnets are old and outdated, and producing new
ones may be slow and costly in terms of time and resources. In this paper, we
propose an automatic method for constructing a wordnet from scratch using only
an unlabeled corpus and a sentence embeddings-based language model. Using this,
we produce FilWordNet, a new wordnet that supplants and improves the outdated
Filipino WordNet. We evaluate our automatically-induced senses and synsets by
matching them with senses from the Princeton WordNet, as well as comparing the
synsets to the old Filipino WordNet. We empirically show that our method can
induce existing, as well as potentially new, senses and synsets automatically
without the need for human supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arabic Text-To-Speech (TTS) Data Preparation. (arXiv:2204.03255v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03255">
<div class="article-summary-box-inner">
<span><p>People may be puzzled by the fact that voice over recordings data sets exist
in addition to Text-to-Speech (TTS), Synthesis system advancements, albeit this
is not the case. The goal of this study is to explain the relevance of TTS as
well as the data preparation procedures. TTS relies heavily on recorded data
since it can have a substantial influence on the outcomes of TTS modules.
Furthermore, whether the domain is specialized or general, appropriate data
should be developed to address all predicted language variants and domains.
Different recording methodologies, taking into account quality and behavior,
may also be advantageous in the development of the module. In light of the lack
of Arabic language in present synthesizing systems, numerous variables that
impact the flow of recorded utterances are being considered in order to
manipulate an Arabic TTS module. In this study, two viewpoints will be
discussed: linguistics and the creation of high-quality recordings for TTS. The
purpose of this work is to offer light on how ground-truth utterances may
influence the evolution of speech systems in terms of naturalness,
intelligibility, and understanding. Well provide voice actor specs as well as
data specs that will assist both voice actors and voice coaches in the studio
as well as the annotators who will be evaluating the audios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Korean Online Hate Speech Dataset for Multilabel Classification: How Can Social Science Aid Developing Better Hate Speech Dataset?. (arXiv:2204.03262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03262">
<div class="article-summary-box-inner">
<span><p>We suggest a multilabel Korean online hate speech dataset that covers seven
categories of hate speech: (1) Race and Nationality, (2) Religion, (3)
Regionalism, (4) Ageism, (5) Misogyny, (6) Sexual Minorities, and (7) Male. Our
35K dataset consists of 24K online comments with Krippendorff's Alpha label
accordance of .713, 2.2K neutral sentences from Wikipedia, 1.7K additionally
labeled sentences generated by the Human-in-the-Loop procedure and
rule-generated 7.1K neutral sentences. The base model with 24K initial dataset
achieved the accuracy of LRAP .892, but improved to .919 after being combined
with 11K additional data. Unlike the conventional binary hate and non-hate
dichotomy approach, we designed a dataset considering both the cultural and
linguistic context to overcome the limitations of western culture-based English
texts. Thus, this paper is not only limited to presenting a local hate speech
dataset but extends as a manual for building a more generalized hate speech
dataset with diverse cultural backgrounds based on social science perspectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03276">
<div class="article-summary-box-inner">
<span><p>Currently, pre-trained models can be considered the default choice for a wide
range of NLP tasks. Despite their SoTA results, there is practical evidence
that these models may require a different number of computing layers for
different input sequences, since evaluating all layers leads to overconfidence
on wrong predictions (namely overthinking). This problem can potentially be
solved by implementing adaptive computation time approaches, which were first
designed to improve inference speed. Recently proposed PonderNet may be a
promising solution for performing an early exit by treating the exit layers
index as a latent variable. However, the originally proposed exit criterion,
relying on sampling from trained posterior distribution on the probability of
exiting from i-th layer, introduces major variance in model outputs,
significantly reducing the resulting models performance. In this paper, we
propose Ponder ALBERT (PALBERT): an improvement to PonderNet with a novel
deterministic Q-exit criterion and a revisited model architecture. We compared
PALBERT with recent methods for performing an early exit. We observed that the
proposed changes can be considered significant improvements on the original
PonderNet architecture and outperform PABEE on a wide range of GLUE tasks. In
addition, we also performed an in-depth ablation study of the proposed
architecture to further understand Lambda layers and their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entailment Graph Learning with Textual Entailment and Soft Transitivity. (arXiv:2204.03286v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03286">
<div class="article-summary-box-inner">
<span><p>Typed entailment graphs try to learn the entailment relations between
predicates from text and model them as edges between predicate nodes. The
construction of entailment graphs usually suffers from severe sparsity and
unreliability of distributional similarity. We propose a two-stage method,
Entailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns
local entailment relations by recognizing possible textual entailment between
template sentences formed by typed CCG-parsed predicates. Based on the
generated local graph, EGT2 then uses three novel soft transitivity constraints
to consider the logical transitivity in entailment structures. Experiments on
benchmark datasets show that EGT2 can well model the transitivity in entailment
graph to alleviate the sparsity issue, and lead to significant improvement over
current state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-Based Extractive Summarisation for Scientific Articles. (arXiv:2204.03301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03301">
<div class="article-summary-box-inner">
<span><p>This paper presents the results of research on supervised extractive text
summarisation for scientific articles. We show that a simple sequential tagging
model based only on the text within a document achieves high results against a
simple classification model. Improvements can be achieved through additional
sentence-level features, though these were minimal. Through further analysis,
we show the potential of the sequential model relying on the structure of the
document depending on the academic discipline which the document is from.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-trained DNN-HMM-Based Acoustic-Phonetic Model. (arXiv:2204.03315v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03315">
<div class="article-summary-box-inner">
<span><p>In spoken language understanding (SLU), what the user says is converted to
his/her intent. Recent work on end-to-end SLU has shown that accuracy can be
improved via pre-training approaches. We revisit ideas presented by Lugosch et
al. using speech pre-training and three-module modeling; however, to ease
construction of the end-to-end SLU model, we use as our phoneme module an
open-source acoustic-phonetic model from a DNN-HMM hybrid automatic speech
recognition (ASR) system instead of training one from scratch. Hence we
fine-tune on speech only for the word module, and we apply multi-target
learning (MTL) on the word and intent modules to jointly optimize SLU
performance. MTL yields a relative reduction of 40% in intent-classification
error rates (from 1.0% to 0.6%). Note that our three-module model is a
streaming method. The final outcome of the proposed three-module modeling
approach yields an intent accuracy of 99.4% on FluentSpeech, an intent error
rate reduction of 50% compared to that of Lugosch et al. Although we focus on
real-time streaming methods, we also list non-streaming methods for comparison.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation. (arXiv:2204.03324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03324">
<div class="article-summary-box-inner">
<span><p>An ultimate goal of artificial intelligence is to build computer systems that
can understand human languages. Understanding commonsense knowledge about the
world expressed in text is one of the foundational and challenging problems to
create such intelligent systems. As a step towards this goal, we present in
this paper ALMEn, an Autoencoding Language Model based Ensemble learning method
for commonsense validation and explanation. By ensembling several advanced
pre-trained language models including RoBERTa, DeBERTa, and ELECTRA with
Siamese neural networks, our method can distinguish natural language statements
that are against commonsense (validation subtask) and correctly identify the
reason for making against commonsense (explanation selection subtask).
Experimental results on the benchmark dataset of SemEval-2020 Task 4 show that
our method outperforms state-of-the-art models, reaching 97.9% and 95.4%
accuracies on the validation and explanation selection subtasks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Abstractive Question Answering over Tables or Text. (arXiv:2204.03357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03357">
<div class="article-summary-box-inner">
<span><p>A long-term ambition of information seeking QA systems is to reason over
multi-modal contexts and generate natural answers to user queries. Today,
memory intensive pre-trained language models are adapted to downstream tasks
such as QA by fine-tuning the model on QA data in a specific modality like
unstructured text or structured tables. To avoid training such memory-hungry
models while utilizing a uniform architecture for each modality,
parameter-efficient adapters add and train small task-specific bottle-neck
layers between transformer layers. In this work, we study parameter-efficient
abstractive QA in encoder-decoder models over structured tabular data and
unstructured textual data using only 1.5% additional parameters for each
modality. We also ablate over adapter layers in both encoder and decoder
modules to study the efficiency-performance trade-off and demonstrate that
reducing additional trainable parameters down to 0.7%-1.0% leads to comparable
results. Our models out-perform current state-of-the-art models on tabular QA
datasets such as Tablesum and FeTaQA, and achieve comparable performance on a
textual QA dataset such as NarrativeQA using significantly less trainable
parameters than fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances. (arXiv:2204.03375v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03375">
<div class="article-summary-box-inner">
<span><p>Dialogue State Tracking (DST) is primarily evaluated using Joint Goal
Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue
state exactly matches the prediction. Generally in DST, the dialogue state or
belief state for a given turn contains all the intents shown by the user till
that turn. Due to this cumulative nature of the belief state, it is difficult
to get a correct prediction once a misprediction has occurred. Thus, although
being a useful metric, it can be harsh at times and underestimate the true
potential of a DST model. Moreover, an improvement in JGA can sometimes
decrease the performance of turn-level or non-cumulative belief state
prediction due to inconsistency in annotations. So, using JGA as the only
metric for model selection may not be ideal for all scenarios. In this work, we
discuss various evaluation metrics used for DST along with their shortcomings.
To address the existing issues, we propose a new evaluation metric named
Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike
JGA, it tries to give penalized rewards to mispredictions that are locally
correct i.e. the root cause of the error is an earlier turn. By doing so, FGA
considers the performance of both cumulative and turn-level prediction flexibly
and provides a better insight than the existing metrics. We also show that FGA
is a better discriminator of DST model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAESTRO: Matched Speech Text Representations through Modality Matching. (arXiv:2204.03409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03409">
<div class="article-summary-box-inner">
<span><p>We present Maestro, a self-supervised training method to unify
representations learnt from speech and text modalities. Self-supervised
learning from speech signals aims to learn the latent structure inherent in the
signal, while self-supervised learning from text attempts to capture lexical
information. Learning aligned representations from unpaired speech and text
sequences is a challenging task. Previous work either implicitly enforced the
representations learnt from these two modalities to be aligned in the latent
space through multitasking and parameter sharing or explicitly through
conversion of modalities via speech synthesis. While the former suffers from
interference between the two modalities, the latter introduces additional
complexity. In this paper, we propose Maestro, a novel algorithm to learn
unified representations from both these modalities simultaneously that can
transfer to diverse downstream tasks such as Automated Speech Recognition (ASR)
and Speech Translation (ST). Maestro learns unified representations through
sequence alignment, duration prediction and matching embeddings in the learned
space through an aligned masked-language model loss. We establish a new
state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 11% relative
reduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)
and 21 languages to English multilingual ST on CoVoST 2 with an improvement of
2.8 BLEU averaged over 21 languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0. (arXiv:2204.03417v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03417">
<div class="article-summary-box-inner">
<span><p>Stuttering is a varied speech disorder that harms an individual's
communication ability. Persons who stutter (PWS) often use speech therapy to
cope with their condition. Improving speech recognition systems for people with
such non-typical speech or tracking the effectiveness of speech therapy would
require systems that can detect dysfluencies while at the same time being able
to detect speech techniques acquired in therapy.
</p>
<p>This paper shows that fine-tuning wav2vec 2.0 for the classification of
stuttering on a sizeable English corpus containing stuttered speech, in
conjunction with multi-task learning, boosts the effectiveness of the
general-purpose wav2vec 2.0 features for detecting stuttering in speech; both
within and across languages. We evaluate our method on Fluencybank and the
German therapy-centric Kassel State of Fluency (KSoF) dataset by training
Support Vector Machine classifiers using features extracted from the fine-tuned
models for six different stuttering-related events types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. Using embeddings from the
fine-tuned models leads to relative classification performance gains up to 27\%
w.r.t. F1-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Vocal Fatigue with Neural Embeddings. (arXiv:2204.03428v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03428">
<div class="article-summary-box-inner">
<span><p>Vocal fatigue refers to the feeling of tiredness and weakness of voice due to
extended utilization. This paper investigates the effectiveness of neural
embeddings for the detection of vocal fatigue. We compare x-vectors,
ECAPA-TDNN, and wav2vec 2.0 embeddings on a corpus of academic spoken English.
Low-dimensional mappings of the data reveal that neural embeddings capture
information about the change in vocal characteristics of a speaker during
prolonged voice usage. We show that vocal fatigue can be reliably predicted
using all three kinds of neural embeddings after only 50 minutes of continuous
speaking when temporal smoothing and normalization are applied to the extracted
embeddings. We employ support vector machines for classification and achieve
accuracy scores of 81% using x-vectors, 85% using ECAPA-TDNN embeddings, and
82% using wav2vec 2.0 embeddings as input features. We obtain an accuracy score
of 76%, when the trained system is applied to a different speaker and recording
environment without any adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTuit: Understanding Spanish language in Twitter through a native transformer. (arXiv:2204.03465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03465">
<div class="article-summary-box-inner">
<span><p>The appearance of complex attention-based language models such as BERT,
Roberta or GPT-3 has allowed to address highly complex tasks in a plethora of
scenarios. However, when applied to specific domains, these models encounter
considerable difficulties. This is the case of Social Networks such as Twitter,
an ever-changing stream of information written with informal and complex
language, where each message requires careful evaluation to be understood even
by humans given the important role that context plays. Addressing tasks in this
domain through Natural Language Processing involves severe challenges. When
powerful state-of-the-art multilingual language models are applied to this
scenario, language specific nuances use to get lost in translation. To face
these challenges we present \textbf{BERTuit}, the larger transformer proposed
so far for Spanish language, pre-trained on a massive dataset of 230M Spanish
tweets using RoBERTa optimization. Our motivation is to provide a powerful
resource to better understand Spanish Twitter and to be used on applications
focused on this social network, with special emphasis on solutions devoted to
tackle the spreading of misinformation in this platform. BERTuit is evaluated
on several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very
competitive multilingual transformers. The utility of our approach is shown
with applications, in this case: a zero-shot methodology to visualize groups of
hoaxes and profiling authors spreading disinformation.
</p>
<p>Misinformation spreads wildly on platforms such as Twitter in languages other
than English, meaning performance of transformers may suffer when transferred
outside English speaking communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delta Keyword Transformer: Bringing Transformers to the Edge through Dynamically Pruned Multi-Head Self-Attention. (arXiv:2204.03479v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03479">
<div class="article-summary-box-inner">
<span><p>Multi-head self-attention forms the core of Transformer networks. However,
their quadratically growing complexity with respect to the input sequence
length impedes their deployment on resource-constrained edge devices. We
address this challenge by proposing a dynamic pruning method, which exploits
the temporal stability of data across tokens to reduce inference cost. The
threshold-based method only retains significant differences between the
subsequent tokens, effectively reducing the number of multiply-accumulates, as
well as the internal tensor data sizes. The approach is evaluated on the Google
Speech Commands Dataset for keyword spotting, and the performance is compared
against the baseline Keyword Transformer. Our experiments show that we can
reduce ~80% of operations while maintaining the original 98.4% accuracy.
Moreover, a reduction of ~87-94% operations can be achieved when only degrading
the accuracy by 1-4%, speeding up the multi-head self-attention inference by a
factor of ~7.5-16.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position-based Prompting for Health Outcome Generation. (arXiv:2204.03489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03489">
<div class="article-summary-box-inner">
<span><p>Probing Pre-trained Language Models (PLMs) using prompts has indirectly
implied that language models (LMs) can be treated as knowledge bases. To this
end, this phenomena has been effective especially when these LMs are fine-tuned
towards not just data of a specific domain, but also to the style or linguistic
pattern of the prompts themselves. We observe that, satisfying a particular
linguistic pattern in prompts is an unsustainable constraint that unnecessarily
lengthens the probing task, especially because, they are often manually
designed and the range of possible prompt template patterns can vary depending
on the prompting objective and domain. We therefore explore an idea of using a
position-attention mechanism to capture positional information of each word in
a prompt relative to the mask to be filled, hence avoiding the need to
re-construct prompts when the prompts linguistic pattern changes. Using our
approach, we demonstrate the ability of eliciting answers to rare prompt
templates (in a case study on health outcome generation) such as Postfix and
Mixed patterns whose missing information is respectively at the start and in
multiple random places of the prompt. More so, using various biomedical PLMs,
our approach consistently outperforms a baseline in which the default mask
language model (MLM) representation is used to predict masked tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Understanding based Multi-Document Machine Reading Comprehension. (arXiv:2204.03494v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03494">
<div class="article-summary-box-inner">
<span><p>Most existing multi-document machine reading comprehension models mainly
focus on understanding the interactions between the input question and
documents, but ignore following two kinds of understandings. First, to
understand the semantic meaning of words in the input question and documents
from the perspective of each other. Second, to understand the supporting cues
for a correct answer from the perspective of intra-document and
inter-documents. Ignoring these two kinds of important understandings would
make the models oversee some important information that may be helpful for
inding correct answers. To overcome this deiciency, we propose a deep
understanding based model for multi-document machine reading comprehension. It
has three cascaded deep understanding modules which are designed to understand
the accurate semantic meaning of words, the interactions between the input
question and documents, and the supporting cues for the correct answer. We
evaluate our model on two large scale benchmark datasets, namely TriviaQA Web
and DuReader. Extensive experiments show that our model achieves
state-of-the-art results on both datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers. (arXiv:2204.03503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03503">
<div class="article-summary-box-inner">
<span><p>Automated short answer grading (ASAG) has gained attention in education as a
means to scale educational tasks to the growing number of students. Recent
progress in Natural Language Processing and Machine Learning has largely
influenced the field of ASAG, of which we survey the recent research
advancements. We complement previous surveys by providing a comprehensive
analysis of recently published methods that deploy deep learning approaches. In
particular, we focus our analysis on the transition from hand engineered
features to representation learning approaches, which learn representative
features for the task at hand automatically from large corpora of data. We
structure our analysis of deep learning methods along three categories: word
embeddings, sequential models, and attention-based methods. Deep learning
impacted ASAG differently than other fields of NLP, as we noticed that the
learned representations alone do not contribute to achieve the best results,
but they rather show to work in a complementary way with hand-engineered
features. The best performance are indeed achieved by methods that combine the
carefully hand-engineered features with the power of the semantic descriptions
provided by the latest models, like transformers architectures. We identify
challenges and provide an outlook on research direction that can be addressed
in the future
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Rankings into Quantized Scores in Peer Review. (arXiv:2204.03505v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03505">
<div class="article-summary-box-inner">
<span><p>In peer review, reviewers are usually asked to provide scores for the papers.
The scores are then used by Area Chairs or Program Chairs in various ways in
the decision-making process. The scores are usually elicited in a quantized
form to accommodate the limited cognitive ability of humans to describe their
opinions in numerical values. It has been found that the quantized scores
suffer from a large number of ties, thereby leading to a significant loss of
information. To mitigate this issue, conferences have started to ask reviewers
to additionally provide a ranking of the papers they have reviewed. There are
however two key challenges. First, there is no standard procedure for using
this ranking information and Area Chairs may use it in different ways
(including simply ignoring them), thereby leading to arbitrariness in the
peer-review process. Second, there are no suitable interfaces for judicious use
of this data nor methods to incorporate it in existing workflows, thereby
leading to inefficiencies. We take a principled approach to integrate the
ranking information into the scores. The output of our method is an updated
score pertaining to each review that also incorporates the rankings. Our
approach addresses the two aforementioned challenges by: (i) ensuring that
rankings are incorporated into the updates scores in the same manner for all
papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use
existing interfaces and workflows designed for scores. We empirically evaluate
our method on synthetic datasets as well as on peer reviews from the ICLR 2017
conference, and find that it reduces the error by approximately 30% as compared
to the best performing baseline on the ICLR 2017 data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QCRI's COVID-19 Disinformation Detector: A System to Fight the COVID-19 Infodemic in Social Media. (arXiv:2204.03506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03506">
<div class="article-summary-box-inner">
<span><p>Fighting the ongoing COVID-19 infodemic has been declared as one of the most
important focus areas by the World Health Organization since the onset of the
COVID-19 pandemic. While the information that is consumed and disseminated
consists of promoting fake cures, rumors, and conspiracy theories to spreading
xenophobia and panic, at the same time there is information (e.g., containing
advice, promoting cure) that can help different stakeholders such as
policy-makers. Social media platforms enable the infodemic and there has been
an effort to curate the content on such platforms, analyze and debunk them.
While a majority of the research efforts consider one or two aspects (e.g.,
detecting factuality) of such information, in this study we focus on a
multifaceted approach, including an
API,\url{https://app.swaggerhub.com/apis/yifan2019/Tanbih/0.8.0/} and a demo
system,\url{https://covid19.tanbih.org}, which we made freely and publicly
available. We believe that this will facilitate researchers and different
stakeholders. A screencast of the API services and demo is
available.\url{https://youtu.be/zhbcSvxEKMk}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods. (arXiv:2204.03508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03508">
<div class="article-summary-box-inner">
<span><p>Multi-task learning (MTL) has become increasingly popular in natural language
processing (NLP) because it improves the performance of related tasks by
exploiting their commonalities and differences. Nevertheless, it is still not
understood very well how multi-task learning can be implemented based on the
relatedness of training tasks. In this survey, we review recent advances of
multi-task learning methods in NLP, with the aim of summarizing them into two
general multi-task training methods based on their task relatedness: (i) joint
training and (ii) multi-step training. We present examples in various NLP
downstream applications, summarize the task relationships and discuss future
directions of this promising topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging pre-trained language models for conversational information seeking from text. (arXiv:2204.03542v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03542">
<div class="article-summary-box-inner">
<span><p>Recent advances in Natural Language Processing, and in particular on the
construction of very large pre-trained language representation models, is
opening up new perspectives on the construction of conversational information
seeking (CIS) systems. In this paper we investigate the usage of in-context
learning and pre-trained language representation models to address the problem
of information extraction from process description documents, in an incremental
question and answering oriented fashion. In particular we investigate the usage
of the native GPT-3 (Generative Pre-trained Transformer 3) model, together with
two in-context learning customizations that inject conceptual definitions and a
limited number of samples in a few shot-learning fashion. The results highlight
the potential of the approach and the usefulness of the in-context learning
customizations, which can substantially contribute to address the "training
data challenge" of deep learning based NLP techniques the BPM field. It also
highlight the challenge posed by control flow relations for which further
training needs to be devised.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic. (arXiv:2204.03558v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03558">
<div class="article-summary-box-inner">
<span><p>As natural language processing systems become more widespread, it is
necessary to address fairness issues in their implementation and deployment to
ensure that their negative impacts on society are understood and minimized.
However, there is limited work that studies fairness using a multilingual and
intersectional framework or on downstream tasks. In this paper, we introduce
four multilingual Equity Evaluation Corpora, supplementary test sets designed
to measure social biases, and a novel statistical framework for studying
unisectional and intersectional social biases in natural language processing.
We use these tools to measure gender, racial, ethnic, and intersectional social
biases across five models trained on emotion regression tasks in English,
Spanish, and Arabic. We find that many systems demonstrate statistically
significant unisectional and intersectional social biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotional Speech Recognition with Pre-trained Deep Visual Models. (arXiv:2204.03561v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03561">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new methodology for emotional speech recognition
using visual deep neural network models. We employ the transfer learning
capabilities of the pre-trained computer vision deep models to have a mandate
for the emotion recognition in speech task. In order to achieve that, we
propose to use a composite set of acoustic features and a procedure to convert
them into images. Besides, we present a training paradigm for these models
taking into consideration the different characteristics between acoustic-based
images and regular ones. In our experiments, we use the pre-trained VGG-16
model and test the overall methodology on the Berlin EMO-DB dataset for
speaker-independent emotion recognition. We evaluate the proposed model on the
full list of the seven emotions and the results set a new state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03574">
<div class="article-summary-box-inner">
<span><p>We introduce compositional soft prompting (CSP), a parameter-efficient
learning technique to improve the zero-shot compositionality of large-scale
pretrained vision-language models (VLMs) without the overhead of fine-tuning
the entire model. VLMs can represent arbitrary classes as natural language
prompts in their flexible text encoders but they underperform state-of-the-art
methods on compositional zero-shot benchmark tasks. To improve VLMs, we propose
a novel form of soft prompting. We treat the attributes and objects that are
composed to define classes as learnable tokens of vocabulary and tune them on
multiple prompt compositions. During inference, we recompose the learned
attribute-object vocabulary in new combinations and show that CSP outperforms
the original VLM on benchmark datasets by an average of 14.7 percentage points
of accuracy. CSP also achieves new state-of-the-art accuracies on two out of
three benchmark datasets, while only fine-tuning a small number of parameters.
Further, we show that CSP improves generalization to higher-order
attribute-attribute-object compositions and combinations of pretrained
attributes and fine-tuned objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing the limits of natural language models for predicting human language judgments. (arXiv:2204.03592v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03592">
<div class="article-summary-box-inner">
<span><p>Neural network language models can serve as computational hypotheses about
how humans process language. We compared the model-human consistency of diverse
language models using a novel experimental approach: controversial sentence
pairs. For each controversial sentence pair, two language models disagree about
which sentence is more likely to occur in natural text. Considering nine
language models (including n-gram, recurrent neural networks, and transformer
models), we created hundreds of such controversial sentence pairs by either
selecting sentences from a corpus or synthetically optimizing sentence pairs to
be highly controversial. Human subjects then provided judgments indicating for
each pair which of the two sentences is more likely. Controversial sentence
pairs proved highly effective at revealing model failures and identifying
models that aligned most closely with human judgments. The most
human-consistent model tested was GPT-2, although experiments also revealed
significant shortcomings of its alignment with human perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Label Correlations for Second-Order Semantic Dependency Parsing with Mean-Field Inference. (arXiv:2204.03619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03619">
<div class="article-summary-box-inner">
<span><p>Second-order semantic parsing with end-to-end mean-field inference has been
shown good performance. In this work we aim to improve this method by modeling
label correlations between adjacent arcs. However, direct modeling leads to
memory explosion because second-order score tensors have sizes of $O(n^3L^2)$
($n$ is the sentence length and $L$ is the number of labels), which is not
affordable. To tackle this computational challenge, we leverage tensor
decomposition techniques, and interestingly, we show that the large
second-order score tensors have no need to be materialized during mean-field
inference, thereby reducing the computational complexity from cubic to
quadratic. We conduct experiments on SemEval 2015 Task 18 English datasets,
showing the effectiveness of modeling label correlations. Our code is publicly
available at https://github.com/sustcsonglin/mean-field-dep-parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">tmVar 3.0: an improved variant concept recognition and normalization tool. (arXiv:2204.03637v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03637">
<div class="article-summary-box-inner">
<span><p>Previous studies have shown that automated text-mining tools are becoming
increasingly important for successfully unlocking variant information in
scientific literature at large scale. Despite multiple attempts in the past,
existing tools are still of limited recognition scope and precision. We propose
tmVar 3.0: an improved variant recognition and normalization tool. Compared to
its predecessors, tmVar 3.0 is able to recognize a wide spectrum of variant
related entities (e.g., allele and copy number variants), and to group
different variant mentions belonging to the same concept in an article for
improved accuracy. Moreover, tmVar3 provides additional variant normalization
options such as allele-specific identifiers from the ClinGen Allele Registry.
tmVar3 exhibits a state-of-the-art performance with over 90% accuracy in
F-measure in variant recognition and normalization, when evaluated on three
independent benchmarking datasets. tmVar3 is freely available for download. We
have also processed the entire PubMed and PMC with tmVar3 and released its
annotations on our FTP. Availability: <a href="ftp://ftp.ncbi.nlm.nih.gov/pub/lu/tmVar3">this ftp URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Entity Linking: A Survey of Models Based on Deep Learning. (arXiv:2006.00575v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.00575">
<div class="article-summary-box-inner">
<span><p>This survey presents a comprehensive description of recent neural entity
linking (EL) systems developed since 2015 as a result of the "deep learning
revolution" in natural language processing. Its goal is to systemize design
features of neural entity linking systems and compare their performance to the
remarkable classic methods on common benchmarks. This work distills a generic
architecture of a neural EL system and discusses its components, such as
candidate generation, mention-context encoding, and entity ranking, summarizing
prominent methods for each of them. The vast variety of modifications of this
general architecture are grouped by several common themes: joint entity mention
detection and disambiguation, models for global linking, domain-independent
techniques including zero-shot and distant supervision methods, and
cross-lingual approaches. Since many neural models take advantage of entity and
mention/context embeddings to represent their meaning, this work also overviews
prominent entity embedding techniques. Finally, the survey touches on
applications of entity linking, focusing on the recently emerged use-case of
enhancing deep pre-trained masked language models based on the Transformer
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Improving Selective Prediction Ability of NLP Systems. (arXiv:2008.09371v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09371">
<div class="article-summary-box-inner">
<span><p>It's better to say "I can't answer" than to answer incorrectly. This
selective prediction ability is crucial for NLP systems to be reliably deployed
in real-world applications. Prior work has shown that existing selective
prediction techniques fail to perform well, especially in the out-of-domain
setting. In this work, we propose a method that improves probability estimates
of models by calibrating them using prediction confidence and difficulty score
of instances. Using these two signals, we first annotate held-out instances and
then train a calibrator to predict the likelihood of correctness of the model's
prediction. We instantiate our method with Natural Language Inference (NLI) and
Duplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and
Out-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the
representations learned by our calibrator result in an improvement of (15.81%,
5.64%) and (6.19%, 13.9%) over 'MaxProb' -- a selective prediction baseline --
on NLI and DD tasks respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributed NLI: Learning to Predict Human Opinion Distributions for Language Reasoning. (arXiv:2104.08676v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08676">
<div class="article-summary-box-inner">
<span><p>We introduce distributed NLI, a new NLU task with a goal to predict the
distribution of human judgements for natural language inference. We show that
by applying additional distribution estimation methods, namely, Monte Carlo
(MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation,
models can capture human judgement distribution more effectively than the
softmax baseline. We show that MC Dropout is able to achieve decent performance
without any distribution annotations while Re-Calibration can give further
improvements with extra distribution annotations, suggesting the value of
multiple annotations for one example in modeling the distribution of human
judgements. Despite these improvements, the best results are still far below
the estimated human upper-bound, indicating that predicting the distribution of
human judgements is still an open, challenging problem with a large room for
improvements. We showcase the common errors for MC Dropout and Re-Calibration.
Finally, we give guidelines on the usage of these methods with different levels
of data availability and encourage future work on modeling the human opinion
distribution for language reasoning. Our code and data are publicly available
at https://github.com/easonnie/ChaosNLI
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lambek pregroups are Frobenius spiders in preorders. (arXiv:2105.03038v4 [math.CT] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03038">
<div class="article-summary-box-inner">
<span><p>"Spider" is a nickname of special Frobenius algebras, a fundamental structure
from mathematics, physics, and computer science. Pregroups are a fundamental
structure from linguistics. Pregroups and spiders have been used together in
natural language processing: one for syntax, the other for semantics. It turns
out that pregroups themselves can be characterized as pointed spiders in the
category of preordered relations, where they naturally arise from grammars. The
other way around, preordered spider algebras in general can be characterized as
unions of pregroups. This extends the characterization of relational spider
algebras as disjoint unions of groups. The compositional framework that emerged
with the results suggests new ways to understand and apply the basis structures
in machine learning and data analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection. (arXiv:2106.04564v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04564">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformer-based models were reported to be robust in intent
classification. In this work, we first point out the importance of in-domain
out-of-scope detection in few-shot intent recognition tasks and then illustrate
the vulnerability of pre-trained Transformer-based models against samples that
are in-domain but out-of-scope (ID-OOS). We construct two new datasets, and
empirically show that pre-trained models do not perform well on both ID-OOS
examples and general out-of-scope examples, especially on fine-grained few-shot
intent detection tasks. To figure out how the models mistakenly classify ID-OOS
intents as in-scope intents, we further conduct analysis on confidence scores
and the overlapping keywords, as well as point out several prospective
directions for future work. Resources are available on
https://github.com/jianguoz/Few-Shot-Intent-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Controlled Generation with Encoder-Decoder Transformers. (arXiv:2106.06411v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06411">
<div class="article-summary-box-inner">
<span><p>Controlling neural network-based models for natural language generation (NLG)
has broad applications in numerous areas such as machine translation, document
summarization, and dialog systems. Approaches that enable such control in a
zero-shot manner would be of great importance as, among other reasons, they
remove the need for additional annotated data and training. In this work, we
propose novel approaches for controlling encoder-decoder transformer-based NLG
models in zero-shot. This is done by introducing three control knobs, namely,
attention biasing, decoder mixing, and context augmentation, that are applied
to these models at generation time. These knobs control the generation process
by directly manipulating trained NLG models (e.g., biasing cross-attention
layers) to realize the desired attributes in the generated outputs. We show
that not only are these NLG models robust to such manipulations, but also their
behavior could be controlled without an impact on their generation performance.
These results, to the best of our knowledge, are the first of their kind.
Through these control knobs, we also investigate the role of transformer
decoder's self-attention module and show strong evidence that its primary role
is maintaining fluency of sentences generated by these models. Based on this
hypothesis, we show that alternative architectures for transformer decoders
could be viable options. We also study how this hypothesis could lead to more
efficient ways for training encoder-decoder transformer models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KazNERD: Kazakh Named Entity Recognition Dataset. (arXiv:2111.13419v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13419">
<div class="article-summary-box-inner">
<span><p>We present the development of a dataset for Kazakh named entity recognition.
The dataset was built as there is a clear need for publicly available annotated
corpora in Kazakh, as well as annotation guidelines containing
straightforward--but rigorous--rules and examples. The dataset annotation,
based on the IOB2 scheme, was carried out on television news text by two native
Kazakh speakers under the supervision of the first author. The resulting
dataset contains 112,702 sentences and 136,333 annotations for 25 entity
classes. State-of-the-art machine learning models to automatise Kazakh named
entity recognition were also built, with the best-performing model achieving an
exact match F1-score of 97.22% on the test set. The annotated dataset,
guidelines, and codes used to train the models are freely available for
download under the CC BY 4.0 licence from https://github.com/IS2AI/KazNERD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Data-based Curricula Work?. (arXiv:2112.06510v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06510">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art NLP systems use large neural networks that require
lots of computational resources for training. Inspired by human knowledge
acquisition, researchers have proposed curriculum learning, - sequencing of
tasks (task-based curricula) or ordering and sampling of the datasets
(data-based curricula) that facilitate training. This work investigates the
benefits of data-based curriculum learning for large modern language models
such as BERT and T5. We experiment with various curricula based on a range of
complexity measures and different sampling strategies. Extensive experiments on
different NLP tasks show that curricula based on various complexity measures
rarely has any benefits while random sampling performs either as well or better
than curricula.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. (arXiv:2202.07305v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07305">
<div class="article-summary-box-inner">
<span><p>Image narrative generation is a task to create a story from an image with a
subjective viewpoint. Given the importance of the subjective feelings of
writers, readers, and characters in storytelling, an image narrative generation
method should consider human emotion. In this study, we propose a novel method
of image narrative generation called ViNTER (Visual Narrative Transformer with
Emotion arc Representation), which takes "emotion arc" as input to capture a
sequence of emotional changes. Since emotion arcs represent the trajectory of
emotional change, it is expected that we can include detailed information about
the emotional changes in the story to the model. We present experimental
results of both automatic and manual evaluations on the Image Narrative dataset
and demonstrate the effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing. (arXiv:2202.08712v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08712">
<div class="article-summary-box-inner">
<span><p>To date, there are no effective treatments for most neurodegenerative
diseases. Knowledge graphs can provide comprehensive and semantic
representation for heterogeneous data, and have been successfully leveraged in
many biomedical applications including drug repurposing. Our objective is to
construct a knowledge graph from literature to study relations between
Alzheimer's disease (AD) and chemicals, drugs and dietary supplements in order
to identify opportunities to prevent or delay neurodegenerative progression. We
collected biomedical annotations and extracted their relations using SemRep via
SemMedDB. We used both a BERT-based classifier and rule-based methods during
data preprocessing to exclude noise while preserving most AD-related semantic
triples. The 1,672,110 filtered triples were used to train with knowledge graph
completion algorithms (i.e., TransE, DistMult, and ComplEx) to predict
candidates that might be helpful for AD treatment or prevention. Among three
knowledge graph completion models, TransE outperformed the other two (MR =
13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further
evaluate the prediction results. We found supporting evidence for most highly
ranked candidates predicted by our model which indicates that our approach can
inform reliable new knowledge. This paper shows that our graph mining model can
predict reliable new relationships between AD and other entities (i.e., dietary
supplements, chemicals, and drugs). The knowledge graph constructed can
facilitate data-driven knowledge discoveries and the generation of novel
hypotheses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13366">
<div class="article-summary-box-inner">
<span><p>For a long period, different recommendation tasks typically require designing
task-specific architectures and training objectives. As a result, it is hard to
transfer the learned knowledge and representations from one task to another,
thus restricting the generalization ability of existing recommendation
approaches, e.g., a sequential recommendation model can hardly be applied or
transferred to a review generation method. To deal with such issues,
considering that language grounding is a powerful medium to describe and
represent various problems or tasks, we present a flexible and unified
text-to-text paradigm called "Pretrain, Personalized Prompt, and Predict
Paradigm" (P5) for recommendation, which unifies various recommendation tasks
in a shared framework. In P5, all data such as user-item interactions, item
metadata, and user reviews are converted to a common format -- natural language
sequences. The rich information from natural language assist P5 to capture
deeper semantics for recommendation. P5 learns different tasks with the same
language modeling objective during pretraining. Thus, it possesses the
potential to serve as the foundation model for downstream recommendation tasks,
allows easy integration with other modalities, and enables instruction-based
recommendation, which will revolutionize the technical form of recommender
system towards universal recommendation engine. With adaptive personalized
prompt for different users, P5 is able to make predictions in a zero-shot or
few-shot manner and largely reduces the necessity for extensive fine-tuning. On
several recommendation benchmarks, we conduct experiments to show the
effectiveness of our generative approach. We will release our prompts and
pretrained P5 language model to help advance future research on Recommendation
as Language Processing (RLP) and Personalized Foundation Models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues. (arXiv:2203.13926v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13926">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of dialogue reasoning with contextualized
commonsense inference. We curate CICERO, a dataset of dyadic conversations with
five types of utterance-level reasoning-based inferences: cause, subsequent
event, prerequisite, motivation, and emotional reaction. The dataset contains
53,105 of such inferences from 5,672 dialogues. We use this dataset to solve
relevant generative and discriminative tasks: generation of cause and
subsequent event; generation of prerequisite, motivation, and listener's
emotional reaction; and selection of plausible alternatives. Our results
ascertain the value of such dialogue-centric commonsense knowledge datasets. It
is our hope that CICERO will open new research avenues into commonsense-based
dialogue reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15937">
<div class="article-summary-box-inner">
<span><p>Current leading mispronunciation detection and diagnosis (MDD) systems
achieve promising performance via end-to-end phoneme recognition. One challenge
of such end-to-end solutions is the scarcity of human-annotated phonemes on
natural L2 speech. In this work, we leverage unlabeled L2 speech via a
pseudo-labeling (PL) procedure and extend the fine-tuning approach based on
pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec
2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples
plus the created pseudo-labeled L2 speech samples. Our pseudo labels are
dynamic and are produced by an ensemble of the online model on-the-fly, which
ensures that our model is robust to pseudo label noise. We show that
fine-tuning with pseudo labels gains a 5.35% phoneme error rate reduction and
2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning
baseline. The proposed PL method is also shown to outperform conventional
offline PL methods. Compared to the state-of-the-art MDD systems, our MDD
solution achieves a more accurate and consistent phonetic error diagnosis. In
addition, we conduct an open test on a separate UTD-4Accents dataset, where our
system recognition outputs show a strong correlation with human perception,
based on accentedness and intelligibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model. (arXiv:2203.17090v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17090">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce PanGu-Bot, a Chinese pre-trained open-domain
dialogue generation model based on a large pre-trained language model (PLM)
PANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue
models trained over a massive amount of dialogue data from scratch, we aim to
build a powerful dialogue model with relatively fewer data and computation
costs by inheriting valuable language capabilities and knowledge from PLMs. To
this end, we train PanGu-Bot from the large PLM PANGU-alpha, which has been
proven well-performed on a variety of Chinese natural language tasks. We
investigate different aspects of responses generated by PanGu-Bot, including
response quality, knowledge, and safety. We show that PanGu-Bot outperforms
state-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA
(Zhou et al., 2021)) w.r.t. the above three aspects. We also demonstrate that
PanGu-Bot can be easily deployed to generate emotional responses without
further training. Throughout our empirical analysis, we also point out that the
PanGu-Bot response quality, knowledge correctness, and safety are still far
from perfect, and further explorations are indispensable to building reliable
and smart dialogue systems. Our model and code will be available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot
soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaLM: Scaling Language Modeling with Pathways. (arXiv:2204.02311v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02311">
<div class="article-summary-box-inner">
<span><p>Large language models have been shown to achieve remarkable performance
across a variety of natural language tasks using few-shot learning, which
drastically reduces the number of task-specific training examples needed to
adapt the model to a particular application. To further our understanding of
the impact of scale on few-shot learning, we trained a 540-billion parameter,
densely activated, Transformer language model, which we call Pathways Language
Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML
system which enables highly efficient training across multiple TPU Pods. We
demonstrate continued benefits of scaling by achieving state-of-the-art
few-shot learning results on hundreds of language understanding and generation
benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough
performance, outperforming the finetuned state-of-the-art on a suite of
multi-step reasoning tasks, and outperforming average human performance on the
recently released BIG-bench benchmark. A significant number of BIG-bench tasks
showed discontinuous improvements from model scale, meaning that performance
steeply increased as we scaled to our largest model. PaLM also has strong
capabilities in multilingual tasks and source code generation, which we
demonstrate on a wide array of benchmarks. We additionally provide a
comprehensive analysis on bias and toxicity, and study the extent of training
data memorization with respect to model scale. Finally, we discuss the ethical
considerations related to large language models and discuss potential
mitigation strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Effective Unsupervised Speech Synthesis. (arXiv:2204.02524v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02524">
<div class="article-summary-box-inner">
<span><p>We introduce the first unsupervised speech synthesis system based on a
simple, yet effective recipe. The framework leverages recent work in
unsupervised speech recognition as well as existing neural-based speech
synthesis. Using only unlabeled speech audio and unlabeled text as well as a
lexicon, our method enables speech synthesis without the need for a
human-labeled corpus. Experiments demonstrate the unsupervised system can
synthesize speech similar to a supervised counterpart in terms of naturalness
and intelligibility measured by human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM. (arXiv:2204.02821v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02821">
<div class="article-summary-box-inner">
<span><p>This paper describes our system for SemEval-2022 Task 2 Multilingual
Idiomaticity Detection and Sentence Embedding sub-task B. We modify a standard
BERT sentence transformer by adding embeddings for each idioms, which are
created using BERTRAM and a small number of contexts. We show that this
technique increases the quality of idiom representations and leads to better
performance on the task. We also perform analysis on our final results and show
that the quality of the produced idiom embeddings is highly sensitive to the
quality of the input contexts.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Follow My Eye: Using Gaze to Supervise Computer-Aided Diagnosis. (arXiv:2204.02976v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02976">
<div class="article-summary-box-inner">
<span><p>When deep neural network (DNN) was first introduced to the medical image
analysis community, researchers were impressed by its performance. However, it
is evident now that a large number of manually labeled data is often a must to
train a properly functioning DNN. This demand for supervision data and labels
is a major bottleneck in current medical image analysis, since collecting a
large number of annotations from experienced experts can be time-consuming and
expensive. In this paper, we demonstrate that the eye movement of radiologists
reading medical images can be a new form of supervision to train the DNN-based
computer-aided diagnosis (CAD) system. Particularly, we record the tracks of
the radiologists' gaze when they are reading images. The gaze information is
processed and then used to supervise the DNN's attention via an Attention
Consistency module. To the best of our knowledge, the above pipeline is among
the earliest efforts to leverage expert eye movement for deep-learning-based
CAD. We have conducted extensive experiments on knee X-ray images for
osteoarthritis assessment. The results show that our method can achieve
considerable improvement in diagnosis performance, with the help of gaze
supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Scale Memory-Based Video Deblurring. (arXiv:2204.02977v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02977">
<div class="article-summary-box-inner">
<span><p>Video deblurring has achieved remarkable progress thanks to the success of
deep neural networks. Most methods solve for the deblurring end-to-end with
limited information propagation from the video sequence. However, different
frame regions exhibit different characteristics and should be provided with
corresponding relevant information. To achieve fine-grained deblurring, we
designed a memory branch to memorize the blurry-sharp feature pairs in the
memory bank, thus providing useful information for the blurry query input. To
enrich the memory of our memory bank, we further designed a bidirectional
recurrency and multi-scale strategy based on the memory bank. Experimental
results demonstrate that our model outperforms other state-of-the-art methods
while keeping the model complexity and inference time low. The code is
available at https://github.com/jibo27/MemDeblur.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Different Losses for Deep Learning Image Colorization. (arXiv:2204.02980v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02980">
<div class="article-summary-box-inner">
<span><p>Image colorization aims to add color information to a grayscale image in a
realistic way. Recent methods mostly rely on deep learning strategies. While
learning to automatically colorize an image, one can define well-suited
objective functions related to the desired color output. Some of them are based
on a specific type of error between the predicted image and ground truth one,
while other losses rely on the comparison of perceptual properties. But, is the
choice of the objective function that crucial, i.e., does it play an important
role in the results? In this chapter, we aim to answer this question by
analyzing the impact of the loss function on the estimated colorization
results. To that goal, we review the different losses and evaluation metrics
that are used in the literature. We then train a baseline network with several
of the reviewed objective functions: classic L1 and L2 losses, as well as more
complex combinations such as Wasserstein GAN and VGG-based LPIPS loss.
Quantitative results show that the models trained with VGG-based LPIPS provide
overall slightly better results for most evaluation metrics. Qualitative
results exhibit more vivid colors when with Wasserstein GAN plus the L2 loss or
again with the VGG-based LPIPS. Finally, the convenience of quantitative user
studies is also discussed to overcome the difficulty of properly assessing on
colorized images, notably for the case of old archive photographs where no
ground truth is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientCellSeg: Efficient Volumetric Cell Segmentation Using Context Aware Pseudocoloring. (arXiv:2204.03014v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03014">
<div class="article-summary-box-inner">
<span><p>Volumetric cell segmentation in fluorescence microscopy images is important
to study a wide variety of cellular processes. Applications range from the
analysis of cancer cells to behavioral studies of cells in the embryonic stage.
Like in other computer vision fields, most recent methods use either large
convolutional neural networks (CNNs) or vision transformer models (ViTs). Since
the number of available 3D microscopy images is typically limited in
applications, we take a different approach and introduce a small CNN for
volumetric cell segmentation. Compared to previous CNN models for cell
segmentation, our model is efficient and has an asymmetric encoder-decoder
structure with very few parameters in the decoder. Training efficiency is
further improved via transfer learning. In addition, we introduce Context Aware
Pseudocoloring to exploit spatial context in z-direction of 3D images while
performing volumetric cell segmentation slice-wise. We evaluated our method
using different 3D datasets from the Cell Segmentation Benchmark of the Cell
Tracking Challenge. Our segmentation method achieves top-ranking results, while
our CNN model has an up to 25x lower number of parameters than other
top-ranking methods. Code and pretrained models are available at:
https://github.com/roydenwa/efficient-cell-seg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Untrimmed Videos: Self-Supervised Video Representation Learning with Hierarchical Consistency. (arXiv:2204.03017v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03017">
<div class="article-summary-box-inner">
<span><p>Natural videos provide rich visual contents for self-supervised learning. Yet
most existing approaches for learning spatio-temporal representations rely on
manually trimmed videos, leading to limited diversity in visual patterns and
limited performance gain. In this work, we aim to learn representations by
leveraging more abundant information in untrimmed videos. To this end, we
propose to learn a hierarchy of consistencies in videos, i.e., visual
consistency and topical consistency, corresponding respectively to clip pairs
that tend to be visually similar when separated by a short time span and share
similar topics when separated by a long time span. Specifically, a hierarchical
consistency learning framework HiCo is presented, where the visually consistent
pairs are encouraged to have the same representation through contrastive
learning, while the topically consistent pairs are coupled through a topical
classifier that distinguishes whether they are topic related. Further, we
impose a gradual sampling algorithm for proposed hierarchical consistency
learning, and demonstrate its theoretical superiority. Empirically, we show
that not only HiCo can generate stronger representations on untrimmed videos,
it also improves the representation quality when applied to trimmed videos.
This is in contrast to standard contrastive learning that fails to learn
appropriate representations from untrimmed videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical Model Criticism of Variational Auto-Encoders. (arXiv:2204.03030v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03030">
<div class="article-summary-box-inner">
<span><p>We propose a framework for the statistical evaluation of variational
auto-encoders (VAEs) and test two instances of this framework in the context of
modelling images of handwritten digits and a corpus of English text. Our take
on evaluation is based on the idea of statistical model criticism, popular in
Bayesian data analysis, whereby a statistical model is evaluated in terms of
its ability to reproduce statistics of an unknown data generating process from
which we can obtain samples. A VAE learns not one, but two joint distributions
over a shared sample space, each exploiting a choice of factorisation that
makes sampling tractable in one of two directions (latent-to-data,
data-to-latent). We evaluate samples from these distributions, assessing their
(marginal) fit to the observed data and our choice of prior, and we also
evaluate samples through a pipeline that connects the two distributions
starting from a data sample, assessing whether together they exploit and reveal
latent factors of variation that are useful to a practitioner. We show that
this methodology offers possibilities for model selection qualitatively beyond
intrinsic evaluation metrics and at a finer granularity than commonly used
statistics can offer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSGN++: Exploiting Visual-Spatial Relation forStereo-based 3D Detectors. (arXiv:2204.03039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03039">
<div class="article-summary-box-inner">
<span><p>Camera-based 3D object detectors are welcome due to their wider deployment
and lower price than LiDAR sensors. We revisit the prior stereo modeling DSGN
about the stereo volume constructions for representing both 3D geometry and
semantics. We polish the stereo modeling and propose our approach, DSGN++,
aiming for improving information flow throughout the 2D-to-3D pipeline in the
following three main aspects. First, to effectively lift the 2D information to
stereo volume, we propose depth-wise plane sweeping (DPS) that allows denser
connections and extracts depth-guided features. Second, for better grasping
differently spaced features, we present a novel stereo volume -- Dual-view
Stereo Volume (DSV) that integrates front-view and top-view features and
reconstructs sub-voxel depth in the camera frustum. Third, as the foreground
region becomes less dominant in 3D space, we firstly propose a multi-modal data
editing strategy -- Stereo-LiDAR Copy-Paste, which ensures cross-modal
alignment and improves data efficiency. Without bells and whistles, extensive
experiments in various modality setups on the popular KITTI benchmark show that
our method consistently outperforms other camera-based 3D detectors for all
categories. Code will be released at https://github.com/chenyilun95/DSGN2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing finetuned models for better pretraining. (arXiv:2204.03044v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03044">
<div class="article-summary-box-inner">
<span><p>Pretrained models are the standard starting point for training. This approach
consistently outperforms the use of a random initialization. However,
pretraining is a costly endeavour that few can undertake.
</p>
<p>In this paper, we create better base models at hardly any cost, by fusing
multiple existing fine tuned models into one. Specifically, we fuse by
averaging the weights of these models. We show that the fused model results
surpass the pretrained model ones. We also show that fusing is often better
than intertraining.
</p>
<p>We find that fusing is less dependent on the target task. Furthermore, weight
decay nullifies intertraining effects but not those of fusing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thermal to Visible Image Synthesis under Atmospheric Turbulence. (arXiv:2204.03057v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03057">
<div class="article-summary-box-inner">
<span><p>In many practical applications of long-range imaging such as biometrics and
surveillance, thermal imagining modalities are often used to capture images in
low-light and nighttime conditions. However, such imaging systems often suffer
from atmospheric turbulence, which introduces severe blur and deformation
artifacts to the captured images. Such an issue is unavoidable in long-range
imaging and significantly decreases the face verification accuracy. In this
paper, we first investigate the problem with a turbulence simulation method on
real-world thermal images. An end-to-end reconstruction method is then proposed
which can directly transform thermal images into visible-spectrum images by
utilizing natural image priors based on a pre-trained StyleGAN2 network.
Compared with the existing two-steps methods of consecutive turbulence
mitigation and thermal to visible image translation, our method is demonstrated
to be effective in terms of both the visual quality of the reconstructed
results and face verification accuracy. Moreover, to the best of our knowledge,
this is the first work that studies the problem of thermal to visible image
translation under atmospheric turbulence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Late multimodal fusion for image and audio music transcription. (arXiv:2204.03063v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03063">
<div class="article-summary-box-inner">
<span><p>Music transcription, which deals with the conversion of music sources into a
structured digital format, is a key problem for Music Information Retrieval
(MIR). When addressing this challenge in computational terms, the MIR community
follows two lines of research: music documents, which is the case of Optical
Music Recognition (OMR), or audio recordings, which is the case of Automatic
Music Transcription (AMT). The different nature of the aforementioned input
data has conditioned these fields to develop modality-specific frameworks.
However, their recent definition in terms of sequence labeling tasks leads to a
common output representation, which enables research on a combined paradigm. In
this respect, multimodal image and audio music transcription comprises the
challenge of effectively combining the information conveyed by image and audio
modalities. In this work, we explore this question at a late-fusion level: we
study four combination approaches in order to merge, for the first time, the
hypotheses regarding end-to-end OMR and AMT systems in a lattice-based search
space. The results obtained for a series of performance scenarios -- in which
the corresponding single-modality models yield different error rates -- showed
interesting benefits of these approaches. In addition, two of the four
strategies considered significantly improve the corresponding unimodal standard
recognition frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Self-Optimal-Transport Feature Transform. (arXiv:2204.03065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03065">
<div class="article-summary-box-inner">
<span><p>The Self-Optimal-Transport (SOT) feature transform is designed to upgrade the
set of features of a data instance to facilitate downstream matching or
grouping related tasks. The transformed set encodes a rich representation of
high order relations between the instance features. Distances between
transformed features capture their direct original similarity and their third
party agreement regarding similarity to other features in the set. A particular
min-cost-max-flow fractional matching problem, whose entropy regularized
version can be approximated by an optimal transport (OT) optimization, results
in our transductive transform which is efficient, differentiable, equivariant,
parameterless and probabilistically interpretable. Empirically, the transform
is highly effective and flexible in its use, consistently improving networks it
is inserted into, in a variety of tasks and training schemes. We demonstrate
its merits through the problem of unsupervised clustering and its efficiency
and wide applicability for few-shot-classification, with state-of-the-art
results, and large-scale person re-identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSCARS: An Outlier-Sensitive Content-Based Radiography Retrieval System. (arXiv:2204.03074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03074">
<div class="article-summary-box-inner">
<span><p>Improving the retrieval relevance on noisy datasets is an emerging need for
the curation of a large-scale clean dataset in the medical domain. While
existing methods can be applied for class-wise retrieval (aka. inter-class),
they cannot distinguish the granularity of likeness within the same class (aka.
intra-class). The problem is exacerbated on medical external datasets, where
noisy samples of the same class are treated equally during training. Our goal
is to identify both intra/inter-class similarities for fine-grained retrieval.
To achieve this, we propose an Outlier-Sensitive Content-based rAdiologhy
Retrieval System (OSCARS), consisting of two steps. First, we train an outlier
detector on a clean internal dataset in an unsupervised manner. Then we use the
trained detector to generate the anomaly scores on the external dataset, whose
distribution will be used to bin intra-class variations. Second, we propose a
quadruplet (a, p, nintra, ninter) sampling strategy, where intra-class
negatives nintra are sampled from bins of the same class other than the bin
anchor a belongs to, while niner are randomly sampled from inter-classes. We
suggest a weighted metric learning objective to balance the intra and
inter-class feature learning. We experimented on two representative public
radiography datasets. Experiments show the effectiveness of our approach. The
training and evaluation code can be found in
https://github.com/XiaoyuanGuo/oscars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance Segmentation of Unlabeled Modalities via Cyclic Segmentation GAN. (arXiv:2204.03082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03082">
<div class="article-summary-box-inner">
<span><p>Instance segmentation for unlabeled imaging modalities is a challenging but
essential task as collecting expert annotation can be expensive and
time-consuming. Existing works segment a new modality by either deploying a
pre-trained model optimized on diverse training data or conducting domain
translation and image segmentation as two independent steps. In this work, we
propose a novel Cyclic Segmentation Generative Adversarial Network (CySGAN)
that conducts image translation and instance segmentation jointly using a
unified framework. Besides the CycleGAN losses for image translation and
supervised losses for the annotated source domain, we introduce additional
self-supervised and segmentation-based adversarial objectives to improve the
model performance by leveraging unlabeled target domain images. We benchmark
our approach on the task of 3D neuronal nuclei segmentation with annotated
electron microscopy (EM) images and unlabeled expansion microscopy (ExM) data.
Our CySGAN outperforms both pretrained generalist models and the baselines that
sequentially conduct image translation and segmentation. Our implementation and
the newly collected, densely annotated ExM nuclei dataset, named NucExM, are
available at https://connectomics-bazaar.github.io/proj/CySGAN/index.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Visual Person-of-Interest DeepFake Detection. (arXiv:2204.03083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03083">
<div class="article-summary-box-inner">
<span><p>Face manipulation technology is advancing very rapidly, and new methods are
being proposed day by day. The aim of this work is to propose a deepfake
detector that can cope with the wide variety of manipulation methods and
scenarios encountered in the real world. Our key insight is that each person
has specific biometric characteristics that a synthetic generator cannot likely
reproduce. Accordingly, we extract high-level audio-visual biometric features
which characterize the identity of a person, and use them to create a
person-of-interest (POI) deepfake detector. We leverage a contrastive learning
paradigm to learn the moving-face and audio segments embeddings that are most
discriminative for each identity. As a result, when the video and/or audio of a
person is manipulated, its representation in the embedding space becomes
inconsistent with the real identity, allowing reliable detection. Training is
carried out exclusively on real talking-face videos, thus the detector does not
depend on any specific manipulation method and yields the highest
generalization ability. In addition, our method can detect both single-modality
(audio-only, video-only) and multi-modality (audio-video) attacks, and is
robust to low-quality or corrupted videos by building only on high-level
semantic features. Experiments on a wide variety of datasets confirm that our
method ensures a SOTA performance, with an average improvement in terms of AUC
of around 3%, 10%, and 7% for high-quality, low quality and attacked videos,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Self-supervised Representation Learning for Movie Understanding. (arXiv:2204.03101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03101">
<div class="article-summary-box-inner">
<span><p>Most self-supervised video representation learning approaches focus on action
recognition. In contrast, in this paper we focus on self-supervised video
learning for movie understanding and propose a novel hierarchical
self-supervised pretraining strategy that separately pretrains each level of
our hierarchical movie understanding model (based on [37]). Specifically, we
propose to pretrain the low-level video backbone using a contrastive learning
objective, while pretrain the higher-level video contextualizer using an event
mask prediction task, which enables the usage of different data sources for
pretraining different levels of the hierarchy. We first show that our
self-supervised pretraining strategies are effective and lead to improved
performance on all tasks and metrics on VidSitu benchmark [37] (e.g., improving
on semantic role prediction from 47% to 61% CIDEr scores). We further
demonstrate the effectiveness of our contextualized event features on LVU tasks
[54], both when used alone and when combined with instance features, showing
their complementarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis. (arXiv:2204.03105v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03105">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of texture representation for 3D shapes
for the challenging and underexplored tasks of texture transfer and synthesis.
Previous works either apply spherical texture maps which may lead to large
distortions, or use continuous texture fields that yield smooth outputs lacking
details. We argue that the traditional way of representing textures with images
and linking them to a 3D mesh via UV mapping is more desirable, since
synthesizing 2D images is a well-studied problem. We propose AUV-Net which
learns to embed 3D surfaces into a 2D aligned UV space, by mapping the
corresponding semantic parts of different 3D shapes to the same location in the
UV space. As a result, textures are aligned across objects, and can thus be
easily synthesized by generative models of images. Texture alignment is learned
in an unsupervised manner by a simple yet effective texture alignment module,
taking inspiration from traditional works on linear subspace learning. The
learned UV mapping and aligned texture representations enable a variety of
applications including texture transfer, texture synthesis, and textured single
view 3D reconstruction. We conduct experiments on multiple datasets to
demonstrate the effectiveness of our method. Project page:
https://nv-tlabs.github.io/AUV-NET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UIGR: Unified Interactive Garment Retrieval. (arXiv:2204.03111v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03111">
<div class="article-summary-box-inner">
<span><p>Interactive garment retrieval (IGR) aims to retrieve a target garment image
based on a reference garment image along with user feedback on what to change
on the reference garment. Two IGR tasks have been studied extensively:
text-guided garment retrieval (TGR) and visually compatible garment retrieval
(VCR). The user feedback for the former indicates what semantic attributes to
change with the garment category preserved, while the category is the only
thing to be changed explicitly for the latter, with an implicit requirement on
style preservation. Despite the similarity between these two tasks and the
practical need for an efficient system tackling both, they have never been
unified and modeled jointly. In this paper, we propose a Unified Interactive
Garment Retrieval (UIGR) framework to unify TGR and VCR. To this end, we first
contribute a large-scale benchmark suited for both problems. We further propose
a strong baseline architecture to integrate TGR and VCR in one model. Extensive
experiments suggest that unifying two tasks in one framework is not only more
efficient by requiring a single model only, it also leads to better
performance. Code and datasets are available at
https://github.com/BrandonHanx/CompFashion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoCOR: Autonomous Condylar Offset Ratio Calculator on TKA-Postoperative Lateral Knee X-ray. (arXiv:2204.03120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03120">
<div class="article-summary-box-inner">
<span><p>The postoperative range of motion is one of the crucial factors indicating
the outcome of Total Knee Arthroplasty (TKA). Although the correlation between
range of knee flexion and posterior condylar offset (PCO) is controversial in
the literature, PCO maintains its importance on evaluation of TKA. Due to
limitations on PCO measurement, two novel parameters, posterior condylar offset
ratio (PCOR) and anterior condylar offset ratio (ACOR), were introduced.
Nowadays, the calculation of PCOR and ACOR on plain lateral radiographs is done
manually by orthopedic surgeons. In this regard, we developed a software,
AutoCOR, to calculate PCOR and ACOR autonomously, utilizing unsupervised
machine learning algorithm (k-means clustering) and digital image processing
techniques. The software AutoCOR is capable of detecting the anterior/posterior
edge points and anterior/posterior cortex of the femoral shaft on true
postoperative lateral conventional radiographs. To test the algorithm, 50
postoperative true lateral radiographs from Istanbul Kosuyolu Medipol Hospital
Database were used (32 patients). The mean PCOR was 0.984 (SD 0.235) in
software results and 0.972 (SD 0.164) in ground truth values. It shows strong
and significant correlation between software and ground truth values (Pearson
r=0.845 p&lt;0.0001). The mean ACOR was 0.107 (SD 0.092) in software results and
0.107 (SD 0.070) in ground truth values. It shows moderate and significant
correlation between software and ground truth values (Spearman's rs=0.519
p=0.0001412). We suggest that AutoCOR is a useful tool that can be used in
clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects. (arXiv:2204.03139v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03139">
<div class="article-summary-box-inner">
<span><p>Research in manipulation of deformable objects is typically conducted on a
limited range of scenarios, because handling each scenario on hardware takes
significant effort. Realistic simulators with support for various types of
deformations and interactions have the potential to speed up experimentation
with novel tasks and algorithms. However, for highly deformable objects it is
challenging to align the output of a simulator with the behavior of real
objects. Manual tuning is not intuitive, hence automated methods are needed. We
view this alignment problem as a joint perception-inference challenge and
demonstrate how to use recent neural network architectures to successfully
perform simulation parameter inference from real point clouds. We analyze the
performance of various architectures, comparing their data and training
requirements. Furthermore, we propose to leverage differentiable point cloud
sampling and differentiable simulation to significantly reduce the time to
achieve the alignment. We employ an efficient way to propagate gradients from
point clouds to simulated meshes and further through to the physical simulation
parameters, such as mass and stiffness. Experiments with highly deformable
objects show that our method can achieve comparable or better alignment with
real object behavior, while reducing the time needed to achieve this by more
than an order of magnitude. Videos and supplementary material are available at
https://tinyurl.com/diffcloud.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Machine Learning Attacks Against Video Anomaly Detection Systems. (arXiv:2204.03141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03141">
<div class="article-summary-box-inner">
<span><p>Anomaly detection in videos is an important computer vision problem with
various applications including automated video surveillance. Although
adversarial attacks on image understanding models have been heavily
investigated, there is not much work on adversarial machine learning targeting
video understanding models and no previous work which focuses on video anomaly
detection. To this end, we investigate an adversarial machine learning attack
against video anomaly detection systems, that can be implemented via an
easy-to-perform cyber-attack. Since surveillance cameras are usually connected
to the server running the anomaly detection model through a wireless network,
they are prone to cyber-attacks targeting the wireless connection. We
demonstrate how Wi-Fi deauthentication attack, a notoriously easy-to-perform
and effective denial-of-service (DoS) attack, can be utilized to generate
adversarial data for video anomaly detection systems. Specifically, we apply
several effects caused by the Wi-Fi deauthentication attack on video quality
(e.g., slow down, freeze, fast forward, low resolution) to the popular
benchmark datasets for video anomaly detection. Our experiments with several
state-of-the-art anomaly detection models show that the attackers can
significantly undermine the reliability of video anomaly detection systems by
causing frequent false alarms and hiding physical anomalies from the
surveillance system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Cross-Domain Pretrained Model for Hyperspectral Image Classification. (arXiv:2204.03144v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03144">
<div class="article-summary-box-inner">
<span><p>A pretrain-finetune strategy is widely used to reduce the overfitting that
can occur when data is insufficient for CNN training. First few layers of a CNN
pretrained on a large-scale RGB dataset are capable of acquiring general image
characteristics which are remarkably effective in tasks targeted for different
RGB datasets. However, when it comes down to hyperspectral domain where each
domain has its unique spectral properties, the pretrain-finetune strategy no
longer can be deployed in a conventional way while presenting three major
issues: 1) inconsistent spectral characteristics among the domains (e.g.,
frequency range), 2) inconsistent number of data channels among the domains,
and 3) absence of large-scale hyperspectral dataset.
</p>
<p>We seek to train a universal cross-domain model which can later be deployed
for various spectral domains. To achieve, we physically furnish multiple inlets
to the model while having a universal portion which is designed to handle the
inconsistent spectral characteristics among different domains. Note that only
the universal portion is used in the finetune process. This approach naturally
enables the learning of our model on multiple domains simultaneously which acts
as an effective workaround for the issue of the absence of large-scale dataset.
</p>
<p>We have carried out a study to extensively compare models that were trained
using cross-domain approach with ones trained from scratch. Our approach was
found to be superior both in accuracy and in training efficiency. In addition,
we have verified that our approach effectively reduces the overfitting issue,
enabling us to deepen the model up to 13 layers (from 9) without compromising
the accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just-Noticeable-Difference Based Edge Map Quality Measure. (arXiv:2204.03155v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03155">
<div class="article-summary-box-inner">
<span><p>The performance of an edge detector can be improved when assisted with an
effective edge map quality measure. Several evaluation methods have been
proposed resulting in different performance score for the same candidate edge
map. However, an effective measure is the one that can be automated and which
correlates with human judgement perceived quality of the edge map.
Distance-based edge map measures are widely used for assessment of edge map
quality. These methods consider distance and statistical properties of edge
pixels to estimate a performance score. The existing methods can be automated;
however, they lack perceptual features. This paper presents edge map quality
measure based on Just-Noticeable-Difference (JND) feature of human visual
system, to compensate the shortcomings of distance-based edge measures. For
this purpose, we have designed constant stimulus experiment to measure the JND
value for two spatial alternative. Experimental results show that JND based
distance calculation outperforms existing distance-based measures according to
subjective evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible Sampling for Long-tailed Skin Lesion Classification. (arXiv:2204.03161v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03161">
<div class="article-summary-box-inner">
<span><p>Most of the medical tasks naturally exhibit a long-tailed distribution due to
the complex patient-level conditions and the existence of rare diseases.
Existing long-tailed learning methods usually treat each class equally to
re-balance the long-tailed distribution. However, considering that some
challenging classes may present diverse intra-class distributions, re-balancing
all classes equally may lead to a significant performance drop. To address
this, in this paper, we propose a curriculum learning-based framework called
Flexible Sampling for the long-tailed skin lesion classification task.
Specifically, we initially sample a subset of training data as anchor points
based on the individual class prototypes. Then, these anchor points are used to
pre-train an inference model to evaluate the per-class learning difficulty.
Finally, we use a curriculum sampling module to dynamically query new samples
from the rest training samples with the learning difficulty-aware sampling
probability. We evaluated our model against several state-of-the-art methods on
the ISIC dataset. The results with two long-tailed settings have demonstrated
the superiority of our proposed training strategy, which achieves a new
benchmark for long-tailed skin lesion classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. (arXiv:2204.03162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03162">
<div class="article-summary-box-inner">
<span><p>We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Dose CT Denoising via Sinogram Inner-Structure Transformer. (arXiv:2204.03163v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03163">
<div class="article-summary-box-inner">
<span><p>Low-Dose Computed Tomography (LDCT) technique, which reduces the radiation
harm to human bodies, is now attracting increasing interest in the medical
imaging field. As the image quality is degraded by low dose radiation, LDCT
exams require specialized reconstruction methods or denoising algorithms.
However, most of the recent effective methods overlook the inner-structure of
the original projection data (sinogram) which limits their denoising ability.
The inner-structure of the sinogram represents special characteristics of the
data in the sinogram domain. By maintaining this structure while denoising, the
noise can be obviously restrained. Therefore, we propose an LDCT denoising
network namely Sinogram Inner-Structure Transformer (SIST) to reduce the noise
by utilizing the inner-structure in the sinogram domain. Specifically, we study
the CT imaging mechanism and statistical characteristics of sinogram to design
the sinogram inner-structure loss including the global and local
inner-structure for restoring high-quality CT images. Besides, we propose a
sinogram transformer module to better extract sinogram features. The
transformer architecture using a self-attention mechanism can exploit
interrelations between projections of different view angles, which achieves an
outstanding performance in sinogram denoising. Furthermore, in order to improve
the performance in the image domain, we propose the image reconstruction module
to complementarily denoise both in the sinogram and image domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDA GAN: Adversarial-Learning-based 3-D Seismic Data Interpolation and Reconstruction for Complex Missing. (arXiv:2204.03197v1 [physics.geo-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03197">
<div class="article-summary-box-inner">
<span><p>The interpolation and reconstruction of missing traces is a crucial step in
seismic data processing, moreover it is also a highly ill-posed problem,
especially for complex cases such as high-ratio random discrete missing,
continuous missing and missing in rich fault or salt body surveys. These
complex cases are rarely mentioned in current sparse or low-rank priorbased and
deep learning-based approaches. To cope with complex missing cases, we propose
Multi-Dimensional Adversarial GAN (MDA GAN), a novel 3-D GAN framework. It
employs three discriminators to ensure the consistency of the reconstructed
data with the original data distribution in each dimension. The feature
splicing module (FSM) is designed and embedded into the generator of this
framework, which automatically splices the features of the unmissing part with
those of the reconstructed part (missing part), thus fully preserving the
information of the unmissing part. To prevent pixel distortion in the seismic
data caused by the adversarial learning process, we propose a new
reconstruction loss Tanh Cross Entropy (TCE) loss to provide smoother
gradients. We experimentally verified the effectiveness of the individual
components of the study and then tested the method on multiple publicly
available data. The method achieves reasonable reconstructions for up to 95% of
random discrete missing, 100 traces of continuous missing and more complex
hybrid missing. In surveys of fault-rich and salt bodies, the method can
achieve promising reconstructions with up to 75% missing in each of the three
directions (98.2% in total).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Network for Early Pulmonary Embolism Detection via Computed Tomography Pulmonary Angiography. (arXiv:2204.03204v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03204">
<div class="article-summary-box-inner">
<span><p>This study was conducted to develop a computer-aided detection (CAD) system
for triaging patients with pulmonary embolism (PE). The purpose of the system
was to reduce the death rate during the waiting period. Computed tomography
pulmonary angiography (CTPA) is used for PE diagnosis. Because CTPA reports
require a radiologist to review the case and suggest further management, this
creates a waiting period during which patients may die. Our proposed CAD method
was thus designed to triage patients with PE from those without PE. In contrast
to related studies involving CAD systems that identify key PE lesion images to
expedite PE diagnosis, our system comprises a novel classification-model
ensemble for PE detection and a segmentation model for PE lesion labeling. The
models were trained using data from National Cheng Kung University Hospital and
open resources. The classification model yielded 0.73 for receiver operating
characteristic curve (accuracy = 0.85), while the mean intersection over union
was 0.689 for the segmentation model. The proposed CAD system can distinguish
between patients with and without PE and automatically label PE lesions to
expedite PE diagnosis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation. (arXiv:2204.03206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03206">
<div class="article-summary-box-inner">
<span><p>Mining precise class-aware attention maps, a.k.a, class activation maps, is
essential for weakly supervised semantic segmentation. In this paper, we
present L2G, a simple online local-to-global knowledge transfer framework for
high-quality object attention mining. We observe that classification models can
discover object regions with more details when replacing the input image with
its local patches. Taking this into account, we first leverage a local
classification network to extract attentions from multiple local patches
randomly cropped from the input image. Then, we utilize a global network to
learn complementary attention knowledge across multiple local attention maps
online. Our framework conducts the global network to learn the captured rich
object detail knowledge from a global view and thereby produces high-quality
attention maps that can be directly used as pseudo annotations for semantic
segmentation networks. Experiments show that our method attains 72.1% and 44.2%
mIoU scores on the validation set of PASCAL VOC 2012 and MS COCO 2014,
respectively, setting new state-of-the-art records. Code is available at
https://github.com/PengtaoJiang/L2G.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MC-UNet Multi-module Concatenation based on U-shape Network for Retinal Blood Vessels Segmentation. (arXiv:2204.03213v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03213">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation of the blood vessels of the retina is an important step
in clinical diagnosis of ophthalmic diseases. Many deep learning frameworks
have come up for retinal blood vessels segmentation tasks. However, the complex
vascular structure and uncertain pathological features make the blood vessel
segmentation still very challenging. A novel U-shaped network named
Multi-module Concatenation which is based on Atrous convolution and
multi-kernel pooling is put forward to retinal vessels segmentation in this
paper. The proposed network structure retains three layers the essential
structure of U-Net, in which the atrous convolution combining the multi-kernel
pooling blocks are designed to obtain more contextual information. The spatial
attention module is concatenated with dense atrous convolution module and
multi-kernel pooling module to form a multi-module concatenation. And different
dilation rates are selected by cascading to acquire a larger receptive field in
atrous convolution. Adequate comparative experiments are conducted on these
public retinal datasets: DRIVE, STARE and CHASE_DB1. The results show that the
proposed method is effective, especially for microvessels. The code will be put
out at https://github.com/Rebeccala/MC-UNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What You See is What You Get: Distributional Generalization for Algorithm Design in Deep Learning. (arXiv:2204.03230v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03230">
<div class="article-summary-box-inner">
<span><p>We investigate and leverage a connection between Differential Privacy (DP)
and the recently proposed notion of Distributional Generalization (DG).
Applying this connection, we introduce new conceptual tools for designing
deep-learning methods that bypass "pathologies" of standard stochastic gradient
descent (SGD). First, we prove that differentially private methods satisfy a
"What You See Is What You Get (WYSIWYG)" generalization guarantee: whatever a
model does on its train data is almost exactly what it will do at test time.
This guarantee is formally captured by distributional generalization. WYSIWYG
enables principled algorithm design in deep learning by reducing
$\textit{generalization}$ concerns to $\textit{optimization}$ ones: in order to
mitigate unwanted behavior at test time, it is provably sufficient to mitigate
this behavior on the train data. This is notably false for standard (non-DP)
methods, hence this observation has applications even when privacy is not
required. For example, importance sampling is known to fail for standard SGD,
but we show that it has exactly the intended effect for DP-trained models.
Thus, with DP-SGD, unlike with SGD, we can influence test-time behavior by
making principled train-time interventions. We use these insights to construct
simple algorithms which match or outperform SOTA in several distributional
robustness applications, and to significantly improve the privacy vs. disparate
impact trade-off of DP-SGD. Finally, we also improve on known theoretical
bounds relating differential privacy, stability, and distributional
generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIT-UAV: A High-altitude Infrared Thermal Dataset for Unmanned Aerial Vehicles. (arXiv:2204.03245v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03245">
<div class="article-summary-box-inner">
<span><p>This paper presents a High-altitude infrared thermal dataset, HIT-UAV, for
object detection applications on Unmanned Aerial Vehicles (UAVs). HIT-UAV
contains 2898 infrared thermal images extracted from 43470 frames. These images
are collected by UAV from schools, parking lots, roads, playgrounds, etc.
HIT-UAV provides different flight data for each place, including flight
altitude (from 60 to 130 meters), camera perspective (from 30 to 90 degrees),
date, and daylight intensity. For each image, the HIT-UAV manual annotates
object instances with two types of the bounding box (oriented and standard) to
address the challenge that object instances have a significant overlap in
aerial images. To the best of our knowledge, HIT-UAV is the first publicly
available high-altitude infrared thermal UAV dataset for persons and vehicles
detection. Moreover, we trained and evaluated the benchmark detection
algorithms (YOLOv4 and YOLOv4-tiny) on HIT-UAV. Compared to the visual light
dataset, the detection algorithms have excellent performance on HIT-UAV because
the infrared thermal images do not contain a significant quantity of irrelevant
information with detection objects. This indicates that infrared thermal
datasets can significantly promote the development of object detection
applications. We hope HIT-UAV contributes to UAV applications such as traffic
surveillance and city monitoring at night. The dataset is available at
https://github.com/suojiashun/HIT-UAV-Infrared-Thermal-Dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pan-cancer computational histopathology reveals tumor mutational burden status through weakly-supervised deep learning. (arXiv:2204.03257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03257">
<div class="article-summary-box-inner">
<span><p>Tumor mutational burden (TMB) is a potential genomic biomarker that can help
identify patients who will benefit from immunotherapy across a variety of
cancers. We included whole slide images (WSIs) of 3228 diagnostic slides from
the Cancer Genome Atlas and 531 WSIs from the Clinical Proteomic Tumor Analysis
Consortium for the development and verification of a pan-cancer TMB prediction
model (PC-TMB). We proposed a multiscale weakly-supervised deep learning
framework for predicting TMB of seven types of tumors based only on routinely
used hematoxylin-eosin (H&amp;E)-stained WSIs. PC-TMB achieved a mean area under
curve (AUC) of 0.818 (0.804-0.831) in the cross-validation cohort, which was
superior to the best single-scale model. In comparison with the
state-of-the-art TMB prediction model from previous publications, our
multiscale model achieved better performance over previously reported models.
In addition, the improvements of PC-TMB over the single-tumor models were also
confirmed by the ablation tests on 10x magnification. The PC-TMB algorithm also
exhibited good generalization on external validation cohort with AUC of 0.732
(0.683-0.761). PC-TMB possessed a comparable survival-risk stratification
performance to the TMB measured by whole exome sequencing, but with low cost
and being time-efficient for providing a prognostic biomarker of multiple solid
tumors. Moreover, spatial heterogeneity of TMB within tumors was also
identified through our PC-TMB, which might enable image-based screening for
molecular biomarkers with spatial variation and potential exploring for
genotype-spatial heterogeneity relationships.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Sensitive Temporal Feature Learning for Gait Recognition. (arXiv:2204.03270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03270">
<div class="article-summary-box-inner">
<span><p>Although gait recognition has drawn increasing research attention recently,
it remains challenging to learn discriminative temporal representation, since
the silhouette differences are quite subtle in spatial domain. Inspired by the
observation that human can distinguish gaits of different subjects by
adaptively focusing on temporal clips with different time scales, we propose a
context-sensitive temporal feature learning (CSTL) network for gait
recognition. CSTL produces temporal features in three scales, and adaptively
aggregates them according to the contextual information from local and global
perspectives. Specifically, CSTL contains an adaptive temporal aggregation
module that subsequently performs local relation modeling and global relation
modeling to fuse the multi-scale features. Besides, in order to remedy the
spatial feature corruption caused by temporal operations, CSTL incorporates a
salient spatial feature learning (SSFL) module to select groups of
discriminative spatial features. Particularly, we utilize transformers to
implement the global relation modeling and the SSFL module. To the best of our
knowledge, this is the first work that adopts transformer in gait recognition.
Extensive experiments conducted on three datasets demonstrate the
state-of-the-art performance. Concretely, we achieve rank-1 accuracies of
98.7%, 96.2% and 88.7% under normal-walking, bag-carrying and coat-wearing
conditions on CASIA-B, 97.5% on OU-MVLP and 50.6% on GREW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Real Time Satellite Pose Estimation on Low Power Edge TPU. (arXiv:2204.03296v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03296">
<div class="article-summary-box-inner">
<span><p>Pose estimation of an uncooperative space resident object is a key asset
towards autonomy in close proximity operations. In this context monocular
cameras are a valuable solution because of their low system requirements.
However, the associated image processing algorithms are either too
computationally expensive for real time on-board implementation, or not enough
accurate. In this paper we propose a pose estimation software exploiting neural
network architectures which can be scaled to different accuracy-latency
trade-offs. We designed our pipeline to be compatible with Edge Tensor
Processing Units to show how low power machine learning accelerators could
enable Artificial Intelligence exploitation in space. The neural networks were
tested both on the benchmark Spacecraft Pose Estimation Dataset, and on the
purposely developed Cosmo Photorealistic Dataset, which depicts a COSMO-SkyMed
satellite in a variety of random poses and steerable solar panels orientations.
The lightest version of our architecture achieves state-of-the-art accuracy on
both datasets but at a fraction of networks complexity, running at 7.7 frames
per second on a Coral Dev Board Mini consuming just 2.2W.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swarm behavior tracking based on a deep vision algorithm. (arXiv:2204.03319v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03319">
<div class="article-summary-box-inner">
<span><p>The intelligent swarm behavior of social insects (such as ants) springs up in
different environments, promising to provide insights for the study of embodied
intelligence. Researching swarm behavior requires that researchers could
accurately track each individual over time. Obviously, manually labeling
individual insects in a video is labor-intensive. Automatic tracking methods,
however, also poses serious challenges: (1) individuals are small and similar
in appearance; (2) frequent interactions with each other cause severe and
long-term occlusion. With the advances of artificial intelligence and computing
vision technologies, we are hopeful to provide a tool to automate monitor
multiple insects to address the above challenges. In this paper, we propose a
detection and tracking framework for multi-ant tracking in the videos by: (1)
adopting a two-stage object detection framework using ResNet-50 as backbone and
coding the position of regions of interest to locate ants accurately; (2) using
the ResNet model to develop the appearance descriptors of ants; (3)
constructing long-term appearance sequences and combining them with motion
information to achieve online tracking. To validate our method, we construct an
ant database including 10 videos of ants from different indoor and outdoor
scenes. We achieve a state-of-the-art performance of 95.7\% mMOTA and 81.1\%
mMOTP in indoor videos, 81.8\% mMOTA and 81.9\% mMOTP in outdoor videos.
Additionally, Our method runs 6-10 times faster than existing methods for
insect tracking. Experimental results demonstrate that our method provides a
powerful tool for accelerating the unraveling of the mechanisms underlying the
swarm behavior of social insects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Sample $\zeta$-mixup: Richer, More Realistic Synthetic Samples from a $p$-Series Interpolant. (arXiv:2204.03323v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03323">
<div class="article-summary-box-inner">
<span><p>Modern deep learning training procedures rely on model regularization
techniques such as data augmentation methods, which generate training samples
that increase the diversity of data and richness of label information. A
popular recent method, mixup, uses convex combinations of pairs of original
samples to generate new samples. However, as we show in our experiments, mixup
can produce undesirable synthetic samples, where the data is sampled off the
manifold and can contain incorrect labels. We propose $\zeta$-mixup, a
generalization of mixup with provably and demonstrably desirable properties
that allows convex combinations of $N \geq 2$ samples, leading to more
realistic and diverse outputs that incorporate information from $N$ original
samples by using a $p$-series interpolant. We show that, compared to mixup,
$\zeta$-mixup better preserves the intrinsic dimensionality of the original
datasets, which is a desirable property for training generalizable models.
Furthermore, we show that our implementation of $\zeta$-mixup is faster than
mixup, and extensive evaluation on controlled synthetic and 24 real-world
natural and medical image classification datasets shows that $\zeta$-mixup
outperforms mixup and traditional data augmentation techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review of Sign Language Recognition: Different Types, Modalities, and Datasets. (arXiv:2204.03328v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03328">
<div class="article-summary-box-inner">
<span><p>A machine can understand human activities, and the meaning of signs can help
overcome the communication barriers between the inaudible and ordinary people.
Sign Language Recognition (SLR) is a fascinating research area and a crucial
task concerning computer vision and pattern recognition. Recently, SLR usage
has increased in many applications, but the environment, background image
resolution, modalities, and datasets affect the performance a lot. Many
researchers have been striving to carry out generic real-time SLR models. This
review paper facilitates a comprehensive overview of SLR and discusses the
needs, challenges, and problems associated with SLR. We study related works
about manual and non-manual, various modalities, and datasets. Research
progress and existing state-of-the-art SLR models over the past decade have
been reviewed. Finally, we find the research gap and limitations in this domain
and suggest future directions. This review paper will be helpful for readers
and researchers to get complete guidance about SLR and the progressive design
of the state-of-the-art SLR model
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Feature Mining for Video Semantic Segmentation. (arXiv:2204.03330v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03330">
<div class="article-summary-box-inner">
<span><p>The contextual information plays a core role in semantic segmentation. As for
video semantic segmentation, the contexts include static contexts and motional
contexts, corresponding to static content and moving content in a video clip,
respectively. The static contexts are well exploited in image semantic
segmentation by learning multi-scale and global/long-range features. The
motional contexts are studied in previous video semantic segmentation. However,
there is no research about how to simultaneously learn static and motional
contexts which are highly correlated and complementary to each other. To
address this problem, we propose a Coarse-to-Fine Feature Mining (CFFM)
technique to learn a unified presentation of static contexts and motional
contexts. This technique consists of two parts: coarse-to-fine feature
assembling and cross-frame feature mining. The former operation prepares data
for further processing, enabling the subsequent joint learning of static and
motional contexts. The latter operation mines useful information/contexts from
the sequential frames to enhance the video contexts of the features of the
target frame. The enhanced features can be directly applied for the final
prediction. Experimental results on popular benchmarks demonstrate that the
proposed CFFM performs favorably against state-of-the-art methods for video
semantic segmentation. Our implementation is available at
https://github.com/GuoleiSun/VSS-CFFM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Optical Flow-Based Line Feature Tracking. (arXiv:2204.03331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03331">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a novel sparse optical flow (SOF)-based line feature
tracking method for the camera pose estimation problem. This method is inspired
by the point-based SOF algorithm and developed based on an observation that two
adjacent images in time-varying image sequences satisfy brightness invariant.
Based on this observation, we re-define the goal of line feature tracking:
track two endpoints of a line feature instead of the entire line based on gray
value matching instead of descriptor matching. To achieve this goal, an
efficient two endpoint tracking (TET) method is presented: first, describe a
given line feature with its two endpoints; next, track the two endpoints based
on SOF to obtain two new tracked endpoints by minimizing a pixel-level
grayscale residual function; finally, connect the two tracked endpoints to
generate a new line feature. The correspondence is established between the
given and the new line feature. Compared with current descriptor-based methods,
our TET method needs not to compute descriptors and detect line features
repeatedly. Naturally, it has an obvious advantage over computation.
Experiments in several public benchmark datasets show our method yields highly
competitive accuracy with an obvious advantage over speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Sieve: Prediction of Grading Curves from Images of Concrete Aggregate. (arXiv:2204.03333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03333">
<div class="article-summary-box-inner">
<span><p>A large component of the building material concrete consists of aggregate
with varying particle sizes between 0.125 and 32 mm. Its actual size
distribution significantly affects the quality characteristics of the final
concrete in both, the fresh and hardened states. The usually unknown variations
in the size distribution of the aggregate particles, which can be large
especially when using recycled aggregate materials, are typically compensated
by an increased usage of cement which, however, has severe negative impacts on
economical and ecological aspects of the concrete production. In order to allow
a precise control of the target properties of the concrete, unknown variations
in the size distribution have to be quantified to enable a proper adaptation of
the concrete's mixture design in real time. To this end, this paper proposes a
deep learning based method for the determination of concrete aggregate grading
curves. In this context, we propose a network architecture applying multi-scale
feature extraction modules in order to handle the strongly diverse object sizes
of the particles. Furthermore, we propose and publish a novel dataset of
concrete aggregate used for the quantitative evaluation of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSTR: End-to-End One-Step Person Search With Transformers. (arXiv:2204.03340v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03340">
<div class="article-summary-box-inner">
<span><p>We propose a novel one-step transformer-based person search framework, PSTR,
that jointly performs person detection and re-identification (re-id) in a
single architecture. PSTR comprises a person search-specialized (PSS) module
that contains a detection encoder-decoder for person detection along with a
discriminative re-id decoder for person re-id. The discriminative re-id decoder
utilizes a multi-level supervision scheme with a shared decoder for
discriminative re-id feature learning and also comprises a part attention block
to encode relationship between different parts of a person. We further
introduce a simple multi-scale scheme to support re-id across person instances
at different scales. PSTR jointly achieves the diverse objectives of
object-level recognition (detection) and instance-level matching (re-id). To
the best of our knowledge, we are the first to propose an end-to-end one-step
transformer-based person search framework. Experiments are performed on two
popular benchmarks: CUHK-SYSU and PRW. Our extensive ablations reveal the
merits of the proposed contributions. Further, the proposed PSTR sets a new
state-of-the-art on both benchmarks. On the challenging PRW benchmark, PSTR
achieves a mean average precision (mAP) score of 56.5%. The source code is
available at \url{https://github.com/JialeCao001/PSTR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implementing a Real-Time, YOLOv5 based Social Distancing Measuring System for Covid-19. (arXiv:2204.03350v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03350">
<div class="article-summary-box-inner">
<span><p>The purpose of this work is, to provide a YOLOv5 deep learning-based social
distance monitoring framework using an overhead view perspective. In addition,
we have developed a custom defined model YOLOv5 modified CSP (Cross Stage
Partial Network) and assessed the performance on COCO and Visdrone dataset with
and without transfer learning. Our findings show that the developed model
successfully identifies the individual who violates the social distances. The
accuracy of 81.7% for the modified bottleneck CSP without transfer learning is
observed on COCO dataset after training the model for 300 epochs whereas for
the same epochs, the default YOLOv5 model is attaining 80.1% accuracy with
transfer learning. This shows an improvement in accuracy by our modified
bottleneck CSP model. For the Visdrone dataset, we are able to achieve an
accuracy of upto 56.5% for certain classes and especially an accuracy of 40%
for people and pedestrians with transfer learning using the default YOLOv5s
model for 30 epochs. While the modified bottleneck CSP is able to perform
slightly better than the default model with an accuracy score of upto 58.1% for
certain classes and an accuracy of ~40.4% for people and pedestrians.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Online Multi-Sensor Depth Fusion. (arXiv:2204.03353v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03353">
<div class="article-summary-box-inner">
<span><p>Many hand-held or mixed reality devices are used with a single sensor for 3D
reconstruction, although they often comprise multiple sensors. Multi-sensor
depth fusion is able to substantially improve the robustness and accuracy of 3D
reconstruction methods, but existing techniques are not robust enough to handle
sensors which operate with diverse value ranges as well as noise and outlier
statistics. To this end, we introduce SenFuNet, a depth fusion approach that
learns sensor-specific noise and outlier statistics and combines the data
streams of depth frames from different sensors in an online fashion. Our method
fuses multi-sensor depth streams regardless of time synchronization and
calibration and generalizes well with little training data. We conduct
experiments with various sensor combinations on the real-world CoRBS and
Scene3D datasets, as well as the Replica dataset. Experiments demonstrate that
our fusion strategy outperforms traditional and recent online depth fusion
approaches. In addition, the combination of multiple sensors yields more robust
outlier handling and precise surface reconstruction than the use of a single
sensor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Transformer. A sparse-aware solution for efficient event data processing. (arXiv:2204.03355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03355">
<div class="article-summary-box-inner">
<span><p>Event cameras are sensors of great interest for many applications that run in
low-resource and challenging environments. They log sparse illumination changes
with high temporal resolution and high dynamic range, while they present
minimal power consumption. However, top-performing methods often ignore
specific event-data properties, leading to the development of generic but
computationally expensive algorithms. Efforts toward efficient solutions
usually do not achieve top-accuracy results for complex tasks. This work
proposes a novel framework, Event Transformer (EvT), that effectively takes
advantage of event-data properties to be highly efficient and accurate. We
introduce a new patch-based event representation and a compact transformer-like
architecture to process it. EvT is evaluated on different event-based
benchmarks for action and gesture recognition. Evaluation results show better
or comparable accuracy to the state-of-the-art while requiring significantly
less computation resources, which makes EvT able to work with minimal latency
both on GPU and CPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO. (arXiv:2204.03359v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03359">
<div class="article-summary-box-inner">
<span><p>Image-Test matching (ITM) is a common task for evaluating the quality of
Vision and Language (VL) models. However, existing ITM benchmarks have a
significant limitation. They have many missing correspondences, originating
from the data construction process itself. For example, a caption is only
matched with one image although the caption can be matched with other similar
images, and vice versa. To correct the massive false negatives, we construct
the Extended COCO Validation (ECCV) Caption dataset by supplying the missing
associations with machine and human annotators. We employ five state-of-the-art
ITM models with diverse properties for our annotation process. Our dataset
provides x3.6 positive image-to-caption associations and x8.5 caption-to-image
associations compared to the original MS-COCO. We also propose to use an
informative ranking-based metric, rather than the popular Recall@K(R@K). We
re-evaluate the existing 25 VL models on existing and proposed benchmarks. Our
findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K R@K,
CxC R@1 are highly correlated with each other, while the rankings change when
we shift to the ECCV mAP. Lastly, we delve into the effect of the bias
introduced by the choice of machine annotator. Source code and dataset are
available at https://github.com/naver-ai/eccv-caption
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Distracted Driver using Convolution Neural Network. (arXiv:2204.03371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03371">
<div class="article-summary-box-inner">
<span><p>With over 50 million car sales annually and over 1.3 million deaths every
year due to motor accidents we have chosen this space. India accounts for 11
per cent of global death in road accidents. Drivers are held responsible for
78% of accidents. Road safety problems in developing countries is a major
concern and human behavior is ascribed as one of the main causes and
accelerators of road safety problems. Driver distraction has been identified as
the main reason for accidents. Distractions can be caused due to reasons such
as mobile usage, drinking, operating instruments, facial makeup, social
interaction. For the scope of this project, we will focus on building a highly
efficient ML model to classify different driver distractions at runtime using
computer vision. We would also analyze the overall speed and scalability of the
model in order to be able to set it up on an edge device. We use CNN, VGG-16,
RestNet50 and ensemble of CNN to predict the classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HunYuan_tvr for Text-Video Retrivial. (arXiv:2204.03382v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03382">
<div class="article-summary-box-inner">
<span><p>Text-Video Retrieval plays an important role in multi-modal understanding and
has attracted increasing attention in recent years. Most existing methods focus
on constructing contrastive pairs between whole videos and complete caption
sentences, while ignoring fine-grained cross-modal relationships, e.g., short
clips and phrases or single frame and word. In this paper, we propose a novel
method, named HunYuan\_tvr, to explore hierarchical cross-modal interactions by
simultaneously exploring video-sentence, clip-phrase, and frame-word
relationships. Considering intrinsic semantic relations between frames,
HunYuan\_tvr first performs self-attention to explore frame-wise correlations
and adaptively clusters correlated frames into clip-level representations.
Then, the clip-wise correlation is explored to aggregate clip representations
into a compact one to describe the video globally. In this way, we can
construct hierarchical video representations for frame-clip-video
granularities, and also explore word-wise correlations to form
word-phrase-sentence embeddings for the text modality. Finally, hierarchical
contrastive learning is designed to explore cross-modal
relationships,~\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which
enables HunYuan\_tvr to achieve a comprehensive multi-modal understanding.
Further boosted by adaptive label denosing and marginal sample enhancement,
HunYuan\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,
Rank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,
DiDemo, and ActivityNet respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Vision Transformers: Flexible Attention-Based Modelling of Biomedical Surfaces. (arXiv:2204.03408v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03408">
<div class="article-summary-box-inner">
<span><p>Recent state-of-the-art performances of Vision Transformers (ViT) in computer
vision tasks demonstrate that a general-purpose architecture, which implements
long-range self-attention, could replace the local feature learning operations
of convolutional neural networks. In this paper, we extend ViTs to surfaces by
reformulating the task of surface learning as a sequence-to-sequence learning
problem, by proposing patching mechanisms for general surface meshes. Sequences
of patches are then processed by a transformer encoder and used for
classification or regression. We validate our method on a range of different
biomedical surface domains and tasks: brain age prediction in the developing
Human Connectome Project (dHCP), fluid intelligence prediction in the Human
Connectome Project (HCP), and coronary artery calcium score classification
using surfaces from the Scottish Computed Tomography of the Heart (SCOT-HEART)
dataset, and investigate the impact of pretraining and data augmentation on
model performance. Results suggest that Surface Vision Transformers (SiT)
demonstrate consistent improvement over geometric deep learning methods for
brain age and fluid intelligence prediction and achieve comparable performance
on calcium score classification to standard metrics used in clinical practice.
Furthermore, analysis of transformer attention maps offers clear and
individualised predictions of the features driving each task. Code is available
on Github: https://github.com/metrics-lab/surface-vision-transformers
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Prototype Prompt-tuning with Pre-trained Representation for Class Incremental Learning. (arXiv:2204.03410v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03410">
<div class="article-summary-box-inner">
<span><p>Class incremental learning has attracted much attention, but most existing
works still continually fine-tune the representation model, resulting in much
catastrophic forgetting. Instead of struggling to fight against such forgetting
by replaying or distillation like most of the existing methods, we take the
pre-train-and-prompt-tuning paradigm to sequentially learn new visual concepts
based on a fixed semantic rich pre-trained representation model by incremental
prototype prompt-tuning (IPP), which substantially reduces the catastrophic
forgetting. In addition, an example prototype classification is proposed to
compensate for semantic drift, the problem caused by learning bias at different
phases. Extensive experiments conducted on the three incremental learning
benchmarks demonstrate that our method consistently outperforms other
state-of-the-art methods with a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Aware Active Learning for Endoscopic Image Analysis. (arXiv:2204.03440v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03440">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of polyps and depth estimation are two important
research problems in endoscopic image analysis. One of the main obstacles to
conduct research on these research problems is lack of annotated data.
Endoscopic annotations necessitate the specialist knowledge of expert
endoscopists and due to this, it can be difficult to organise, expensive and
time consuming. To address this problem, we investigate an active learning
paradigm to reduce the number of training examples by selecting the most
discriminative and diverse unlabelled examples for the task taken into
consideration. Most of the existing active learning pipelines are task-agnostic
in nature and are often sub-optimal to the end task. In this paper, we propose
a novel task-aware active learning pipeline and applied for two important tasks
in endoscopic image analysis: semantic segmentation and depth estimation. We
compared our method with the competitive baselines. From the experimental
results, we observe a substantial improvement over the compared baselines.
Codes are available at https://github.com/thetna/endo-active-learn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Visual Geo-localization Benchmark. (arXiv:2204.03444v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03444">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new open-source benchmarking framework for Visual
Geo-localization (VG) that allows to build, train, and test a wide range of
commonly used architectures, with the flexibility to change individual
components of a geo-localization pipeline. The purpose of this framework is
twofold: i) gaining insights into how different components and design choices
in a VG pipeline impact the final results, both in terms of performance
(recall@N metric) and system requirements (such as execution time and memory
consumption); ii) establish a systematic evaluation protocol for comparing
different methods. Using the proposed framework, we perform a large suite of
experiments which provide criteria for choosing backbone, aggregation and
negative mining depending on the use-case and requirements. We also assess the
impact of engineering techniques like pre/post-processing, data augmentation
and image resizing, showing that better performance can be obtained through
somewhat simple procedures: for example, downscaling the images' resolution to
80% can lead to similar results with a 36% savings in extraction time and
dataset storage requirement. Code and trained models are available at
https://deep-vg-bench.herokuapp.com/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Diffusion Models. (arXiv:2204.03458v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03458">
<div class="article-summary-box-inner">
<span><p>Generating temporally coherent high fidelity video is an important milestone
in generative modeling research. We make progress towards this milestone by
proposing a diffusion model for video generation that shows very promising
initial results. Our model is a natural extension of the standard image
diffusion architecture, and it enables jointly training from image and video
data, which we find to reduce the variance of minibatch gradients and speed up
optimization. To generate long and higher resolution videos we introduce a new
conditional sampling technique for spatial and temporal video extension that
performs better than previously proposed methods. We present the first results
on a large text-conditioned video generation task, as well as state-of-the-art
results on an established unconditional video generation benchmark.
Supplementary material is available at https://video-diffusion.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results. (arXiv:2204.03475v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03475">
<div class="article-summary-box-inner">
<span><p>ImageNet serves as the primary dataset for evaluating the quality of
computer-vision models. The common practice today is training each architecture
with a tailor-made scheme, designed and tuned by an expert. In this paper, we
present a unified scheme for training any backbone on ImageNet. The scheme,
named USI (Unified Scheme for ImageNet), is based on knowledge distillation and
modern tricks. It requires no adjustments or hyper-parameters tuning between
different models, and is efficient in terms of training times. We test USI on a
wide variety of architectures, including CNNs, Transformers, Mobile-oriented
and MLP-only. On all models tested, USI outperforms previous state-of-the-art
results. Hence, we are able to transform training on ImageNet from an
expert-oriented task to an automatic seamless routine. Since USI accepts any
backbone and trains it to top results, it also enables to perform methodical
comparisons, and identify the most efficient backbones along the speed-accuracy
Pareto curve. Implementation is available
at:https://github.com/Alibaba-MIIL/Solving_ImageNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProbNVS: Fast Novel View Synthesis with Learned Probability-Guided Sampling. (arXiv:2204.03476v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03476">
<div class="article-summary-box-inner">
<span><p>Existing state-of-the-art novel view synthesis methods rely on either fairly
accurate 3D geometry estimation or sampling of the entire space for neural
volumetric rendering, which limit the overall efficiency. In order to improve
the rendering efficiency by reducing sampling points without sacrificing
rendering quality, we propose to build a novel view synthesis framework based
on learned MVS priors that enables general, fast and photo-realistic view
synthesis simultaneously. Specifically, fewer but important points are sampled
under the guidance of depth probability distributions extracted from the
learned MVS architecture. Based on the learned probability-guided sampling, a
neural volume rendering module is elaborately devised to fully aggregate source
view information as well as the learned scene structures to synthesize
photorealistic target view images. Finally, the rendering results in uncertain,
occluded and unreferenced regions can be further improved by incorporating a
confidence-aware refinement module. Experiments show that our method achieves
15 to 40 times faster rendering compared to state-of-the-art baselines, with
strong generalization capacity and comparable high-quality novel view synthesis
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing the Long-Term Behaviour of Deep Reinforcement Learning for Pushing and Grasping. (arXiv:2204.03487v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03487">
<div class="article-summary-box-inner">
<span><p>We investigate the "Visual Pushing for Grasping" (VPG) system by Zeng et al.
and the "Hourglass" system by Ewerton et al., an evolution of the former. The
focus of our work is the investigation of the capabilities of both systems to
learn long-term rewards and policies. Zeng et al. original task only needs a
limited amount of foresight. Ewerton et al. attain their best performance using
an agent which only takes the most immediate action under consideration. We are
interested in the ability of their models and training algorithms to accurately
predict long-term Q-Values. To evaluate this ability, we design a new bin
sorting task and reward function. Our task requires agents to accurately
estimate future rewards and therefore use high discount factors in their
Q-Value calculation. We investigate the behaviour of an adaptation of the VPG
training algorithm on our task. We show that this adaptation can not accurately
predict the required long-term action sequences. In addition to the limitations
identified by Ewerton et al., it suffers from the known Deep Q-Learning problem
of overestimated Q-Values. In an effort to solve our task, we turn to the
Hourglass models and combine them with the Double Q-Learning approach. We show
that this approach enables the models to accurately predict long-term action
sequences when trained with large discount factors. Our results show that the
Double Q-Learning technique is essential for training with very high discount
factors, as the models Q-Value predictions diverge otherwise. We also
experiment with different approaches for discount factor scheduling, loss
calculation and exploration procedures. Our results show that the latter
factors do not visibly influence the model's performance for our task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Distributed Learning using Vision Transformer with Random Patch Permutation. (arXiv:2204.03500v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03500">
<div class="article-summary-box-inner">
<span><p>The widespread application of artificial intelligence in health research is
currently hampered by limitations in data availability. Distributed learning
methods such as federated learning (FL) and shared learning (SL) are introduced
to solve this problem as well as data management and ownership issues with
their different strengths and weaknesses. The recent proposal of federated
split task-agnostic (FeSTA) learning tries to reconcile the distinct merits of
FL and SL by enabling the multi-task collaboration between participants through
Vision Transformer (ViT) architecture, but they suffer from higher
communication overhead. To address this, here we present a multi-task
distributed learning using ViT with random patch permutation. Instead of using
a CNN based head as in FeSTA, p-FeSTA adopts a randomly permuting simple patch
embedder, improving the multi-task learning performance without sacrificing
privacy. Experimental results confirm that the proposed method significantly
enhances the benefit of multi-task collaboration, communication efficiency, and
privacy preservation, shedding light on practical multi-task distributed
learning in the field of medical imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Many-to-many Splatting for Efficient Video Frame Interpolation. (arXiv:2204.03513v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03513">
<div class="article-summary-box-inner">
<span><p>Motion-based video frame interpolation commonly relies on optical flow to
warp pixels from the inputs to the desired interpolation instant. Yet due to
the inherent challenges of motion estimation (e.g. occlusions and
discontinuities), most state-of-the-art interpolation approaches require
subsequent refinement of the warped result to generate satisfying outputs,
which drastically decreases the efficiency for multi-frame interpolation. In
this work, we propose a fully differentiable Many-to-Many (M2M) splatting
framework to interpolate frames efficiently. Specifically, given a frame pair,
we estimate multiple bidirectional flows to directly forward warp the pixels to
the desired time step, and then fuse any overlapping pixels. In doing so, each
source pixel renders multiple target pixels and each target pixel can be
synthesized from a larger area of visual context. This establishes a
many-to-many splatting scheme with robustness to artifacts like holes.
Moreover, for each input frame pair, M2M only performs motion estimation once
and has a minuscule computational overhead when interpolating an arbitrary
number of in-between frames, hence achieving fast multi-frame interpolation. We
conducted extensive experiments to analyze M2M, and found that it significantly
improves efficiency while maintaining high effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale. (arXiv:2204.03514v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03514">
<div class="article-summary-box-inner">
<span><p>We present a large-scale study of imitating human demonstrations on tasks
that require a virtual robot to search for objects in new environments -- (1)
ObjectGoal Navigation (e.g. 'find &amp; go to a chair') and (2) Pick&amp;Place (e.g.
'find mug, pick mug, find counter, place mug on counter'). First, we develop a
virtual teleoperation data-collection infrastructure -- connecting Habitat
simulator running in a web browser to Amazon Mechanical Turk, allowing remote
users to teleoperate virtual robots, safely and at scale. We collect 80k
demonstrations for ObjectNav and 12k demonstrations for Pick&amp;Place, which is an
order of magnitude larger than existing human demonstration datasets in
simulation or on real robots.
</p>
<p>Second, we attempt to answer the question -- how does large-scale imitation
learning (IL) (which hasn't been hitherto possible) compare to reinforcement
learning (RL) (which is the status quo)? On ObjectNav, we find that IL (with no
bells or whistles) using 70k human demonstrations outperforms RL using 240k
agent-gathered trajectories. The IL-trained agent demonstrates efficient
object-search behavior -- it peeks into rooms, checks corners for small
objects, turns in place to get a panoramic view -- none of these are exhibited
as prominently by the RL agent, and to induce these behaviors via RL would
require tedious reward engineering. Finally, accuracy vs. training data size
plots show promising scaling behavior, suggesting that simply collecting more
demonstrations is likely to advance the state of art further. On Pick&amp;Place,
the comparison is starker -- IL agents achieve ${\sim}$18% success on episodes
with new object-receptacle locations when trained with 9.5k human
demonstrations, while RL agents fail to get beyond 0%. Overall, our work
provides compelling evidence for investing in large-scale imitation learning.
</p>
<p>Project page: https://ram81.github.io/projects/habitat-web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizing Deep Neural Networks with Topographic Activation Maps. (arXiv:2204.03528v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03528">
<div class="article-summary-box-inner">
<span><p>Machine Learning with Deep Neural Networks (DNNs) has become a successful
tool in solving tasks across various fields of application. The success of DNNs
is strongly connected to their high complexity in terms of the number of
network layers or of neurons in each layer, which severely complicates to
understand how DNNs solve their learned task. To improve the explainability of
DNNs, we adapt methods from neuroscience because this field has a rich
experience in analyzing complex and opaque systems. In this work, we draw
inspiration from how neuroscience uses topographic maps to visualize the
activity of the brain when it performs certain tasks. Transferring this
approach to DNNs can help to visualize and understand their internal processes
more intuitively, too. However, the inner structures of brains and DNNs differ
substantially. Therefore, to be able to visualize activations of neurons in
DNNs as topographic maps, we research techniques to layout the neurons in a
two-dimensional space in which neurons of similar activity are in the vicinity
of each other. In this work, we introduce and compare different methods to
obtain a topographic layout of the neurons in a network layer. Moreover, we
demonstrate how to use the resulting topographic activation maps to identify
errors or encoded biases in DNNs or data sets. Our novel visualization
technique improves the transparency of DNN-based algorithmic decision-making
systems and is accessible to a broad audience because topographic maps are
intuitive to interpret without expert-knowledge in Machine Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Multiscale Object-based Superpixel Framework. (arXiv:2204.03533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03533">
<div class="article-summary-box-inner">
<span><p>Superpixel segmentation can be used as an intermediary step in many
applications, often to improve object delineation and reduce computer workload.
However, classical methods do not incorporate information about the desired
object. Deep-learning-based approaches consider object information, but their
delineation performance depends on data annotation. Additionally, the
computational time of object-based methods is usually much higher than desired.
In this work, we propose a novel superpixel framework, named Superpixels
through Iterative CLEarcutting (SICLE), which exploits object information being
able to generate a multiscale segmentation on-the-fly. SICLE starts off from
seed oversampling and repeats optimal connectivity-based superpixel delineation
and object-based seed removal until a desired number of superpixels is reached.
It generalizes recent superpixel methods, surpassing them and other
state-of-the-art approaches in efficiency and effectiveness according to
multiple delineation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Zero-Shot HOI Detection via Vision and Language Knowledge Distillation. (arXiv:2204.03541v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03541">
<div class="article-summary-box-inner">
<span><p>Most existing Human-Object Interaction~(HOI) Detection methods rely heavily
on full annotations with predefined HOI categories, which is limited in
diversity and costly to scale further. We aim at advancing zero-shot HOI
detection to detect both seen and unseen HOIs simultaneously. The fundamental
challenges are to discover potential human-object pairs and identify novel HOI
categories. To overcome the above challenges, we propose a novel end-to-end
zero-shot HOI Detection (EoID) framework via vision-language knowledge
distillation. We first design an Interactive Score module combined with a
Two-stage Bipartite Matching algorithm to achieve interaction distinguishment
for human-object pairs in an action-agnostic manner. Then we transfer the
distribution of action probability from the pretrained vision-language teacher
as well as the seen ground truth to the HOI model to attain zero-shot HOI
classification. Extensive experiments on HICO-Det dataset demonstrate that our
model discovers potential interactive pairs and enables the recognition of
unseen HOIs. Finally, our method outperforms the previous SOTA by 8.92% on
unseen mAP and 10.18% on overall mAP under UA setting, by 6.02% on unseen mAP
and 9.1% on overall mAP under UC setting. Moreover, our method is generalizable
to large-scale object detection data to further scale up the action sets. The
source code will be available at: https://github.com/mrwu-mac/EoID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Procedures for Establishing Generative Adversarial Network-based Stochastic Image Models in Medical Imaging. (arXiv:2204.03547v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03547">
<div class="article-summary-box-inner">
<span><p>Modern generative models, such as generative adversarial networks (GANs),
hold tremendous promise for several areas of medical imaging, such as
unconditional medical image synthesis, image restoration, reconstruction and
translation, and optimization of imaging systems. However, procedures for
establishing stochastic image models (SIMs) using GANs remain generic and do
not address specific issues relevant to medical imaging. In this work,
canonical SIMs that simulate realistic vessels in angiography images are
employed to evaluate procedures for establishing SIMs using GANs. The GAN-based
SIM is compared to the canonical SIM based on its ability to reproduce those
statistics that are meaningful to the particular medically realistic SIM
considered. It is shown that evaluating GANs using classical metrics and
medically relevant metrics may lead to different conclusions about the fidelity
of the trained GANs. This work highlights the need for the development of
objective metrics for evaluating GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Digital Disguises: Leveraging Face Swaps to Protect Patient Privacy. (arXiv:2204.03559v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03559">
<div class="article-summary-box-inner">
<span><p>With rapid advancements in image generation technology, face swapping for
privacy protection has emerged as an active area of research. The ultimate
benefit is improved access to video datasets, e.g. in healthcare settings.
Recent literature has proposed deep network-based architectures to perform
facial swaps and reported the associated reduction in facial recognition
accuracy. However, there is not much reporting on how well these methods
preserve the types of semantic information needed for the privatized videos to
remain useful for their intended application. Our main contribution is a novel
end-to-end face swapping pipeline for recorded videos of standardized
assessments of autism symptoms in children. Through this design, we are the
first to provide a methodology for assessing the privacy-utility trade-offs for
the face swapping approach to patient privacy protection. Our methodology can
show, for example, that current deep network based face swapping is
bottle-necked by face detection in real world videos, and the extent to which
gaze and expression information is preserved by face swaps relative to baseline
privatization methods such as blurring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotional Speech Recognition with Pre-trained Deep Visual Models. (arXiv:2204.03561v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03561">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new methodology for emotional speech recognition
using visual deep neural network models. We employ the transfer learning
capabilities of the pre-trained computer vision deep models to have a mandate
for the emotion recognition in speech task. In order to achieve that, we
propose to use a composite set of acoustic features and a procedure to convert
them into images. Besides, we present a training paradigm for these models
taking into consideration the different characteristics between acoustic-based
images and regular ones. In our experiments, we use the pre-trained VGG-16
model and test the overall methodology on the Berlin EMO-DB dataset for
speaker-independent emotion recognition. We evaluate the proposed model on the
full list of the seven emotions and the results set a new state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explicit and Implicit Pattern Relation Analysis for Discovering Actionable Negative Sequences. (arXiv:2204.03571v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03571">
<div class="article-summary-box-inner">
<span><p>Real-life events, behaviors and interactions produce sequential data. An
important but rarely explored problem is to analyze those nonoccurring (also
called negative) yet important sequences, forming negative sequence analysis
(NSA). A typical NSA area is to discover negative sequential patterns (NSPs)
consisting of important non-occurring and occurring elements and patterns. The
limited existing work on NSP mining relies on frequentist and downward closure
property-based pattern selection, producing large and highly redundant NSPs,
nonactionable for business decision-making. This work makes the first attempt
for actionable NSP discovery. It builds an NSP graph representation, quantify
both explicit occurrence and implicit non-occurrence-based element and pattern
relations, and then discover significant, diverse and informative NSPs in the
NSP graph to represent the entire NSP set for discovering actionable NSPs. A
DPP-based NSP representation and actionable NSP discovery method EINSP
introduces novel and significant contributions for NSA and sequence analysis:
(1) it represents NSPs by a determinantal point process (DPP) based graph; (2)
it quantifies actionable NSPs in terms of their statistical significance,
diversity, and strength of explicit/implicit element/pattern relations; and (3)
it models and measures both explicit and implicit element/pattern relations in
the DPP-based NSP graph to represent direct and indirect couplings between NSP
items, elements and patterns. We substantially analyze the effectiveness of
EINSP in terms of various theoretical and empirical aspects including
complexity, item/pattern coverage, pattern size and diversity, implicit pattern
relation strength, and data factors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Pathology-Based Machine Learning Method to Assist in Epithelial Dysplasia Diagnosis. (arXiv:2204.03572v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03572">
<div class="article-summary-box-inner">
<span><p>The Epithelial Dysplasia (ED) is a tissue alteration commonly present in
lesions preceding oral cancer, being its presence one of the most important
factors in the progression toward carcinoma. This study proposes a method to
design a low computational cost classification system to support the detection
of dysplastic epithelia, contributing to reduce the variability of pathologist
assessments. We employ a multilayer artificial neural network (MLP-ANN) and
defining the regions of the epithelium to be assessed based on the knowledge of
the pathologist. The performance of the proposed solution was statistically
evaluated. The implemented MLP-ANN presented an average accuracy of 87%, with a
variability much inferior to that obtained from three trained evaluators.
Moreover, the proposed solution led to results which are very close to those
obtained using a convolutional neural network (CNN) implemented by transfer
learning, with 100 times less computational complexity. In conclusion, our
results show that a simple neural network structure can lead to a performance
equivalent to that of much more complex structures, which are routinely used in
the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03574">
<div class="article-summary-box-inner">
<span><p>We introduce compositional soft prompting (CSP), a parameter-efficient
learning technique to improve the zero-shot compositionality of large-scale
pretrained vision-language models (VLMs) without the overhead of fine-tuning
the entire model. VLMs can represent arbitrary classes as natural language
prompts in their flexible text encoders but they underperform state-of-the-art
methods on compositional zero-shot benchmark tasks. To improve VLMs, we propose
a novel form of soft prompting. We treat the attributes and objects that are
composed to define classes as learnable tokens of vocabulary and tune them on
multiple prompt compositions. During inference, we recompose the learned
attribute-object vocabulary in new combinations and show that CSP outperforms
the original VLM on benchmark datasets by an average of 14.7 percentage points
of accuracy. CSP also achieves new state-of-the-art accuracies on two out of
three benchmark datasets, while only fine-tuning a small number of parameters.
Further, we show that CSP improves generalization to higher-order
attribute-attribute-object compositions and combinations of pretrained
attributes and fine-tuned objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoRF: Learning 3D Object Radiance Fields from Single View Observations. (arXiv:2204.03593v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03593">
<div class="article-summary-box-inner">
<span><p>We introduce AutoRF - a new approach for learning neural 3D object
representations where each object in the training set is observed by only a
single view. This setting is in stark contrast to the majority of existing
works that leverage multiple views of the same object, employ explicit priors
during training, or require pixel-perfect annotations. To address this
challenging setting, we propose to learn a normalized, object-centric
representation whose embedding describes and disentangles shape, appearance,
and pose. Each encoding provides well-generalizable, compact information about
the object of interest, which is decoded in a single-shot into a new target
view, thus enabling novel view synthesis. We further improve the reconstruction
quality by optimizing shape and appearance codes at test time by fitting the
representation tightly to the input image. In a series of experiments, we show
that our method generalizes well to unseen objects, even across different
datasets of challenging real-world street scenes such as nuScenes, KITTI, and
Mapillary Metropolis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pin the Memory: Learning to Generalize Semantic Segmentation. (arXiv:2204.03609v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03609">
<div class="article-summary-box-inner">
<span><p>The rise of deep neural networks has led to several breakthroughs for
semantic segmentation. In spite of this, a model trained on source domain often
fails to work properly in new challenging domains, that is directly concerned
with the generalization capability of the model. In this paper, we present a
novel memory-guided domain generalization method for semantic segmentation
based on meta-learning framework. Especially, our method abstracts the
conceptual knowledge of semantic classes into categorical memory which is
constant beyond the domains. Upon the meta-learning concept, we repeatedly
train memory-guided networks and simulate virtual test to 1) learn how to
memorize a domain-agnostic and distinct information of classes and 2) offer an
externally settled memory as a class-guidance to reduce the ambiguity of
representation in the test data of arbitrary unseen domain. To this end, we
also propose memory divergence and feature cohesion losses, which encourage to
learn memory reading and update processes for category-aware domain
generalization. Extensive experiments for semantic segmentation demonstrate the
superior generalization capability of our method over state-of-the-art works on
various benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Contrastive Learning in Image-Text-Label Space. (arXiv:2204.03610v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03610">
<div class="article-summary-box-inner">
<span><p>Visual recognition is recently learned via either supervised learning on
human-annotated image-label data or language-image contrastive learning with
webly-crawled image-text pairs. While supervised learning may result in a more
discriminative representation, language-image pretraining shows unprecedented
zero-shot recognition capability, largely due to the different properties of
data sources and learning objectives. In this work, we introduce a new
formulation by combining the two data sources into a common image-text-label
space. In this space, we propose a new learning paradigm, called Unified
Contrastive Learning (UniCL) with a single learning objective to seamlessly
prompt the synergy of two data types. Extensive experiments show that our UniCL
is an effective way of learning semantically rich yet discriminative
representations, universally for image recognition in zero-shot, linear-probe,
fully finetuning and transfer learning scenarios. Particularly, it attains
gains up to 9.2% and 14.5% in average on zero-shot recognition benchmarks over
the language-image contrastive learning and supervised learning methods,
respectively. In linear probe setting, it also boosts the performance over the
two methods by 7.3% and 3.4%, respectively. Our study also indicates that UniCL
stand-alone is a good learner on pure image-label data, rivaling the supervised
learning methods across three image classification datasets and two types of
vision backbones, ResNet and Swin Transformer. Code is available at
https://github.com/microsoft/UniCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pneumonia Detection in Chest X-Rays using Neural Networks. (arXiv:2204.03618v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03618">
<div class="article-summary-box-inner">
<span><p>With the advancement in AI, deep learning techniques are widely used to
design robust classification models in several areas such as medical diagnosis
tasks in which it achieves good performance. In this paper, we have proposed
the CNN model (Convolutional Neural Network) for the classification of Chest
X-ray images for Radiological Society of North America Pneumonia (RSNA)
datasets. The study also tries to achieve the same RSNA benchmark results using
the limited computational resources by trying out various approaches to the
methodologies that have been implemented in recent years. The proposed method
is based on a non-complex CNN and the use of transfer learning algorithms like
Xception, InceptionV3/V4, EfficientNetB7. Along with this, the study also tries
to achieve the same RSNA benchmark results using the limited computational
resources by trying out various approaches to the methodologies that have been
implemented in recent years. The RSNA benchmark MAP score is 0.25, but using
the Mask RCNN model on a stratified sample of 3017 along with image
augmentation gave a MAP score of 0.15. Meanwhile, the YoloV3 without any
hyperparameter tuning gave the MAP score of 0.32 but still, the loss keeps
decreasing. Running the model for a greater number of iterations can give
better results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effects of Regularization and Data Augmentation are Class Dependent. (arXiv:2204.03632v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03632">
<div class="article-summary-box-inner">
<span><p>Regularization is a fundamental technique to prevent over-fitting and to
improve generalization performances by constraining a model's complexity.
Current Deep Networks heavily rely on regularizers such as Data-Augmentation
(DA) or weight-decay, and employ structural risk minimization, i.e.
cross-validation, to select the optimal regularization hyper-parameters. In
this study, we demonstrate that techniques such as DA or weight decay produce a
model with a reduced complexity that is unfair across classes. The optimal
amount of DA or weight decay found from cross-validation leads to disastrous
model performances on some classes e.g. on Imagenet with a resnet50, the "barn
spider" classification test accuracy falls from $68\%$ to $46\%$ only by
introducing random crop DA during training. Even more surprising, such
performance drop also appears when introducing uninformative regularization
techniques such as weight decay. Those results demonstrate that our search for
ever increasing generalization performance -- averaged over all classes and
samples -- has left us with models and regularizers that silently sacrifice
performances on some classes. This scenario can become dangerous when deploying
a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on
INaturalist sees its performances fall from $70\%$ to $30\%$ on class \#8889
when introducing random crop DA during the Imagenet pre-training phase. Those
results demonstrate that designing novel regularizers without class-dependent
bias remains an open research question.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Incremental Learning with Strong Pre-trained Models. (arXiv:2204.03634v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03634">
<div class="article-summary-box-inner">
<span><p>Class-incremental learning (CIL) has been widely studied under the setting of
starting from a small number of classes (base classes). Instead, we explore an
understudied real-world setting of CIL that starts with a strong model
pre-trained on a large number of base classes. We hypothesize that a strong
base model can provide a good representation for novel classes and incremental
learning can be done with small adaptations. We propose a 2-stage training
scheme, i) feature augmentation -- cloning part of the backbone and fine-tuning
it on the novel data, and ii) fusion -- combining the base and novel
classifiers into a unified classifier. Experiments show that the proposed
method significantly outperforms state-of-the-art CIL methods on the
large-scale ImageNet dataset (e.g. +10% overall accuracy than the best). We
also propose and analyze understudied practical CIL scenarios, such as
base-novel overlap with distribution shift. Our proposed method is robust and
generalizes to all analyzed CIL settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Category-Level Object Pose Estimation. (arXiv:2204.03635v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03635">
<div class="article-summary-box-inner">
<span><p>Object pose estimation is an important component of most vision pipelines for
embodied agents, as well as in 3D vision more generally. In this paper we
tackle the problem of estimating the pose of novel object categories in a
zero-shot manner. This extends much of the existing literature by removing the
need for pose-labelled datasets or category-specific CAD models for training or
inference. Specifically, we make the following contributions. First, we
formalise the zero-shot, category-level pose estimation problem and frame it in
a way that is most applicable to real-world embodied agents. Secondly, we
propose a novel method based on semantic correspondences from a self-supervised
vision transformer to solve the pose estimation problem. We further re-purpose
the recent CO3D dataset to present a controlled and realistic test setting.
Finally, we demonstrate that all baselines for our proposed task perform
poorly, and show that our method provides a six-fold improvement in average
rotation accuracy at 30 degrees. Our code is available at
https://github.com/applied-ai-lab/zero-shot-pose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation. (arXiv:2204.03636v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03636">
<div class="article-summary-box-inner">
<span><p>Depth estimation from images serves as the fundamental step of 3D perception
for autonomous driving and is an economical alternative to expensive depth
sensors like LiDAR. The temporal photometric consistency enables
self-supervised depth estimation without labels, further facilitating its
application. However, most existing methods predict the depth solely based on
each monocular image and ignore the correlations among multiple surrounding
cameras, which are typically available for modern self-driving vehicles. In
this paper, we propose a SurroundDepth method to incorporate the information
from multiple surrounding views to predict depth maps across cameras.
Specifically, we employ a joint network to process all the surrounding views
and propose a cross-view transformer to effectively fuse the information from
multiple views. We apply cross-view self-attention to efficiently enable the
global interactions between multi-camera feature maps. Different from
self-supervised monocular depth estimation, we are able to predict real-world
scales given multi-camera extrinsic matrices. To achieve this goal, we adopt
structure-from-motion to extract scale-aware pseudo depths to pretrain the
models. Further, instead of predicting the ego-motion of each individual
camera, we estimate a universal ego-motion of the vehicle and transfer it to
each view to achieve multi-view consistency. In experiments, our method
achieves the state-of-the-art performance on the challenging multi-camera depth
estimation datasets DDAD and nuScenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer. (arXiv:2204.03638v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03638">
<div class="article-summary-box-inner">
<span><p>Videos are created to express emotion, exchange information, and share
experiences. Video synthesis has intrigued researchers for a long time. Despite
the rapid progress driven by advances in visual synthesis, most existing
studies focus on improving the frames' quality and the transitions between
them, while little progress has been made in generating longer videos. In this
paper, we present a method that builds on 3D-VQGAN and transformers to generate
videos with thousands of frames. Our evaluation shows that our model trained on
16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,
and Taichi-HD datasets can generate diverse, coherent, and high-quality long
videos. We also showcase conditional extensions of our approach for generating
meaningful long videos by incorporating temporal information with text and
audio. Videos and code can be found at
https://songweige.github.io/projects/tats/index.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equivariance Discovery by Learned Parameter-Sharing. (arXiv:2204.03640v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03640">
<div class="article-summary-box-inner">
<span><p>Designing equivariance as an inductive bias into deep-nets has been a
prominent approach to build effective models, e.g., a convolutional neural
network incorporates translation equivariance. However, incorporating these
inductive biases requires knowledge about the equivariance properties of the
data, which may not be available, e.g., when encountering a new domain. To
address this, we study how to discover interpretable equivariances from data.
Specifically, we formulate this discovery process as an optimization problem
over a model's parameter-sharing schemes. We propose to use the partition
distance to empirically quantify the accuracy of the recovered equivariance.
Also, we theoretically analyze the method for Gaussian data and provide a bound
on the mean squared gap between the studied discovery scheme and the oracle
scheme. Empirically, we show that the approach recovers known equivariances,
such as permutations and shifts, on sum of numbers and spatially-invariant
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Image-to-Image Translation with Generative Prior. (arXiv:2204.03641v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03641">
<div class="article-summary-box-inner">
<span><p>Unsupervised image-to-image translation aims to learn the translation between
two visual domains without paired data. Despite the recent progress in image
translation models, it remains challenging to build mappings between complex
domains with drastic visual discrepancies. In this work, we present a novel
framework, Generative Prior-guided UNsupervised Image-to-image Translation
(GP-UNIT), to improve the overall quality and applicability of the translation
algorithm. Our key insight is to leverage the generative prior from pre-trained
class-conditional GANs (e.g., BigGAN) to learn rich content correspondences
across various domains. We propose a novel coarse-to-fine scheme: we first
distill the generative prior to capture a robust coarse-level content
representation that can link objects at an abstract semantic level, based on
which fine-level content features are adaptively learned for more accurate
multi-level content correspondences. Extensive experiments demonstrate the
superiority of our versatile framework over state-of-the-art methods in robust,
high-quality and diversified translations, even for challenging and distant
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-train, Self-train, Distill: A simple recipe for Supersizing 3D Reconstruction. (arXiv:2204.03642v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03642">
<div class="article-summary-box-inner">
<span><p>Our work learns a unified model for single-view 3D reconstruction of objects
from hundreds of semantic categories. As a scalable alternative to direct 3D
supervision, our work relies on segmented image collections for learning 3D of
generic categories. Unlike prior works that use similar supervision but learn
independent category-specific models from scratch, our approach of learning a
unified model simplifies the training process while also allowing the model to
benefit from the common structure across categories. Using image collections
from standard recognition datasets, we show that our approach allows learning
3D inference for over 150 object categories. We evaluate using two datasets and
qualitatively and quantitatively show that our unified reconstruction approach
improves over prior category-specific reconstruction baselines. Our final 3D
reconstruction model is also capable of zero-shot inference on images from
unseen object categories and we empirically show that increasing the number of
training categories improves the reconstruction quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Total Variation Optimization Layers for Computer Vision. (arXiv:2204.03643v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03643">
<div class="article-summary-box-inner">
<span><p>Optimization within a layer of a deep-net has emerged as a new direction for
deep-net layer design. However, there are two main challenges when applying
these layers to computer vision tasks: (a) which optimization problem within a
layer is useful?; (b) how to ensure that computation within a layer remains
efficient? To study question (a), in this work, we propose total variation (TV)
minimization as a layer for computer vision. Motivated by the success of total
variation in image processing, we hypothesize that TV as a layer provides
useful inductive bias for deep-nets too. We study this hypothesis on five
computer vision tasks: image classification, weakly supervised object
localization, edge-preserving smoothing, edge detection, and image denoising,
improving over existing baselines. To achieve these results we had to address
question (b): we developed a GPU-based projected-Newton method which is
$37\times$ faster than existing solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DaViT: Dual Attention Vision Transformers. (arXiv:2204.03645v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03645">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce Dual Attention Vision Transformers (DaViT), a
simple yet effective vision transformer architecture that is able to capture
global context while maintaining computational efficiency. We propose
approaching the problem from an orthogonal angle: exploiting self-attention
mechanisms with both "spatial tokens" and "channel tokens". With spatial
tokens, the spatial dimension defines the token scope, and the channel
dimension defines the token feature dimension. With channel tokens, we have the
inverse: the channel dimension defines the token scope, and the spatial
dimension defines the token feature dimension. We further group tokens along
the sequence direction for both spatial and channel tokens to maintain the
linear complexity of the entire model. We show that these two self-attentions
complement each other: (i) since each channel token contains an abstract
representation of the entire image, the channel attention naturally captures
global interactions and representations by taking all spatial positions into
account when computing attention scores between channels; (ii) the spatial
attention refines the local representations by performing fine-grained
interactions across spatial locations, which in turn helps the global
information modeling in channel attention. Extensive experiments show our DaViT
achieves state-of-the-art performance on four different tasks with efficient
computations. Without extra data, DaViT-Tiny, DaViT-Small, and DaViT-Base
achieve 82.8%, 84.2%, and 84.6% top-1 accuracy on ImageNet-1K with 28.3M,
49.7M, and 87.9M parameters, respectively. When we further scale up DaViT with
1.5B weakly supervised image and text pairs, DaViT-Gaint reaches 90.4% top-1
accuracy on ImageNet-1K. Code is available at https://github.com/dingmyu/davit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment. (arXiv:2204.03646v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03646">
<div class="article-summary-box-inner">
<span><p>Most existing action quality assessment methods rely on the deep features of
an entire video to predict the score, which is less reliable due to the
non-transparent inference process and poor interpretability. We argue that
understanding both high-level semantics and internal temporal structures of
actions in competitive sports videos is the key to making predictions accurate
and interpretable. Towards this goal, we construct a new fine-grained dataset,
called FineDiving, developed on diverse diving events with detailed annotations
on action procedures. We also propose a procedure-aware approach for action
quality assessment, learned by a new Temporal Segmentation Attention module.
Specifically, we propose to parse pairwise query and exemplar action instances
into consecutive steps with diverse semantic and temporal correspondences. The
procedure-aware cross-attention is proposed to learn embeddings between query
and exemplar steps to discover their semantic, spatial, and temporal
correspondences, and further serve for fine-grained contrastive regression to
derive a reliable scoring mechanism. Extensive experiments demonstrate that our
approach achieves substantial improvements over state-of-the-art methods with
better interpretability. The dataset and code are available at
\url{https://github.com/xujinglin/FineDiving}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting CLIP For Phrase Localization Without Further Training. (arXiv:2204.03647v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03647">
<div class="article-summary-box-inner">
<span><p>Supervised or weakly supervised methods for phrase localization (textual
grounding) either rely on human annotations or some other supervised models,
e.g., object detectors. Obtaining these annotations is labor-intensive and may
be difficult to scale in practice. We propose to leverage recent advances in
contrastive language-vision models, CLIP, pre-trained on image and caption
pairs collected from the internet. In its original form, CLIP only outputs an
image-level embedding without any spatial resolution. We adapt CLIP to generate
high-resolution spatial feature maps. Importantly, we can extract feature maps
from both ViT and ResNet CLIP model while maintaining the semantic properties
of an image embedding. This provides a natural framework for phrase
localization. Our method for phrase localization requires no human annotations
or additional training. Extensive experiments show that our method outperforms
existing no-training methods in zero-shot phrase localization, and in some
cases, it even outperforms supervised methods. Code is available at
https://github.com/pals-ttic/adapting-CLIP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage. (arXiv:2204.03648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03648">
<div class="article-summary-box-inner">
<span><p>Outdoor portrait photographs are often marred by the harsh shadows cast under
direct sunlight. To resolve this, one can use post-capture lighting
manipulation techniques, but these methods either require complex hardware
(e.g., a light stage) to capture each individual, or rely on image-based priors
and thus fail to reconstruct many of the subtle facial details that vary from
person to person. In this paper, we present SunStage, a system for accurate,
individually-tailored, and lightweight reconstruction of facial geometry and
reflectance that can be used for general portrait relighting with cast shadows.
Our method only requires the user to capture a selfie video outdoors, rotating
in place, and uses the varying angles between the sun and the face as
constraints in the joint reconstruction of facial geometry, reflectance
properties, and lighting parameters. Aside from relighting, we show that our
reconstruction can be used for applications like reflectance editing and view
synthesis. Results and interactive demos are available at
https://grail.cs.washington.edu/projects/sunstage/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Prompt Learning for Vision-Language Models. (arXiv:2204.03649v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03649">
<div class="article-summary-box-inner">
<span><p>Contrastive vision-language models like CLIP have shown great progress in
zero-shot transfer learning. This new paradigm uses large-scale image-text
pairs for training and aligns images and texts in a common embedding space. In
the inference stage, the proper text description, known as prompt, needs to be
carefully designed for zero-shot transfer. To avoid laborious prompt
engineering and simultaneously improve transfer performance, recent works such
as CoOp, CLIP-Adapter and Tip-Adapter propose to adapt vision-language models
for downstream image recognition tasks by either optimizing the continuous
prompt representations or training an additional adapter network on top of the
pre-trained vision-language models on a small set of labeled data. Though
promising improvements are achieved, using labeled images from target datasets
may violate the intention of zero-shot transfer of pre-trained vision-language
models. In this paper, we propose an unsupervised prompt learning (UPL)
framework, which does not require any annotations of the target dataset, to
improve the zero-shot transfer of CLIP-like vision-language models.
Experimentally, for zero-shot transfer, our UPL outperforms original CLIP with
prompt engineering and on ImageNet as well as other 10 datasets. An enhanced
version of UPL is even on par with the 8-shot CoOp and the 8-shot TIP-Adapter
on most datasets while our method does not need any labeled images for
training. Code and models are available at
https://github.com/tonyhuang2022/UPL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Hard Examples for Pixel-wise Classification. (arXiv:1812.05447v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.05447">
<div class="article-summary-box-inner">
<span><p>Pixel-wise classification in remote sensing identifies entities in
large-scale satellite-based images at the pixel level. Few fully annotated
large-scale datasets for pixel-wise classification exist due to the challenges
of annotating individual pixels. Training data scarcity inevitably ensues from
the annotation challenge, leading to overfitting classifiers and degraded
classification performance. The lack of annotated pixels also necessarily
results in few hard examples of various entities critical for generating a
robust classification hyperplane. To overcome the problem of the data scarcity
and lack of hard examples in training, we introduce a two-step hard example
generation (HEG) approach that first generates hard example candidates and then
mines actual hard examples. In the first step, a generator that creates hard
example candidates is learned via the adversarial learning framework by fooling
a discriminator and a pixel-wise classification model at the same time. In the
second step, mining is performed to build a fixed number of hard examples from
a large pool of real and artificially generated examples. To evaluate the
effectiveness of the proposed HEG approach, we design a 9-layer fully
convolutional network suitable for pixel-wise classification. Experiments show
that using generated hard examples from the proposed HEG approach improves the
pixel-wise classification model's accuracy on red tide detection and
hyperspectral image classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Driven Multi-Camera Pedestrian Detection. (arXiv:1812.10779v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.10779">
<div class="article-summary-box-inner">
<span><p>In the current worldwide situation, pedestrian detection has reemerged as a
pivotal tool for intelligent video-based systems aiming to solve tasks such as
pedestrian tracking, social distancing monitoring or pedestrian mass counting.
Pedestrian detection methods, even the top performing ones, are highly
sensitive to occlusions among pedestrians, which dramatically degrades their
performance in crowded scenarios. The generalization of multi-camera set-ups
permits to better confront occlusions by combining information from different
viewpoints. In this paper, we present a multi-camera approach to globally
combine pedestrian detections leveraging automatically extracted scene context.
Contrarily to the majority of the methods of the state-of-the-art, the proposed
approach is scene-agnostic, not requiring a tailored adaptation to the target
scenario\textemdash e.g., via fine-tunning. This noteworthy attribute does not
require \textit{ad hoc} training with labelled data, expediting the deployment
of the proposed method in real-world situations. Context information, obtained
via semantic segmentation, is used 1) to automatically generate a common Area
of Interest for the scene and all the cameras, avoiding the usual need of
manually defining it; and 2) to obtain detections for each camera by solving a
global optimization problem that maximizes coherence of detections both in each
2D image and in the 3D scene. This process yields tightly-fitted bounding boxes
that circumvent occlusions or miss-detections. Experimental results on five
publicly available datasets show that the proposed approach outperforms
state-of-the-art multi-camera pedestrian detectors, even some specifically
trained on the target scenario, signifying the versatility and robustness of
the proposed method without requiring ad-hoc annotations nor human-guided
configuration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GGNN: Graph-based GPU Nearest Neighbor Search. (arXiv:1912.01059v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.01059">
<div class="article-summary-box-inner">
<span><p>Approximate nearest neighbor (ANN) search in high dimensions is an integral
part of several computer vision systems and gains importance in deep learning
with explicit memory representations. Since PQT, FAISS, and SONG started to
leverage the massive parallelism offered by GPUs, GPU-based implementations are
a crucial resource for today's state-of-the-art ANN methods. While most of
these methods allow for faster queries, less emphasis is devoted to
accelerating the construction of the underlying index structures. In this
paper, we propose a novel GPU-friendly search structure based on nearest
neighbor graphs and information propagation on graphs. Our method is designed
to take advantage of GPU architectures to accelerate the hierarchical
construction of the index structure and for performing the query. Empirical
evaluation shows that GGNN significantly surpasses the state-of-the-art CPU-
and GPU-based systems in terms of build-time, accuracy and search speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator Search. (arXiv:2011.02166v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.02166">
<div class="article-summary-box-inner">
<span><p>The convolutional neural network has achieved great success in fulfilling
computer vision tasks despite large computation overhead against efficient
deployment. Structured (channel) pruning is usually applied to reduce the model
redundancy while preserving the network structure, such that the pruned network
can be easily deployed in practice. However, existing structured pruning
methods require hand-crafted rules which may lead to tremendous pruning space.
In this paper, we introduce Differentiable Annealing Indicator Search (DAIS)
that leverages the strength of neural architecture search in the channel
pruning and automatically searches for the effective pruned model with given
constraints on computation overhead. Specifically, DAIS relaxes the binarized
channel indicators to be continuous and then jointly learns both indicators and
model parameters via bi-level optimization. To bridge the non-negligible
discrepancy between the continuous model and the target binarized model, DAIS
proposes an annealing-based procedure to steer the indicator convergence
towards binarized states. Moreover, DAIS designs various regularizations based
on a priori structural knowledge to control the pruning sparsity and to improve
model performance. Experimental results show that DAIS outperforms
state-of-the-art pruning methods on CIFAR-10, CIFAR-100, and ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-informed neural networks for myocardial perfusion MRI quantification. (arXiv:2011.12844v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12844">
<div class="article-summary-box-inner">
<span><p>Tracer-kinetic models allow for the quantification of kinetic parameters such
as blood flow from dynamic contrast-enhanced magnetic resonance (MR) images.
Fitting the observed data with multi-compartment exchange models is desirable,
as they are physiologically plausible and resolve directly for blood flow and
microvascular function. However, the reliability of model fitting is limited by
the low signal-to-noise ratio, temporal resolution, and acquisition length.
This may result in inaccurate parameter estimates.
</p>
<p>This study introduces physics-informed neural networks (PINNs) as a means to
perform myocardial perfusion MR quantification, which provides a versatile
scheme for the inference of kinetic parameters. These neural networks can be
trained to fit the observed perfusion MR data while respecting the underlying
physical conservation laws described by a multi-compartment exchange model.
Here, we provide a framework for the implementation of PINNs in myocardial
perfusion MR.
</p>
<p>The approach is validated both in silico and in vivo. In the in silico study,
an overall reduction in mean-squared error with the ground-truth parameters was
observed compared to a standard non-linear least squares fitting approach. The
in vivo study demonstrates that the method produces parameter values comparable
to those previously found in literature, as well as providing parameter maps
which match the clinical diagnosis of patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08850">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has recently seen tremendous success in self-supervised
learning. So far, however, it is largely unclear why the learned
representations generalize so effectively to a large variety of downstream
tasks. We here prove that feedforward models trained with objectives belonging
to the commonly used InfoNCE family learn to implicitly invert the underlying
generative model of the observed data. While the proofs make certain
statistical assumptions about the generative model, we observe empirically that
our findings hold even if these assumptions are severely violated. Our theory
highlights a fundamental connection between contrastive learning, generative
modeling, and nonlinear independent component analysis, thereby furthering our
understanding of the learned representations as well as providing a theoretical
foundation to derive more effective contrastive losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Federated Peer Learning for Skin Lesion Classification. (arXiv:2103.03703v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03703">
<div class="article-summary-box-inner">
<span><p>Globally, Skin carcinoma is among the most lethal diseases. Millions of
people are diagnosed with this cancer every year. Sill, early detection can
decrease the medication cost and mortality rate substantially. The recent
improvement in automated cancer classification using deep learning methods has
reached a human-level performance requiring a large amount of annotated data
assembled in one location, yet, finding such conditions usually is not
feasible. Recently, federated learning (FL) has been proposed to train
decentralized models in a privacy-preserved fashion depending on labeled data
at the client-side, which is usually not available and costly. To address this,
we propose \verb!FedPerl!, a semi-supervised federated learning method. Our
method is inspired by peer learning from educational psychology and ensemble
averaging from committee machines. FedPerl builds communities based on clients'
similarities. Then it encourages communities members to learn from each other
to generate more accurate pseudo labels for the unlabeled data. We also
proposed the peer anonymization (PA) technique to anonymize clients. As a core
component of our method, PA is orthogonal to other methods without additional
complexity and reduces the communication cost while enhancing performance.
Finally, we propose a dynamic peer-learning policy that controls the learning
stream to avoid any degradation in the performance, especially for individual
clients. Our experimental setup consists of 71,000 skin lesion images collected
from 5 publicly available datasets. We test our method in four different
scenarios in SSFL. With few annotated data, FedPerl is on par with a
state-of-the-art method in skin lesion classification in the standard setup
while outperforming SSFLs and the baselines by 1.8% and 15.8%, respectively.
Also, it generalizes better to unseen clients while being less sensitive to
noisy ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Clustering of Hyperspectral Images Through Spectral-Spatial Diffusion Geometry. (arXiv:2103.15783v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15783">
<div class="article-summary-box-inner">
<span><p>Clustering algorithms partition a dataset into groups of similar points. The
primary contribution of this article is the Multiscale Spatially-Regularized
Diffusion Learning (M-SRDL) clustering algorithm, which uses
spatially-regularized diffusion distances to efficiently and accurately learn
multiple scales of latent structure in hyperspectral images. The M-SRDL
clustering algorithm extracts clusterings at many scales from a hyperspectral
image and outputs these clusterings' variation of information-barycenter as an
exemplar for all underlying cluster structure. We show that incorporating
spatial regularization into a multiscale clustering framework results in
smoother and more coherent clusters when applied to hyperspectral data,
yielding more accurate clustering labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A tutorial on $\mathbf{SE}(3)$ transformation parameterizations and on-manifold optimization. (arXiv:2103.15980v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15980">
<div class="article-summary-box-inner">
<span><p>An arbitrary rigid transformation in $\mathbf{SE}(3)$ can be separated into
two parts, namely, a translation and a rigid rotation. This technical report
reviews, under a unifying viewpoint, three common alternatives to representing
the rotation part: sets of three (yaw-pitch-roll) Euler angles, orthogonal
rotation matrices from $\mathbf{SO}(3)$ and quaternions. It will be described:
(i) the equivalence between these representations and the formulas for
transforming one to each other (in all cases considering the translational and
rotational parts as a whole), (ii) how to compose poses with poses and poses
with points in each representation and (iii) how the uncertainty of the poses
(when modeled as Gaussian distributions) is affected by these transformations
and compositions. Some brief notes are also given about the Jacobians required
to implement least-squares optimization on manifolds, an very promising
approach in recent engineering literature. The text reflects which MRPT C++
library functions implement each of the described algorithms. All formulas and
their implementation have been thoroughly validated by means of unit testing
and numerical estimation of the Jacobians
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14836">
<div class="article-summary-box-inner">
<span><p>Representations of the world environment play a crucial role in artificial
intelligence. It is often inefficient to conduct reasoning and inference
directly in the space of raw sensory representations, such as pixel values of
images. Representation learning allows us to automatically discover suitable
representations from raw sensory data. For example, given raw sensory data, a
deep neural network learns nonlinear representations at its hidden layers,
which are subsequently used for classification at its output layer. This
happens implicitly during training through minimizing a supervised or
unsupervised loss. In this paper, we study the dynamics of such implicit
nonlinear representation learning. We identify a pair of a new assumption and a
novel condition, called the common model structure assumption and the
data-architecture alignment condition. Under the common model structure
assumption, the data-architecture alignment condition is shown to be sufficient
for the global convergence and necessary for the global optimality. Moreover,
our theory explains how and when increasing the network size does and does not
improve the training behaviors in the practical regime. Our results provide
practical guidance for designing a model structure: e.g., the common model
structure assumption can be used as a justification for using a particular
model structure instead of others. We also derive a new training framework,
which satisfies the data-architecture alignment condition by automatically
modifying any given training algorithm. Given a standard training algorithm,
the framework running its modified version is empirically shown to maintain
competitive test performances while providing global convergence guarantees for
deep residual neural networks with convolutions, skip connections, and batch
normalization with datasets, including MNIST, CIFAR-10, CIFAR-100, Semeion,
KMNIST and SVHN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scarce Data Driven Deep Learning of Drones via Generalized Data Distribution Space. (arXiv:2108.08244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08244">
<div class="article-summary-box-inner">
<span><p>Increased drone proliferation in civilian and professional settings has
created new threat vectors for airports and national infrastructures. The
economic damage for a single major airport from drone incursions is estimated
to be millions per day. Due to the lack of diverse drone training data,
accurate training of deep learning detection algorithms under scarce data is an
open challenge. Existing methods largely rely on collecting diverse and
comprehensive experimental drone footage data, artificially induced data
augmentation, transfer and meta-learning, as well as physics-informed learning.
However, these methods cannot guarantee capturing diverse drone designs and
fully understanding the deep feature space of drones. Here, we show how
understanding the general distribution of the drone data via a Generative
Adversarial Network (GAN) and explaining the missing features using Topological
Data Analysis (TDA) - can allow us to acquire missing data to achieve rapid and
more accurate learning. We demonstrate our results on a drone image dataset,
which contains both real drone images as well as simulated images from
computer-aided design. When compared to random data collection (usual practice
- discriminator accuracy of 94.67\% after 200 epochs), our proposed GAN-TDA
informed data collection method offers a significant 4\% improvement (99.42\%
after 200 epochs). We believe that this approach of exploiting general data
distribution knowledge form neural networks can be applied to a wide range of
scarce data open challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Cycle-consistent Generative Adversarial Networks for Pan-sharpening. (arXiv:2109.09395v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09395">
<div class="article-summary-box-inner">
<span><p>Deep learning based pan-sharpening has received significant research interest
in recent years. Most of existing methods fall into the supervised learning
framework in which they down-sample the multi-spectral (MS) and panchromatic
(PAN) images and regard the original MS images as ground truths to form
training samples. Although impressive performance could be achieved, they have
difficulties generalizing to the original full-scale images due to the scale
gap, which makes them lack of practicability. In this paper, we propose an
unsupervised generative adversarial framework that learns from the full-scale
images without the ground truths to alleviate this problem. We extract the
modality-specific features from the PAN and MS images with a two-stream
generator, perform fusion in the feature domain, and then reconstruct the
pan-sharpened images. Furthermore, we introduce a novel hybrid loss based on
the cycle-consistency and adversarial scheme to improve the performance.
Comparison experiments with the state-of-the-art methods are conducted on
GaoFen-2 and WorldView-3 satellites. Results demonstrate that the proposed
method can greatly improve the pan-sharpening performance on the full-scale
images, which clearly show its practical value. Codes are available at
https://github.com/zhysora/UCGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning based Medical Image Deepfake Detection: A Comparative Study. (arXiv:2109.12800v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12800">
<div class="article-summary-box-inner">
<span><p>Deep generative networks in recent years have reinforced the need for caution
while consuming various modalities of digital information. One avenue of
deepfake creation is aligned with injection and removal of tumors from medical
scans. Failure to detect medical deepfakes can lead to large setbacks on
hospital resources or even loss of life. This paper attempts to address the
detection of such attacks with a structured case study. Specifically, we
evaluate eight different machine learning algorithms, which including three
conventional machine learning methods, support vector machine, random forest,
decision tree, and five deep learning models, DenseNet121, DenseNet201,
ResNet50, ResNet101, VGG19, on distinguishing between tampered and untampered
images.For deep learning models, the five models are used for feature
extraction, then fine-tune for each pre-trained model is performed. The
findings of this work show near perfect accuracy in detecting instances of
tumor injections and removals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05319">
<div class="article-summary-box-inner">
<span><p>Data-driven fault detection has been regarded as a 3D image segmentation
task. The models trained from synthetic data are difficult to generalize in
some surveys. Recently, training 3D fault segmentation using sparse manual 2D
slices is thought to yield promising results, but manual labeling has many
false negative labels (abnormal annotations), which is detrimental to training
and consequently to detection performance. Motivated to train 3D fault
segmentation networks under sparse 2D labels while suppressing false negative
labels, we analyze the training process gradient and propose the Mask Dice (MD)
loss. Moreover, the fault is an edge feature, and current encoder-decoder
architectures widely used for fault detection (e.g., U-shape network) are not
conducive to edge representation. Consequently, Fault-Net is proposed, which is
designed for the characteristics of faults, employs high-resolution propagation
features, and embeds MultiScale Compression Fusion block to fuse multi-scale
information, which allows the edge information to be fully preserved during
propagation and fusion, thus enabling advanced performance via few
computational resources. Experimental demonstrates that MD loss supports the
inclusion of human experience in training and suppresses false negative labels
therein, enabling baseline models to improve performance and generalize to more
surveys. Fault-Net is capable to provide a more stable and reliable
interpretation of faults, it uses extremely low computational resources and
inference is significantly faster than other models. Our method indicates
optimal performance in comparison with several mainstream methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ByteTrack: Multi-Object Tracking by Associating Every Detection Box. (arXiv:2110.06864v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06864">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) aims at estimating bounding boxes and identities
of objects in videos. Most methods obtain identities by associating detection
boxes whose scores are higher than a threshold. The objects with low detection
scores, e.g. occluded objects, are simply thrown away, which brings
non-negligible true object missing and fragmented trajectories. To solve this
problem, we present a simple, effective and generic association method,
tracking by associating almost every detection box instead of only the high
score ones. For the low score detection boxes, we utilize their similarities
with tracklets to recover true objects and filter out the background
detections. When applied to 9 different state-of-the-art trackers, our method
achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To
put forwards the state-of-the-art performance of MOT, we design a simple and
strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3
IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a
single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20,
HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with
deploy versions and tutorials of applying to other trackers are released at
https://github.com/ifzhang/ByteTrack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Lightweight Single Object Tracking with UHP-SOT++. (arXiv:2111.07548v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07548">
<div class="article-summary-box-inner">
<span><p>An unsupervised, lightweight and high-performance single object tracker,
called UHP-SOT, was proposed by Zhou et al. recently. As an extension, we
present an enhanced version and name it UHP-SOT++ in this work. Built upon the
foundation of the discriminative-correlation-filters-based (DCF-based) tracker,
two new ingredients are introduced in UHP-SOT and UHP-SOT++: 1) background
motion modeling and 2) object box trajectory modeling. The main difference
between UHP-SOT and UHP-SOT++ is the fusion strategy of proposals from three
models (i.e., DCF, background motion and object box trajectory models). An
improved fusion strategy is adopted by UHP-SOT++ for more robust tracking
performance against large-scale tracking datasets. Our second contribution lies
in an extensive evaluation of the performance of state-of-the-art supervised
and unsupervised methods by testing them on four SOT benchmark datasets -
OTB2015, TC128, UAV123 and LaSOT. Experiments show that UHP-SOT++ outperforms
all previous unsupervised methods and several deep-learning (DL) methods in
tracking accuracy. Since UHP-SOT++ has extremely small model size, high
tracking performance, and low computational complexity (operating at a rate of
20 FPS on an i5 CPU even without code optimization), it is an ideal solution in
real-time object tracking on resource-limited platforms. Based on the
experimental results, we compare pros and cons of supervised and unsupervised
trackers and provide a new perspective to understand the performance gap
between supervised and unsupervised methods, which is the third contribution of
this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StylePart: Image-based Shape Part Manipulation. (arXiv:2111.10520v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10520">
<div class="article-summary-box-inner">
<span><p>Due to a lack of image-based "part controllers", shape manipulation of
man-made shape images, such as resizing the backrest of a chair or replacing a
cup handle is not intuitive. To tackle this problem, we present StylePart, a
framework that enables direct shape manipulation of an image by leveraging
generative models of both images and 3D shapes. Our key contribution is a
shape-consistent latent mapping function that connects the image generative
latent space and the 3D man-made shape attribute latent space. Our method
"forwardly maps" the image content to its corresponding 3D shape attributes,
where the shape part can be easily manipulated. The attribute codes of the
manipulated 3D shape are then "backwardly mapped" to the image latent code to
obtain the final manipulated image. We demonstrate our approach through various
manipulation tasks, including part replacement, part resizing, and viewpoint
manipulation, and evaluate its effectiveness through extensive ablation
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred Objects in Videos. (arXiv:2111.14465v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14465">
<div class="article-summary-box-inner">
<span><p>We propose a method for jointly estimating the 3D motion, 3D shape, and
appearance of highly motion-blurred objects from a video. To this end, we model
the blurred appearance of a fast moving object in a generative fashion by
parametrizing its 3D position, rotation, velocity, acceleration, bounces,
shape, and texture over the duration of a predefined time window spanning
multiple frames. Using differentiable rendering, we are able to estimate all
parameters by minimizing the pixel-wise reprojection error to the input video
via backpropagating through a rendering pipeline that accounts for motion blur
by averaging the graphics output over short time intervals. For that purpose,
we also estimate the camera exposure gap time within the same optimization. To
account for abrupt motion changes like bounces, we model the motion trajectory
as a piece-wise polynomial, and we are able to estimate the specific time of
the bounce at sub-frame accuracy. Experiments on established benchmark datasets
demonstrate that our method outperforms previous methods for fast moving object
deblurring and 3D reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPIN: Simplifying Polar Invariance for Neural networks Application to vision-based irradiance forecasting. (arXiv:2111.14507v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14507">
<div class="article-summary-box-inner">
<span><p>Translational invariance induced by pooling operations is an inherent
property of convolutional neural networks, which facilitates numerous computer
vision tasks such as classification. Yet to leverage rotational invariant
tasks, convolutional architectures require specific rotational invariant layers
or extensive data augmentation to learn from diverse rotated versions of a
given spatial configuration. Unwrapping the image into its polar coordinates
provides a more explicit representation to train a convolutional architecture
as the rotational invariance becomes translational, hence the visually distinct
but otherwise equivalent rotated versions of a given scene can be learnt from a
single image. We show with two common vision-based solar irradiance forecasting
challenges (i.e. using ground-taken sky images or satellite images), that this
preprocessing step significantly improves prediction results by standardising
the scene representation, while decreasing training time by a factor of 4
compared to augmenting data with rotations. In addition, this transformation
magnifies the area surrounding the centre of the rotation, leading to more
accurate short-term irradiance predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation. (arXiv:2111.14826v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14826">
<div class="article-summary-box-inner">
<span><p>The nonuniform quantization strategy for compressing neural networks usually
achieves better performance than its counterpart, i.e., uniform strategy, due
to its superior representational capacity. However, many nonuniform
quantization methods overlook the complicated projection process in
implementing the nonuniformly quantized weights/activations, which incurs
non-negligible time and space overhead in hardware deployment. In this study,
we propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can
maintain the strong representation ability of nonuniform methods while being
hardware-friendly and efficient as the uniform quantization for model
inference. We achieve this through learning the flexible in-equidistant input
thresholds to better fit the underlying distribution while quantizing these
real-valued inputs into equidistant output levels. To train the quantized
network with learnable input thresholds, we introduce a generalized
straight-through estimator (G-STE) for intractable backward derivative
calculation w.r.t. threshold parameters. Additionally, we consider entropy
preserving regularization to further reduce information loss in weight
quantization. Even under this adverse constraint of imposing uniformly
quantized weights and activations, our N2UQ outperforms state-of-the-art
nonuniform quantization methods by 0.5~1.7 on ImageNet, demonstrating the
contribution of N2UQ design. Code and models are available at:
https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Depth Priors for Neural Radiance Fields from Sparse Input Views. (arXiv:2112.03288v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03288">
<div class="article-summary-box-inner">
<span><p>Neural radiance fields (NeRF) encode a scene into a neural representation
that enables photo-realistic rendering of novel views. However, a successful
reconstruction from RGB images requires a large number of input views taken
under static conditions - typically up to a few hundred images for room-size
scenes. Our method aims to synthesize novel views of whole rooms from an order
of magnitude fewer images. To this end, we leverage dense depth priors in order
to constrain the NeRF optimization. First, we take advantage of the sparse
depth data that is freely available from the structure from motion (SfM)
preprocessing step used to estimate camera poses. Second, we use depth
completion to convert these sparse points into dense depth maps and uncertainty
estimates, which are used to guide NeRF optimization. Our method enables
data-efficient novel view synthesis on challenging indoor scenes, using as few
as 18 images for an entire scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07225">
<div class="article-summary-box-inner">
<span><p>The long-tailed class distribution in visual recognition tasks poses great
challenges for neural networks on how to handle the biased predictions between
head and tail classes, i.e., the model tends to classify tail classes as head
classes. While existing research focused on data resampling and loss function
engineering, in this paper, we take a different perspective: the classification
margins. We study the relationship between the margins and logits
(classification scores) and empirically observe the biased margins and the
biased logits are positively correlated. We propose MARC, a simple yet
effective MARgin Calibration function to dynamically calibrate the biased
margins for unbiased logits. We validate MARC through extensive experiments on
common long-tailed benchmarks including CIFAR-LT, ImageNet-LT, Places-LT, and
iNaturalist-LT. Experimental results demonstrate that our MARC achieves
favorable results on these benchmarks. In addition, MARC is extremely easy to
implement with just three lines of code. We hope this simple method will
motivate people to rethink the biased margins and biased logits in long-tailed
visual recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An effective coaxiality measurement for twist drill based on line structured light sensor. (arXiv:2112.09873v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09873">
<div class="article-summary-box-inner">
<span><p>Aiming at the accurate and effective coaxiality measurement for twist drill
with irregular surface, an optical measurement mechanism is proposed in this
paper. First, A high-precision rotation instrument based on four core units is
designed, which can obtain the 3-D point cloud data of full angle for the twist
drill. Second, in the data processing stage, an improved robust Gaussian
mixture model is established for accurate and rapid blade back segmentation. To
improve measurement efficiency, a rapid reconstruction method of the twist
drill axis based on orthogonal synthesis is provided to locate the axial
position of the maximum deviation from the benchmark by utilizing the extracted
blade back data. Finally, by calculating the maximum radial Euclidean distance
from the benchmark, the coaxiality error of the twist drill is obtained.
Comparing with other measurement methods, experimental results show that our
proposed method is effective with high precision of 3 um and high efficiency of
less than 3 s/pc. The result demonstrate that the proposed method is effective,
robust and automatic, it can be applied in many actual industrial scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Raw High-Definition Radar for Multi-Task Learning. (arXiv:2112.10646v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10646">
<div class="article-summary-box-inner">
<span><p>With their robustness to adverse weather conditions and ability to measure
speeds, radar sensors have been part of the automotive landscape for more than
two decades. Recent progress toward High Definition (HD) Imaging radar has
driven the angular resolution below the degree, thus approaching laser scanning
performance. However, the amount of data a HD radar delivers and the
computational cost to estimate the angular positions remain a challenge. In
this paper, we propose a novel HD radar sensing model, FFT-RadNet, that
eliminates the overhead of computing the range-azimuth-Doppler 3D tensor,
learning instead to recover angles from a range-Doppler spectrum. FFT-RadNet is
trained both to detect vehicles and to segment free driving space. On both
tasks, it competes with the most recent radar-based models while requiring less
compute and memory. Also, we collected and annotated 2-hour worth of raw data
from synchronized automotive-grade sensors (camera, laser, HD radar) in various
environments (city street, highway, countryside road). This unique dataset,
nick-named RADIal for "Radar, Lidar et al.", is available at
https://github.com/valeoai/RADIal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metrics for saliency map evaluation of deep learning explanation methods. (arXiv:2201.13291v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13291">
<div class="article-summary-box-inner">
<span><p>Due to the black-box nature of deep learning models, there is a recent
development of solutions for visual explanations of CNNs. Given the high cost
of user studies, metrics are necessary to compare and evaluate these different
methods. In this paper, we critically analyze the Deletion Area Under Curve
(DAUC) and Insertion Area Under Curve (IAUC) metrics proposed by Petsiuk et al.
(2018). These metrics were designed to evaluate the faithfulness of saliency
maps generated by generic methods such as Grad-CAM or RISE. First, we show that
the actual saliency score values given by the saliency map are ignored as only
the ranking of the scores is taken into account. This shows that these metrics
are insufficient by themselves, as the visual appearance of a saliency map can
change significantly without the ranking of the scores being modified.
Secondly, we argue that during the computation of DAUC and IAUC, the model is
presented with images that are out of the training distribution which might
lead to an unreliable behavior of the model being explained. To complement
DAUC/IAUC, we propose new metrics that quantify the sparsity and the
calibration of explanation methods, two previously unstudied properties.
Finally, we give general remarks about the metrics studied in this paper and
discuss how to evaluate them in a user study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04175">
<div class="article-summary-box-inner">
<span><p>Multi-institutional efforts can facilitate training of deep MRI
reconstruction models, albeit privacy risks arise during cross-site sharing of
imaging data. Federated learning (FL) has recently been introduced to address
privacy concerns by enabling distributed training without transfer of imaging
data. Existing FL methods for MRI reconstruction employ conditional models to
map from undersampled to fully-sampled acquisitions via explicit knowledge of
the imaging operator. Since conditional models generalize poorly across
different acceleration rates or sampling densities, imaging operators must be
fixed between training and testing, and they are typically matched across
sites. To improve generalization and flexibility in multi-institutional
collaborations, here we introduce a novel method for MRI reconstruction based
on Federated learning of Generative IMage Priors (FedGIMP). FedGIMP leverages a
two-stage approach: cross-site learning of a generative MRI prior, and
subject-specific injection of the imaging operator. The global MRI prior is
learned via an unconditional adversarial model that synthesizes high-quality MR
images based on latent variables. Specificity in the prior is preserved via a
mapper subnetwork that produces site-specific latents. During inference, the
prior is combined with subject-specific imaging operators to enable
reconstruction, and further adapted to individual test samples by minimizing
data-consistency loss. Comprehensive experiments on multi-institutional
datasets clearly demonstrate enhanced generalization performance of FedGIMP
against site-specific and federated methods based on conditional models, as
well as traditional reconstruction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection. (arXiv:2202.06934v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06934">
<div class="article-summary-box-inner">
<span><p>Detection of small objects and objects far away in the scene is a major
challenge in surveillance applications. Such objects are represented by small
number of pixels in the image and lack sufficient details, making them
difficult to detect using conventional detectors. In this work, an open-source
framework called Slicing Aided Hyper Inference (SAHI) is proposed that provides
a generic slicing aided inference and fine-tuning pipeline for small object
detection. The proposed technique is generic in the sense that it can be
applied on top of any available object detector without any fine-tuning.
Experimental evaluations, using object detection baselines on the Visdrone and
xView aerial object detection datasets show that the proposed inference method
can increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and
TOOD detectors, respectively. Moreover, the detection accuracy can be further
increased with a slicing aided fine-tuning, resulting in a cumulative increase
of 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been
integrated with Detectron2, MMDetection and YOLOv5 models and it is publicly
available at https://github.com/obss/sahi.git .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminability-enforcing loss to improve representation learning. (arXiv:2202.07073v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07073">
<div class="article-summary-box-inner">
<span><p>During the training process, deep neural networks implicitly learn to
represent the input data samples through a hierarchy of features, where the
size of the hierarchy is determined by the number of layers. In this paper, we
focus on enforcing the discriminative power of the high-level representations,
that are typically learned by the deeper layers (closer to the output). To this
end, we introduce a new loss term inspired by the Gini impurity, which is aimed
at minimizing the entropy (increasing the discriminative power) of individual
high-level features with respect to the class labels. Although our Gini loss
induces highly-discriminative features, it does not ensure that the
distribution of the high-level features matches the distribution of the
classes. As such, we introduce another loss term to minimize the
Kullback-Leibler divergence between the two distributions. We conduct
experiments on two image classification data sets (CIFAR-100 and Caltech 101),
considering multiple neural architectures ranging from convolutional networks
(ResNet-17, ResNet-18, ResNet-50) to transformers (CvT). Our empirical results
show that integrating our novel loss terms into the training objective
consistently outperforms the models trained with cross-entropy alone, without
increasing the inference time at all.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. (arXiv:2202.07305v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07305">
<div class="article-summary-box-inner">
<span><p>Image narrative generation is a task to create a story from an image with a
subjective viewpoint. Given the importance of the subjective feelings of
writers, readers, and characters in storytelling, an image narrative generation
method should consider human emotion. In this study, we propose a novel method
of image narrative generation called ViNTER (Visual Narrative Transformer with
Emotion arc Representation), which takes "emotion arc" as input to capture a
sequence of emotional changes. Since emotion arcs represent the trajectory of
emotional change, it is expected that we can include detailed information about
the emotional changes in the story to the model. We present experimental
results of both automatic and manual evaluations on the Image Narrative dataset
and demonstrate the effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realistic Blur Synthesis for Learning Image Deblurring. (arXiv:2202.08771v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08771">
<div class="article-summary-box-inner">
<span><p>Training learning-based deblurring methods demands a tremendous amount of
blurred and sharp image pairs. Unfortunately, existing synthetic datasets are
not realistic enough, and deblurring models trained on them cannot handle real
blurred images effectively. While real datasets have recently been proposed,
they provide limited diversity of scenes and camera settings, and capturing
real datasets for diverse settings is still challenging. This paper analyzes
various factors that introduce differences between real and synthetic blurred
images, and presents a novel blur synthesis pipeline to synthesize more
realistic blur. We also present RSBlur, a novel dataset with real blurred
images and the corresponding sharp image sequences to enable detailed analysis
on the differences between real and synthetic blur. With our blur synthesis
pipeline and RSBlur dataset, we reveal the effects of different factors in blur
synthesis. We also show that our synthesis method can improve the deblurring
performance on real blurred images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03605">
<div class="article-summary-box-inner">
<span><p>We present DINO (\textbf{D}ETR with \textbf{I}mproved de\textbf{N}oising
anch\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in
this paper. DINO improves over previous DETR-like models in performance and
efficiency by using a contrastive way for denoising training, a mixed query
selection method for anchor initialization, and a look forward twice scheme for
box prediction. DINO achieves $48.3$AP in $12$ epochs and $51.0$AP in $36$
epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a
significant improvement of $\textbf{+4.9}$\textbf{AP} and
$\textbf{+2.4}$\textbf{AP}, respectively, compared to DN-DETR, the previous
best DETR-like model. DINO scales well in both model size and data size.
Without bells and whistles, after pre-training on the Objects365 dataset with a
SwinL backbone, DINO obtains the best results on both COCO \texttt{val2017}
($\textbf{63.2}$\textbf{AP}) and \texttt{test-dev}
(\textbf{$\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO
significantly reduces its model size and pre-training data size while achieving
better results. Our code will be available at
\url{https://github.com/IDEACVR/DINO}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible Amortized Variational Inference in qBOLD MRI. (arXiv:2203.05845v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05845">
<div class="article-summary-box-inner">
<span><p>Streamlined qBOLD acquisitions enable experimentally straightforward
observations of brain oxygen metabolism. $R_2^\prime$ maps are easily inferred;
however, the Oxygen extraction fraction (OEF) and deoxygenated blood volume
(DBV) are more ambiguously determined from the data. As such, existing
inference methods tend to yield very noisy and underestimated OEF maps, while
overestimating DBV.
</p>
<p>This work describes a novel probabilistic machine learning approach that can
infer plausible distributions of OEF and DBV. Initially, we create a model that
produces informative voxelwise prior distribution based on synthetic training
data. Contrary to prior work, we model the joint distribution of OEF and DBV
through a scaled multivariate logit-Normal distribution, which enables the
values to be constrained within a plausible range. The prior distribution model
is used to train an efficient amortized variational Bayesian inference model.
This model learns to infer OEF and DBV by predicting real image data, with few
training data required, using the signal equations as a forward model.
</p>
<p>We demonstrate that our approach enables the inference of smooth OEF and DBV
maps, with a physiologically plausible distribution that can be adapted through
specification of an informative prior distribution. Other benefits include
model comparison (via the evidence lower bound) and uncertainty quantification
for identifying image artefacts. Results are demonstrated on a small study
comparing subjects undergoing hyperventilation and at rest. We illustrate that
the proposed approach allows measurement of gray matter differences in OEF and
DBV and enables voxelwise comparison between conditions, where we observe
significant increases in OEF and $R_2^\prime$ during hyperventilation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data. (arXiv:2203.09475v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09475">
<div class="article-summary-box-inner">
<span><p>Vision-based segmentation of the robotic tool during robot-assisted surgery
enables downstream applications, such as augmented reality feedback, while
allowing for inaccuracies in robot kinematics. With the introduction of deep
learning, many methods were presented to solve instrument segmentation directly
and solely from images. While these approaches made remarkable progress on
benchmark datasets, fundamental challenges pertaining to their robustness
remain. We present CaRTS, a causality-driven robot tool segmentation algorithm,
that is designed based on a complementary causal model of the robot tool
segmentation task. Rather than directly inferring segmentation masks from
observed images, CaRTS iteratively aligns tool models with image observations
by updating the initially incorrect robot kinematic parameters through forward
kinematics and differentiable rendering to optimize image feature similarity
end-to-end. We benchmark CaRTS with competing techniques on both synthetic as
well as real data from the dVRK, generated in precisely controlled scenarios to
allow for counterfactual synthesis. On training-domain test data, CaRTS
achieves a Dice score of 93.4 that is preserved well (Dice score of 91.8) when
tested on counterfactual altered test data, exhibiting low brightness, smoke,
blood, and altered background patterns. This compares favorably to Dice scores
of 95.0 and 62.8, respectively, of a purely image-based method trained and
tested on the same data. Future work will involve accelerating CaRTS to achieve
video framerate and estimating the impact occlusion has in practice. Despite
these limitations, our results are promising: In addition to achieving high
segmentation accuracy, CaRTS provides estimates of the true robot kinematics,
which may benefit applications such as force estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Few-Shot Learning via Implanting and Compressing. (arXiv:2203.10297v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10297">
<div class="article-summary-box-inner">
<span><p>This work focuses on tackling the challenging but realistic visual task of
Incremental Few-Shot Learning (IFSL), which requires a model to continually
learn novel classes from only a few examples while not forgetting the base
classes on which it was pre-trained. Our study reveals that the challenges of
IFSL lie in both inter-class separation and novel-class representation. Dur to
intra-class variation, a novel class may implicitly leverage the knowledge from
multiple base classes to construct its feature representation. Hence, simply
reusing the pre-trained embedding space could lead to a scattered feature
distribution and result in category confusion. To address such issues, we
propose a two-step learning strategy referred to as \textbf{Im}planting and
\textbf{Co}mpressing (\textbf{IMCO}), which optimizes both feature space
partition and novel class reconstruction in a systematic manner. Specifically,
in the \textbf{Implanting} step, we propose to mimic the data distribution of
novel classes with the assistance of data-abundant base set, so that a model
could learn semantically-rich features that are beneficial for discriminating
between the base and other unseen classes. In the \textbf{Compressing} step, we
adapt the feature extractor to precisely represent each novel class for
enhancing intra-class compactness, together with a regularized parameter
updating rule for preventing aggressive model updating. Finally, we demonstrate
that IMCO outperforms competing baselines with a significant margin, both in
image classification task and more challenging object detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAN: a Segmentation-free Document Attention Network for Handwritten Document Recognition. (arXiv:2203.12273v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12273">
<div class="article-summary-box-inner">
<span><p>Unconstrained handwritten text recognition is a challenging computer vision
task. It is traditionally handled by a two-step approach combining line
segmentation followed by text line recognition. For the first time, we propose
an end-to-end segmentation-free architecture for the task of handwritten
document recognition: the Document Attention Network. In addition to the text
recognition, the model is trained to label text parts using begin and end tags
in an XML-like fashion. This model is made up of an FCN encoder for feature
extraction and a stack of transformer decoder layers for a recurrent
token-by-token prediction process. It takes whole text documents as input and
sequentially outputs characters, as well as logical layout tokens. Contrary to
the existing segmentation-based approaches, the model is trained without using
any segmentation label. We achieve competitive results on the READ 2016 dataset
at page level, as well as double-page level with a CER of 3.53% and 3.69%,
respectively. We also provide results for the RIMES 2009 dataset at page level,
reaching 4.54% of CER.
</p>
<p>We provide all source code and pre-trained model weights at
https://github.com/FactoDeepLearning/DAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-augmented histopathologic review using image analysis to optimize DNA yield and tumor purity from FFPE slides. (arXiv:2203.13948v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13948">
<div class="article-summary-box-inner">
<span><p>To achieve minimum DNA input and tumor purity requirements for
next-generation sequencing (NGS), pathologists visually estimate
macrodissection and slide count decisions. Misestimation may cause tissue waste
and increased laboratory costs. We developed an AI-augmented smart pathology
review system (SmartPath) to empower pathologists with quantitative metrics for
determining tissue extraction parameters. Using digitized H&amp;E-stained FFPE
slides as inputs, SmartPath segments tumors, extracts cell-based features, and
suggests macrodissection areas. To predict DNA yield per slide, the extracted
features are correlated with known DNA yields. Then, a pathologist-defined
target yield divided by the predicted DNA yield/slide gives the number of
slides to scrape. Following model development, an internal validation trial was
conducted within the Tempus Labs molecular sequencing laboratory. We evaluated
our system on 501 clinical colorectal cancer slides, where half received
SmartPath-augmented review and half traditional pathologist review. The
SmartPath cohort had 25% more DNA yields within a desired target range of
100-2000ng. The SmartPath system recommended fewer slides to scrape for large
tissue sections, saving tissue in these cases. Conversely, SmartPath
recommended more slides to scrape for samples with scant tissue sections,
helping prevent costly re-extraction due to insufficient extraction yield. A
statistical analysis was performed to measure the impact of covariates on the
results, offering insights on how to improve future applications of SmartPath.
Overall, the study demonstrated that AI-augmented histopathologic review using
SmartPath could decrease tissue waste, sequencing time, and laboratory costs by
optimizing DNA yields and tumor purity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimT: Handling Open-set Noise for Domain Adaptive Semantic Segmentation. (arXiv:2203.15202v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15202">
<div class="article-summary-box-inner">
<span><p>This paper studies a practical domain adaptive (DA) semantic segmentation
problem where only pseudo-labeled target data is accessible through a black-box
model. Due to the domain gap and label shift between two domains,
pseudo-labeled target data contains mixed closed-set and open-set label noises.
In this paper, we propose a simplex noise transition matrix (SimT) to model the
mixed noise distributions in DA semantic segmentation and formulate the problem
as estimation of SimT. By exploiting computational geometry analysis and
properties of segmentation, we design three complementary regularizers, i.e.
volume regularization, anchor guidance, convex guarantee, to approximate the
true SimT. Specifically, volume regularization minimizes the volume of simplex
formed by rows of the non-square SimT, which ensures outputs of segmentation
model to fit into the ground truth label distribution. To compensate for the
lack of open-set knowledge, anchor guidance and convex guarantee are devised to
facilitate the modeling of open-set noise distribution and enhance the
discriminative feature learning among closed-set and open-set classes. The
estimated SimT is further utilized to correct noise issues in pseudo labels and
promote the generalization ability of segmentation model on target domain data.
Extensive experimental results demonstrate that the proposed SimT can be
flexibly plugged into existing DA methods to boost the performance. The source
code is available at https://github.com/CityU-AIM-Group/SimT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pay Attention to Hidden States for Video Deblurring: Ping-Pong Recurrent Neural Networks and Selective Non-Local Attention. (arXiv:2203.16063v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16063">
<div class="article-summary-box-inner">
<span><p>Video deblurring models exploit information in the neighboring frames to
remove blur caused by the motion of the camera and the objects. Recurrent
Neural Networks~(RNNs) are often adopted to model the temporal dependency
between frames via hidden states. When motion blur is strong, however, hidden
states are hard to deliver proper information due to the displacement between
different frames. While there have been attempts to update the hidden states,
it is difficult to handle misaligned features beyond the receptive field of
simple modules. Thus, we propose 2 modules to supplement the RNN architecture
for video deblurring. First, we design Ping-Pong RNN~(PPRNN) that acts on
updating the hidden states by referring to the features from the current and
the previous time steps alternately. PPRNN gathers relevant information from
the both features in an iterative and balanced manner by utilizing its
recurrent architecture. Second, we use a Selective Non-Local Attention~(SNLA)
module to additionally refine the hidden state by aligning it with the
positional information from the input frame feature. The attention score is
scaled by the relevance to the input feature to focus on the necessary
information. By paying attention to hidden states with both modules, which have
strong synergy, our PAHS framework improves the representation powers of RNN
structures and achieves state-of-the-art deblurring performance on standard
benchmarks and real-world videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CADG: A Model Based on Cross Attention for Domain Generalization. (arXiv:2203.17067v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17067">
<div class="article-summary-box-inner">
<span><p>In Domain Generalization (DG) tasks, models are trained by using only
training data from the source domains to achieve generalization on an unseen
target domain, this will suffer from the distribution shift problem. So it's
important to learn a classifier to focus on the common representation which can
be used to classify on multi-domains, so that this classifier can achieve a
high performance on an unseen target domain as well. With the success of cross
attention in various cross-modal tasks, we find that cross attention is a
powerful mechanism to align the features come from different distributions. So
we design a model named CADG (cross attention for domain generalization),
wherein cross attention plays a important role, to address distribution shift
problem. Such design makes the classifier can be adopted on multi-domains, so
the classifier will generalize well on an unseen domain. Experiments show that
our proposed method achieves state-of-the-art performance on a variety of
domain generalization benchmarks compared with other single model and can even
achieve a better performance than some ensemble-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do learned representations respect causal relationships?. (arXiv:2204.00762v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00762">
<div class="article-summary-box-inner">
<span><p>Data often has many semantic attributes that are causally associated with
each other. But do attribute-specific learned representations of data also
respect the same causal relations? We answer this question in three steps.
First, we introduce NCINet, an approach for observational causal discovery from
high-dimensional data. It is trained purely on synthetically generated
representations and can be applied to real representations, and is specifically
designed to mitigate the domain gap between the two. Second, we apply NCINet to
identify the causal relations between image representations of different pairs
of attributes with known and unknown causal relations between the labels. For
this purpose, we consider image representations learned for predicting
attributes on the 3D Shapes, CelebA, and the CASIA-WebFace datasets, which we
annotate with multiple multi-class attributes. Third, we analyze the effect on
the underlying causal relation between learned representations induced by
various design choices in representation learning. Our experiments indicate
that (1) NCINet significantly outperforms existing observational causal
discovery approaches for estimating the causal relation between pairs of random
samples, both in the presence and absence of an unobserved confounder, (2)
under controlled scenarios, learned representations can indeed satisfy the
underlying causal relations between their respective labels, and (3) the causal
relations are positively correlated with the predictive capability of the
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images. (arXiv:2204.00790v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00790">
<div class="article-summary-box-inner">
<span><p>Airports have an important role in both military and civilian domains. The
synthetic aperture radar (SAR) based airport detection has received increasing
attention in recent years. However, due to the high cost of SAR imaging and
annotation process, there is no publicly available SAR dataset for airport
detection. As a result, deep learning methods have not been fully used in
airport detection tasks. To provide a benchmark for airport detection research
in SAR images, this paper introduces a large-scale SAR Airport Dataset (SAD).
In order to adequately reflect the demands of real world applications, it
contains 624 SAR images from Sentinel 1B and covers 104 airfield instances with
different scales, orientations and shapes. The experiments of multiple deep
learning approach on this dataset proves its effectiveness. It developing
state-of-the-art airport area detection algorithms or other relevant tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rotated Object Detection via Scale-invariant Mahalanobis Distance in Aerial Images. (arXiv:2204.00840v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00840">
<div class="article-summary-box-inner">
<span><p>Rotated object detection in aerial images is a meaningful yet challenging
task as objects are densely arranged and have arbitrary orientations. The
eight-parameter (coordinates of box vectors) methods in rotated object
detection usually use ln-norm losses (L1 loss, L2 loss, and smooth L1 loss) as
loss functions. As ln-norm losses are mainly based on non-scale-invariant
Minkowski distance, using ln-norm losses will lead to inconsistency with the
detection metric rotational Intersection-over-Union (IoU) and training
instability. To address the problems, we use Mahalanobis distance to calculate
loss between the predicted and the target box vertices' vectors, proposing a
new loss function called Mahalanobis Distance Loss (MDL) for eight-parameter
rotated object detection. As Mahalanobis distance is scale-invariant, MDL is
more consistent with detection metric and more stable during training than
ln-norm losses. To alleviate the problem of boundary discontinuity like all
other eight-parameter methods, we further take the minimum loss value to make
MDL continuous at boundary cases. We achieve state-of-art performance on
DOTA-v1.0 with the proposed method MDL. Furthermore, compared to the experiment
that uses smooth L1 loss, we find that MDL performs better in rotated object
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploration of Active Learning for Affective Digital Phenotyping. (arXiv:2204.01915v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01915">
<div class="article-summary-box-inner">
<span><p>Some of the most severe bottlenecks preventing widespread development of
machine learning models for human behavior include a dearth of labeled training
data and difficulty of acquiring high quality labels. Active learning is a
paradigm for using algorithms to computationally select a useful subset of data
points to label using metrics for model uncertainty and data similarity. We
explore active learning for naturalistic computer vision emotion data, a
particularly heterogeneous and complex data space due to inherently subjective
labels. Using frames collected from gameplay acquired from a therapeutic
smartphone game for children with autism, we run a simulation of active
learning using gameplay prompts as metadata to aid in the active learning
process. We find that active learning using information generated during
gameplay slightly outperforms random selection of the same number of labeled
frames. We next investigate a method to conduct active learning with subjective
data, such as in affective computing, and where multiple crowdsourced labels
can be acquired for each image. Using the Child Affective Facial Expression
(CAFE) dataset, we simulate an active learning process for crowdsourcing many
labels and find that prioritizing frames using the entropy of the crowdsourced
label distribution results in lower categorical cross-entropy loss compared to
random frame selection. Collectively, these results demonstrate pilot
evaluations of two novel active learning approaches for subjective affective
data collected in noisy settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSDoodle: Searching for App Screens via Interactive Sketching. (arXiv:2204.01968v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01968">
<div class="article-summary-box-inner">
<span><p>Keyword-based mobile screen search does not account for screen content and
fails to operate as a universal tool for all levels of users. Visual searching
(e.g., image, sketch) is structured and easy to adopt. Current visual search
approaches count on a complete screen and are therefore slow and tedious.
PSDoodle employs a deep neural network to recognize partial screen element
drawings instantly on a digital drawing interface and shows results in
real-time. PSDoodle is the first tool that utilizes partial sketches and
searches for screens in an interactive iterative way. PSDoodle supports
different drawing styles and retrieves search results that are relevant to the
user's sketch query. A short video demonstration is available online at:
https://youtu.be/3cVLHFm5pY4
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Visual Geo-localization for Large-Scale Applications. (arXiv:2204.02287v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02287">
<div class="article-summary-box-inner">
<span><p>Visual Geo-localization (VG) is the task of estimating the position where a
given photo was taken by comparing it with a large database of images of known
locations. To investigate how existing techniques would perform on a real-world
city-wide VG application, we build San Francisco eXtra Large, a new dataset
covering a whole city and providing a wide range of challenging cases, with a
size 30x bigger than the previous largest dataset for visual geo-localization.
We find that current methods fail to scale to such large datasets, therefore we
design a new highly scalable training technique, called CosPlace, which casts
the training as a classification problem avoiding the expensive mining needed
by the commonly used contrastive learning. We achieve state-of-the-art
performance on a wide range of datasets and find that CosPlace is robust to
heavy domain changes. Moreover, we show that, compared to the previous
state-of-the-art, CosPlace requires roughly 80% less GPU memory at train time,
and it achieves better results with 8x smaller descriptors, paving the way for
city-wide real-world visual geo-localization. Dataset, code and trained models
are available for research purposes at https://github.com/gmberton/CosPlace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference. (arXiv:2204.02585v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02585">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Fields (NeRF) has emerged as the state-of-the-art method for
novel view generation of complex scenes, but is very slow during inference.
Recently, there have been multiple works on speeding up NeRF inference, but the
state of the art methods for real-time NeRF inference rely on caching the
neural network output, which occupies several giga-bytes of disk space that
limits their real-world applicability. As caching the neural network of
original NeRF network is not feasible, Garbin et al. proposed "FastNeRF" which
factorizes the problem into 2 sub-networks - one which depends only on the 3D
coordinate of a sample point and one which depends only on the 2D camera
viewing direction. Although this factorization enables them to reduce the cache
size and perform inference at over 200 frames per second, the memory overhead
is still substantial. In this work, we propose SqueezeNeRF, which is more than
60 times memory-efficient than the sparse cache of FastNeRF and is still able
to render at more than 190 frames per second on a high spec GPU during
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification. (arXiv:2204.02611v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02611">
<div class="article-summary-box-inner">
<span><p>Recently, large-scale synthetic datasets are shown to be very useful for
generalizable person re-identification. However, synthesized persons in
existing datasets are mostly cartoon-like and in random dress collocation,
which limits their performance. To address this, in this work, an automatic
approach is proposed to directly clone the whole outfits from real-world person
images to virtual 3D characters, such that any virtual person thus created will
appear very similar to its real-world counterpart. Specifically, based on UV
texture mapping, two cloning methods are designed, namely registered clothes
mapping and homogeneous cloth expansion. Given clothes keypoints detected on
person images and labeled on regular UV maps with clear clothes structures,
registered mapping applies perspective homography to warp real-world clothes to
the counterparts on the UV map. As for invisible clothes parts and irregular UV
maps, homogeneous expansion segments a homogeneous area on clothes as a
realistic cloth pattern or cell, and expand the cell to fill the UV map.
Furthermore, a similarity-diversity expansion strategy is proposed, by
clustering person images, sampling images per cluster, and cloning outfits for
3D character generation. This way, virtual persons can be scaled up densely in
visual similarity to challenge model learning, and diversely in population to
enrich sample distribution. Finally, by rendering the cloned characters in
Unity3D scenes, a more realistic virtual dataset called ClonedPerson is
created, with 5,621 identities and 887,766 images. Experimental results show
that the model trained on ClonedPerson has a better generalization performance,
superior to that trained on other popular real-world and synthetic person
re-identification datasets. The ClonedPerson project is available at
https://github.com/Yanan-Wang-cs/ClonedPerson.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards An End-to-End Framework for Flow-Guided Video Inpainting. (arXiv:2204.02663v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02663">
<div class="article-summary-box-inner">
<span><p>Optical flow, which captures motion information across frames, is exploited
in recent video inpainting methods through propagating pixels along its
trajectories. However, the hand-crafted flow-based processes in these methods
are applied separately to form the whole inpainting pipeline. Thus, these
methods are less efficient and rely heavily on the intermediate results from
earlier stages. In this paper, we propose an End-to-End framework for
Flow-Guided Video Inpainting (E$^2$FGVI) through elaborately designed three
trainable modules, namely, flow completion, feature propagation, and content
hallucination modules. The three modules correspond with the three stages of
previous flow-based methods but can be jointly optimized, leading to a more
efficient and effective inpainting process. Experimental results demonstrate
that the proposed method outperforms state-of-the-art methods both
qualitatively and quantitatively and shows promising efficiency. The code is
available at https://github.com/MCG-NKU/E2FGVI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02810">
<div class="article-summary-box-inner">
<span><p>Face frontalization consists of synthesizing a frontally-viewed face from an
arbitrarily-viewed one. The main contribution of this paper is a frontalization
methodology that preserves non-rigid facial deformations in order to boost the
performance of visually assisted speech communication. The method alternates
between the estimation of (i)~the rigid transformation (scale, rotation, and
translation) and (ii)~the non-rigid deformation between an arbitrarily-viewed
face and a face model. The method has two important merits: it can deal with
non-Gaussian errors in the data and it incorporates a dynamical face
deformation model. For that purpose, we use the generalized Student
t-distribution in combination with a linear dynamic system in order to account
for both rigid head motions and time-varying facial deformations caused by
speech production. We propose to use the zero-mean normalized cross-correlation
(ZNCC) score to evaluate the ability of the method to preserve facial
expressions. The method is thoroughly evaluated and compared with several state
of the art methods, either based on traditional geometric models or on deep
learning. Moreover, we show that the method, when incorporated into deep
learning pipelines, namely lip reading and speech enhancement, improves word
recognition and speech intelligibilty scores by a considerable margin.
Supplemental material is accessible at
https://team.inria.fr/robotlearn/research/facefrontalization-benchmark/
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-10 23:07:36.162648135 UTC">2022-04-10 23:07:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>