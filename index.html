<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-29T01:30:00Z">10-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Emoji-aware Co-attention Network with EmoGraph2vec Model for Sentiment Anaylsis. (arXiv:2110.14636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14636">
<div class="article-summary-box-inner">
<span><p>In social media platforms, emojis have an extremely high occurrence in
computer-mediated communications. Many emojis are used to strengthen the
emotional expressions and the emojis that co-occurs in a sentence also have a
strong sentiment connection. However, when it comes to emoji representation
learning, most studies have only utilized the fixed descriptions provided by
the Unicode Consortium, without consideration of actual usage scenario. As for
the sentiment analysis task, many researchers ignore the emotional impact of
the interaction between text and emojis. It results that the emotional
semantics of emojis cannot be fully explored. In this work, we propose a method
to learn emoji representations called EmoGraph2vec and design an emoji-aware
co-attention network that learns the mutual emotional semantics between text
and emojis on short texts of social media. In EmoGraph2vec, we form an emoji
co-occurrence network on real social data and enrich the semantic information
based on an external knowledge base EmojiNet to obtain emoji node embeddings.
Our model designs a co-attention mechanism to incorporate the text and emojis,
and integrates a squeeze-and-excitation (SE) block into a convolutional neural
network as a classifier. Finally, we use the transfer learning method to
increase converge speed and achieve higher accuracy. Experimental results show
that the proposed model can outperform several baselines for sentiment analysis
on benchmark datasets. Additionally, we conduct a series of ablation and
comparison experiments to investigate the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Realistic Single-Task Continuous Learning Research for NER. (arXiv:2110.14694v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14694">
<div class="article-summary-box-inner">
<span><p>There is an increasing interest in continuous learning (CL), as data privacy
is becoming a priority for real-world machine learning applications. Meanwhile,
there is still a lack of academic NLP benchmarks that are applicable for
realistic CL settings, which is a major challenge for the advancement of the
field. In this paper we discuss some of the unrealistic data characteristics of
public datasets, study the challenges of realistic single-task continuous
learning as well as the effectiveness of data rehearsal as a way to mitigate
accuracy loss. We construct a CL NER dataset from an existing publicly
available dataset and release it along with the code to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly-Injected Deep Support Vector Data Description for Text Outlier Detection. (arXiv:2110.14729v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14729">
<div class="article-summary-box-inner">
<span><p>Anomaly detection or outlier detection is a common task in various domains,
which has attracted significant research efforts in recent years. Existing
works mainly focus on structured data such as numerical or categorical data;
however, anomaly detection on unstructured textual data is less attended. In
this work, we target the textual anomaly detection problem and propose a deep
anomaly-injected support vector data description (AI-SVDD) framework. AI-SVDD
not only learns a more compact representation of the data hypersphere but also
adopts a small number of known anomalies to increase the discriminative power.
To tackle text input, we employ a multilayer perceptron (MLP) network in
conjunction with BERT to obtain enriched text representations. We conduct
experiments on three text anomaly detection applications with multiple
datasets. Experimental results show that the proposed AI-SVDD is promising and
outperforms existing works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Review-based Recommenders. (arXiv:2110.14747v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14747">
<div class="article-summary-box-inner">
<span><p>Just as user preferences change with time, item reviews also reflect those
same preference changes. In a nutshell, if one is to sequentially incorporate
review content knowledge into recommender systems, one is naturally led to
dynamical models of text. In the present work we leverage the known power of
reviews to enhance rating predictions in a way that (i) respects the causality
of review generation and (ii) includes, in a bidirectional fashion, the ability
of ratings to inform language review models and vice-versa, language
representations that help predict ratings end-to-end. Moreover, our
representations are time-interval aware and thus yield a continuous-time
representation of the dynamics. We provide experiments on real-world datasets
and show that our methodology is able to outperform several state-of-the-art
models. Source code for all models can be found at [1].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Funnelling: Ensemble Learning and Heterogeneous Document Embeddings for Cross-Lingual Text Classification. (arXiv:2110.14764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14764">
<div class="article-summary-box-inner">
<span><p>\emph{Funnelling} (Fun) is a recently proposed method for cross-lingual text
classification (CLTC) based on a two-tier learning ensemble for heterogeneous
transfer learning (HTL). In this ensemble method, 1st-tier classifiers, each
working on a different and language-dependent feature space, return a vector of
calibrated posterior probabilities (with one dimension for each class) for each
document, and the final classification decision is taken by a metaclassifier
that uses this vector as its input. The metaclassifier can thus exploit
class-class correlations, and this (among other things) gives Fun an edge over
CLTC systems in which these correlations cannot be brought to bear. In this
paper we describe \emph{Generalized Funnelling} (gFun), a generalization of Fun
consisting of an HTL architecture in which 1st-tier components can be arbitrary
\emph{view-generating functions}, i.e., language-dependent functions that each
produce a language-independent representation ("view") of the document. We
describe an instance of gFun in which the metaclassifier receives as input a
vector of calibrated posterior probabilities (as in Fun) aggregated to other
embedded representations that embody other types of correlations, such as
word-class correlations (as encoded by \emph{Word-Class Embeddings}), word-word
correlations (as encoded by \emph{Multilingual Unsupervised or Supervised
Embeddings}), and word-context correlations (as encoded by \emph{multilingual
BERT}). We show that this instance of \textsc{gFun} substantially improves over
Fun and over state-of-the-art baselines, by reporting experimental results
obtained on two large, standard datasets for multilingual multilabel text
classification. Our code that implements gFun is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine Grained Human Evaluation for English-to-Chinese Machine Translation: A Case Study on Scientific Text. (arXiv:2110.14766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14766">
<div class="article-summary-box-inner">
<span><p>Recent research suggests that neural machine translation (MT) in the news
domain has reached human-level performance, but for other professional domains,
it is far below the level. In this paper, we conduct a fine-grained systematic
human evaluation for four widely used Chinese-English NMT systems on scientific
abstracts which are collected from published journals and books. Our human
evaluation results show that all the systems return with more than 10\% error
rates on average, which requires much post editing effort for real academic
use. Furthermore, we categorize six main error types and and provide some real
examples. Our findings emphasise the needs that research attention in the MT
community should be shifted from short text generic translation to professional
machine translation and build large scale bilingual corpus for these specific
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Dementia from Speech and Transcripts using Transformers. (arXiv:2110.14769v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14769">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious
consequences to peoples' everyday lives, if it is not diagnosed early since
there is no available cure. Because of the cost of examinations for diagnosing
dementia, i.e., Magnetic Resonance Imaging (MRI), electroencephalogram (EEG)
signals etc., current work has been focused on diagnosing dementia from
spontaneous speech. However, little work has been done regarding the conversion
of speech data to Log-Mel spectrograms and Mel-frequency cepstral coefficients
(MFCCs) and the usage of pretrained models. Concurrently, little work has been
done in terms of both the usage of transformer networks and the way the two
modalities, i.e., speech and transcripts, are combined in a single neural
network. To address these limitations, first we employ several pretrained
models, with Vision Transformer (ViT) achieving the highest evaluation results.
Secondly, we propose multimodal models. More specifically, our introduced
models include Gated Multimodal Unit in order to control the influence of each
modality towards the final classification and crossmodal attention so as to
capture in an effective way the relationships between the two modalities.
Extensive experiments conducted on the ADReSS Challenge dataset demonstrate the
effectiveness of the proposed models and their superiority over
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Vagueness Detection with Deep Learning to Identify Fake News. (arXiv:2110.14780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14780">
<div class="article-summary-box-inner">
<span><p>In this paper, we combine two independent detection methods for identifying
fake news: the algorithm VAGO uses semantic rules combined with NLP techniques
to measure vagueness and subjectivity in texts, while the classifier FAKE-CLF
relies on Convolutional Neural Network classification and supervised deep
learning to classify texts as biased or legitimate. We compare the results of
the two methods on four corpora. We find a positive correlation between the
vagueness and subjectivity measures obtained by VAGO, and the classification of
text as biased by FAKE-CLF. The comparison yields mutual benefits: VAGO helps
explain the results of FAKE-CLF. Conversely FAKE-CLF helps us corroborate and
expand VAGO's database. The use of two complementary techniques (rule-based vs
data-driven) proves a fruitful approach for the challenging problem of
identifying fake news.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer. (arXiv:2110.14782v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14782">
<div class="article-summary-box-inner">
<span><p>While recent work on multilingual language models has demonstrated their
capacity for cross-lingual zero-shot transfer on downstream tasks, there is a
lack of consensus in the community as to what shared properties between
languages enable such transfer. Analyses involving pairs of natural languages
are often inconclusive and contradictory since languages simultaneously differ
in many linguistic aspects. In this paper, we perform a large-scale empirical
study to isolate the effects of various linguistic properties by measuring
zero-shot transfer between four diverse natural languages and their
counterparts constructed by modifying aspects such as the script, word order,
and syntax. Among other things, our experiments show that the absence of
sub-word overlap significantly affects zero-shot transfer when languages differ
in their word order, and there is a strong correlation between transfer
performance and word embedding alignment between languages (e.g., R=0.94 on the
task of NLI). Our results call for focus in multilingual models on explicitly
improving word embedding alignment between languages rather than relying on its
implicit emergence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate Speech Classifiers Learn Human-Like Social Stereotypes. (arXiv:2110.14839v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14839">
<div class="article-summary-box-inner">
<span><p>Social stereotypes negatively impact individuals' judgements about different
groups and may have a critical role in how people understand language directed
toward minority social groups. Here, we assess the role of social stereotypes
in the automated detection of hateful language by examining the relation
between individual annotator biases and erroneous classification of texts by
hate speech classifiers. Specifically, in Study 1 we investigate the impact of
novice annotators' stereotypes on their hate-speech-annotation behavior. In
Study 2 we examine the effect of language-embedded stereotypes on expert
annotators' aggregated judgements in a large annotated corpus. Finally, in
Study 3 we demonstrate how language-embedded stereotypes are associated with
systematic prediction errors in a neural-network hate speech classifier. Our
results demonstrate that hate speech classifiers learn human-like biases which
can further perpetuate social inequalities when propagated at scale. This
framework, combining social psychological and computational linguistic methods,
provides insights into additional sources of bias in hate speech moderation,
informing ongoing debates regarding fairness in machine learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Sequence to Sequence Model for Extracting Multiple Product Name Entities from Dialog. (arXiv:2110.14843v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14843">
<div class="article-summary-box-inner">
<span><p>E-commerce voice ordering systems need to recognize multiple product name
entities from ordering utterances. Existing voice ordering systems such as
Amazon Alexa can capture only a single product name entity. This restrains
users from ordering multiple items with one utterance. In recent years,
pre-trained language models, e.g., BERT and GPT-2, have shown promising results
on NLP benchmarks like Super-GLUE. However, they can't perfectly generalize to
this Multiple Product Name Entity Recognition (MPNER) task due to the ambiguity
in voice ordering utterances. To fill this research gap, we propose Entity
Transformer (ET) neural network architectures which recognize up to 10 items in
an utterance. In our evaluation, the best ET model (conveRT + ngram + ET) has a
performance improvement of 12% on our test set compared to the non-neural
model, and outperforms BERT with ET as well. This helps customers finalize
their shopping cart via voice dialog, which improves shopping efficiency and
experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14883">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture has improved the performance of deep learning
models in domains such as Computer Vision and Natural Language Processing.
Together with better performance come larger model sizes. This imposes
challenges to the memory wall of the current accelerator hardware such as GPU.
It is never ideal to train large models such as Vision Transformer, BERT, and
GPT on a single GPU or a single machine. There is an urgent demand to train
models in a distributed environment. However, distributed training, especially
model parallelism, often requires domain expertise in computer systems and
architecture. It remains a challenge for AI researchers to implement complex
distributed training solutions for their models.
</p>
<p>In this paper, we introduce Colossal-AI, which is a unified parallel training
system designed to seamlessly integrate different paradigms of parallelization
techniques including data parallelism, pipeline parallelism, multiple tensor
parallelism, and sequence parallelism. Colossal-AI aims to support the AI
community to write distributed models in the same way as how they write models
normally. This allows them to focus on developing the model architecture and
separates the concerns of distributed training from the development process.
The documentations can be found at https://www.colossalai.org and the source
code can be found at https://github.com/hpcaitech/ColossalAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Siamese Bi-encoder Neural Ranking Model Using Lightweight Fine-Tuning. (arXiv:2110.14943v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14943">
<div class="article-summary-box-inner">
<span><p>A BERT-based Neural Ranking Model (NRM) can be either a cross-encoder or a
bi-encoder. Between the two, bi-encoder is highly efficient because all the
documents can be pre-processed before the actual query time. Although query and
document are independently encoded, the existing bi-encoder NRMs are Siamese
models where a single language model is used for consistently encoding both of
query and document. In this work, we show two approaches for improving the
performance of BERT-based bi-encoders. The first approach is to replace the
full fine-tuning step with a lightweight fine-tuning. We examine lightweight
fine-tuning methods that are adapter-based, prompt-based, and hybrid of the
two. The second approach is to develop semi-Siamese models where queries and
documents are handled with a limited amount of difference. The limited
difference is realized by learning two lightweight fine-tuning modules, where
the main language model of BERT is kept common for both query and document. We
provide extensive experiment results for monoBERT, TwinBERT, and ColBERT where
three performance metrics are evaluated over Robust04, ClueWeb09b, and MS-MARCO
datasets. The results confirm that both lightweight fine-tuning and
semi-Siamese are considerably helpful for improving BERT-based bi-encoders. In
fact, lightweight fine-tuning is helpful for cross-encoder, too.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preventing posterior collapse in variational autoencoders for text generation via decoder regularization. (arXiv:2110.14945v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14945">
<div class="article-summary-box-inner">
<span><p>Variational autoencoders trained to minimize the reconstruction error are
sensitive to the posterior collapse problem, that is the proposal posterior
distribution is always equal to the prior. We propose a novel regularization
method based on fraternal dropout to prevent posterior collapse. We evaluate
our approach using several metrics and observe improvements in all the tested
configurations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Speech Emotion Recognition: Challenges of Real-Life Emergency Call Centers Data Recordings. (arXiv:2110.14957v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14957">
<div class="article-summary-box-inner">
<span><p>Recognizing a speaker's emotion from their speech can be a key element in
emergency call centers. End-to-end deep learning systems for speech emotion
recognition now achieve equivalent or even better results than conventional
machine learning approaches. In this paper, in order to validate the
performance of our neural network architecture for emotion recognition from
speech, we first trained and tested it on the widely used corpus accessible by
the community, IEMOCAP. We then used the same architecture as the real life
corpus, CEMO, composed of 440 dialogs (2h16m) from 485 speakers. The most
frequent emotions expressed by callers in these real life emergency dialogues
are fear, anger and positive emotions such as relief. In the IEMOCAP general
topic conversations, the most frequent emotions are sadness, anger and
happiness. Using the same end-to-end deep learning architecture, an Unweighted
Accuracy Recall (UA) of 63% is obtained on IEMOCAP and a UA of 45.6% on CEMO,
each with 4 classes. Using only 2 classes (Anger, Neutral), the results for
CEMO are 76.9% UA compared to 81.1% UA for IEMOCAP. We expect that these
encouraging results with CEMO can be improved by combining the audio channel
with the linguistic channel. Real-life emotions are clearly more complex than
acted ones, mainly due to the large diversity of emotional expressions of
speakers. Index Terms-emotion detection, end-to-end deep learning architecture,
call center, real-life database, complex emotions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Analysis of Korean Public AI Hub Parallel Corpora and in-depth Analysis using LIWC. (arXiv:2110.15023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15023">
<div class="article-summary-box-inner">
<span><p>Machine translation (MT) system aims to translate source language into target
language. Recent studies on MT systems mainly focus on neural machine
translation (NMT). One factor that significantly affects the performance of NMT
is the availability of high-quality parallel corpora. However, high-quality
parallel corpora concerning Korean are relatively scarce compared to those
associated with other high-resource languages, such as German or Italian. To
address this problem, AI Hub recently released seven types of parallel corpora
for Korean. In this study, we conduct an in-depth verification of the quality
of corresponding parallel corpora through Linguistic Inquiry and Word Count
(LIWC) and several relevant experiments. LIWC is a word-counting software
program that can analyze corpora in multiple ways and extract linguistic
features as a dictionary base. To the best of our knowledge, this study is the
first to use LIWC to analyze parallel corpora in the field of NMT. Our findings
suggest the direction of further research toward obtaining the improved quality
parallel corpora through our correlation analysis in LIWC and NMT performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Multimodal and Multisensory Empathic Technologies for Enhanced Human Communication. (arXiv:2110.15054v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15054">
<div class="article-summary-box-inner">
<span><p>As digital social platforms and mobile technologies are becoming more
prevalent and robust, the use of Artificial Intelligence (AI) in facilitating
human communication will grow. This, in turn, will pave the way for the
development of intuitive, adaptive, and effective empathic AI interfaces that
better address the needs of socially and culturally diverse communities. I
believe such developments must consider a principled framework that includes
the human perceptual senses in the digital design process right from the start,
for a more accurate, as well as a more aesthetic, memorable, and soothing
experience. In this position paper, I suggest features, identify some
challenges that need to be addressed in the process, and propose some future
research directions that I think should be part of the design and
implementation. Such an approach will allow various communities of practice to
investigate the areas of intersection between artificial intelligence, on one
side, and human communication, perceptual needs and social and cultural values,
on the other.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SenTag: a Web-based Tool for Semantic Annotation of Textual Documents. (arXiv:2110.15062v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15062">
<div class="article-summary-box-inner">
<span><p>In this work, we present SenTag, a lightweight web-based tool focused on
semantic annotation of textual documents. The platform allows multiple users to
work on a corpus of documents. The tool enables to tag a corpus of documents
through an intuitive and easy-to-use user interface that adopts the Extensible
Markup Language (XML) as output format. The main goal of the application is
two-fold: facilitating the tagging process and reducing or avoiding for errors
in the output documents. Moreover, it allows to identify arguments and other
entities that are used to build an arguments graph. It is also possible to
assess the level of agreement of annotators working on a corpus of text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition. (arXiv:2110.15063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15063">
<div class="article-summary-box-inner">
<span><p>TEXTOIR is the first integrated and visualized platform for text open intent
recognition. It is composed of two main modules: open intent detection and open
intent discovery. Each module integrates most of the state-of-the-art
algorithms and benchmark intent datasets. It also contains an overall framework
connecting the two modules in a pipeline scheme. In addition, this platform has
visualized tools for data and model management, training, evaluation and
analysis of the performance from different aspects. TEXTOIR provides useful
toolkits and convenient visualized interfaces for each sub-module (Toolkit
code: https://github.com/thuiar/TEXTOIR), and designs a framework to implement
a complete process to both identify known intents and discover open intents
(Demo code: https://github.com/thuiar/TEXTOIR-DEMO).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Fine-Grained Reasoning for Fake News Detection. (arXiv:2110.15064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15064">
<div class="article-summary-box-inner">
<span><p>The detection of fake news often requires sophisticated reasoning skills,
such as logically combining information by considering word-level subtle clues.
In this paper, we move towards fine-grained reasoning for fake news detection
by better reflecting the logical processes of human thinking and enabling the
modeling of subtle clues. In particular, we propose a fine-grained reasoning
framework by following the human's information-processing model, introduce a
mutual-reinforcement-based method for incorporating human knowledge about which
evidence is more important, and design a prior-aware bi-channel kernel graph
network to model subtle differences between pieces of evidence. Extensive
experiments show that our model outperforms the state-of-art methods and
demonstrate the explainability of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification. (arXiv:2110.15116v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15116">
<div class="article-summary-box-inner">
<span><p>Scientific claim verification can help the researchers to easily find the
target scientific papers with the sentence evidence from a large corpus for the
given claim. Some existing works propose pipeline models on the three tasks of
abstract retrieval, rationale selection and stance prediction. Such works have
the problems of error propagation among the modules in the pipeline and lack of
sharing valuable information among modules. We thus propose an approach, named
as ARSJoint, that jointly learns the modules for the three tasks with a machine
reading comprehension framework by including claim information. In addition, we
enhance the information exchanges and constraints among tasks by proposing a
regularization term between the sentence attention scores of abstract retrieval
and the estimated outputs of rational selection. The experimental results on
the benchmark dataset SciFact show that our approach outperforms the existing
works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confounds and Overestimations in Fake Review Detection: Experimentally Controlling for Product-Ownership and Data-Origin. (arXiv:2110.15130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15130">
<div class="article-summary-box-inner">
<span><p>The popularity of online shopping is steadily increasing. At the same time,
fake product reviewsare published widely and have the potential to affect
consumer purchasing behavior. In response,previous work has developed automated
methods for the detection of deceptive product reviews.However, studies vary
considerably in terms of classification performance, and many use data
thatcontain potential confounds, which makes it difficult to determine their
validity. Two possibleconfounds are data-origin (i.e., the dataset is composed
of more than one source) and productownership (i.e., reviews written by
individuals who own or do not own the reviewed product). Inthe present study,
we investigate the effect of both confounds for fake review detection. Using
anexperimental design, we manipulate data-origin, product ownership, review
polarity, and veracity.Supervised learning analysis suggests that review
veracity (60.26 - 69.87%) is somewhat detectablebut reviews additionally
confounded with product-ownership (66.19 - 74.17%), or with data-origin(84.44 -
86.94%) are easier to classify. Review veracity is most easily classified if
confounded withproduct-ownership and data-origin combined (87.78 - 88.12%),
suggesting overestimations of thetrue performance in other work. These findings
are moderated by review polarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Table Vector Representations. (arXiv:2110.15132v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15132">
<div class="article-summary-box-inner">
<span><p>High-quality Web tables are rich sources of information that can be used to
populate Knowledge Graphs (KG). The focus of this paper is an evaluation of
methods for table-to-class annotation, which is a sub-task of Table
Interpretation (TI). We provide a formal definition for table classification as
a machine learning task. We propose an experimental setup and we evaluate 5
fundamentally different approaches to find the best method for generating
vector table representations. Our findings indicate that although transfer
learning methods achieve high F1 score on the table classification task,
dedicated table encoding models are a promising direction as they appear to
capture richer semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis of Programming Course Evaluations Before and After the Introduction of an Autograder. (arXiv:2110.15134v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15134">
<div class="article-summary-box-inner">
<span><p>Commonly, introductory programming courses in higher education institutions
have hundreds of participating students eager to learn to program. The manual
effort for reviewing the submitted source code and for providing feedback can
no longer be managed. Manually reviewing the submitted homework can be
subjective and unfair, particularly if many tutors are responsible for grading.
Different autograders can help in this situation; however, there is a lack of
knowledge about how autograders can impact students' overall perception of
programming classes and teaching. This is relevant for course organizers and
institutions to keep their programming courses attractive while coping with
increasing students.
</p>
<p>This paper studies the answers to the standardized university evaluation
questionnaires of multiple large-scale foundational computer science courses
which recently introduced autograding. The differences before and after this
intervention are analyzed. By incorporating additional observations, we
hypothesize how the autograder might have contributed to the significant
changes in the data, such as, improved interactions between tutors and
students, improved overall course quality, improved learning success, increased
time spent, and reduced difficulty. This qualitative study aims to provide
hypotheses for future research to define and conduct quantitative surveys and
data analysis. The autograder technology can be validated as a teaching method
to improve student satisfaction with programming courses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diversity-Driven Combination for Grammatical Error Correction. (arXiv:2110.15149v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15149">
<div class="article-summary-box-inner">
<span><p>Grammatical error correction (GEC) is the task of detecting and correcting
errors in a written text. The idea of combining multiple system outputs has
been successfully used in GEC. To achieve successful system combination,
multiple component systems need to produce corrected sentences that are both
diverse and of comparable quality. However, most existing state-of-the-art GEC
approaches are based on similar sequence-to-sequence neural networks, so the
gains are limited from combining the outputs of component systems similar to
one another. In this paper, we present Diversity-Driven Combination (DDC) for
GEC, a system combination strategy that encourages diversity among component
systems. We evaluate our system combination strategy on the CoNLL-2014 shared
task and the BEA-2019 shared task. On both benchmarks, DDC achieves significant
performance gain with a small number of training examples and outperforms the
component systems by a large margin. Our source code is available at
https://github.com/nusnlp/gec-ddc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTian Poetics: Constrained Composition with Masked LMs. (arXiv:2110.15181v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15181">
<div class="article-summary-box-inner">
<span><p>Masked language models have recently been interpreted as energy-based
sequence models that can be generated from using a Metropolis--Hastings
sampler. This short paper demonstrates how this can be instrumentalized for
constrained composition and explores the poetics implied by such a usage. Our
focus on constraints makes it especially apt to understand the generated text
through the poetics of the OuLiPo movement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Add-On for Empowering Google Forms to be an Automatic Question Generator in Online Assessments. (arXiv:2110.15220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15220">
<div class="article-summary-box-inner">
<span><p>This research suggests an add-on to empower Google Forms to be an automatic
machine for generating multiple-choice questions (MCQs) used in online
assessments. In this paper, we elaborate an add-on design mainly comprising
question-formulating software and data storage. The algorithm as an
intellectual mechanism of this software can produce MCQs at an analytical
level. In an experiment, we found the MCQs could assess levels of students'
knowledge comparably with those generated by human experts. This add-on can be
applied generally to formulate MCQs for any rational concepts. With no effort
from an instructor at runtime, the add-on can transform a few data instances
describing rational concepts to be variety sets of MCQs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word-level confidence estimation for RNN transducers. (arXiv:2110.15222v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15222">
<div class="article-summary-box-inner">
<span><p>Confidence estimate is an often requested feature in applications such as
medical transcription where errors can impact patient care and the confidence
estimate could be used to alert medical professionals to verify potential
errors in recognition.
</p>
<p>In this paper, we present a lightweight neural confidence model tailored for
Automatic Speech Recognition (ASR) system with Recurrent Neural Network
Transducers (RNN-T). Compared to other existing approaches, our model utilizes:
(a) the time information associated with recognized words, which reduces the
computational complexity, and (b) a simple and elegant trick for mapping
between sub-word and word sequences. The mapping addresses the non-unique
tokenization and token deletion problems while amplifying differences between
confusable words. Through extensive empirical evaluations on two different
long-form test sets, we demonstrate that the model achieves a performance of
0.4 Normalized Cross Entropy (NCE) and 0.05 Expected Calibration Error (ECE).
It is robust across different ASR configurations, including target types
(graphemes vs. morphemes), traffic conditions (streaming vs. non-streaming),
and encoder types. We further discuss the importance of evaluation metrics to
reflect practical applications and highlight the need for further work in
improving Area Under the Curve (AUC) for Negative Precision Rate (NPV) and True
Negative Rate (TNR).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning Attention Heads of Transformer Models Using A* Search: A Novel Approach to Compress Big NLP Architectures. (arXiv:2110.15225v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15225">
<div class="article-summary-box-inner">
<span><p>Recent years have seen a growing adoption of Transformer models such as BERT
in Natural Language Processing and even in Computer Vision. However, due to the
size, there has been limited adoption of such models within
resource-constrained computing environments This paper proposes novel pruning
algorithms to compress transformer models by eliminating redundant Attention
Heads. We apply the A* search algorithm to obtain a pruned model with minimal
accuracy guarantees. Our results indicate that the method could eliminate as
much as 40% of the attention heads in the BERT transformer model with almost no
loss in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-stage Clarification in Conversational AI: The case of Question-Answering Dialogue Systems. (arXiv:2110.15235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15235">
<div class="article-summary-box-inner">
<span><p>Clarification resolution plays an important role in various information
retrieval tasks such as interactive question answering and conversational
search. In such context, the user often formulates their information needs as
short and ambiguous queries, some popular search interfaces then prompt the
user to confirm her intent (e.g. "Did you mean ... ?") or to rephrase if
needed. When it comes to dialogue systems, having fluid user-bot exchanges is
key to good user experience. In the absence of such clarification mechanism,
one of the following responses is given to the user: 1) A direct answer, which
can potentially be non-relevant if the intent was not clear, 2) a generic
fallback message informing the user that the retrieval tool is incapable of
handling the query. Both scenarios might raise frustration and degrade the user
experience. To this end, we propose a multi-stage clarification mechanism for
prompting clarification and query selection in the context of a question
answering dialogue system. We show that our proposed mechanism improves the
overall user experience and outperforms competitive baselines with two
datasets, namely the public in-scope out-of-scope dataset and a commercial
dataset based on real user logs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">\'UFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5. (arXiv:2110.15248v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15248">
<div class="article-summary-box-inner">
<span><p>We present the winning entry to the Multilingual Lexical Normalization
(MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which
evaluates lexical-normalization systems on 12 social media datasets in 11
languages. We base our solution on a pre-trained byte-level language model,
ByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then
fine-tune on authentic normalization data. Our system achieves the best
performance by a wide margin in intrinsic evaluation, and also the best
performance in extrinsic evaluation through dependency parsing. The source code
is released at https://github.com/ufal/multilexnorm2021 and the fine-tuned
models at https://huggingface.co/ufal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cognitive network science quantifies feelings expressed in suicide letters and Reddit mental health communities. (arXiv:2110.15269v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15269">
<div class="article-summary-box-inner">
<span><p>Writing messages is key to expressing feelings. This study adopts cognitive
network science to reconstruct how individuals report their feelings in
clinical narratives like suicide notes or mental health posts. We achieve this
by reconstructing syntactic/semantic associations between conceptsin texts as
co-occurrences enriched with affective data. We transform 142 suicide notes and
77,000 Reddit posts from the r/anxiety, r/depression, r/schizophrenia, and
r/do-it-your-own (r/DIY) forums into 5 cognitive networks, each one expressing
meanings and emotions as reported by authors. These networks reconstruct the
semantic frames surrounding \textit{feel}, enabling a quantification of
prominent associations and emotions focused around feelings. We find strong
feelings of sadness across all clinical Reddit boards, added to fear
r/depression, and replaced by joy/anticipation in r/DIY. Semantic communities
and topic modelling both highlight key narrative topics of \textit{regret},
\textit{unhealthy lifestyle} and \textit{low mental well-being}. Importantly,
negative associations and emotions co-existed with trustful/positive language,
focused on \textit{getting better}. This emotional polarisation provides
quantitative evidence that online clinical boards possess a complex structure,
where users mix both positive and negative outlooks. This dichotomy is absent
in the r/DIY reference board and in suicide notes, where negative emotional
associations about regret and pain persist but are overwhelmed by positive
jargon addressing loved ones. Our quantitative comparisons provide strong
evidence that suicide notes encapsulate different ways of expressing feelings
compared to online Reddit boards, the latter acting more like personal diaries
and relief valve. Our findings provide an interpretable, quantitative aid for
supporting psychological inquiries of human feelings in digital and clinical
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework. (arXiv:2110.15317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15317">
<div class="article-summary-box-inner">
<span><p>Despite great success on many machine learning tasks, deep neural networks
are still vulnerable to adversarial samples. While gradient-based adversarial
attack methods are well-explored in the field of computer vision, it is
impractical to directly apply them in natural language processing due to the
discrete nature of text. To bridge this gap, we propose a general framework to
adapt existing gradient-based methods to craft textual adversarial samples. In
this framework, gradient-based continuous perturbations are added to the
embedding layer and are amplified in the forward propagation process. Then the
final perturbed latent representations are decoded with a mask language model
head to obtain potential adversarial samples. In this paper, we instantiate our
framework with \textbf{T}extual \textbf{P}rojected \textbf{G}radient
\textbf{D}escent (\textbf{TPGD}). We conduct comprehensive experiments to
evaluate our framework by performing transfer black-box attacks on BERT,
RoBERTa and ALBERT on three benchmark datasets. Experimental results
demonstrate our method achieves an overall better performance and produces more
fluent and grammatical adversarial samples compared to strong baseline methods.
All the code and data will be made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Ground Multi-Agent Communication with Autoencoders. (arXiv:2110.15349v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15349">
<div class="article-summary-box-inner">
<span><p>Communication requires having a common language, a lingua franca, between
agents. This language could emerge via a consensus process, but it may require
many generations of trial and error. Alternatively, the lingua franca can be
given by the environment, where agents ground their language in representations
of the observed world. We demonstrate a simple way to ground language in
learned representations, which facilitates decentralized multi-agent
communication and coordination. We find that a standard representation learning
algorithm -- autoencoding -- is sufficient for arriving at a grounded common
language. When agents broadcast these representations, they learn to understand
and respond to each other's utterances and achieve surprisingly strong task
performance across a variety of multi-agent communication environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry matters: Exploring language examples at the decision boundary. (arXiv:2010.07212v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07212">
<div class="article-summary-box-inner">
<span><p>A growing body of recent evidence has highlighted the limitations of natural
language processing (NLP) datasets and classifiers. These include the presence
of annotation artifacts in datasets, classifiers relying on shallow features
like a single word (e.g., if a movie review has the word "romantic", the review
tends to be positive), or unnecessary words (e.g., learning a proper noun to
classify a movie as positive or negative). The presence of such artifacts has
subsequently led to the development of challenging datasets to force the model
to generalize better. While a variety of heuristic strategies, such as
counterfactual examples and contrast sets, have been proposed, the theoretical
justification about what makes these examples difficult for the classifier is
often lacking or unclear. In this paper, using tools from information geometry,
we propose a theoretical way to quantify the difficulty of an example in NLP.
Using our approach, we explore difficult examples for several deep learning
architectures. We discover that both BERT, CNN and fasttext are susceptible to
word substitutions in high difficulty examples. These classifiers tend to
perform poorly on the FIM test set. (generated by sampling and perturbing
difficult examples, with accuracy dropping below 50%). We replicate our
experiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews,
YelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a
correlation coefficient of -0.4 between resilience to perturbations and the
difficulty score. Similarly we observe a correlation of 0.35 between the
difficulty score and the empirical success probability of random substitutions.
Our approach is simple, architecture agnostic and can be used to study the
fragilities of text classification models. All the code used will be made
publicly available, including a tool to explore the difficult examples for
other datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Speaker Diarization: Recent Advances with Deep Learning. (arXiv:2101.09624v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09624">
<div class="article-summary-box-inner">
<span><p>Speaker diarization is a task to label audio or video recordings with classes
that correspond to speaker identity, or in short, a task to identify "who spoke
when". In the early years, speaker diarization algorithms were developed for
speech recognition on multispeaker audio recordings to enable speaker adaptive
processing. These algorithms also gained their own value as a standalone
application over time to provide speaker-specific metainformation for
downstream tasks such as audio retrieval. More recently, with the emergence of
deep learning technology, which has driven revolutionary changes in research
and practices across speech application domains, rapid advancements have been
made for speaker diarization. In this paper, we review not only the historical
development of speaker diarization technology but also the recent advancements
in neural speaker diarization approaches. Furthermore, we discuss how speaker
diarization systems have been integrated with speech recognition applications
and how the recent surge of deep learning is leading the way of jointly
modeling these two components to be complementary to each other. By considering
such exciting technical trends, we believe that this paper is a valuable
contribution to the community to provide a survey work by consolidating the
recent developments with neural methods and thus facilitating further progress
toward a more efficient speaker diarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models. (arXiv:2102.04130v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04130">
<div class="article-summary-box-inner">
<span><p>The capabilities of natural language models trained on large-scale data have
increased immensely over the past few years. Open source libraries such as
HuggingFace have made these models easily available and accessible. While prior
research has identified biases in large language models, this paper considers
biases contained in the most popular versions of these models when applied
`out-of-the-box' for downstream tasks. We focus on generative language models
as they are well-suited for extracting biases inherited from training data.
Specifically, we conduct an in-depth analysis of GPT-2, which is the most
downloaded text generation model on HuggingFace, with over half a million
downloads per month. We assess biases related to occupational associations for
different protected categories by intersecting gender with religion, sexuality,
ethnicity, political affiliation, and continental name origin. Using a
template-based data collection pipeline, we collect 396K sentence completions
made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and
more stereotypical for women than for men, especially for intersections; (ii)
Intersectional interactions are highly relevant for occupational associations,
which we quantify by fitting 262 logistic models; (iii) For most occupations,
GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor
Bureau data, and even pulls the societally-skewed distribution towards gender
parity in cases where its predictions deviate from real labor market
observations. This raises the normative question of what language models should
learn - whether they should reflect or correct for existing inequalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DOBF: A Deobfuscation Pre-Training Objective for Programming Languages. (arXiv:2102.07492v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07492">
<div class="article-summary-box-inner">
<span><p>Recent advances in self-supervised learning have dramatically improved the
state of the art on a wide variety of tasks. However, research in language
model pre-training has mostly focused on natural languages, and it is unclear
whether models like BERT and its variants provide the best pre-training when
applied to other modalities, such as source code. In this paper, we introduce a
new pre-training objective, DOBF, that leverages the structural aspect of
programming languages and pre-trains a model to recover the original version of
obfuscated source code. We show that models pre-trained with DOBF significantly
outperform existing approaches on multiple downstream tasks, providing relative
improvements of up to 13% in unsupervised code translation, and 24% in natural
language code search. Incidentally, we found that our pre-trained model is able
to de-obfuscate fully obfuscated source files, and to suggest descriptive
variable names.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">#PraCegoVer: A Large Dataset for Image Captioning in Portuguese. (arXiv:2103.11474v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11474">
<div class="article-summary-box-inner">
<span><p>Automatically describing images using natural sentences is an important task
to support visually impaired people's inclusion onto the Internet. It is still
a big challenge that requires understanding the relation of the objects present
in the image and their attributes and actions they are involved in. Then,
visual interpretation methods are needed, but linguistic models are also
necessary to verbally describe the semantic relations. This problem is known as
Image Captioning. Although many datasets were proposed in the literature, the
majority contains only English captions, whereas datasets with captions
described in other languages are scarce. Recently, a movement called PraCegoVer
arose on the Internet, stimulating users from social media to publish images,
tag #PraCegoVer and add a short description of their content. Thus, inspired by
this movement, we have proposed the #PraCegoVer, a multi-modal dataset with
Portuguese captions based on posts from Instagram. It is the first large
dataset for image captioning in Portuguese with freely annotated images.
Further, the captions in our dataset bring additional challenges to the
problem: first, in contrast to popular datasets such as MS COCO Captions,
#PraCegoVer has only one reference to each image; also, both mean and variance
of our reference sentence length are significantly greater than those in the MS
COCO Captions. These two characteristics contribute to making our dataset
interesting due to the linguistic aspect and the challenges that it introduces
to the image captioning problem. We publicly-share the dataset at
https://github.com/gabrielsantosrv/PraCegoVer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphTMT: Unsupervised Graph-based Topic Modeling from Video Transcripts. (arXiv:2105.01466v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01466">
<div class="article-summary-box-inner">
<span><p>To unfold the tremendous amount of multimedia data uploaded daily to social
media platforms, effective topic modeling techniques are needed. Existing work
tends to apply topic models on written text datasets. In this paper, we propose
a topic extractor on video transcripts. Exploiting neural word embeddings
through graph-based clustering, we aim to improve usability and semantic
coherence. Unlike most topic models, this approach works without knowing the
true number of topics, which is important when no such assumption can or should
be made. Experimental results on the real-life multimodal dataset MuSe-CaR
demonstrates that our approach GraphTMT extracts coherent and meaningful topics
and outperforms baseline methods. Furthermore, we successfully demonstrate the
applicability of our approach on the popular Citysearch corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations. (arXiv:2106.00786v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00786">
<div class="article-summary-box-inner">
<span><p>Feature importance (FI) estimates are a popular form of explanation, and they
are commonly created and evaluated by computing the change in model confidence
caused by removing certain input features at test time. For example, in the
standard Sufficiency metric, only the top-k most important tokens are kept. In
this paper, we study several under-explored dimensions of FI explanations,
providing conceptual and empirical improvements for this form of explanation.
First, we advance a new argument for why it can be problematic to remove
features from an input when creating or evaluating explanations: the fact that
these counterfactual inputs are out-of-distribution (OOD) to models implies
that the resulting explanations are socially misaligned. The crux of the
problem is that the model prior and random weight initialization influence the
explanations (and explanation metrics) in unintended ways. To resolve this
issue, we propose a simple alteration to the model training process, which
results in more socially aligned explanations and metrics. Second, we compare
among five approaches for removing features from model inputs. We find that
some methods produce more OOD counterfactuals than others, and we make
recommendations for selecting a feature-replacement function. Finally, we
introduce four search-based methods for identifying FI explanations and compare
them to strong baselines, including LIME, Anchors, and Integrated Gradients.
Through experiments with six diverse text classification datasets, we find that
the only method that consistently outperforms random search is a Parallel Local
Search (PLS) that we introduce. Improvements over the second-best method are as
large as 5.4 points for Sufficiency and 17 points for Comprehensiveness. All
supporting code for experiments in this paper is publicly available at
https://github.com/peterbhase/ExplanationSearch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence. (arXiv:2107.02173v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02173">
<div class="article-summary-box-inner">
<span><p>Topic model evaluation, like evaluation of other unsupervised methods, can be
contentious. However, the field has coalesced around automated estimates of
topic coherence, which rely on the frequency of word co-occurrences in a
reference corpus. Contemporary neural topic models surpass classical ones
according to these metrics. At the same time, topic model evaluation suffers
from a validation gap: automated coherence, developed for classical models, has
not been validated using human experimentation for neural models. In addition,
a meta-analysis of topic modeling literature reveals a substantial
standardization gap in automated topic modeling benchmarks. To address the
validation gap, we compare automated coherence with the two most widely
accepted human judgment tasks: topic rating and word intrusion. To address the
standardization gap, we systematically evaluate a dominant classical model and
two state-of-the-art neural models on two commonly used datasets. Automated
evaluations declare a winning model when corresponding human evaluations do
not, calling into question the validity of fully automatic evaluations
independent of human judgments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05768">
<div class="article-summary-box-inner">
<span><p>Transformers provide a class of expressive architectures that are extremely
effective for sequence modeling. However, the key limitation of transformers is
their quadratic memory and time complexity $\mathcal{O}(L^2)$ with respect to
the sequence length in attention layers, which restricts application in
extremely long sequences. Most existing approaches leverage sparsity or
low-rank assumptions in the attention matrix to reduce cost, but sacrifice
expressiveness. Instead, we propose Combiner, which provides full attention
capability in each attention head while maintaining low computation and memory
complexity. The key idea is to treat the self-attention mechanism as a
conditional expectation over embeddings at each location, and approximate the
conditional distribution with a structured factorization. Each location can
attend to all other locations, either via direct attention, or through indirect
attention to abstractions, which are again conditional expectations of
embeddings from corresponding local regions. We show that most sparse attention
patterns used in existing sparse transformers are able to inspire the design of
such factorization for full attention, resulting in the same sub-quadratic cost
($\mathcal{O}(L\log(L))$ or $\mathcal{O}(L\sqrt{L})$). Combiner is a drop-in
replacement for attention layers in existing transformers and can be easily
implemented in common frameworks. An experimental evaluation on both
autoregressive and bidirectional sequence tasks demonstrates the effectiveness
of this approach, yielding state-of-the-art results on several image and text
modeling tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval. (arXiv:2107.11976v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11976">
<div class="article-summary-box-inner">
<span><p>We present Cross-lingual Open-Retrieval Answer Generation (CORA), the first
unified many-to-many question answering (QA) model that can answer questions
across many languages, even for ones without language-specific annotated data
or knowledge sources. We introduce a new dense passage retrieval algorithm that
is trained to retrieve documents across languages for a question. Combined with
a multilingual autoregressive generation model, CORA answers directly in the
target language without any translation or in-language retrieval modules as
used in prior work. We propose an iterative training method that automatically
extends annotated data available only in high-resource languages to
low-resource ones. Our results show that CORA substantially outperforms the
previous state of the art on multilingual open QA benchmarks across 26
languages, 9 of which are unseen during training. Our analyses show the
significance of cross-lingual retrieval and generation in many languages,
particularly under low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Disagreement in the Scientific Literature. (arXiv:2107.14641v2 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14641">
<div class="article-summary-box-inner">
<span><p>Disagreement is essential to scientific progress. However, the extent of
disagreement in science, its evolution over time, and the fields in which it
happens, remains poorly understood. Leveraging a massive collection of
English-language scientific texts, we develop a cue-phrase based approach to
identify instances of disagreement citations across more than four million
scientific articles. Using this method, we construct an indicator of
disagreement across scientific fields over the 2000-2015 period. In contrast
with black-box text classification methods, our framework is transparent and
easily interpretable. We reveal a disciplinary spectrum of disagreement, with
higher disagreement in the social sciences and lower disagreement in physics
and mathematics. However, detailed disciplinary analysis demonstrates
heterogeneity across sub-fields, revealing the importance of local disciplinary
cultures and epistemic characteristics of disagreement. Paper-level analysis
reveals notable episodes of disagreement in science, and illustrates how
methodological artifacts can confound analyses of scientific texts. These
findings contribute to a broader understanding of disagreement and establish a
foundation for future research to understanding key processes underlying
scientific progress.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Interplay Between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis. (arXiv:2110.01147v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01147">
<div class="article-summary-box-inner">
<span><p>Are end-to-end text-to-speech (TTS) models over-parametrized? To what extent
can these models be pruned, and what happens to their synthesis capabilities?
This work serves as a starting point to explore pruning both spectrogram
prediction networks and vocoders. We thoroughly investigate the tradeoffs
between sparsity and its subsequent effects on synthetic speech. Additionally,
we explored several aspects of TTS pruning: amount of finetuning data versus
sparsity, TTS-Augmentation to utilize unspoken text, and combining knowledge
distillation and pruning. Our findings suggest that not only are end-to-end TTS
models highly prunable, but also, perhaps surprisingly, pruned TTS models can
produce synthetic speech with equal or higher naturalness and intelligibility,
with similar prosody. All of our experiments are conducted on publicly
available models, and findings in this work are backed by large-scale
subjective tests and objective measures. Code and 200 pruned models are made
available to facilitate future research on efficiency in TTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Wav2vec 2.0 fine-tuning for improved speech emotion recognition. (arXiv:2110.06309v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06309">
<div class="article-summary-box-inner">
<span><p>While wav2vec 2.0 has been proposed for speech recognition (ASR), it can also
be used for speech emotion recognition (SER); its performance can be
significantly improved using different fine-tuning strategies. Two baseline
methods, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT) are
first presented. We show that V-FT is able to outperform state-of-the-art
models on the IEMOCAP dataset. TAPT, an existing NLP fine-tuning strategy,
further improves the performance on SER. We also introduce a novel fine-tuning
method termed P-TAPT, which modifies the TAPT objective to learn contextualized
emotion representations. Experiments show that P-TAPT performs better than TAPT
especially under low-resource settings. Compared to prior works in this
literature, our top-line system achieved a 7.4% absolute improvement on
unweighted accuracy (UA) over the state-of-the-art performance on IEMOCAP. Our
code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Super-Resolution Performance using Meta-Attention Layers. (arXiv:2110.14638v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14638">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) have achieved impressive results across
many super-resolution (SR) and image restoration tasks. While many such
networks can upscale low-resolution (LR) images using just the raw pixel-level
information, the ill-posed nature of SR can make it difficult to accurately
super-resolve an image which has undergone multiple different degradations.
Additional information (metadata) describing the degradation process (such as
the blur kernel applied, compression level, etc.) can guide networks to
super-resolve LR images with higher fidelity to the original source. Previous
attempts at informing SR networks with degradation parameters have indeed been
able to improve performance in a number of scenarios. However, due to the
fully-convolutional nature of many SR networks, most of these metadata fusion
methods either require a complete architectural change, or necessitate the
addition of significant extra complexity. Thus, these approaches are difficult
to introduce into arbitrary SR networks without considerable design
alterations. In this paper, we introduce meta-attention, a simple mechanism
which allows any SR CNN to exploit the information available in relevant
degradation parameters. The mechanism functions by translating the metadata
into a channel attention vector, which in turn selectively modulates the
network's feature maps. Incorporating meta-attention into SR networks is
straightforward, as it requires no specific type of architecture to function
correctly. Extensive testing has shown that meta-attention can consistently
improve the pixel-level accuracy of state-of-the-art (SOTA) networks when
provided with relevant degradation metadata. For PSNR, the gain on
blurred/downsampled (X4) images is of 0.2969 dB (on average) and 0.3320 dB for
SOTA general and face SR models, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensing Anomalies as Potential Hazards: Datasets and Benchmarks. (arXiv:2110.14706v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14706">
<div class="article-summary-box-inner">
<span><p>We consider the problem of detecting, in the visual sensing data stream of an
autonomous mobile robot, semantic patterns that are unusual (i.e., anomalous)
with respect to the robot's previous experience in similar environments. These
anomalies might indicate unforeseen hazards and, in scenarios where failure is
costly, can be used to trigger an avoidance behavior. We contribute three novel
image-based datasets acquired in robot exploration scenarios, comprising a
total of more than 200k labeled frames, spanning various types of anomalies. On
these datasets, we study the performance of an anomaly detection approach based
on autoencoders operating at different scales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sharp-GAN: Sharpness Loss Regularized GAN for Histopathology Image Synthesis. (arXiv:2110.14709v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14709">
<div class="article-summary-box-inner">
<span><p>Existing deep learning-based approaches for histopathology image analysis
require large annotated training sets to achieve good performance; but
annotating histopathology images is slow and resource-intensive. Conditional
generative adversarial networks have been applied to generate synthetic
histopathology images to alleviate this issue, but current approaches fail to
generate clear contours for overlapped and touching nuclei. In this study, We
propose a sharpness loss regularized generative adversarial network to
synthesize realistic histopathology images. The proposed network uses
normalized nucleus distance map rather than the binary mask to encode nuclei
contour information. The proposed sharpness loss enhances the contrast of
nuclei contour pixels. The proposed method is evaluated using four image
quality metrics and segmentation results on two public datasets. Both
quantitative and qualitative results demonstrate that the proposed approach can
generate realistic histopathology images with clear nuclei contours.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Self-Supervised and Few-Shot Object Detection. (arXiv:2110.14711v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14711">
<div class="article-summary-box-inner">
<span><p>Labeling data is often expensive and time-consuming, especially for tasks
such as object detection and instance segmentation, which require dense
labeling of the image. While few-shot object detection is about training a
model on novel (unseen) object classes with little data, it still requires
prior training on many labeled examples of base (seen) classes. On the other
hand, self-supervised methods aim at learning representations from unlabeled
data which transfer well to downstream tasks such as object detection.
Combining few-shot and self-supervised object detection is a promising research
direction. In this survey, we review and characterize the most recent
approaches on few-shot and self-supervised object detection. Then, we give our
main takeaways and discuss future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lung Cancer Lesion Detection in Histopathology Images Using Graph-Based Sparse PCA Network. (arXiv:2110.14728v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14728">
<div class="article-summary-box-inner">
<span><p>Early detection of lung cancer is critical for improvement of patient
survival. To address the clinical need for efficacious treatments, genetically
engineered mouse models (GEMM) have become integral in identifying and
evaluating the molecular underpinnings of this complex disease that may be
exploited as therapeutic targets. Assessment of GEMM tumor burden on
histopathological sections performed by manual inspection is both time
consuming and prone to subjective bias. Therefore, an interplay of needs and
challenges exists for computer-aided diagnostic tools, for accurate and
efficient analysis of these histopathology images. In this paper, we propose a
simple machine learning approach called the graph-based sparse principal
component analysis (GS-PCA) network, for automated detection of cancerous
lesions on histological lung slides stained by hematoxylin and eosin (H&amp;E). Our
method comprises four steps: 1) cascaded graph-based sparse PCA, 2) PCA binary
hashing, 3) block-wise histograms, and 4) support vector machine (SVM)
classification. In our proposed architecture, graph-based sparse PCA is
employed to learn the filter banks of the multiple stages of a convolutional
network. This is followed by PCA hashing and block histograms for indexing and
pooling. The meaningful features extracted from this GS-PCA are then fed to an
SVM classifier. We evaluate the performance of the proposed algorithm on H&amp;E
slides obtained from an inducible K-rasG12D lung cancer mouse model using
precision/recall rates, F-score, Tanimoto coefficient, and area under the curve
(AUC) of the receiver operator characteristic (ROC) and show that our algorithm
is efficient and provides improved detection accuracy compared to existing
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer for Classification of Breast Ultrasound Images. (arXiv:2110.14731v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14731">
<div class="article-summary-box-inner">
<span><p>Medical ultrasound (US) imaging has become a prominent modality for breast
cancer imaging due to its ease-of-use, low-cost and safety. In the past decade,
convolutional neural networks (CNNs) have emerged as the method of choice in
vision applications and have shown excellent potential in automatic
classification of US images. Despite their success, their restricted local
receptive field limits their ability to learn global context information.
Recently, Vision Transformer (ViT) designs that are based on self-attention
between image patches have shown great potential to be an alternative to CNNs.
In this study, for the first time, we utilize ViT to classify breast US images
using different augmentation strategies. The results are provided as
classification accuracy and Area Under the Curve (AUC) metrics, and the
performance is compared with the state-of-the-art CNNs. The results indicate
that the ViT models have comparable efficiency with or even better than the
CNNs in classification of US breast images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algorithmic encoding of protected characteristics and its implications on disparities across subgroups. (arXiv:2110.14755v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14755">
<div class="article-summary-box-inner">
<span><p>It has been rightfully emphasized that the use of AI for clinical decision
making could amplify health disparities. A machine learning model may pick up
undesirable correlations, for example, between a patient's racial identity and
clinical outcome. Such correlations are often present in (historical) data used
for model development. There has been an increase in studies reporting biases
in disease detection models across patient subgroups. Besides the scarcity of
data from underserved populations, very little is known about how these biases
are encoded and how one may reduce or even remove disparate performance. There
is some speculation whether algorithms may recognize patient characteristics
such as biological sex or racial identity, and then directly or indirectly use
this information when making predictions. But it remains unclear how we can
establish whether such information is actually used. This article aims to shed
some light on these issues by exploring new methodology allowing intuitive
inspections of the inner working of machine learning models for image-based
detection of disease. We also evaluate an effective yet debatable technique for
addressing disparities leveraging the automatic prediction of patient
characteristics, resulting in models with comparable true and false positive
rates across subgroups. Our findings may stimulate the discussion about safe
and ethical use of AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regularized Frank-Wolfe for Dense CRFs: Generalizing Mean Field and Beyond. (arXiv:2110.14759v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14759">
<div class="article-summary-box-inner">
<span><p>We introduce regularized Frank-Wolfe, a general and effective algorithm for
inference and learning of dense conditional random fields (CRFs). The algorithm
optimizes a nonconvex continuous relaxation of the CRF inference problem using
vanilla Frank-Wolfe with approximate updates, which are equivalent to
minimizing a regularized energy function. Our proposed method is a
generalization of existing algorithms such as mean field or concave-convex
procedure. This perspective not only offers a unified analysis of these
algorithms, but also allows an easy way of exploring different variants that
potentially yield better performance. We illustrate this in our empirical
results on standard semantic segmentation datasets, where several
instantiations of our regularized Frank-Wolfe outperform mean field inference,
both as a standalone component and as an end-to-end trainable layer in a neural
network. We also show that dense CRFs, coupled with our new algorithms, produce
significant improvements over strong CNN baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Dementia from Speech and Transcripts using Transformers. (arXiv:2110.14769v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14769">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious
consequences to peoples' everyday lives, if it is not diagnosed early since
there is no available cure. Because of the cost of examinations for diagnosing
dementia, i.e., Magnetic Resonance Imaging (MRI), electroencephalogram (EEG)
signals etc., current work has been focused on diagnosing dementia from
spontaneous speech. However, little work has been done regarding the conversion
of speech data to Log-Mel spectrograms and Mel-frequency cepstral coefficients
(MFCCs) and the usage of pretrained models. Concurrently, little work has been
done in terms of both the usage of transformer networks and the way the two
modalities, i.e., speech and transcripts, are combined in a single neural
network. To address these limitations, first we employ several pretrained
models, with Vision Transformer (ViT) achieving the highest evaluation results.
Secondly, we propose multimodal models. More specifically, our introduced
models include Gated Multimodal Unit in order to control the influence of each
modality towards the final classification and crossmodal attention so as to
capture in an effective way the relationships between the two modalities.
Extensive experiments conducted on the ADReSS Challenge dataset demonstrate the
effectiveness of the proposed models and their superiority over
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SiamPolar: Semi-supervised Realtime Video Object Segmentation with Polar Representation. (arXiv:2110.14773v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14773">
<div class="article-summary-box-inner">
<span><p>Video object segmentation (VOS) is an essential part of autonomous vehicle
navigation. The real-time speed is very important for the autonomous vehicle
algorithms along with the accuracy metric. In this paper, we propose a
semi-supervised real-time method based on the Siamese network using a new polar
representation. The input of bounding boxes is initialized rather than the
object masks, which are applied to the video object detection tasks. The polar
representation could reduce the parameters for encoding masks with subtle
accuracy loss so that the algorithm speed can be improved significantly. An
asymmetric siamese network is also developed to extract the features from
different spatial scales. Moreover, the peeling convolution is proposed to
reduce the antagonism among the branches of the polar head. The repeated
cross-correlation and semi-FPN are designed based on this idea. The
experimental results on the DAVIS-2016 dataset and other public datasets
demonstrate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BI-GCN: Boundary-Aware Input-Dependent Graph Convolution Network for Biomedical Image Segmentation. (arXiv:2110.14775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14775">
<div class="article-summary-box-inner">
<span><p>Segmentation is an essential operation of image processing. The convolution
operation suffers from a limited receptive field, while global modelling is
fundamental to segmentation tasks. In this paper, we apply graph convolution
into the segmentation task and propose an improved \textit{Laplacian}.
Different from existing methods, our \textit{Laplacian} is data-dependent, and
we introduce two attention diagonal matrices to learn a better vertex
relationship. In addition, it takes advantage of both region and boundary
information when performing graph-based information propagation. Specifically,
we model and reason about the boundary-aware region-wise correlations of
different classes through learning graph representations, which is capable of
manipulating long range semantic reasoning across various regions with the
spatial enhancement along the object's boundary. Our model is well-suited to
obtain global semantic region information while also accommodates local spatial
boundary characteristics simultaneously. Experiments on two types of
challenging datasets demonstrate that our method outperforms the
state-of-the-art approaches on the segmentation of polyps in colonoscopy images
and of the optic disc and optic cup in colour fundus images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCALP -- Supervised Contrastive Learning for Cardiopulmonary Disease Classification and Localization in Chest X-rays using Patient Metadata. (arXiv:2110.14787v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14787">
<div class="article-summary-box-inner">
<span><p>Computer-aided diagnosis plays a salient role in more accessible and accurate
cardiopulmonary diseases classification and localization on chest radiography.
Millions of people get affected and die due to these diseases without an
accurate and timely diagnosis. Recently proposed contrastive learning heavily
relies on data augmentation, especially positive data augmentation. However,
generating clinically-accurate data augmentations for medical images is
extremely difficult because the common data augmentation methods in computer
vision, such as sharp, blur, and crop operations, can severely alter the
clinical settings of medical images. In this paper, we proposed a novel and
simple data augmentation method based on patient metadata and supervised
knowledge to create clinically accurate positive and negative augmentations for
chest X-rays. We introduce an end-to-end framework, SCALP, which extends the
self-supervised contrastive approach to a supervised setting. Specifically,
SCALP pulls together chest X-rays from the same patient (positive keys) and
pushes apart chest X-rays from different patients (negative keys). In addition,
it uses ResNet-50 along with the triplet-attention mechanism to identify
cardiopulmonary diseases, and Grad-CAM++ to highlight the abnormal regions. Our
extensive experiments demonstrate that SCALP outperforms existing baselines
with significant margins in both classification and localization tasks.
Specifically, the average classification AUCs improve from 82.8% (SOTA using
DenseNet-121) to 83.9% (SCALP using ResNet-50), while the localization results
improve on average by 3.7% over different IoU thresholds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification. (arXiv:2110.14795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14795">
<div class="article-summary-box-inner">
<span><p>We introduce MedMNIST v2, a large-scale MNIST-like dataset collection of
standardized biomedical images, including 12 datasets for 2D and 6 datasets for
3D. All images are pre-processed into a small size of 28x28 (2D) or 28x28x28
(3D) with the corresponding classification labels so that no background
knowledge is required for users. Covering primary data modalities in biomedical
images, MedMNIST v2 is designed to perform classification on lightweight 2D and
3D images with various dataset scales (from 100 to 100,000) and diverse tasks
(binary/multi-class, ordinal regression, and multi-label). The resulting
dataset, consisting of 708,069 2D images and 10,214 3D images in total, could
support numerous research / educational purposes in biomedical image analysis,
computer vision, and machine learning. We benchmark several baseline methods on
MedMNIST v2, including 2D / 3D neural networks and open-source / commercial
AutoML tools. The data and code are publicly available at
https://medmnist.com/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intermediate Layers Matter in Momentum Contrastive Self Supervised Learning. (arXiv:2110.14805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14805">
<div class="article-summary-box-inner">
<span><p>We show that bringing intermediate layers' representations of two augmented
versions of an image closer together in self-supervised learning helps to
improve the momentum contrastive (MoCo) method. To this end, in addition to the
contrastive loss, we minimize the mean squared error between the intermediate
layer representations or make their cross-correlation matrix closer to an
identity matrix. Both loss objectives either outperform standard MoCo, or
achieve similar performances on three diverse medical imaging datasets:
NIH-Chest Xrays, Breast Cancer Histopathology, and Diabetic Retinopathy. The
gains of the improved MoCo are especially large in a low-labeled data regime
(e.g. 1% labeled data) with an average gain of 5% across three datasets. We
analyze the models trained using our novel approach via feature similarity
analysis and layer-wise probing. Our analysis reveals that models trained via
our approach have higher feature reuse compared to a standard MoCo and learn
informative features earlier in the network. Finally, by comparing the output
probability distribution of models fine-tuned on small versus large labeled
data, we conclude that our proposed method of pre-training leads to lower
Kolmogorov-Smirnov distance, as compared to a standard MoCo. This provides
additional evidence that our proposed method learns more informative features
in the pre-training phase which could be leveraged in a low-labeled data
regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing and Taming Resolution in Convolutional Neural Networks. (arXiv:2110.14819v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14819">
<div class="article-summary-box-inner">
<span><p>Image resolution has a significant effect on the accuracy and computational,
storage, and bandwidth costs of computer vision model inference. These costs
are exacerbated when scaling out models to large inference serving systems and
make image resolution an attractive target for optimization. However, the
choice of resolution inherently introduces additional tightly coupled choices,
such as image crop size, image detail, and compute kernel implementation that
impact computational, storage, and bandwidth costs. Further complicating this
setting, the optimal choices from the perspective of these metrics are highly
dependent on the dataset and problem scenario. We characterize this tradeoff
space, quantitatively studying the accuracy and efficiency tradeoff via
systematic and automated tuning of image resolution, image quality and
convolutional neural network operators. With the insights from this study, we
propose a dynamic resolution mechanism that removes the need to statically
choose a resolution ahead of time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ODMTCNet: An Interpretable Multi-view Deep Neural Network Architecture for Image Feature Representation. (arXiv:2110.14830v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14830">
<div class="article-summary-box-inner">
<span><p>This work proposes an interpretable multi-view deep neural network
architecture, namely optimal discriminant multi-view tensor convolutional
network (ODMTCNet), by integrating statistical machine learning (SML)
principles with the deep neural network (DNN) architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-visual Representation Learning for Anomaly Events Detection in Crowds. (arXiv:2110.14862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14862">
<div class="article-summary-box-inner">
<span><p>In recent years, anomaly events detection in crowd scenes attracts many
researchers' attention, because of its importance to public safety. Existing
methods usually exploit visual information to analyze whether any abnormal
events have occurred due to only visual sensors are generally equipped in
public places. However, when an abnormal event in crowds occurs, sound
information may be discriminative to assist the crowd analysis system to
determine whether there is an abnormality. Compare with vision information that
is easily occluded, audio signals have a certain degree of penetration. Thus,
this paper attempt to exploit multi-modal learning for modeling the audio and
visual signals simultaneously. To be specific, we design a two-branch network
to model different types of information. The first is a typical 3D CNN model to
extract temporal appearance features from video clips. The second is an audio
CNN for encoding Log Mel-Spectrogram of audio signals. Finally, by fusing the
above features, a more accurate prediction will be produced. We conduct the
experiments on SHADE dataset, a synthetic audio-visual dataset in surveillance
scenes, and find introducing audio signals effectively improves the performance
of anomaly events detection and outperforms other state-of-the-art methods.
Furthermore, we will release the code and the pre-trained models as soon as
possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Depthwise-Separable Convolutions for Adversarially Robust and Efficient Neural Networks. (arXiv:2110.14871v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14871">
<div class="article-summary-box-inner">
<span><p>Despite their tremendous successes, convolutional neural networks (CNNs)
incur high computational/storage costs and are vulnerable to adversarial
perturbations. Recent works on robust model compression address these
challenges by combining model compression techniques with adversarial training.
But these methods are unable to improve throughput (frames-per-second) on
real-life hardware while simultaneously preserving robustness to adversarial
perturbations. To overcome this problem, we propose the method of Generalized
Depthwise-Separable (GDWS) convolution -- an efficient, universal,
post-training approximation of a standard 2D convolution. GDWS dramatically
improves the throughput of a standard pre-trained network on real-life hardware
while preserving its robustness. Lastly, GDWS is scalable to large problem
sizes since it operates on pre-trained models and doesn't require any
additional training. We establish the optimality of GDWS as a 2D convolution
approximator and present exact algorithms for constructing optimal GDWS
convolutions under complexity and error constraints. We demonstrate the
effectiveness of GDWS via extensive experiments on CIFAR-10, SVHN, and ImageNet
datasets. Our code can be found at https://github.com/hsndbk4/GDWS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14883">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture has improved the performance of deep learning
models in domains such as Computer Vision and Natural Language Processing.
Together with better performance come larger model sizes. This imposes
challenges to the memory wall of the current accelerator hardware such as GPU.
It is never ideal to train large models such as Vision Transformer, BERT, and
GPT on a single GPU or a single machine. There is an urgent demand to train
models in a distributed environment. However, distributed training, especially
model parallelism, often requires domain expertise in computer systems and
architecture. It remains a challenge for AI researchers to implement complex
distributed training solutions for their models.
</p>
<p>In this paper, we introduce Colossal-AI, which is a unified parallel training
system designed to seamlessly integrate different paradigms of parallelization
techniques including data parallelism, pipeline parallelism, multiple tensor
parallelism, and sequence parallelism. Colossal-AI aims to support the AI
community to write distributed models in the same way as how they write models
normally. This allows them to focus on developing the model architecture and
separates the concerns of distributed training from the development process.
The documentations can be found at https://www.colossalai.org and the source
code can be found at https://github.com/hpcaitech/ColossalAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Degraded Reference Image Quality Assessment. (arXiv:2110.14899v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14899">
<div class="article-summary-box-inner">
<span><p>In practical media distribution systems, visual content usually undergoes
multiple stages of quality degradation along the delivery chain, but the
pristine source content is rarely available at most quality monitoring points
along the chain to serve as a reference for quality assessment. As a result,
full-reference (FR) and reduced-reference (RR) image quality assessment (IQA)
methods are generally infeasible. Although no-reference (NR) methods are
readily applicable, their performance is often not reliable. On the other hand,
intermediate references of degraded quality are often available, e.g., at the
input of video transcoders, but how to make the best use of them in proper ways
has not been deeply investigated. Here we make one of the first attempts to
establish a new paradigm named degraded-reference IQA (DR IQA). Specifically,
we lay out the architectures of DR IQA and introduce a 6-bit code to denote the
choices of configurations. We construct the first large-scale databases
dedicated to DR IQA and will make them publicly available. We make novel
observations on distortion behavior in multi-stage distortion pipelines by
comprehensively analyzing five multiple distortion combinations. Based on these
observations, we develop novel DR IQA models and make extensive comparisons
with a series of baseline models derived from top-performing FR and NR models.
The results suggest that DR IQA may offer significant performance improvement
in multiple distortion environments, thereby establishing DR IQA as a valid IQA
paradigm that is worth further exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches. (arXiv:2110.14908v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14908">
<div class="article-summary-box-inner">
<span><p>What makes speeches effective has long been a subject for debate, and until
today there is broad controversy among public speaking experts about what
factors make a speech effective as well as the roles of these factors in
speeches. Moreover, there is a lack of quantitative analysis methods to help
understand effective speaking strategies. In this paper, we propose E-ffective,
a visual analytic system allowing speaking experts and novices to analyze both
the role of speech factors and their contribution in effective speeches. From
interviews with domain experts and investigating existing literature, we
identified important factors to consider in inspirational speeches. We obtained
the generated factors from multi-modal data that were then related to
effectiveness data. Our system supports rapid understanding of critical factors
in inspirational speeches, including the influence of emotions by means of
novel visualization methods and interaction. Two novel visualizations include
E-spiral (that shows the emotional shifts in speeches in a visually compact
way) and E-script (that connects speech content with key speech delivery
information). In our evaluation we studied the influence of our system on
experts' domain knowledge about speech factors. We further studied the
usability of the system by speaking novices and experts on assisting analysis
of inspirational speech effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Object Tracking with Transformer. (arXiv:2110.14921v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14921">
<div class="article-summary-box-inner">
<span><p>Feature fusion and similarity computation are two core problems in 3D object
tracking, especially for object tracking using sparse and disordered point
clouds. Feature fusion could make similarity computing more efficient by
including target object information. However, most existing LiDAR-based
approaches directly use the extracted point cloud feature to compute similarity
while ignoring the attention changes of object regions during tracking. In this
paper, we propose a feature fusion network based on transformer architecture.
Benefiting from the self-attention mechanism, the transformer encoder captures
the inter- and intra- relations among different regions of the point cloud. By
using cross-attention, the transformer decoder fuses features and includes more
target cues into the current point cloud feature to compute the region
attentions, which makes the similarity computing more efficient. Based on this
feature fusion network, we propose an end-to-end point cloud object tracking
framework, a simple yet effective method for 3D object tracking using point
clouds. Comprehensive experimental results on the KITTI dataset show that our
method achieves new state-of-the-art performance. Code is available at:
https://github.com/3bobo/lttr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Explanation of Brain Activity Classifiers using Image-to-Image Transfer by Generative Adversarial Network. (arXiv:2110.14927v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14927">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) can accurately decode task-related information
from brain activations. However, because of the nonlinearity of the DNN, the
decisions made by DNNs are hardly interpretable. One of the promising
approaches for explaining such a black-box system is counterfactual
explanation. In this framework, the behavior of a black-box system is explained
by comparing real data and realistic synthetic data that are specifically
generated such that the black-box system outputs an unreal outcome. Here we
introduce a novel generative DNN (counterfactual activation generator, CAG)
that can provide counterfactual explanations for DNN-based classifiers of brain
activations. Importantly, CAG can simultaneously handle image transformation
among multiple classes associated with different behavioral tasks. Using CAG,
we demonstrated counterfactual explanation of DNN-based classifiers that
learned to discriminate brain activations of seven behavioral tasks.
Furthermore, by iterative applications of CAG, we were able to enhance and
extract subtle spatial brain activity patterns that affected the classifier's
decisions. Together, these results demonstrate that the counterfactual
explanation based on image-to-image transformation would be a promising
approach to understand and extend the current application of DNNs in fMRI
analyses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A recursive robust filtering approach for 3D registration. (arXiv:2110.14932v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14932">
<div class="article-summary-box-inner">
<span><p>This work presents a new recursive robust filtering approach for
feature-based 3D registration. Unlike the common state-of-the-art alignment
algorithms, the proposed method has four advantages that have not yet occurred
altogether in any previous solution. For instance, it is able to deal with
inherent noise contaminating sensory data; it is robust to uncertainties caused
by noisy feature localisation; it also combines the advantages of both (Formula
presented.) and (Formula presented.) norms for a higher performance and a more
prospective prevention of local minima. The result is an accurate and stable
rigid body transformation. The latter enables a thorough control over the
convergence regarding the alignment as well as a correct assessment of the
quality of registration. The mathematical rationale behind the proposed
approach is explained, and the results are validated on physical and synthetic
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPU based GMM segmentation of kinect data. (arXiv:2110.14934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14934">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel approach for background/foreground segmentation
of RGBD data with the Gaussian Mixture Models (GMM). We first start by the
background subtraction from the colour and depth images separately. The
foregrounds resulting from both streams are then fused for a more accurate
detection. Our segmentation solution is implemented on the GPU. Thus, it works
at the full frame rate of the sensor (30fps). Test results show its robustness
against illumination change, shadows and reflections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FocusFace: Multi-task Contrastive Learning for Masked Face Recognition. (arXiv:2110.14940v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14940">
<div class="article-summary-box-inner">
<span><p>SARS-CoV-2 has presented direct and indirect challenges to the scientific
community. One of the most prominent indirect challenges advents from the
mandatory use of face masks in a large number of countries. Face recognition
methods struggle to perform identity verification with similar accuracy on
masked and unmasked individuals. It has been shown that the performance of
these methods drops considerably in the presence of face masks, especially if
the reference image is unmasked. We propose FocusFace, a multi-task
architecture that uses contrastive learning to be able to accurately perform
masked face recognition. The proposed architecture is designed to be trained
from scratch or to work on top of state-of-the-art face recognition methods
without sacrificing the capabilities of a existing models in conventional face
recognition tasks. We also explore different approaches to design the
contrastive learning module. Results are presented in terms of masked-masked
(M-M) and unmasked-masked (U-M) face verification performance. For both
settings, the results are on par with published methods, but for M-M
specifically, the proposed method was able to outperform all the solutions that
it was compared to. We further show that when using our method on top of
already existing methods the training computational costs decrease
significantly while retaining similar performances. The implementation and the
trained models are available at GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dispensed Transformer Network for Unsupervised Domain Adaptation. (arXiv:2110.14944v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14944">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation is a crucial step in medical image analysis and
applying supervised machine learning to segment the organs or lesions has been
substantiated effective. However, it is costly to perform data annotation that
provides ground truth labels for training the supervised algorithms, and the
high variance of data that comes from different domains tends to severely
degrade system performance over cross-site or cross-modality datasets. To
mitigate this problem, a novel unsupervised domain adaptation (UDA) method
named dispensed Transformer network (DTNet) is introduced in this paper. Our
novel DTNet contains three modules. First, a dispensed residual transformer
block is designed, which realizes global attention by dispensed interleaving
operation and deals with the excessive computational cost and GPU memory usage
of the Transformer. Second, a multi-scale consistency regularization is
proposed to alleviate the loss of details in the low-resolution output for
better feature alignment. Finally, a feature ranking discriminator is
introduced to automatically assign different weights to domain-gap features to
lessen the feature distribution distance, reducing the performance shift of two
domains. The proposed method is evaluated on large fluorescein angiography (FA)
retinal nonperfusion (RNP) cross-site dataset with 676 images and a wide used
cross-modality dataset from the MM-WHS challenge. Extensive results demonstrate
that our proposed network achieves the best performance in comparison with
several state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Large-Scale Rendering of Simulated Crops for Synthetic Ground Truth Generation on Modular Supercomputers. (arXiv:2110.14946v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14946">
<div class="article-summary-box-inner">
<span><p>Computer Vision problems deal with the semantic extraction of information
from camera images. Especially for field crop images, the underlying problems
are hard to label and even harder to learn, and the availability of
high-quality training data is low. Deep neural networks do a good job of
extracting the necessary models from training examples. However, they rely on
an abundance of training data that is not feasible to generate or label by
expert annotation. To address this challenge, we make use of the Unreal Engine
to render large and complex virtual scenes. We rely on the performance of
individual nodes by distributing plant simulations across nodes and both
generate scenes as well as train neural networks on GPUs, restricting node
communication to parallel learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocScanner: Robust Document Image Rectification with Progressive Learning. (arXiv:2110.14968v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14968">
<div class="article-summary-box-inner">
<span><p>Compared to flatbed scanners, portable smartphones are much more convenient
for physical documents digitizing. However, such digitized documents are often
distorted due to uncontrolled physical deformations, camera positions, and
illumination variations. To this end, this work presents DocScanner, a new deep
network architecture for document image rectification. Different from existing
methods, DocScanner addresses this issue by introducing a progressive learning
mechanism. Specifically, DocScanner maintains a single estimate of the
rectified image, which is progressively corrected with a recurrent
architecture. The iterative refinements make DocScanner converge to a robust
and superior performance, and the lightweight recurrent architecture ensures
the running efficiency. In addition, before the above rectification process,
observing the corrupted rectified boundaries existing in prior works,
DocScanner exploits a document localization module to explicitly segment the
foreground document from the cluttered background environments. To further
improve the rectification quality, based on the geometric priori between the
distorted and the rectified images, a geometric regularization is introduced
during training to further facilitate the performance. Extensive experiments
are conducted on the Doc3D dataset and the DocUNet benchmark dataset, and the
quantitative and qualitative evaluation results verify the effectiveness of
DocScanner, which outperforms previous methods on OCR accuracy, image
similarity, and our proposed distortion metric by a considerable margin.
Furthermore, our DocScanner shows the highest efficiency in inference time and
parameter count.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skeleton-Based Mutually Assisted Interacted Object Localization and Human Action Recognition. (arXiv:2110.14994v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14994">
<div class="article-summary-box-inner">
<span><p>Skeleton data carries valuable motion information and is widely explored in
human action recognition. However, not only the motion information but also the
interaction with the environment provides discriminative cues to recognize the
action of persons. In this paper, we propose a joint learning framework for
mutually assisted "interacted object localization" and "human action
recognition" based on skeleton data. The two tasks are serialized together and
collaborate to promote each other, where preliminary action type derived from
skeleton alone helps improve interacted object localization, which in turn
provides valuable cues for the final human action recognition. Besides, we
explore the temporal consistency of interacted object as constraint to better
localize the interacted object with the absence of ground-truth labels.
Extensive experiments on the datasets of SYSU-3D, NTU60 RGB+D and
Northwestern-UCLA show that our method achieves the best or competitive
performance with the state-of-the-art methods for human action recognition.
Visualization results show that our method can also provide reasonable
interacted object localization results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sliding Sequential CVAE with Time Variant Socially-aware Rethinking for Trajectory Prediction. (arXiv:2110.15016v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15016">
<div class="article-summary-box-inner">
<span><p>Pedestrian trajectory prediction is a key technology in many applications
such as video surveillance, social robot navigation, and autonomous driving,
and significant progress has been made in this research topic. However, there
remain two limitations of previous studies. First, with the continuation of
time, the prediction error at each time step increases significantly, causing
the final displacement error to be impossible to ignore. Second, the prediction
results of multiple pedestrians might be impractical in the prediction horizon,
i.e., the predicted trajectories might collide with each other. To overcome
these limitations, this work proposes a novel trajectory prediction method
called CSR, which consists of a cascaded conditional variational autoencoder
(CVAE) module and a socially-aware regression module. The cascaded CVAE module
first estimates the future trajectories in a sequential pattern. Specifically,
each CVAE concatenates the past trajectories and the predicted points so far as
the input and predicts the location at the following time step. Then, the
socially-aware regression module generates offsets from the estimated future
trajectories to produce the socially compliant final predictions, which are
more reasonable and accurate results than the estimated trajectories. Moreover,
considering the large model parameters of the cascaded CVAE module, a slide
CVAE module is further exploited to improve the model efficiency using one
shared CVAE, in a slidable manner. Experiments results demonstrate that the
proposed method exhibits improvements over state-of-the-art method on the
Stanford Drone Dataset (SDD) and ETH/UCY of approximately 38.0% and 22.2%,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Non Co-occurrence with Unlabeled In-the-wild Data for Incremental Object Detection. (arXiv:2110.15017v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15017">
<div class="article-summary-box-inner">
<span><p>Deep networks have shown remarkable results in the task of object detection.
However, their performance suffers critical drops when they are subsequently
trained on novel classes without any sample from the base classes originally
used to train the model. This phenomenon is known as catastrophic forgetting.
Recently, several incremental learning methods are proposed to mitigate
catastrophic forgetting for object detection. Despite the effectiveness, these
methods require co-occurrence of the unlabeled base classes in the training
data of the novel classes. This requirement is impractical in many real-world
settings since the base classes do not necessarily co-occur with the novel
classes. In view of this limitation, we consider a more practical setting of
complete absence of co-occurrence of the base and novel classes for the object
detection task. We propose the use of unlabeled in-the-wild data to bridge the
non co-occurrence caused by the missing base classes during the training of
additional novel classes. To this end, we introduce a blind sampling strategy
based on the responses of the base-class model and pre-trained novel-class
model to select a smaller relevant dataset from the large in-the-wild dataset
for incremental learning. We then design a dual-teacher distillation framework
to transfer the knowledge distilled from the base- and novel-class teacher
models to the student model using the sampled in-the-wild data. Experimental
results on the PASCAL VOC and MS COCO datasets show that our proposed method
significantly outperforms other state-of-the-art class-incremental object
detection methods when there is no co-occurrence between the base and novel
classes during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformable Registration of Brain MR Images via a Hybrid Loss. (arXiv:2110.15027v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15027">
<div class="article-summary-box-inner">
<span><p>We learn a deformable registration model for T1-weighted MR images by
considering multiple image characteristics via a hybrid loss. Our method
registers the OASIS dataset with high accuracy while preserving deformation
smoothness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Emotion Recognition: A multi-task approach using deep learning. (arXiv:2110.15028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15028">
<div class="article-summary-box-inner">
<span><p>Facial Emotion Recognition is an inherently difficult problem, due to vast
differences in facial structures of individuals and ambiguity in the emotion
displayed by a person. Recently, a lot of work is being done in the field of
Facial Emotion Recognition, and the performance of the CNNs for this task has
been inferior compared to the results achieved by CNNs in other fields like
Object detection, Facial recognition etc. In this paper, we propose a
multi-task learning algorithm, in which a single CNN detects gender, age and
race of the subject along with their emotion. We validate this proposed
methodology using two datasets containing real-world images. The results show
that this approach is significantly better than the current State of the art
algorithms for this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explicitly Modeling the Discriminability for Instance-Aware Visual Object Tracking. (arXiv:2110.15030v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15030">
<div class="article-summary-box-inner">
<span><p>Visual object tracking performance has been dramatically improved in recent
years, but some severe challenges remain open, like distractors and occlusions.
We suspect the reason is that the feature representations of the tracking
targets are only expressively learned but not fully discriminatively modeled.
In this paper, we propose a novel Instance-Aware Tracker (IAT) to explicitly
excavate the discriminability of feature representations, which improves the
classical visual tracking pipeline with an instance-level classifier. First, we
introduce a contrastive learning mechanism to formulate the classification
task, ensuring that every training sample could be uniquely modeled and be
highly distinguishable from plenty of other samples. Besides, we design an
effective negative sample selection scheme to contain various intra and inter
classes in the instance classification branch. Furthermore, we implement two
variants of the proposed IAT, including a video-level one and an object-level
one. They realize the concept of \textbf{instance} in different granularity as
videos and target bounding boxes, respectively. The former enhances the ability
to recognize the target from the background while the latter boosts the
discriminative power for mitigating the target-distractor dilemma. Extensive
experimental evaluations on 8 benchmark datasets show that both two versions of
the proposed IAT achieve leading results against state-of-the-art methods while
running at 30FPS. Code will be available when it is published.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Deep Representation with Energy-Based Self-Expressiveness for Subspace Clustering. (arXiv:2110.15037v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15037">
<div class="article-summary-box-inner">
<span><p>Deep subspace clustering has attracted increasing attention in recent years.
Almost all the existing works are required to load the whole training data into
one batch for learning the self-expressive coefficients in the framework of
deep learning. Although these methods achieve promising results, such a
learning fashion severely prevents from the usage of deeper neural network
architectures (e.g., ResNet), leading to the limited representation abilities
of the models. In this paper, we propose a new deep subspace clustering
framework, motivated by the energy-based models. In contrast to previous
approaches taking the weights of a fully connected layer as the self-expressive
coefficients, we propose to learn an energy-based network to obtain the
self-expressive coefficients by mini-batch training. By this means, it is no
longer necessary to load all data into one batch for learning, and it thus
becomes a reality that we can utilize deeper neural network models for subspace
clustering. Considering the powerful representation ability of the recently
popular self-supervised learning, we attempt to leverage self-supervised
representation learning to learn the dictionary. Finally, we propose a joint
framework to learn both the self-expressive coefficients and dictionary
simultaneously, and train the model in an end-to-end manner. The experiments
are performed on three publicly available datasets, and extensive experimental
results demonstrate our method can significantly outperform the other related
approaches. For instance, on the three datasets, our method can averagely
achieve $13.8\%$, $15.4\%$, $20.8\%$ improvements in terms of Accuracy, NMI,
and ARI over SENet which is proposed very recently and obtains the second best
results in the experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LF-YOLO: A Lighter and Faster YOLO for Weld Defect Detection of X-ray Image. (arXiv:2110.15045v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15045">
<div class="article-summary-box-inner">
<span><p>X-ray image plays an important role in manufacturing for quality assurance,
because it can reflect the internal condition of weld region. However, the
shape and scale of different defect types vary greatly, which makes it
challenging for model to detect weld defects. In this paper, we propose a weld
defect detection method based on convolution neural network (CNN), namely
Lighter and Faster YOLO (LF-YOLO). In particularly, an enhanced multiscale
feature (EMF) module is designed to implement both parameter-based and
parameter-free multi-scale information extracting operation. EMF enables the
extracted feature map capable to represent more plentiful information, which is
achieved by superior hierarchical fusion structure. To improve the performance
of detection network, we propose an efficient feature extraction (EFE) module.
EFE processes input data with extremely low consumption, and improve the
practicability of whole network in actual industry. Experimental results show
that our weld defect network achieves satisfactory balance between performance
and consumption, and reaches 92.9 mAP50 with 61.5 FPS. To further prove the
ability of our method, we test it on public dataset MS COCO, and the results
show that our LF-YOLO has a outstanding versatility detection performance. The
code is available at https://github.com/lmomoy/LF-YOLO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Robustness in Multi-Task Learning: Promises and Illusions. (arXiv:2110.15053v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15053">
<div class="article-summary-box-inner">
<span><p>Vulnerability to adversarial attacks is a well-known weakness of Deep Neural
networks. While most of the studies focus on single-task neural networks with
computer vision datasets, very little research has considered complex
multi-task models that are common in real applications. In this paper, we
evaluate the design choices that impact the robustness of multi-task deep
learning networks. We provide evidence that blindly adding auxiliary tasks, or
weighing the tasks provides a false sense of robustness. Thereby, we tone down
the claim made by previous research and study the different factors which may
affect robustness. In particular, we show that the choice of the task to
incorporate in the loss function are important factors that can be leveraged to
yield more robust models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Guided Metric Learner for Overcoming Class Confusion in Few-Shot Road Object Detection. (arXiv:2110.15074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15074">
<div class="article-summary-box-inner">
<span><p>Localization and recognition of less-occurring road objects have been a
challenge in autonomous driving applications due to the scarcity of data
samples. Few-Shot Object Detection techniques extend the knowledge from
existing base object classes to learn novel road objects given few training
examples. Popular techniques in FSOD adopt either meta or metric learning
techniques which are prone to class confusion and base class forgetting. In
this work, we introduce a novel Meta Guided Metric Learner (MGML) to overcome
class confusion in FSOD. We re-weight the features of the novel classes higher
than the base classes through a novel Squeeze and Excite module and encourage
the learning of truly discriminative class-specific features by applying an
Orthogonality Constraint to the meta learner. Our method outperforms
State-of-the-Art (SoTA) approaches in FSOD on the India Driving Dataset (IDD)
by upto 11 mAP points while suffering from the least class confusion of 20%
given only 10 examples of each novel road object. We further show similar
improvements on the few-shot splits of PASCAL VOC dataset where we outperform
SoTA approaches by upto 5.8 mAP accross all splits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpineOne: A One-Stage Detection Framework for Degenerative Discs and Vertebrae. (arXiv:2110.15082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15082">
<div class="article-summary-box-inner">
<span><p>Spinal degeneration plagues many elders, office workers, and even the younger
generations. Effective pharmic or surgical interventions can help relieve
degenerative spine conditions. However, the traditional diagnosis procedure is
often too laborious. Clinical experts need to detect discs and vertebrae from
spinal magnetic resonance imaging (MRI) or computed tomography (CT) images as a
preliminary step to perform pathological diagnosis or preoperative evaluation.
Machine learning systems have been developed to aid this procedure generally
following a two-stage methodology: first perform anatomical localization, then
pathological classification. Towards more efficient and accurate diagnosis, we
propose a one-stage detection framework termed SpineOne to simultaneously
localize and classify degenerative discs and vertebrae from MRI slices.
SpineOne is built upon the following three key techniques: 1) a new design of
the keypoint heatmap to facilitate simultaneous keypoint localization and
classification; 2) the use of attention modules to better differentiate the
representations between discs and vertebrae; and 3) a novel gradient-guided
objective association mechanism to associate multiple learning objectives at
the later training stage. Empirical results on the Spinal Disease Intelligent
Diagnosis Tianchi Competition (SDID-TC) dataset of 550 exams demonstrate that
our approach surpasses existing methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data. (arXiv:2110.15094v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15094">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation~(KD) aims to craft a compact student model that
imitates the behavior of a pre-trained teacher in a target domain. Prior KD
approaches, despite their gratifying results, have largely relied on the
premise that \emph{in-domain} data is available to carry out the knowledge
transfer. Such an assumption, unfortunately, in many cases violates the
practical setting, since the original training data or even the data domain is
often unreachable due to privacy or copyright reasons. In this paper, we
attempt to tackle an ambitious task, termed as \emph{out-of-domain} knowledge
distillation~(OOD-KD), which allows us to conduct KD using only OOD data that
can be readily obtained at a very low cost. Admittedly, OOD-KD is by nature a
highly challenging task due to the agnostic domain gap. To this end, we
introduce a handy yet surprisingly efficacious approach, dubbed
as~\textit{MosaicKD}. The key insight behind MosaicKD lies in that, samples
from various domains share common local patterns, even though their global
semantic may vary significantly; these shared local patterns, in turn, can be
re-assembled analogous to mosaic tiling, to approximate the in-domain data and
to further alleviating the domain discrepancy. In MosaicKD, this is achieved
through a four-player min-max game, in which a generator, a discriminator, a
student network, are collectively trained in an adversarial manner, partially
under the guidance of a pre-trained teacher. We validate MosaicKD over
{classification and semantic segmentation tasks} across various benchmarks, and
demonstrate that it yields results much superior to the state-of-the-art
counterparts on OOD data. Our code is available at
\url{https://github.com/zju-vipa/MosaicKD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing. (arXiv:2110.15128v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15128">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation which aims to adapt models trained on a
labeled source domain to a completely unlabeled target domain has attracted
much attention in recent years. While many domain adaptation techniques have
been proposed for images, the problem of unsupervised domain adaptation in
videos remains largely underexplored. In this paper, we introduce Contrast and
Mix (CoMix), a new contrastive learning framework that aims to learn
discriminative invariant feature representations for unsupervised video domain
adaptation. First, unlike existing methods that rely on adversarial learning
for feature alignment, we utilize temporal contrastive learning to bridge the
domain gap by maximizing the similarity between encoded representations of an
unlabeled video at two different speeds as well as minimizing the similarity
between different videos played at different speeds. Second, we propose a novel
extension to the temporal contrastive loss by using background mixing that
allows additional positives per anchor, thus adapting contrastive learning to
leverage action semantics shared across both domains. Moreover, we also
integrate a supervised contrastive learning objective using target
pseudo-labels to enhance discriminability of the latent space for video domain
adaptation. Extensive experiments on several benchmark datasets demonstrate the
superiority of our proposed approach over state-of-the-art methods. Project
page: https://cvir.github.io/projects/comix
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blending Anti-Aliasing into Vision Transformer. (arXiv:2110.15156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15156">
<div class="article-summary-box-inner">
<span><p>The transformer architectures, based on self-attention mechanism and
convolution-free design, recently found superior performance and booming
applications in computer vision. However, the discontinuous patch-wise
tokenization process implicitly introduces jagged artifacts into attention
maps, arising the traditional problem of aliasing for vision transformers.
Aliasing effect occurs when discrete patterns are used to produce high
frequency or continuous information, resulting in the indistinguishable
distortions. Recent researches have found that modern convolution networks
still suffer from this phenomenon. In this work, we analyze the uncharted
problem of aliasing in vision transformer and explore to incorporate
anti-aliasing properties. Specifically, we propose a plug-and-play
Aliasing-Reduction Module(ARM) to alleviate the aforementioned issue. We
investigate the effectiveness and generalization of the proposed method across
multiple tasks and various vision transformer families. This lightweight design
consistently attains a clear boost over several famous structures. Furthermore,
our module also improves data efficiency and robustness of vision transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Authentication Attacks on Projection-based Cancelable Biometric Schemes. (arXiv:2110.15163v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15163">
<div class="article-summary-box-inner">
<span><p>Cancelable biometric schemes aim at generating secure biometric templates by
combining user specific tokens, such as password, stored secret or salt, along
with biometric data. This type of transformation is constructed as a
composition of a biometric transformation with a feature extraction algorithm.
The security requirements of cancelable biometric schemes concern the
irreversibility, unlinkability and revocability of templates, without losing in
accuracy of comparison. While several schemes were recently attacked regarding
these requirements, full reversibility of such a composition in order to
produce colliding biometric characteristics, and specifically presentation
attacks, were never demonstrated to the best of our knowledge. In this paper,
we formalize these attacks for a traditional cancelable scheme with the help of
integer linear programming (ILP) and quadratically constrained quadratic
programming (QCQP). Solving these optimization problems allows an adversary to
slightly alter its fingerprint image in order to impersonate any individual.
Moreover, in an even more severe scenario, it is possible to simultaneously
impersonate several individuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Aware Person Detection in Surveillance Data. (arXiv:2110.15171v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15171">
<div class="article-summary-box-inner">
<span><p>Crowd management relies on inspection of surveillance video either by
operators or by object detection models. These models are large, making it
difficult to deploy them on resource constrained edge hardware. Instead, the
computations are often offloaded to a (third party) cloud platform. While crowd
management may be a legitimate application, transferring video from the camera
to remote infrastructure may open the door for extracting additional
information that are infringements of privacy, like person tracking or face
recognition. In this paper, we use adversarial training to obtain a lightweight
obfuscator that transforms video frames to only retain the necessary
information for person detection. Importantly, the obfuscated data can be
processed by publicly available object detectors without retraining and without
significant loss of accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Coarse to Dense 3D Indoor Scene Registration Algorithms. (arXiv:2110.15179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15179">
<div class="article-summary-box-inner">
<span><p>3D alignment has become a very important part of 3D scanning technology. For
instance, we can divide the alignment process into four steps: key point
detection, key point description, initial pose estimation, and alignment
refinement. Researchers have contributed several approaches to the literature
for each step, which suggests a natural need for a comparative study for an
educated more appropriate choice. In this work, we propose a description and an
evaluation of the different methods used for 3D registration with special focus
on RGB-D data to find the best combinations that permit a complete and more
accurate 3D reconstruction of indoor scenes with cheap depth cameras.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The magnitude vector of images. (arXiv:2110.15188v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15188">
<div class="article-summary-box-inner">
<span><p>The magnitude of a finite metric space is a recently-introduced invariant
quantity. Despite beneficial theoretical and practical properties, such as a
general utility for outlier detection, and a close connection to Laplace radial
basis kernels, magnitude has received little attention by the machine learning
community so far. In this work, we investigate the properties of magnitude on
individual images, with each image forming its own metric space. We show that
the known properties of outlier detection translate to edge detection in images
and we give supporting theoretical justifications. In addition, we provide a
proof of concept of its utility by using a novel magnitude layer to defend
against adversarial attacks. Since naive magnitude calculations may be
computationally prohibitive, we introduce an algorithm that leverages the
regular structure of images to dramatically reduce the computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Covariate and Concept Shift for Detection and Calibration of Out-of-Distribution Data. (arXiv:2110.15231v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15231">
<div class="article-summary-box-inner">
<span><p>Moving beyond testing on in-distribution data works on Out-of-Distribution
(OOD) detection have recently increased in popularity. A recent attempt to
categorize OOD data introduces the concept of near and far OOD detection.
Specifically, prior works define characteristics of OOD data in terms of
detection difficulty. We propose to characterize the spectrum of OOD data using
two types of distribution shifts: covariate shift and concept shift, where
covariate shift corresponds to change in style, e.g., noise, and concept shift
indicates a change in semantics. This characterization reveals that sensitivity
to each type of shift is important to the detection and confidence calibration
of OOD data. Consequently, we investigate score functions that capture
sensitivity to each type of dataset shift and methods that improve them. To
this end, we theoretically derive two score functions for OOD detection, the
covariate shift score and concept shift score, based on the decomposition of
KL-divergence for both scores, and propose a geometrically-inspired method
(Geometric ODIN) to improve OOD detection under both shifts with only
in-distribution data. Additionally, the proposed method naturally leads to an
expressive post-hoc calibration function which yields state-of-the-art
calibration performance on both in-distribution and out-of-distribution data.
We are the first to propose a method that works well across both OOD detection
and calibration and under different types of shifts. Specifically, we improve
the previous state-of-the-art OOD detection by relatively 7% AUROC on CIFAR100
vs. SVHN and achieve the best calibration performance of 0.084 Expected
Calibration Error on the corrupted CIFAR100C dataset. View project page at
https://sites.google.com/view/geometric-decomposition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guided Evolution for Neural Architecture Search. (arXiv:2110.15232v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15232">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) methods have been successfully applied to
image tasks with excellent results. However, NAS methods are often complex and
tend to converge to local minima as soon as generated architectures seem to
yield good results. In this paper, we propose G-EA, a novel approach for guided
evolutionary NAS. The rationale behind G-EA, is to explore the search space by
generating and evaluating several architectures in each generation at
initialization stage using a zero-proxy estimator, where only the
highest-scoring network is trained and kept for the next generation. This
evaluation at initialization stage allows continuous extraction of knowledge
from the search space without increasing computation, thus allowing the search
to be efficiently guided. Moreover, G-EA forces exploitation of the most
performant networks by descendant generation while at the same time forcing
exploration by parent mutation and by favouring younger architectures to the
detriment of older ones. Experimental results demonstrate the effectiveness of
the proposed method, showing that G-EA achieves state-of-the-art results in
NAS-Bench-201 search space in CIFAR-10, CIFAR-100 and ImageNet16-120, with mean
accuracies of 93.98%, 72.12% and 45.94% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subpixel object segmentation using wavelets and multi resolution analysis. (arXiv:2110.15233v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15233">
<div class="article-summary-box-inner">
<span><p>We propose a novel deep learning framework for fast prediction of boundaries
of two-dimensional simply connected domains using wavelets and Multi Resolution
Analysis (MRA). The boundaries are modelled as (piecewise) smooth closed curves
using wavelets and the so-called Pyramid Algorithm. Our network architecture is
a hybrid analog of the U-Net, where the down-sampling path is a two-dimensional
encoder with learnable filters, and the upsampling path is a one-dimensional
decoder, which builds curves up from low to high resolution levels. Any wavelet
basis induced by a MRA can be used. This flexibility allows for incorporation
of priors on the smoothness of curves. The effectiveness of the proposed method
is demonstrated by delineating boundaries of simply connected domains (organs)
in medical images using Debauches wavelets and comparing performance with a
U-Net baseline. Our model demonstrates up to 5x faster inference speed compared
to the U-Net, while maintaining similar performance in terms of Dice score and
Hausdorff distance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Learning the Partial Permutation Matrix for Robust 3D Point Cloud Registration. (arXiv:2110.15250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15250">
<div class="article-summary-box-inner">
<span><p>Even though considerable progress has been made in deep learning-based 3D
point cloud processing, how to obtain accurate correspondences for robust
registration remains a major challenge because existing hard assignment methods
cannot deal with outliers naturally. Alternatively, the soft matching-based
methods have been proposed to learn the matching probability rather than hard
assignment. However, in this paper, we prove that these methods have an
inherent ambiguity causing many deceptive correspondences. To address the above
challenges, we propose to learn a partial permutation matching matrix, which
does not assign corresponding points to outliers, and implements hard
assignment to prevent ambiguity. However, this proposal poses two new problems,
i.e., existing hard assignment algorithms can only solve a full rank
permutation matrix rather than a partial permutation matrix, and this desired
matrix is defined in the discrete space, which is non-differentiable. In
response, we design a dedicated soft-to-hard (S2H) matching procedure within
the registration pipeline consisting of two steps: solving the soft matching
matrix (S-step) and projecting this soft matrix to the partial permutation
matrix (H-step). Specifically, we augment the profit matrix before the hard
assignment to solve an augmented permutation matrix, which is cropped to
achieve the final partial permutation matrix. Moreover, to guarantee end-to-end
learning, we supervise the learned partial permutation matrix but propagate the
gradient to the soft matrix instead. Our S2H matching procedure can be easily
integrated with existing registration frameworks, which has been verified in
representative frameworks including DCP, RPMNet, and DGR. Extensive experiments
have validated our method, which creates a new state-of-the-art performance for
robust 3D point cloud registration. The code will be made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning Disentangled Group Representation as Feature. (arXiv:2110.15255v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15255">
<div class="article-summary-box-inner">
<span><p>A good visual representation is an inference map from observations (images)
to features (vectors) that faithfully reflects the hidden modularized
generative factors (semantics). In this paper, we formulate the notion of
"good" representation from a group-theoretic view using Higgins' definition of
disentangled representation, and show that existing Self-Supervised Learning
(SSL) only disentangles simple augmentation features such as rotation and
colorization, thus unable to modularize the remaining semantics. To break the
limitation, we propose an iterative SSL algorithm: Iterative Partition-based
Invariant Risk Minimization (IP-IRM), which successfully grounds the abstract
semantics and the group acting on them into concrete contrastive learning. At
each iteration, IP-IRM first partitions the training samples into two subsets
that correspond to an entangled group element. Then, it minimizes a
subset-invariant contrastive loss, where the invariance guarantees to
disentangle the group element. We prove that IP-IRM converges to a fully
disentangled representation and show its effectiveness on various benchmarks.
Codes are available at https://github.com/Wangt-CN/IP-IRM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UltraPose: Synthesizing Dense Pose with 1 Billion Points by Human-body Decoupling 3D Model. (arXiv:2110.15267v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15267">
<div class="article-summary-box-inner">
<span><p>Recovering dense human poses from images plays a critical role in
establishing an image-to-surface correspondence between RGB images and the 3D
surface of the human body, serving the foundation of rich real-world
applications, such as virtual humans, monocular-to-3d reconstruction. However,
the popular DensePose-COCO dataset relies on a sophisticated manual annotation
system, leading to severe limitations in acquiring the denser and more accurate
annotated pose resources. In this work, we introduce a new 3D human-body model
with a series of decoupled parameters that could freely control the generation
of the body. Furthermore, we build a data generation system based on this
decoupling 3D model, and construct an ultra dense synthetic benchmark
UltraPose, containing around 1.3 billion corresponding points. Compared to the
existing manually annotated DensePose-COCO dataset, the synthetic UltraPose has
ultra dense image-to-surface correspondences without annotation cost and error.
Our proposed UltraPose provides the largest benchmark and data resources for
lifting the model capability in predicting more accurate dense poses. To
promote future researches in this field, we also propose a transformer-based
method to model the dense correspondence between 2D and 3D worlds. The proposed
model trained on synthetic UltraPose can be applied to real-world scenarios,
indicating the effectiveness of our benchmark and model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Continuous Face Representation with Explicit Functions. (arXiv:2110.15268v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15268">
<div class="article-summary-box-inner">
<span><p>How to represent a face pattern? While it is presented in a continuous way in
our visual system, computers often store and process the face image in a
discrete manner with 2D arrays of pixels. In this study, we attempt to learn a
continuous representation for face images with explicit functions. First, we
propose an explicit model (EmFace) for human face representation in the form of
a finite sum of mathematical terms, where each term is an analytic function
element. Further, to estimate the unknown parameters of EmFace, a novel neural
network, EmNet, is designed with an encoder-decoder structure and trained using
the backpropagation algorithm, where the encoder is defined by a deep
convolutional neural network and the decoder is an explicit mathematical
expression of EmFace. Experimental results show that EmFace has a higher
representation performance on faces with various expressions, postures, and
other factors, compared to that of other methods. Furthermore, EmFace achieves
reasonable performance on several face image processing tasks, including face
image restoration, denoising, and transformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEGAN: Memory Enhanced Graph Attention Network for Space-Time Video Super-Resolution. (arXiv:2110.15327v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15327">
<div class="article-summary-box-inner">
<span><p>Space-time video super-resolution (STVSR) aims to construct a high space-time
resolution video sequence from the corresponding low-frame-rate, low-resolution
video sequence. Inspired by the recent success to consider spatial-temporal
information for space-time super-resolution, our main goal in this work is to
take full considerations of spatial and temporal correlations within the video
sequences of fast dynamic events. To this end, we propose a novel one-stage
memory enhanced graph attention network (MEGAN) for space-time video
super-resolution. Specifically, we build a novel long-range memory graph
aggregation (LMGA) module to dynamically capture correlations along the channel
dimensions of the feature maps and adaptively aggregate channel features to
enhance the feature representations. We introduce a non-local residual block,
which enables each channel-wise feature to attend global spatial hierarchical
features. In addition, we adopt a progressive fusion module to further enhance
the representation ability by extensively exploiting spatial-temporal
correlations from multiple frames. Experiment results demonstrate that our
method achieves better results compared with the state-of-the-art methods
quantitatively and visually.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Relaxation for Multi-view Representation Learning. (arXiv:2110.15348v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15348">
<div class="article-summary-box-inner">
<span><p>Multi-view methods learn representations by aligning multiple views of the
same image and their performance largely depends on the choice of data
augmentation. In this paper, we notice that some other useful augmentations,
such as image rotation, are harmful for multi-view methods because they cause a
semantic shift that is too large to be aligned well. This observation motivates
us to relax the exact alignment objective to better cultivate stronger
augmentations. Taking image rotation as a case study, we develop a generic
approach, Pretext-aware Residual Relaxation (Prelax), that relaxes the exact
alignment by allowing an adaptive residual vector between different views and
encoding the semantic shift through pretext-aware learning. Extensive
experiments on different backbones show that our method can not only improve
multi-view methods with existing augmentations, but also benefit from stronger
image augmentations like rotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XDEEP-MSI: Explainable Bias-Rejecting Microsatellite Instability Deep Learning System In Colorectal Cancer. (arXiv:2110.15350v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15350">
<div class="article-summary-box-inner">
<span><p>We present a system for the prediction of microsatellite instability (MSI)
from H&amp;E images of colorectal cancer using deep learning (DL) techniques
customized for tissue microarrays (TMAs). The system incorporates an end-to-end
image preprocessing module that produces tiles at multiple magnifications in
the regions of interest as guided by a tissue classifier module, and a
multiple-bias rejecting module. The training and validation TMA samples were
obtained from the EPICOLON project and further enriched with samples from a
single institution. A systematic study of biases at tile level identified three
protected (bias) variables associated with the learned representations of a
baseline model: the project of origin of samples, the patient spot and the TMA
glass where each spot was placed. A multiple bias rejecting technique based on
adversarial training is implemented at the DL architecture so to directly avoid
learning the batch effects of those variables. The learned features from the
bias-ablated model have maximum discriminative power with respect to the task
and minimal statistical mean dependence with the biases. The impact of
different magnifications, types of tissues and the model performance at tile vs
patient level is analyzed. The AUC at tile level, and including all three
selected tissues (tumor epithelium, mucine and lymphocytic regions) and 4
magnifications, was 0.87 +/- 0.03 and increased to 0.9 +/- 0.03 at patient
level. To the best of our knowledge, this is the first work that incorporates a
multiple bias ablation technique at the DL architecture in digital pathology,
and the first using TMAs for the MSI prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning. (arXiv:2110.15352v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15352">
<div class="article-summary-box-inner">
<span><p>Tiny deep learning on microcontroller units (MCUs) is challenging due to the
limited memory size. We find that the memory bottleneck is due to the
imbalanced memory distribution in convolutional neural network (CNN) designs:
the first several blocks have an order of magnitude larger memory usage than
the rest of the network. To alleviate this issue, we propose a generic
patch-by-patch inference scheduling, which operates only on a small spatial
region of the feature map and significantly cuts down the peak memory. However,
naive implementation brings overlapping patches and computation overhead. We
further propose network redistribution to shift the receptive field and FLOPs
to the later stage and reduce the computation overhead. Manually redistributing
the receptive field is difficult. We automate the process with neural
architecture search to jointly optimize the neural architecture and inference
scheduling, leading to MCUNetV2. Patch-based inference effectively reduces the
peak memory usage of existing networks by 4-8x. Co-designed with neural
networks, MCUNetV2 sets a record ImageNet accuracy on MCU (71.8%), and achieves
&gt;90% accuracy on the visual wake words dataset under only 32kB SRAM. MCUNetV2
also unblocks object detection on tiny devices, achieving 16.9% higher mAP on
Pascal VOC compared to the state-of-the-art result. Our study largely addressed
the memory bottleneck in tinyML and paved the way for various vision
applications beyond image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language. (arXiv:2110.15358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15358">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a unified framework, called Visual Reasoning with
Differ-entiable Physics (VRDP), that can jointly learn visual concepts and
infer physics models of objects and their interactions from videos and
language. This is achieved by seamlessly integrating three components: a visual
perception module, a concept learner, and a differentiable physics engine. The
visual perception module parses each video frame into object-centric
trajectories and represents them as latent scene representations. The concept
learner grounds visual concepts (e.g., color, shape, and material) from these
object-centric representations based on the language, thus providing prior
knowledge for the physics engine. The differentiable physics model, implemented
as an impulse-based differentiable rigid-body simulator, performs
differentiable physical simulation based on the grounded concepts to infer
physical properties, such as mass, restitution, and velocity, by fitting the
simulated trajectories into the video observations. Consequently, these learned
concepts and physical models can explain what we have seen and imagine what is
about to happen in future and counterfactual scenarios. Integrating
differentiable physics into the dynamic reasoning framework offers several
appealing benefits. More accurate dynamics prediction in learned physics models
enables state-of-the-art performance on both synthetic and real-world
benchmarks while still maintaining high transparency and interpretability; most
notably, VRDP improves the accuracy of predictive and counterfactual questions
by 4.5% and 11.5% compared to its best counterpart. VRDP is also highly
data-efficient: physical parameters can be optimized from very few videos, and
even a single video can be sufficient. Finally, with all physical parameters
inferred, VRDP can quickly learn new concepts from a few examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives. (arXiv:2110.15360v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15360">
<div class="article-summary-box-inner">
<span><p>Despite the potential of reinforcement learning (RL) for building
general-purpose robotic systems, training RL agents to solve robotics tasks
still remains challenging due to the difficulty of exploration in purely
continuous action spaces. Addressing this problem is an active area of research
with the majority of focus on improving RL methods via better optimization or
more efficient exploration. An alternate but important component to consider
improving is the interface of the RL algorithm with the robot. In this work, we
manually specify a library of robot action primitives (RAPS), parameterized
with arguments that are learned by an RL policy. These parameterized primitives
are expressive, simple to implement, enable efficient exploration and can be
transferred across robots, tasks and environments. We perform a thorough
empirical study across challenging tasks in three distinct domains with image
input and a sparse terminal reward. We find that our simple change to the
action interface substantially improves both the learning efficiency and task
performance irrespective of the underlying RL algorithm, significantly
outperforming prior methods which learn skills from offline expert data. Code
and videos at https://mihdalal.github.io/raps/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-adaptive Crowd Counting via High-quality Image Translation and Density Reconstruction. (arXiv:1912.03677v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.03677">
<div class="article-summary-box-inner">
<span><p>Recently, crowd counting using supervised learning achieves a remarkable
improvement. Nevertheless, most counters rely on a large amount of manually
labeled data. With the release of synthetic crowd data, a potential alternative
is transferring knowledge from them to real data without any manual label.
However, there is no method to effectively suppress domain gaps and output
elaborate density maps during the transferring. To remedy the above problems,
this paper proposes a Domain-Adaptive Crowd Counting (DACC) framework, which
consists of a high-quality image translation and density map reconstruction. To
be specific, the former focuses on translating synthetic data to realistic
images, which prompts the translation quality by segregating
domain-shared/independent features and designing content-aware consistency
loss. The latter aims at generating pseudo labels on real scenes to improve the
prediction quality. Next, we retrain a final counter using these pseudo labels.
Adaptation experiments on six real-world datasets demonstrate that the proposed
method outperforms the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRAPHITE: A Practical Framework for Generating Automatic Physical Adversarial Machine Learning Attacks. (arXiv:2002.07088v5 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.07088">
<div class="article-summary-box-inner">
<span><p>This paper investigates an adversary's ease of attack in generating
adversarial examples for real-world scenarios. We address three key
requirements for practical attacks for the real-world: 1) automatically
constraining the size and shape of the attack so it can be applied with
stickers, 2) transform-robustness, i.e., robustness of a attack to
environmental physical variations such as viewpoint and lighting changes, and
3) supporting attacks in both white-box and black-box hard-label scenarios, so
that the adversary can attack proprietary models. In particular, the art of
automatically picking which areas to perturb remains largely unexplored -- an
efficient solution would remove the need to search over possible locations,
shapes, and sizes as in current patch attacks. In this work, we propose
GRAPHITE, an efficient and general framework for generating attacks that
satisfy the above three key requirements. GRAPHITE takes advantage of
transform-robustness, a metric based on expectation over transforms (EoT), to
automatically generate small masks and optimize with gradient-free
optimization. GRAPHITE is also flexible as it can easily trade-off
transform-robustness, perturbation size, and query count in black-box settings.
On a GTSRB model in a hard-label black-box setting, we are able to find attacks
on all possible 1,806 victim-target class pairs with averages of 77.8%
transform-robustness, perturbation size of 16.63% of the victim images, and
126K queries per pair. For digital-only attacks where achieving
transform-robustness is not a requirement, GRAPHITE is able to find successful
small-patch attacks with an average of only 566 queries for 92.2% of
victim-target pairs. GRAPHITE is also able to find successful attacks using
perturbations that modify small areas of the input image against PatchGuard, a
recently proposed defense against patch-based attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.08773">
<div class="article-summary-box-inner">
<span><p>Data augmentations are important ingredients in the recipe for training
robust neural networks, especially in computer vision. A fundamental question
is whether neural network features encode data augmentation transformations. To
answer this question, we introduce a systematic approach to investigate which
layers of neural networks are the most predictive of augmentation
transformations. Our approach uses features in pre-trained vision models with
minimal additional processing to predict common properties transformed by
augmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).
Surprisingly, neural network features not only predict data augmentation
transformations, but they predict many transformations with high accuracy.
After validating that neural networks encode features corresponding to
augmentation transformations, we show that these features are encoded in the
early layers of modern CNNs, though the augmentation signal fades in deeper
layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Monocular Depth Reconstruction of Non-Rigid Scenes. (arXiv:2012.15680v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15680">
<div class="article-summary-box-inner">
<span><p>Monocular depth reconstruction of complex and dynamic scenes is a highly
challenging problem. While for rigid scenes learning-based methods have been
offering promising results even in unsupervised cases, there exists little to
no literature addressing the same for dynamic and deformable scenes. In this
work, we present an unsupervised monocular framework for dense depth estimation
of dynamic scenes, which jointly reconstructs rigid and non-rigid parts without
explicitly modelling the camera motion. Using dense correspondences, we derive
a training objective that aims to opportunistically preserve pairwise distances
between reconstructed 3D points. In this process, the dense depth map is
learned implicitly using the as-rigid-as-possible hypothesis. Our method
provides promising results, demonstrating its capability of reconstructing 3D
from challenging videos of non-rigid scenes. Furthermore, the proposed method
also provides unsupervised motion segmentation results as an auxiliary output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation. (arXiv:2101.11223v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11223">
<div class="article-summary-box-inner">
<span><p>A key assumption of top-down human pose estimation approaches is their
expectation of having a single person/instance present in the input bounding
box. This often leads to failures in crowded scenes with occlusions. We propose
a novel solution to overcome the limitations of this fundamental assumption.
Our Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose
instances within a given bounding box. We introduce a Multi-Instance Modulation
Block (MIMB) that can adaptively modulate channel-wise feature responses for
each instance and is parameter efficient. We demonstrate the efficacy of our
approach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically,
we achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant
improvement of 2.4 AP and 6.5 AP over the prior art, respectively. When using
ground truth bounding boxes for inference, MIPNet achieves an improvement of
0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets
compared to HRNet. Interestingly, when fewer, high confidence bounding boxes
are used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet
maintains a relatively stable performance (drop of 1 AP) for the same inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evolving GAN Formulations for Higher Quality Image Synthesis. (arXiv:2102.08578v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08578">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have extended deep learning to complex
generation and translation tasks across different data modalities. However,
GANs are notoriously difficult to train: Mode collapse and other instabilities
in the training process often degrade the quality of the generated results,
such as images. This paper presents a new technique called TaylorGAN for
improving GANs by discovering customized loss functions for each of its two
networks. The loss functions are parameterized as Taylor expansions and
optimized through multiobjective evolution. On an image-to-image translation
benchmark task, this approach qualitatively improves generated image quality
and quantitatively improves two independent GAN performance metrics. It
therefore forms a promising approach for applying GANs to more challenging
tasks in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Decoupling Augmentation for Weakly Supervised Semantic Segmentation. (arXiv:2103.01795v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01795">
<div class="article-summary-box-inner">
<span><p>Data augmentation is vital for deep learning neural networks. By providing
massive training samples, it helps to improve the generalization ability of the
model. Weakly supervised semantic segmentation (WSSS) is a challenging problem
that has been deeply studied in recent years, conventional data augmentation
approaches for WSSS usually employ geometrical transformations, random cropping
and color jittering. However, merely increasing the same contextual semantic
data does not bring much gain to the networks to distinguish the objects, e.g.,
the correct image-level classification of "aeroplane" may be not only due to
the recognition of the object itself, but also its co-occurrence context like
"sky", which will cause the model to focus less on the object features. To this
end, we present a Context Decoupling Augmentation (CDA) method, to change the
inherent context in which the objects appear and thus drive the network to
remove the dependence between object instances and contextual information. To
validate the effectiveness of the proposed method, extensive experiments on
PASCAL VOC 2012 dataset with several alternative network architectures
demonstrate that CDA can boost various popular WSSS methods to the new
state-of-the-art by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings. (arXiv:2103.02886v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02886">
<div class="article-summary-box-inner">
<span><p>Recent advances in off-policy deep reinforcement learning (RL) have led to
impressive success in complex tasks from visual observations. Experience replay
improves sample-efficiency by reusing experiences from the past, and
convolutional neural networks (CNNs) process high-dimensional inputs
effectively. However, such techniques demand high memory and computational
bandwidth. In this paper, we present Stored Embeddings for Efficient
Reinforcement Learning (SEER), a simple modification of existing off-policy RL
methods, to address these computational and memory requirements. To reduce the
computational overhead of gradient updates in CNNs, we freeze the lower layers
of CNN encoders early in training due to early convergence of their parameters.
Additionally, we reduce memory requirements by storing the low-dimensional
latent vectors for experience replay instead of high-dimensional images,
enabling an adaptive increase in the replay buffer capacity, a useful technique
in constrained-memory settings. In our experiments, we show that SEER does not
degrade the performance of RL agents while significantly saving computation and
memory across a diverse set of DeepMind Control environments and Atari games.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Student-Teacher Feature Pyramid Matching for Anomaly Detection. (arXiv:2103.04257v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04257">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is a challenging task and usually formulated as an
one-class learning problem for the unexpectedness of anomalies. This paper
proposes a simple yet powerful approach to this issue, which is implemented in
the student-teacher framework for its advantages but substantially extends it
in terms of both accuracy and efficiency. Given a strong model pre-trained on
image classification as the teacher, we distill the knowledge into a single
student network with the identical architecture to learn the distribution of
anomaly-free images and this one-step transfer preserves the crucial clues as
much as possible. Moreover, we integrate the multi-scale feature matching
strategy into the framework, and this hierarchical feature matching enables the
student network to receive a mixture of multi-level knowledge from the feature
pyramid under better supervision, thus allowing to detect anomalies of various
sizes. The difference between feature pyramids generated by the two networks
serves as a scoring function indicating the probability of anomaly occurring.
Due to such operations, our approach achieves accurate and fast pixel-level
anomaly detection. Very competitive results are delivered on the MVTec anomaly
detection dataset, superior to the state of the art ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-driven Cloud Clustering via a Rotationally Invariant Autoencoder. (arXiv:2103.04885v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04885">
<div class="article-summary-box-inner">
<span><p>Advanced satellite-born remote sensing instruments produce high-resolution
multi-spectral data for much of the globe at a daily cadence. These datasets
open up the possibility of improved understanding of cloud dynamics and
feedback, which remain the biggest source of uncertainty in global climate
model projections. As a step towards answering these questions, we describe an
automated rotation-invariant cloud clustering (RICC) method that leverages deep
learning autoencoder technology to organize cloud imagery within large datasets
in an unsupervised fashion, free from assumptions about predefined classes. We
describe both the design and implementation of this method and its evaluation,
which uses a sequence of testing protocols to determine whether the resulting
clusters: (1) are physically reasonable, (i.e., embody scientifically relevant
distinctions); (2) capture information on spatial distributions, such as
textures; (3) are cohesive and separable in latent space; and (4) are
rotationally invariant, (i.e., insensitive to the orientation of an image).
Results obtained when these evaluation protocols are applied to RICC outputs
suggest that the resultant novel cloud clusters capture meaningful aspects of
cloud physics, are appropriately spatially coherent, and are invariant to
orientations of input images. Our results support the possibility of using an
unsupervised data-driven approach for automated clustering and pattern
discovery in cloud imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Multi-Layer Layout Estimation for Warehouse Racks. (arXiv:2103.09174v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09174">
<div class="article-summary-box-inner">
<span><p>Given a monocular colour image of a warehouse rack, we aim to predict the
bird's-eye view layout for each shelf in the rack, which we term as multi-layer
layout prediction. To this end, we present RackLay, a deep neural network for
real-time shelf layout estimation from a single image. Unlike previous layout
estimation methods, which provide a single layout for the dominant ground plane
alone, RackLay estimates the top-view and front-view layout for each shelf in
the considered rack populated with objects. RackLay's architecture and its
variants are versatile and estimate accurate layouts for diverse scenes
characterized by varying number of visible shelves in an image, large range in
shelf occupancy factor and varied background clutter. Given the extreme paucity
of datasets in this space and the difficulty involved in acquiring real data
from warehouses, we additionally release a flexible synthetic dataset
generation pipeline WareSynth which allows users to control the generation
process and tailor the dataset according to contingent application. The
ablations across architectural variants and comparison with strong prior
baselines vindicate the efficacy of RackLay as an apt architecture for the
novel problem of multi-layered layout estimation. We also show that fusing the
top-view and front-view enables 3D reasoning applications such as metric free
space estimation for the considered rack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">#PraCegoVer: A Large Dataset for Image Captioning in Portuguese. (arXiv:2103.11474v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11474">
<div class="article-summary-box-inner">
<span><p>Automatically describing images using natural sentences is an important task
to support visually impaired people's inclusion onto the Internet. It is still
a big challenge that requires understanding the relation of the objects present
in the image and their attributes and actions they are involved in. Then,
visual interpretation methods are needed, but linguistic models are also
necessary to verbally describe the semantic relations. This problem is known as
Image Captioning. Although many datasets were proposed in the literature, the
majority contains only English captions, whereas datasets with captions
described in other languages are scarce. Recently, a movement called PraCegoVer
arose on the Internet, stimulating users from social media to publish images,
tag #PraCegoVer and add a short description of their content. Thus, inspired by
this movement, we have proposed the #PraCegoVer, a multi-modal dataset with
Portuguese captions based on posts from Instagram. It is the first large
dataset for image captioning in Portuguese with freely annotated images.
Further, the captions in our dataset bring additional challenges to the
problem: first, in contrast to popular datasets such as MS COCO Captions,
#PraCegoVer has only one reference to each image; also, both mean and variance
of our reference sentence length are significantly greater than those in the MS
COCO Captions. These two characteristics contribute to making our dataset
interesting due to the linguistic aspect and the challenges that it introduces
to the image captioning problem. We publicly-share the dataset at
https://github.com/gabrielsantosrv/PraCegoVer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Elastic Lottery Ticket Hypothesis. (arXiv:2103.16547v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16547">
<div class="article-summary-box-inner">
<span><p>Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse
trainable subnetworks, or winning tickets, which can be trained in isolation to
achieve similar or even better performance compared to the full models. Despite
many efforts being made, the most effective method to identify such winning
tickets is still Iterative Magnitude-based Pruning (IMP), which is
computationally expensive and has to be run thoroughly for every different
network. A natural question that comes in is: can we "transform" the winning
ticket found in one network to another with a different architecture, yielding
a winning ticket for the latter at the beginning, without re-doing the
expensive IMP? Answering this question is not only practically relevant for
efficient "once-for-all" winning ticket finding, but also theoretically
appealing for uncovering inherently scalable sparse patterns in networks. We
conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety
of strategies to tweak the winning tickets found from different networks of the
same model family (e.g., ResNets). Based on these results, we articulate the
Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or
dropping) and re-ordering layers for one network, its corresponding winning
ticket could be stretched (or squeezed) into a subnetwork for another deeper
(or shallower) network from the same family, whose performance is nearly the
same competitive as the latter's winning ticket directly found by IMP. We have
also extensively compared E-LTH with pruning-at-initialization and dynamic
sparse training methods, as well as discussed the generalizability of E-LTH to
different model families, layer types, and across datasets. Code is available
at https://github.com/VITA-Group/ElasticLTH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03736">
<div class="article-summary-box-inner">
<span><p>Meta-learning can extract an inductive bias from previous learning experience
and assist the training of new tasks. It is often realized through optimizing a
meta-model with the evaluation loss of task-specific solvers. Most existing
algorithms sample non-overlapping $\mathit{support}$ sets and $\mathit{query}$
sets to train and evaluate the solvers respectively due to simplicity
($\mathcal{S}$/$\mathcal{Q}$ protocol). Different from
$\mathcal{S}$/$\mathcal{Q}$ protocol, we can also evaluate a task-specific
solver by comparing it to a target model $\mathcal{T}$, which is the optimal
model for this task or a model that behaves well enough on this task
($\mathcal{S}$/$\mathcal{T}$ protocol). Although being short of research,
$\mathcal{S}$/$\mathcal{T}$ protocol has unique advantages such as offering
more informative supervision, but it is computationally expensive. This paper
looks into this special evaluation method and takes a step towards putting it
into practice. We find that with a small ratio of tasks armed with target
models, classic meta-learning algorithms can be improved a lot without
consuming many resources. We empirically verify the effectiveness of
$\mathcal{S}$/$\mathcal{T}$ protocol in a typical application of meta-learning,
$\mathit{i.e.}$, few-shot learning. In detail, after constructing target models
by fine-tuning the pre-trained network on those hard tasks, we match the
task-specific solvers and target models via knowledge distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoPE: Conditional image generation using Polynomial Expansions. (arXiv:2104.05077v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05077">
<div class="article-summary-box-inner">
<span><p>Generative modeling has evolved to a notable field of machine learning. Deep
polynomial neural networks (PNNs) have demonstrated impressive results in
unsupervised image generation, where the task is to map an input vector (i.e.,
noise) to a synthesized image. However, the success of PNNs has not been
replicated in conditional generation tasks, such as super-resolution. Existing
PNNs focus on single-variable polynomial expansions which do not fare well to
two-variable inputs, i.e., the noise variable and the conditional variable. In
this work, we introduce a general framework, called CoPE, that enables a
polynomial expansion of two input variables and captures their auto- and
cross-correlations. We exhibit how CoPE can be trivially augmented to accept an
arbitrary number of input variables. CoPE is evaluated in five tasks
(class-conditional generation, inverse problems, edges-to-image translation,
image-to-image translation, attribute-guided generation) involving eight
datasets. The thorough evaluation suggests that CoPE can be useful for tackling
diverse conditional generation tasks. The source code of CoPE is available at
\url{https://github.com/grigorisg9gr/polynomial_nets_for_conditional_generation}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning of Global-Local Video Representations. (arXiv:2104.05418v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05418">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has delivered impressive results for various tasks in
the self-supervised regime. However, existing approaches optimize for learning
representations specific to downstream scenarios, i.e., \textit{global}
representations suitable for tasks such as classification or \textit{local}
representations for tasks such as detection and localization. While they
produce satisfactory results in the intended downstream scenarios, they often
fail to generalize to tasks that they were not originally designed for. In this
work, we propose to learn video representations that generalize to both the
tasks which require global semantic information (e.g., classification) and the
tasks that require local fine-grained spatio-temporal information (e.g.,
localization). We achieve this by optimizing two contrastive objectives that
together encourage our model to learn global-local visual information given
audio signals. We show that the two objectives mutually improve the
generalizability of the learned global-local representations, significantly
outperforming their disjointly learned counterparts. We demonstrate our
approach on various tasks including action/sound classification, lip reading,
deepfake detection, event and sound localization
(https://github.com/yunyikristy/global\_local).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Binocular Eye-Tracking SystemWith Stereo Stimuli for 3D Gaze Estimation. (arXiv:2104.12167v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12167">
<div class="article-summary-box-inner">
<span><p>Eye-tracking technologies have been widely used in applications like
psychological studies and human computer interactions (HCI). However, most
current eye trackers focus on 2D point of gaze (PoG) estimation and cannot
provide accurate gaze depth.Concerning future applications such as HCI with 3D
displays, we propose a novel binocular eye tracking device with stereo stimuli
to provide highly accurate 3D PoG estimation. In our device, the 3D stereo
imaging system can provide users with a friendly and immersive 3D visual
experience without wearing any accessories. The eye capturing system can
directly record the users eye movements under 3D stimuli without disturbance. A
regression based 3D eye tracking model is built based on collected eye movement
data under stereo stimuli. Our model estimates users 2D gaze with features
defined by eye region landmarks and further estimates 3D PoG with a multi
source feature set constructed by comprehensive eye movement features and
disparity features from stereo stimuli. Two test stereo scenes with different
depths of field are designed to verify the model effectiveness. Experimental
results show that the average error for 2D gaze estimation was 0.66\degree and
for 3D PoG estimation, the average errors are 1.85~cm/0.15~m over the workspace
volume 50~cm $\times$ 30~cm $\times$ 75~cm/2.4~m $\times$ 4.0~m $\times$ 7.9~m
separately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGMB-Transformer: Anatomy-Guided Multi-Branch Transformer Network for Automated Evaluation of Root Canal Therapy. (arXiv:2105.00381v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00381">
<div class="article-summary-box-inner">
<span><p>Accurate evaluation of the treatment result on X-ray images is a significant
and challenging step in root canal therapy since the incorrect interpretation
of the therapy results will hamper timely follow-up which is crucial to the
patients' treatment outcome. Nowadays, the evaluation is performed in a manual
manner, which is time-consuming, subjective, and error-prone. In this paper, we
aim to automate this process by leveraging the advances in computer vision and
artificial intelligence, to provide an objective and accurate method for root
canal therapy result assessment. A novel anatomy-guided multi-branch
Transformer (AGMB-Transformer) network is proposed, which first extracts a set
of anatomy features and then uses them to guide a multi-branch Transformer
network for evaluation. Specifically, we design a polynomial curve fitting
segmentation strategy with the help of landmark detection to extract the
anatomy features. Moreover, a branch fusion module and a multi-branch structure
including our progressive Transformer and Group Multi-Head Self-Attention
(GMHSA) are designed to focus on both global and local features for an accurate
diagnosis. To facilitate the research, we have collected a large-scale root
canal therapy evaluation dataset with 245 root canal therapy X-ray images, and
the experiment results show that our AGMB-Transformer can improve the diagnosis
accuracy from 57.96% to 90.20% compared with the baseline network. The proposed
AGMB-Transformer can achieve a highly accurate evaluation of root canal
therapy. To our best knowledge, our work is the first to perform automatic root
canal therapy evaluation and has important clinical value to reduce the
workload of endodontists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Graph Embeddings for Open World Compositional Zero-Shot Learning. (arXiv:2105.01017v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01017">
<div class="article-summary-box-inner">
<span><p>Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions
of state and object visual primitives seen during training. A problem with
standard CZSL is the assumption of knowing which unseen compositions will be
available at test time. In this work, we overcome this assumption operating on
the open world setting, where no limit is imposed on the compositional space at
test time, and the search space contains a large number of unseen compositions.
To address this problem, we propose a new approach, Compositional Cosine Graph
Embeddings (Co-CGE), based on two principles. First, Co-CGE models the
dependency between states, objects and their compositions through a graph
convolutional neural network. The graph propagates information from seen to
unseen concepts, improving their representations. Second, since not all unseen
compositions are equally feasible, and less feasible ones may damage the
learned representations, Co-CGE estimates a feasibility score for each unseen
composition, using the scores as margins in a cosine similarity-based loss and
as weights in the adjacency matrix of the graphs. Experiments show that our
approach achieves state-of-the-art performances in standard CZSL while
outperforming previous methods in the open world scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Detection by Human Crowds, Machines, and Machine-informed Crowds. (arXiv:2105.06496v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06496">
<div class="article-summary-box-inner">
<span><p>The recent emergence of machine-manipulated media raises an important
societal question: how can we know if a video that we watch is real or fake? In
two online studies with 15,016 participants, we present authentic videos and
deepfakes and ask participants to identify which is which. We compare the
performance of ordinary human observers against the leading computer vision
deepfake detection model and find them similarly accurate while making
different kinds of mistakes. Together, participants with access to the model's
prediction are more accurate than either alone, but inaccurate model
predictions often decrease participants' accuracy. To probe the relative
strengths and weaknesses of humans and machines as detectors of deepfakes, we
examine human and machine performance across video-level features, and we
evaluate the impact of pre-registered randomized interventions on deepfake
detection. We find that manipulations designed to disrupt visual processing of
faces hinder human participants' performance while mostly not affecting the
model's performance, suggesting a role for specialized cognitive capacities in
explaining human deepfake detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Trees for Learning on Graphs. (arXiv:2105.07264v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07264">
<div class="article-summary-box-inner">
<span><p>Graph Neural Networks (GNNs) have emerged as a flexible and powerful approach
for learning over graphs. Despite this success, existing GNNs are constrained
by their local message-passing architecture and are provably limited in their
expressive power. In this work, we propose a new GNN architecture -- the Neural
Tree. The neural tree architecture does not perform message passing on the
input graph, but on a tree-structured graph, called the H-tree, that is
constructed from the input graph. Nodes in the H-tree correspond to subgraphs
in the input graph, and they are reorganized in a hierarchical manner such that
the parent of a node in the H-tree always corresponds to a larger subgraph in
the input graph. We show that the neural tree architecture can approximate any
smooth probability distribution function over an undirected graph. We also
prove that the number of parameters needed to achieve an
$\epsilon$-approximation of the distribution function is exponential in the
treewidth of the input graph, but linear in its size. We prove that any
continuous $\mathcal{G}$-invariant/equivariant function can be approximated by
a nonlinear combination of such probability distribution functions over
$\mathcal{G}$. We apply the neural tree to semi-supervised node classification
in 3D scene graphs, and show that these theoretical properties translate into
significant gains in prediction accuracy, over the more traditional GNN
architectures. We also show the applicability of the neural tree architecture
to citation networks with large treewidth, by using a graph sub-sampling
technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-Training Sparsity-Aware Quantization. (arXiv:2105.11010v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11010">
<div class="article-summary-box-inner">
<span><p>Quantization is a technique used in deep neural networks (DNNs) to increase
execution performance and hardware efficiency. Uniform post-training
quantization (PTQ) methods are common, since they can be implemented
efficiently in hardware and do not require extensive hardware resources or a
training set. Mapping FP32 models to INT8 using uniform PTQ yields models with
negligible accuracy degradation; however, reducing precision below 8 bits with
PTQ is challenging, as accuracy degradation becomes noticeable, due to the
increase in quantization noise. In this paper, we propose a sparsity-aware
quantization (SPARQ) method, in which the unstructured and dynamic activation
sparsity is leveraged in different representation granularities. 4-bit
quantization, for example, is employed by dynamically examining the bits of
8-bit values and choosing a window of 4 bits, while first skipping zero-value
bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we
focus on pairs of 8-bit activations and examine whether one of the two is equal
to zero. If one is equal to zero, the second can opportunistically use the
other's 4-bit budget; if both do not equal zero, then each is dynamically
quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation
and a practical hardware implementation. The code is available at
https://github.com/gilshm/sparq.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14944">
<div class="article-summary-box-inner">
<span><p>Explaining the decisions of an Artificial Intelligence (AI) model is
increasingly critical in many real-world, high-stake applications. Hundreds of
papers have either proposed new feature attribution methods, discussed or
harnessed these tools in their work. However, despite humans being the target
end-users, most attribution methods were only evaluated on proxy
automatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et
al. 2018). In this paper, we conduct the first user study to measure
attribution map effectiveness in assisting humans in ImageNet classification
and Stanford Dogs fine-grained classification, and when an image is natural or
adversarial (i.e., contains adversarial perturbations). Overall, feature
attribution is surprisingly not more effective than showing humans nearest
training-set examples. On a harder task of fine-grained dog categorization,
presenting attribution maps to humans does not help, but instead hurts the
performance of human-AI teams compared to AI alone. Importantly, we found
automatic attribution-map evaluation measures to correlate poorly with the
actual human-AI team performance. Our findings encourage the community to
rigorously test their methods on the downstream human-in-the-loop applications
and to rethink the existing evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. (arXiv:2105.15203v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15203">
<div class="article-summary-box-inner">
<span><p>We present SegFormer, a simple, efficient yet powerful semantic segmentation
framework which unifies Transformers with lightweight multilayer perception
(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a
novel hierarchically structured Transformer encoder which outputs multiscale
features. It does not need positional encoding, thereby avoiding the
interpolation of positional codes which leads to decreased performance when the
testing resolution differs from training. 2) SegFormer avoids complex decoders.
The proposed MLP decoder aggregates information from different layers, and thus
combining both local attention and global attention to render powerful
representations. We show that this simple and lightweight design is the key to
efficient segmentation on Transformers. We scale our approach up to obtain a
series of models from SegFormer-B0 to SegFormer-B5, reaching significantly
better performance and efficiency than previous counterparts. For example,
SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x
smaller and 2.2% better than the previous best method. Our best model,
SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows
excellent zero-shot robustness on Cityscapes-C. Code will be released at:
github.com/NVlabs/SegFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields. (arXiv:2106.01553v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01553">
<div class="article-summary-box-inner">
<span><p>Multilayer perceptrons (MLPs) have been successfully used to represent 3D
shapes implicitly and compactly, by mapping 3D coordinates to the corresponding
signed distance values or occupancy values. In this paper, we propose a novel
positional encoding scheme, called Spline Positional Encoding, to map the input
coordinates to a high dimensional space before passing them to MLPs, for
helping to recover 3D signed distance fields with fine-scale geometric details
from unorganized 3D point clouds. We verified the superiority of our approach
over other positional encoding schemes on tasks of 3D shape reconstruction from
input point clouds and shape space learning. The efficacy of our approach
extended to image reconstruction is also demonstrated and evaluated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$ Regularization. (arXiv:2106.02923v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02923">
<div class="article-summary-box-inner">
<span><p>There have been many recent advances in representation learning; however,
unsupervised representation learning can still struggle with model
identification issues related to rotations of the latent space. Variational
Auto-Encoders (VAEs) and their extensions such as $\beta$-VAEs have been shown
to improve local alignment of latent variables with PCA directions, which can
help to improve model disentanglement under some conditions. Borrowing
inspiration from Independent Component Analysis (ICA) and sparse coding, we
propose applying an $L_1$ loss to the VAE's generative Jacobian during training
to encourage local latent variable alignment with independent factors of
variation in images of multiple objects or images with multiple parts. We
demonstrate our results on a variety of datasets, giving qualitative and
quantitative results using information theoretic and modularity measures that
show our added $L_1$ cost encourages local axis alignment of the latent
representation with individual factors of variation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Controllable Generation of Physical Scenes with Explicit Knowledge. (arXiv:2106.04066v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04066">
<div class="article-summary-box-inner">
<span><p>Deep Generative Models (DGMs) are known for their superior capability in
generating realistic data. Extending purely data-driven approaches, recent
specialized DGMs may satisfy additional controllable requirements such as
embedding a traffic sign in a driving scene, by manipulating patterns
\textit{implicitly} in the neuron or feature level. In this paper, we introduce
a novel method to incorporate domain knowledge \textit{explicitly} in the
generation process to achieve semantically controllable scene generation. We
categorize our knowledge into two types to be consistent with the composition
of natural scenes, where the first type represents the property of objects and
the second type represents the relationship among objects. We then propose a
tree-structured generative model to learn complex scene representation, whose
nodes and edges are naturally corresponding to the two types of knowledge
respectively. Knowledge can be explicitly integrated to enable semantically
controllable scene generation by imposing semantic rules on properties of nodes
and edges in the tree structure. We construct a synthetic example to illustrate
the controllability and explainability of our method in a clean setting. We
further extend the synthetic example to realistic autonomous vehicle driving
environments and conduct extensive experiments to show that our method
efficiently identifies adversarial traffic scenes against different
state-of-the-art 3D point cloud segmentation models satisfying the traffic
rules specified as the explicit knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. (arXiv:2106.04619v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04619">
<div class="article-summary-box-inner">
<span><p>Self-supervised representation learning has shown remarkable success in a
number of domains. A common practice is to perform data augmentation via
hand-crafted transformations intended to leave the semantics of the data
invariant. We seek to understand the empirical success of this approach from a
theoretical perspective. We formulate the augmentation process as a latent
variable model by postulating a partition of the latent representation into a
content component, which is assumed invariant to augmentation, and a style
component, which is allowed to change. Unlike prior work on disentanglement and
independent component analysis, we allow for both nontrivial statistical and
causal dependencies in the latent space. We study the identifiability of the
latent representation based on pairs of views of the observations and prove
sufficient conditions that allow us to identify the invariant content partition
up to an invertible mapping in both generative and discriminative settings. We
find numerical simulations with dependent latent variables are consistent with
our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,
visually complex images with rich causal dependencies, which we use to study
the effect of data augmentations performed in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data. (arXiv:2106.05001v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05001">
<div class="article-summary-box-inner">
<span><p>A central challenge in training classification models in the real-world
federated system is learning with non-IID data. To cope with this, most of the
existing works involve enforcing regularization in local optimization or
improving the model aggregation scheme at the server. Other works also share
public datasets or synthesized samples to supplement the training of
under-represented classes or introduce a certain level of personalization.
Though effective, they lack a deep understanding of how the data heterogeneity
affects each layer of a deep classification model. In this paper, we bridge
this gap by performing an experimental analysis of the representations learned
by different layers. Our observations are surprising: (1) there exists a
greater bias in the classifier than other layers, and (2) the classification
performance can be significantly improved by post-calibrating the classifier
after federated training. Motivated by the above findings, we propose a novel
and simple algorithm called Classifier Calibration with Virtual Representations
(CCVR), which adjusts the classifier using virtual representations sampled from
an approximated gaussian mixture model. Experimental results demonstrate that
CCVR achieves state-of-the-art performance on popular federated learning
benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple
yet effective method can shed some light on the future research of federated
learning with non-IID data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethink Transfer Learning in Medical Image Classification. (arXiv:2106.05152v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05152">
<div class="article-summary-box-inner">
<span><p>Transfer learning (TL) with deep convolutional neural networks (DCNNs) has
proved successful in medical image classification (MIC). However, the current
practice is puzzling, as MIC typically relies only on low- and/or mid-level
features that are learned in the bottom layers of DCNNs. Following this
intuition, we question the current strategies of TL in MIC. In this paper, we
perform careful experimental comparisons between shallow and deep networks for
classification on two chest x-ray datasets, using different TL strategies. We
find that deep models are not always favorable, and finetuning truncated deep
models almost always yields the best performance, especially in data-poor
regimes.
</p>
<p>Project webpage:
https://sun-umn.github.io/Transfer-Learning-in-Medical-Imaging/
</p>
<p>Keywords: Transfer learning, Medical image classification, Feature hierarchy,
Medical imaging, Evaluation metrics, Imbalanced data
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Canonical Face Embeddings. (arXiv:2106.07822v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07822">
<div class="article-summary-box-inner">
<span><p>We present evidence that many common convolutional neural networks (CNNs)
trained for face verification learn functions that are nearly equivalent under
rotation. More specifically, we demonstrate that one face verification model's
embeddings (i.e. last-layer activations) can be compared directly to another
model's embeddings after only a rotation or linear transformation, with little
performance penalty. This finding is demonstrated using IJB-C 1:1 verification
across the combinations of ten modern off-the-shelf CNN-based face verification
models which vary in training dataset, CNN architecture, method of angular loss
calculation, or some combination of the 3. These networks achieve a mean true
accept rate of 0.96 at a false accept rate of 0.01. When instead evaluating
embeddings generated from two CNNs, where one CNN's embeddings are mapped with
a linear transformation, the mean true accept rate drops to 0.95 using the same
verification paradigm. Restricting these linear maps to only perform rotation
produces a mean true accept rate of 0.91. These mappings' existence suggests
that a common representation is learned by models despite variation in training
or structure. We discuss the broad implications a result like this has,
including an example regarding face template security.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05768">
<div class="article-summary-box-inner">
<span><p>Transformers provide a class of expressive architectures that are extremely
effective for sequence modeling. However, the key limitation of transformers is
their quadratic memory and time complexity $\mathcal{O}(L^2)$ with respect to
the sequence length in attention layers, which restricts application in
extremely long sequences. Most existing approaches leverage sparsity or
low-rank assumptions in the attention matrix to reduce cost, but sacrifice
expressiveness. Instead, we propose Combiner, which provides full attention
capability in each attention head while maintaining low computation and memory
complexity. The key idea is to treat the self-attention mechanism as a
conditional expectation over embeddings at each location, and approximate the
conditional distribution with a structured factorization. Each location can
attend to all other locations, either via direct attention, or through indirect
attention to abstractions, which are again conditional expectations of
embeddings from corresponding local regions. We show that most sparse attention
patterns used in existing sparse transformers are able to inspire the design of
such factorization for full attention, resulting in the same sub-quadratic cost
($\mathcal{O}(L\log(L))$ or $\mathcal{O}(L\sqrt{L})$). Combiner is a drop-in
replacement for attention layers in existing transformers and can be easily
implemented in common frameworks. An experimental evaluation on both
autoregressive and bidirectional sequence tasks demonstrates the effectiveness
of this approach, yielding state-of-the-art results on several image and text
modeling tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Automated Machine Learning Pipeline for Echocardiogram Segmentation. (arXiv:2107.08440v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08440">
<div class="article-summary-box-inner">
<span><p>Nowadays, cardiac diagnosis largely depends on left ventricular function
assessment. With the help of the segmentation deep learning model, the
assessment of the left ventricle becomes more accessible and accurate. However,
deep learning technique still faces two main obstacles: the difficulty in
acquiring sufficient training data and time-consuming in developing quality
models. In the ordinary data acquisition process, the dataset was selected
randomly from a large pool of unlabeled images for labeling, leading to massive
labor time to annotate those images. Besides that, hand-designed model
development is strenuous and also costly. This paper introduces a pipeline that
relies on Active Learning to ease the labeling work and utilizes Neural
Architecture Search's idea to design the adequate deep learning model
automatically. We called this Fully automated machine learning pipeline for
echocardiogram segmentation. The experiment results show that our method
obtained the same IOU accuracy with only two-fifths of the original training
dataset, and the searched model got the same accuracy as the hand-designed
model given the same training dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Alignment Prediction for Few-Shot Video Classification. (arXiv:2107.11960v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11960">
<div class="article-summary-box-inner">
<span><p>The goal of few-shot video classification is to learn a classification model
with good generalization ability when trained with only a few labeled videos.
However, it is difficult to learn discriminative feature representations for
videos in such a setting. In this paper, we propose Temporal Alignment
Prediction (TAP) based on sequence similarity learning for few-shot video
classification. In order to obtain the similarity of a pair of videos, we
predict the alignment scores between all pairs of temporal positions in the two
videos with the temporal alignment prediction function. Besides, the inputs to
this function are also equipped with the context information in the temporal
domain. We evaluate TAP on two video classification benchmarks including
Kinetics and Something-Something V2. The experimental results verify the
effectiveness of TAP and show its superiority over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transductive Maximum Margin Classifier for Few-Shot Learning. (arXiv:2107.11975v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11975">
<div class="article-summary-box-inner">
<span><p>Few-shot learning aims to train a classifier that can generalize well when
just a small number of labeled examples per class are given. We introduce a
transductive maximum margin classifier for few-shot learning (FS-TMMC). The
basic idea of the classical maximum margin classifier is to solve an optimal
prediction function so that the training data can be correctly classified by
the resulting classifer with the largest geometric margin. In few-shot
learning, it is challenging to find such classifiers with good generalization
ability due to the insufficiency of training data in the support set. FS-TMMC
leverages the unlabeled query examples to adjust the separating hyperplane of
the maximum margin classifier such that the prediction function is optimal on
both the support and query sets. Furthermore, we use an efficient and effective
quasi-Newton algorithm, the L-BFGS method for optimization. Experimental
results on three standard few-shot learning benchmarks including miniImagenet,
tieredImagenet and CUB show that our method achieves state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Adversarially Blur Visual Object Tracking. (arXiv:2107.12085v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12085">
<div class="article-summary-box-inner">
<span><p>Motion blur caused by the moving of the object or camera during the exposure
can be a key challenge for visual object tracking, affecting tracking accuracy
significantly. In this work, we explore the robustness of visual object
trackers against motion blur from a new angle, i.e., adversarial blur attack
(ABA). Our main objective is to online transfer input frames to their natural
motion-blurred counterparts while misleading the state-of-the-art trackers
during the tracking process. To this end, we first design the motion blur
synthesizing method for visual tracking based on the generation principle of
motion blur, considering the motion information and the light accumulation
process. With this synthetic method, we propose optimization-based ABA (OP-ABA)
by iteratively optimizing an adversarial objective function against the
tracking w.r.t. the motion and light accumulation parameters. The OP-ABA is
able to produce natural adversarial examples but the iteration can cause heavy
time cost, making it unsuitable for attacking real-time trackers. To alleviate
this issue, we further propose one-step ABA (OS-ABA) where we design and train
a joint adversarial motion and accumulation predictive network (JAMANet) with
the guidance of OP-ABA, which is able to efficiently estimate the adversarial
motion and accumulation parameters in a one-step way. The experiments on four
popular datasets (e.g., OTB100, VOT2018, UAV123, and LaSOT) demonstrate that
our methods are able to cause significant accuracy drops on four
state-of-the-art trackers with high transferability. Please find the source
code at \url{https://github.com/tsingqguo/ABA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Video Object Segmentation by Motion-Aware Mask Propagation. (arXiv:2107.12569v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12569">
<div class="article-summary-box-inner">
<span><p>We propose a self-supervised spatio-temporal matching method, coined
Motion-Aware Mask Propagation (MAMP), for video object segmentation. MAMP
leverages the frame reconstruction task for training without the need for
annotations. During inference, MAMP extracts high-resolution features from each
frame to build a memory bank from the features as well as the predicted masks
of selected past frames. MAMP then propagates the masks from the memory bank to
subsequent frames according to our proposed motion-aware spatio-temporal
matching module to handle fast motion and long-term matching scenarios.
Evaluation on DAVIS-2017 and YouTube-VOS datasets show that MAMP achieves
state-of-the-art performance with stronger generalization ability compared to
existing self-supervised methods, i.e., 4.2% higher mean J&amp;F on DAVIS-2017 and
4.85% higher mean J&amp;F on the unseen categories of YouTube-VOS than the nearest
competitor. Moreover, MAMP performs at par with many supervised video object
segmentation methods. Our code is available at:
https://github.com/bo-miao/MAMP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Transferable Are Self-supervised Features in Medical Image Classification Tasks?. (arXiv:2108.10048v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10048">
<div class="article-summary-box-inner">
<span><p>Transfer learning has become a standard practice to mitigate the lack of
labeled data in medical classification tasks. Whereas finetuning a downstream
task using supervised ImageNet pretrained features is straightforward and
extensively investigated in many works, there is little study on the usefulness
of self-supervised pretraining. In this paper, we assess the transferability of
ImageNet self-supervisedpretraining by evaluating the performance of models
initialized with pretrained features from three self-supervised techniques
(SimCLR, SwAV, and DINO) on selected medical classification tasks. The chosen
tasks cover tumor detection in sentinel axillary lymph node images, diabetic
retinopathy classification in fundus images, and multiple pathological
condition classification in chest X-ray images. We demonstrate that
self-supervised pretrained models yield richer embeddings than their supervised
counterpart, which benefits downstream tasks in view of both linear evaluation
and finetuning. For example, in view of linear evaluation at acritically small
subset of the data, we see an improvement up to 14.79% in Kappa score in the
diabetic retinopathy classification task, 5.4% in AUC in the tumor
classification task, 7.03% AUC in the pneumonia detection, and 9.4% in AUC in
the detection of pathological conditions in chest X-ray. In addition, we
introduce Dynamic Visual Meta-Embedding (DVME) as an end-to-end transfer
learning approach that fuses pretrained embeddings from multiple models. We
show that the collective representation obtained by DVME leads to a significant
improvement in the performance of selected tasks compared to using a single
pretrained model approach and can be generalized to any combination of
pretrained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Transformer for Single Image Super-Resolution. (arXiv:2108.11084v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11084">
<div class="article-summary-box-inner">
<span><p>Single image super-resolution task has witnessed great strides with the
development of deep learning. However, most existing studies focus on building
a more complex neural network with a massive number of layers, bringing heavy
computational cost and memory storage. Recently, as Transformer yields
brilliant results in NLP tasks, more and more researchers start to explore the
application of Transformer in computer vision tasks. But with the heavy
computational cost and high GPU memory occupation of the vision Transformer,
the network can not be designed too deep. To address this problem, we propose a
novel Efficient Super-Resolution Transformer (ESRT) for fast and accurate image
super-resolution. ESRT is a hybrid Transformer where a CNN-based SR network is
first designed in the front to extract deep features. Specifically, there are
two backbones for formatting the ESRT: lightweight CNN backbone (LCB) and
lightweight Transformer backbone (LTB). Among them, LCB is a lightweight SR
network to extract deep SR features at a low computational cost by dynamically
adjusting the size of the feature map. LTB is made up of an efficient
Transformer (ET) with a small GPU memory occupation, which benefited from the
novel efficient multi-head attention (EMHA). In EMHA, a feature split module
(FSM) is proposed to split the long sequence into sub-segments and then these
sub-segments are applied by attention operation. This module can significantly
decrease the GPU memory occupation. Extensive experiments show that our ESRT
achieves competitive results. Compared with the original Transformer which
occupies 16057M GPU memory, the proposed ET only occupies 4191M GPU memory with
better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shifted Chunk Transformer for Spatio-Temporal Representational Learning. (arXiv:2108.11575v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11575">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal representational learning has been widely adopted in various
fields such as action recognition, video object segmentation, and action
anticipation. Previous spatio-temporal representational learning approaches
primarily employ ConvNets or sequential models,e.g., LSTM, to learn the
intra-frame and inter-frame features. Recently, Transformer models have
successfully dominated the study of natural language processing (NLP), image
classification, etc. However, the pure-Transformer based spatio-temporal
learning can be prohibitively costly on memory and computation to extract
fine-grained features from a tiny patch. To tackle the training difficulty and
enhance the spatio-temporal learning, we construct a shifted chunk Transformer
with pure self-attention blocks. Leveraging the recent efficient Transformer
design in NLP, this shifted chunk Transformer can learn hierarchical
spatio-temporal features from a local tiny patch to a global video clip. Our
shifted self-attention can also effectively model complicated inter-frame
variances. Furthermore, we build a clip encoder based on Transformer to model
long-term temporal dependencies. We conduct thorough ablation studies to
validate each component and hyper-parameters in our shifted chunk Transformer,
and it outperforms previous state-of-the-art approaches on Kinetics-400,
Kinetics-600, UCF101, and HMDB51.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking at the whole picture: constrained unsupervised anomaly segmentation. (arXiv:2109.00482v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00482">
<div class="article-summary-box-inner">
<span><p>Current unsupervised anomaly localization approaches rely on generative
models to learn the distribution of normal images, which is later used to
identify potential anomalous regions derived from errors on the reconstructed
images. However, a main limitation of nearly all prior literature is the need
of employing anomalous images to set a class-specific threshold to locate the
anomalies. This limits their usability in realistic scenarios, where only
normal data is typically accessible. Despite this major drawback, only a
handful of works have addressed this limitation, by integrating supervision on
attention maps during training. In this work, we propose a novel formulation
that does not require accessing images with abnormalities to define the
threshold. Furthermore, and in contrast to very recent work, the proposed
constraint is formulated in a more principled manner, leveraging well-known
knowledge in constrained optimization. In particular, the equality constraint
on the attention maps in prior work is replaced by an inequality constraint,
which allows more flexibility. In addition, to address the limitations of
penalty-based functions we employ an extension of the popular log-barrier
methods to handle the constraint. Comprehensive experiments on the popular
BRATS'19 dataset demonstrate that the proposed approach substantially
outperforms relevant literature, establishing new state-of-the-art results for
unsupervised lesion segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection Accuracy for Evaluating Compositional Explanations of Units. (arXiv:2109.07804v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07804">
<div class="article-summary-box-inner">
<span><p>The recent success of deep learning models in solving complex problems and in
different domains has increased interest in understanding what they learn.
Therefore, different approaches have been employed to explain these models, one
of which uses human-understandable concepts as explanations. Two examples of
methods that use this approach are Network Dissection and Compositional
explanations. The former explains units using atomic concepts, while the latter
makes explanations more expressive, replacing atomic concepts with logical
forms. While intuitively, logical forms are more informative than atomic
concepts, it is not clear how to quantify this improvement, and their
evaluation is often based on the same metric that is optimized during the
search-process and on the usage of hyper-parameters to be tuned. In this paper,
we propose to use as evaluation metric the Detection Accuracy, which measures
units' consistency of detection of their assigned explanations. We show that
this metric (1) evaluates explanations of different lengths effectively, (2)
can be used as a stopping criterion for the compositional explanation search,
eliminating the explanation length hyper-parameter, and (3) exposes new
specialized units whose length 1 explanations are the perceptual abstractions
of their longer explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BabelCalib: A Universal Approach to Calibrating Central Cameras. (arXiv:2109.09704v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09704">
<div class="article-summary-box-inner">
<span><p>Existing calibration methods occasionally fail for large field-of-view
cameras due to the non-linearity of the underlying problem and the lack of good
initial values for all parameters of the used camera model. This might occur
because a simpler projection model is assumed in an initial step, or a poor
initial guess for the internal parameters is pre-defined. A lot of the
difficulties of general camera calibration lie in the use of a forward
projection model. We side-step these challenges by first proposing a solver to
calibrate the parameters in terms of a back-projection model and then regress
the parameters for a target forward model. These steps are incorporated in a
robust estimation framework to cope with outlying detections. Extensive
experiments demonstrate that our approach is very reliable and returns the most
accurate calibration parameters as measured on the downstream task of absolute
pose estimation on test sets. The code is released at
https://github.com/ylochman/babelcalib.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Detection in Thermal Spectrum for Advanced Driver-Assistance Systems (ADAS). (arXiv:2109.09854v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09854">
<div class="article-summary-box-inner">
<span><p>Object detection in thermal infrared spectrum provides more reliable data
source in low-lighting conditions and different weather conditions, as it is
useful both in-cabin and outside for pedestrian, animal, and vehicular
detection as well as for detecting street-signs &amp; lighting poles. This paper is
about exploring and adapting state-of-the-art object detection and classifier
framework on thermal vision with seven distinct classes for advanced
driver-assistance systems (ADAS). The trained network variants on public
datasets are validated on test data with three different test approaches which
include test-time with no augmentation, test-time augmentation, and test-time
with model ensembling. Additionally, the efficacy of trained networks is tested
on locally gathered novel test-data captured with an uncooled LWIR prototype
thermal camera in challenging weather and environmental scenarios. The
performance analysis of trained models is investigated by computing precision,
recall, and mean average precision scores (mAP). Furthermore, the trained model
architecture is optimized using TensorRT inference accelerator and deployed on
resource-constrained edge hardware Nvidia Jetson Nano to explicitly reduce the
inference time on GPU as well as edge devices for further real-time onboard
installations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Predict Trustworthiness with Steep Slope Loss. (arXiv:2110.00054v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00054">
<div class="article-summary-box-inner">
<span><p>Understanding the trustworthiness of a prediction yielded by a classifier is
critical for the safe and effective use of AI models. Prior efforts have been
proven to be reliable on small-scale datasets. In this work, we study the
problem of predicting trustworthiness on real-world large-scale datasets, where
the task is more challenging due to high-dimensional features, diverse visual
concepts, and large-scale samples. In such a setting, we observe that the
trustworthiness predictors trained with prior-art loss functions, i.e., the
cross entropy loss, focal loss, and true class probability confidence loss, are
prone to view both correct predictions and incorrect predictions to be
trustworthy. The reasons are two-fold. Firstly, correct predictions are
generally dominant over incorrect predictions. Secondly, due to the data
complexity, it is challenging to differentiate the incorrect predictions from
the correct ones on real-world large-scale datasets. To improve the
generalizability of trustworthiness predictors, we propose a novel steep slope
loss to separate the features w.r.t. correct predictions from the ones w.r.t.
incorrect predictions by two slide-like curves that oppose each other. The
proposed loss is evaluated with two representative deep learning models, i.e.,
Vision Transformer and ResNet, as trustworthiness predictors. We conduct
comprehensive experiments and analyses on ImageNet, which show that the
proposed loss effectively improves the generalizability of trustworthiness
predictors. The code and pre-trained trustworthiness predictors for
reproducibility are available at
https://github.com/luoyan407/predict_trustworthiness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Instance Segmentation with High-Resolution Automotive Radar. (arXiv:2110.01775v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01775">
<div class="article-summary-box-inner">
<span><p>Automotive radar has been widely used in the modern advanced driver
assistance systems (ADAS) and autonomous driving system as it provides reliable
environmental perception in all-weather conditions with affordable cost.
However, automotive radar usually only plays as an auxiliary sensor since it
hardly supplies semantic and geometry information due to the sparsity of radar
detection points. Nonetheless, as development of high-resolution automotive
radar in recent years, more advanced perception functionality like instance
segmentation which has only been well explored using Lidar point clouds,
becomes possible by using automotive radar. Its data comes with rich contexts
such as Radar Cross Section (RCS) and micro-doppler effects which may
potentially be pertinent, and sometimes can even provide detection when the
field of view is completely obscured. Therefore, the effective utilization of
radar detection points data is an integral part of automotive perception. The
outcome from instance segmentation could be seen as comparable result of
clustering, and could be potentially used as the input of tracker for tracking
the targets. In this paper, we propose two efficient methods for instance
segmentation with radar detection points, one is implemented in an end-to-end
deep learning driven fashion using PointNet++ framework, and the other is based
on clustering of the radar detection points with semantic information. Both
approaches can be further improved by implementing visual multi-layer
perceptron (MLP). The effectiveness of the proposed methods is verified using
experimental results on the recent RadarScenes dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-center, multi-vendor automated segmentation of left ventricular anatomy in contrast-enhanced MRI. (arXiv:2110.07360v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07360">
<div class="article-summary-box-inner">
<span><p>Accurate delineation of the left ventricular boundaries in late
gadolinium-enhanced magnetic resonance imaging (LGE-MRI) is an essential step
for scar tissue quantification and patient-specific assessment of myocardial
infarction. Many deep-learning techniques have been proposed to perform
automatic segmentations of the left ventricle (LV) in LGE-MRI showing
segmentations as accurate as those obtained by expert cardiologists. Thus far,
the existing models have been overwhelmingly developed and evaluated with
LGE-MRI datasets from single clinical centers. However, in practice, LGE-MRI
images vary significantly between clinical centers within and across countries,
in particular due to differences in the MRI scanners, imaging conditions,
contrast injection protocols and local clinical practise. This work
investigates for the first time multi-center and multi-vendor LV segmentation
in LGE-MRI, by proposing, implementing and evaluating in detail several
strategies to enhance model generalizability across clinical cites. These
include data augmentation to artificially augment the image variability in the
training sample, image harmonization to align the distributions of LGE-MRI
images across centers, and transfer learning to adjust existing single-center
models to unseen images from new clinical sites. The results obtained based on
a new multi-center LGE-MRI dataset acquired in four clinical centers in Spain,
France and China, show that the combination of data augmentation and transfer
learning can lead to single-center models that generalize well to new clinical
centers not included in the original training. The proposed framework shows the
potential for developing clinical tools for automated LV segmentation in
LGE-MRI that can be deployed in multiple clinical centers across distinct
geographical locations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMDS-7: Environmental Microorganism Image Dataset Seventh Version for Multiple Object Detection Evaluation. (arXiv:2110.07723v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07723">
<div class="article-summary-box-inner">
<span><p>The Environmental Microorganism Image Dataset Seventh Version (EMDS-7) is a
microscopic image data set, including the original Environmental Microorganism
images (EMs) and the corresponding object labeling files in ".XML" format file.
The EMDS-7 data set consists of 41 types of EMs, which has a total of 2365
images and 13216 labeled objects. The EMDS-7 database mainly focuses on the
object detection. In order to prove the effectiveness of EMDS-7, we select the
most commonly used deep learning methods (Faster-RCNN, YOLOv3, YOLOv4, SSD and
RetinaNet) and evaluation indices for testing and evaluation. EMDS-7 is freely
published for non-commercial purpose at:
https://figshare.com/articles/dataset/EMDS-7_DataSet/16869571
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EFENet: Reference-based Video Super-Resolution with Enhanced Flow Estimation. (arXiv:2110.07797v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07797">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problem of reference-based video
super-resolution(RefVSR), i.e., how to utilize a high-resolution (HR) reference
frame to super-resolve a low-resolution (LR) video sequence. The existing
approaches to RefVSR essentially attempt to align the reference and the input
sequence, in the presence of resolution gap and long temporal range. However,
they either ignore temporal structure within the input sequence, or suffer
accumulative alignment errors. To address these issues, we propose EFENet to
exploit simultaneously the visual cues contained in the HR reference and the
temporal information contained in the LR sequence. EFENet first globally
estimates cross-scale flow between the reference and each LR frame. Then our
novel flow refinement module of EFENet refines the flow regarding the furthest
frame using all the estimated flows, which leverages the global temporal
information within the sequence and therefore effectively reduces the alignment
errors. We provide comprehensive evaluations to validate the strengths of our
approach, and to demonstrate that the proposed framework outperforms the
state-of-the-art methods. Code is available at
https://github.com/IndigoPurple/EFENet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping illegal waste dumping sites with neural-network classification of satellite imagery. (arXiv:2110.08599v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08599">
<div class="article-summary-box-inner">
<span><p>Public health and habitat quality are crucial goals of urban planning. In
recent years, the severe social and environmental impact of illegal waste
dumping sites has made them one of the most serious problems faced by cities in
the Global South, in a context of scarce information available for decision
making. To help identify the location of dumping sites and track their
evolution over time we adopt a data-driven model from the machine learning
domain, analyzing satellite images. This allows us to take advantage of the
increasing availability of geo-spatial open-data, high-resolution satellite
imagery, and open source tools to train machine learning algorithms with a
small set of known waste dumping sites in Buenos Aires, and then predict the
location of other sites over vast areas at high speed and low cost. This case
study shows the results of a collaboration between Dymaxion Labs and
Fundaci\'on Bunge y Born to harness this technique in order to create a
comprehensive map of potential locations of illegal waste dumping sites in the
region.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-Based Detection, Classification and Prediction/Prognosis in Medical Imaging: Towards Radiophenomics. (arXiv:2110.10332v2 [physics.med-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10332">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) techniques have significant potential to enable
effective, robust and automated image phenotyping including identification of
subtle patterns. AI-based detection searches the image space to find the
regions of interest based on patterns and features. There is a spectrum of
tumor histologies from benign to malignant that can be identified by AI-based
classification approaches using image features. The extraction of minable
information from images gives way to the field of radiomics and can be explored
via explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics
analysis has the potential to be utilized as a noninvasive technique for the
accurate characterization of tumors to improve diagnosis and treatment
monitoring. This work reviews AI-based techniques, with a special focus on
oncological PET and PET/CT imaging, for different detection, classification,
and prediction/prognosis tasks. We also discuss needed efforts to enable the
translation of AI techniques to routine clinical workflows, and potential
improvements and complementary techniques such as the use of natural language
processing on electronic health records and neuro-symbolic AI techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Illiterate DALL-E Learns to Compose. (arXiv:2110.11405v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11405">
<div class="article-summary-box-inner">
<span><p>Although DALL-E has shown an impressive ability of composition-based
systematic generalization in image generation, it requires the dataset of
text-image pairs and the compositionality is provided by the text. In contrast,
object-centric representation models like the Slot Attention model learn
composable representations without the text prompt. However, unlike DALL-E its
ability to systematically generalize for zero-shot generation is significantly
limited. In this paper, we propose a simple but novel slot-based autoencoding
architecture, called SLATE, for combining the best of both worlds: learning
object-centric representations that allows systematic generalization in
zero-shot image generation without text. As such, this model can also be seen
as an illiterate DALL-E model. Unlike the pixel-mixture decoders of existing
object-centric representation models, we propose to use the Image GPT decoder
conditioned on the slots for capturing complex interactions among the slots and
pixels. In experiments, we show that this simple and easy-to-implement
architecture not requiring a text prompt achieves significant improvement in
in-distribution and out-of-distribution (zero-shot) image generation and
qualitatively comparable or better slot-attention structure than the models
based on mixture decoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Cross-Modal Prediction and Relation Consistency for Semi-Supervised Image Captioning. (arXiv:2110.11767v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11767">
<div class="article-summary-box-inner">
<span><p>The task of image captioning aims to generate captions directly from images
via the automatically learned cross-modal generator. To build a well-performing
generator, existing approaches usually need a large number of described images,
which requires a huge effects on manual labeling. However, in real-world
applications, a more general scenario is that we only have limited amount of
described images and a large number of undescribed images. Therefore, a
resulting challenge is how to effectively combine the undescribed images into
the learning of cross-modal generator. To solve this problem, we propose a
novel image captioning method by exploiting the Cross-modal Prediction and
Relation Consistency (CPRC), which aims to utilize the raw image input to
constrain the generated sentence in the commonly semantic space. In detail,
considering that the heterogeneous gap between modalities always leads to the
supervision difficulty of using the global embedding directly, CPRC turns to
transform both the raw image and corresponding generated sentence into the
shared semantic space, and measure the generated sentence from two aspects: 1)
Prediction consistency. CPRC utilizes the prediction of raw image as soft label
to distill useful supervision for the generated sentence, rather than employing
the traditional pseudo labeling; 2) Relation consistency. CPRC develops a novel
relation consistency between augmented images and corresponding generated
sentences to retain the important relational knowledge. In result, CPRC
supervises the generated sentence from both the informativeness and
representativeness perspectives, and can reasonably use the undescribed images
to learn a more effective generator under the semi-supervised scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerate 3D Object Processing via Spectral Layout. (arXiv:2110.12621v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12621">
<div class="article-summary-box-inner">
<span><p>3D image processing is an important problem in computer vision and pattern
recognition fields. Compared with 2D image processing, its computation
difficulty and cost are much higher due to the extra dimension. To
fundamentally address this problem, we propose to embed the essential
information in a 3D object into 2D space via spectral layout. Specifically, we
construct a 3D adjacency graph to capture spatial structure of the 3D voxel
grid. Then we calculate the eigenvectors corresponding to the second and third
smallest eigenvalues of its graph Laplacian and perform spectral layout to map
each voxel into a pixel in 2D Cartesian coordinate plane. The proposed method
can achieve high quality 2D representations for 3D objects, which enables to
use 2D-based methods to process 3D objects. The experimental results
demonstrate the effectiveness and efficiency of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Comes Dancing with Collaborative Parsing-Flow Video Synthesis. (arXiv:2110.14147v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14147">
<div class="article-summary-box-inner">
<span><p>Transferring human motion from a source to a target person poses great
potential in computer vision and graphics applications. A crucial step is to
manipulate sequential future motion while retaining the appearance
characteristic.Previous work has either relied on crafted 3D human models or
trained a separate model specifically for each target person, which is not
scalable in practice.This work studies a more general setting, in which we aim
to learn a single model to parsimoniously transfer motion from a source video
to any target person given only one image of the person, named as Collaborative
Parsing-Flow Network (CPF-Net). The paucity of information regarding the target
person makes the task particularly challenging to faithfully preserve the
appearance in varying designated poses. To address this issue, CPF-Net
integrates the structured human parsing and appearance flow to guide the
realistic foreground synthesis which is merged into the background by a
spatio-temporal fusion module. In particular, CPF-Net decouples the problem
into stages of human parsing sequence generation, foreground sequence
generation and final video generation. The human parsing generation stage
captures both the pose and the body structure of the target. The appearance
flow is beneficial to keep details in synthesized frames. The integration of
human parsing and appearance flow effectively guides the generation of video
frames with realistic appearance. Finally, the dedicated designed fusion
network ensure the temporal coherence. We further collect a large set of human
dancing videos to push forward this research field. Both quantitative and
qualitative results show our method substantially improves over previous
approaches and is able to generate appealing and photo-realistic target videos
given any input person image. All source code and dataset will be released at
https://github.com/xiezhy6/CPF-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal-attentive Covariance Pooling Networks for Video Recognition. (arXiv:2110.14381v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14381">
<div class="article-summary-box-inner">
<span><p>For video recognition task, a global representation summarizing the whole
contents of the video snippets plays an important role for the final
performance. However, existing video architectures usually generate it by using
a simple, global average pooling (GAP) method, which has limited ability to
capture complex dynamics of videos. For image recognition task, there exist
evidences showing that covariance pooling has stronger representation ability
than GAP. Unfortunately, such plain covariance pooling used in image
recognition is an orderless representative, which cannot model spatio-temporal
structure inherent in videos. Therefore, this paper proposes a
Temporal-attentive Covariance Pooling(TCP), inserted at the end of deep
architectures, to produce powerful video representations. Specifically, our TCP
first develops a temporal attention module to adaptively calibrate
spatio-temporal features for the succeeding covariance pooling, approximatively
producing attentive covariance representations. Then, a temporal covariance
pooling performs temporal pooling of the attentive covariance representations
to characterize both intra-frame correlations and inter-frame
cross-correlations of the calibrated features. As such, the proposed TCP can
capture complex temporal dynamics. Finally, a fast matrix power normalization
is introduced to exploit geometry of covariance representations. Note that our
TCP is model-agnostic and can be flexibly integrated into any video
architectures, resulting in TCPNet for effective video recognition. The
extensive experiments on six benchmarks (e.g., Kinetics, Something-Something V1
and Charades) using various video architectures show our TCPNet is clearly
superior to its counterparts, while having strong generalization ability. The
source code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Geometric Perspective towards Neural Calibration via Sensitivity Decomposition. (arXiv:2110.14577v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14577">
<div class="article-summary-box-inner">
<span><p>It is well known that vision classification models suffer from poor
calibration in the face of data distribution shifts. In this paper, we take a
geometric approach to this problem. We propose Geometric Sensitivity
Decomposition (GSD) which decomposes the norm of a sample feature embedding and
the angular similarity to a target classifier into an instance-dependent and an
instance-independent component. The instance-dependent component captures the
sensitive information about changes in the input while the instance-independent
component represents the insensitive information serving solely to minimize the
loss on the training dataset. Inspired by the decomposition, we analytically
derive a simple extension to current softmax-linear models, which learns to
disentangle the two components during training. On several common vision
models, the disentangled model outperforms other calibration methods on
standard calibration metrics in the face of out-of-distribution (OOD) data and
corruption with significantly less complexity. Specifically, we surpass the
current state of the art by 30.8% relative improvement on corrupted CIFAR100 in
Expected Calibration Error. Code available at
https://github.com/GT-RIPL/Geometric-Sensitivity-Decomposition.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v4 [eess.IV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09780">
<div class="article-summary-box-inner">
<span><p>COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging
exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,
and uses less radiation. Here, we demonstrate the impact of lung segmentation
in COVID-19 identification using CXR images and evaluate which contents of the
image influenced the most. Semantic segmentation was performed using a U-Net
CNN architecture, and the classification using three CNN architectures (VGG,
ResNet, and Inception). Explainable Artificial Intelligence techniques were
employed to estimate the impact of segmentation. A three-classes database was
composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the
impact of creating a CXR image database from different sources, and the
COVID-19 generalization from one source to another. The segmentation achieved a
Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification
using segmented images achieved an F1-Score of 0.88 for the multi-class setup,
and 0.83 for COVID-19 identification. In the cross-dataset scenario, we
obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for
COVID-19 identification using segmented images. Experiments support the
conclusion that even after segmentation, there is a strong bias introduced by
underlying factors from different sources.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-30 23:02:19.466473128 UTC">2021-10-30 23:02:19 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.6</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>