<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-26T01:30:00Z">01-26</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification Of Fake News Headline Based On Neural Networks. (arXiv:2201.09966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09966">
<div class="article-summary-box-inner">
<span><p>Over the last few years, Text classification is one of the fundamental tasks
in natural language processing (NLP) in which the objective is to categorize
text documents into one of the predefined classes. The news is full of our
life. Therefore, news headlines classification is a crucial task to connect
users with the right news. The news headline classification is a kind of text
classification, which can be generally divided into three mainly parts: feature
extraction, classifier selection, and evaluations. In this article, we use the
dataset, containing news over a period of eighteen years provided by Kaggle
platform to classify news headlines. We choose TF-IDF to extract features and
neural network as the classifier, while the evaluation metrics is accuracy.
From the experiment result, it is obvious that our NN model has the best
performance among these models in the metrics of accuracy. The higher the
accuracy is, the better performance the model will gain. Our NN model owns the
accuracy 0.8622, which is highest accuracy among these four models. And it is
0.0134, 0.033, 0.080 higher than its of other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HC4: A New Suite of Test Collections for Ad Hoc CLIR. (arXiv:2201.09992v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09992">
<div class="article-summary-box-inner">
<span><p>HC4 is a new suite of test collections for ad hoc Cross-Language Information
Retrieval (CLIR), with Common Crawl News documents in Chinese, Persian, and
Russian, topics in English and in the document languages, and graded relevance
judgments. New test collections are needed because existing CLIR test
collections built using pooling of traditional CLIR runs have systematic gaps
in their relevance judgments when used to evaluate neural CLIR methods. The HC4
collections contain 60 topics and about half a million documents for each of
Chinese and Persian, and 54 topics and five million documents for Russian.
Active learning was used to determine which documents to annotate after being
seeded using interactive search and judgment. Documents were judged on a
three-grade relevance scale. This paper describes the design and construction
of the new test collections and provides baseline results for demonstrating
their utility for evaluating systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Razmecheno: Named Entity Recognition from Digital Archive of Diaries "Prozhito". (arXiv:2201.09997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09997">
<div class="article-summary-box-inner">
<span><p>The vast majority of existing datasets for Named Entity Recognition (NER) are
built primarily on news, research papers and Wikipedia with a few exceptions,
created from historical and literary texts. What is more, English is the main
source for data for further labelling. This paper aims to fill in multiple gaps
by creating a novel dataset "Razmecheno", gathered from the diary texts of the
project "Prozhito" in Russian. Our dataset is of interest for multiple research
lines: literary studies of diary texts, transfer learning from other domains,
low-resource or cross-lingual named entity recognition. Razmecheno comprises
1331 sentences and 14119 tokens, sampled from diaries, written during the
Perestroika. The annotation schema consists of five commonly used entity tags:
person, characteristics, location, organisation, and facility. The labelling is
carried out on the crowdsourcing platfrom Yandex.Toloka in two stages. First,
workers selected sentences, which contain an entity of particular type. Second,
they marked up entity spans. As a result 1113 entities were obtained. Empirical
evaluation of Razmecheno is carried out with off-the-shelf NER tools and by
fine-tuning pre-trained contextualized encoders. We release the annotated
dataset for open access.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text and Code Embeddings by Contrastive Pre-Training. (arXiv:2201.10005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10005">
<div class="article-summary-box-inner">
<span><p>Text embeddings are useful features in many applications such as semantic
search and computing text similarity. Previous work typically trains models
customized for different use cases, varying in dataset choice, training
objective and model architecture. In this work, we show that contrastive
pre-training on unsupervised data at scale leads to high quality vector
representations of text and code. The same unsupervised text embeddings that
achieve new state-of-the-art results in linear-probe classification also
display impressive semantic search capabilities and sometimes even perform
competitively with fine-tuned models. On linear-probe classification accuracy
averaging over 7 tasks, our best unsupervised model achieves a relative
improvement of 4% and 1.8% over previous best unsupervised and supervised text
embedding models respectively. The same text embeddings when evaluated on
large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and
10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and
TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code
embedding models on (text, code) pairs, obtaining a 20.8% relative improvement
over prior best work on code search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Documenting Geographically and Contextually Diverse Data Sources: The BigScience Catalogue of Language Data and Resources. (arXiv:2201.10066v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10066">
<div class="article-summary-box-inner">
<span><p>In recent years, large-scale data collection efforts have prioritized the
amount of data collected in order to improve the modeling capabilities of large
language models. This prioritization, however, has resulted in concerns with
respect to the rights of data subjects represented in data collections,
particularly when considering the difficulty in interrogating these collections
due to insufficient documentation and tools for analysis. Mindful of these
pitfalls, we present our methodology for a documentation-first, human-centered
data collection project as part of the BigScience initiative. We identified a
geographically diverse set of target language groups (Arabic, Basque, Chinese,
Catalan, English, French, Indic languages, Indonesian, Niger-Congo languages,
Portuguese, Spanish, and Vietnamese, as well as programming languages) for
which to collect metadata on potential data sources. To structure this effort,
we developed our online catalogue as a supporting tool for gathering metadata
through organized public hackathons. We present our development process;
analyses of the resulting resource metadata, including distributions over
languages, regions, and resource types; and our lessons learned in this
endeavor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two heads are better than one: Enhancing medical representations by pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10113">
<div class="article-summary-box-inner">
<span><p>The massive context of electronic health records (EHRs) has created enormous
potentials for improving healthcare, among which structured (coded) data and
unstructured (text) data are two important textual modalities. They do not
exist in isolation and can complement each other in most real-life clinical
scenarios. Most existing researches in medical informatics, however, either
only focus on a particular modality or straightforwardly concatenate the
information from different modalities, which ignore the interaction and
information sharing between them. To address these issues, we proposed a
unified deep learning-based medical pre-trained language model, named UMM-PLM,
to automatically learn representative features from multimodal EHRs that
consist of both structured data and unstructured data. Specifically, we first
developed parallel unimodal information representation modules to capture the
unimodal-specific characteristic, where unimodal representations were learned
from each data source separately. A cross-modal module was further introduced
to model the interactions between different modalities. We pre-trained the
model on a large EHRs dataset containing both structured data and unstructured
data and verified the effectiveness of the model on three downstream clinical
tasks, i.e., medication recommendation, 30-day readmission and ICD coding
through extensive experiments. The results demonstrate the power of UMM-PLM
compared with benchmark methods and state-of-the-art baselines. Analyses show
that UMM-PLM can effectively concern with multimodal textual information and
has the potential to provide more comprehensive interpretations for clinical
decision making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training. (arXiv:2201.10207v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10207">
<div class="article-summary-box-inner">
<span><p>We introduce a new approach for speech pre-training named SPIRAL which works
by learning denoising representation of perturbed data in a teacher-student
framework. Specifically, given a speech utterance, we first feed the utterance
to a teacher network to obtain corresponding representation. Then the same
utterance is perturbed and fed to a student network. The student network is
trained to output representation resembling that of the teacher. At the same
time, the teacher network is updated as moving average of student's weights
over training steps. In order to prevent representation collapse, we apply an
in-utterance contrastive loss as pre-training objective and impose position
randomization on the input to the teacher. SPIRAL achieves competitive or
better results compared to state-of-the-art speech pre-training method wav2vec
2.0, with significant reduction of training cost (80% for Base model, 65% for
Large model). Furthermore, we address the problem of noise-robustness that is
critical to real-world speech applications. We propose multi-condition
pre-training by perturbing the student's input with various types of additive
noise. We demonstrate that multi-condition pre-trained SPIRAL models are more
robust to noisy speech (9.0% - 13.3% relative word error rate reduction on real
noisy test data), compared to applying multi-condition training solely in the
fine-tuning stage. The code will be released after publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanatory Learning: Beyond Empiricism in Neural Networks. (arXiv:2201.10222v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10222">
<div class="article-summary-box-inner">
<span><p>We introduce Explanatory Learning (EL), a framework to let machines use
existing knowledge buried in symbolic sequences -- e.g. explanations written in
hieroglyphic -- by autonomously learning to interpret them. In EL, the burden
of interpreting symbols is not left to humans or rigid human-coded compilers,
as done in Program Synthesis. Rather, EL calls for a learned interpreter, built
upon a limited collection of symbolic sequences paired with observations of
several phenomena. This interpreter can be used to make predictions on a novel
phenomenon given its explanation, and even to find that explanation using only
a handful of observations, like human scientists do. We formulate the EL
problem as a simple binary classification task, so that common end-to-end
approaches aligned with the dominant empiricist view of machine learning could,
in principle, solve it. To these models, we oppose Critical Rationalist
Networks (CRNs), which instead embrace a rationalist view on the acquisition of
knowledge. CRNs express several desired properties by construction, they are
truly explainable, can adjust their processing at test-time for harder
inferences, and can offer strong confidence guarantees on their predictions. As
a final contribution, we introduce Odeen, a basic EL environment that simulates
a small flatland-style universe full of phenomena to explain. Using Odeen as a
testbed, we show how CRNs outperform empiricist end-to-end approaches of
similar size and architecture (Transformers) in discovering explanations for
novel phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the fusion of acoustic and text representations in RNN-T. (arXiv:2201.10240v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10240">
<div class="article-summary-box-inner">
<span><p>The recurrent neural network transducer (RNN-T) has recently become the
mainstream end-to-end approach for streaming automatic speech recognition
(ASR). To estimate the output distributions over subword units, RNN-T uses a
fully connected layer as the joint network to fuse the acoustic representations
extracted using the acoustic encoder with the text representations obtained
using the prediction network based on the previous subword units. In this
paper, we propose to use gating, bilinear pooling, and a combination of them in
the joint network to produce more expressive representations to feed into the
output layer. A regularisation method is also proposed to enable better
acoustic encoder training by reducing the gradients back-propagated into the
prediction network at the beginning of RNN-T training. Experimental results on
a multilingual ASR setting for voice search over nine languages show that the
joint use of the proposed methods can result in 4%--5% relative word error rate
reductions with only a few million extra parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Transformers Encode a Foundational Ontology? Probing Abstract Classes in Natural Language. (arXiv:2201.10262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10262">
<div class="article-summary-box-inner">
<span><p>With the methodological support of probing (or diagnostic classification),
recent studies have demonstrated that Transformers encode syntactic and
semantic information to some extent. Following this line of research, this
paper aims at taking semantic probing to an abstraction extreme with the goal
of answering the following research question: can contemporary
Transformer-based models reflect an underlying Foundational Ontology? To this
end, we present a systematic Foundational Ontology (FO) probing methodology to
investigate whether Transformers-based models encode abstract semantic
information. Following different pre-training and fine-tuning regimes, we
present an extensive evaluation of a diverse set of large-scale language models
over three distinct and complementary FO tagging experiments. Specifically, we
present and discuss the following conclusions: (1) The probing results indicate
that Transformer-based models incidentally encode information related to
Foundational Ontologies during the pre-training pro-cess; (2) Robust FO taggers
(accuracy of 90 percent)can be efficiently built leveraging on this knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-channel Attentive Graph Convolutional Network With Sentiment Fusion For Multimodal Sentiment Analysis. (arXiv:2201.10274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10274">
<div class="article-summary-box-inner">
<span><p>Nowadays, with the explosive growth of multimodal reviews on social media
platforms, multimodal sentiment analysis has recently gained popularity because
of its high relevance to these social media posts. Although most previous
studies design various fusion frameworks for learning an interactive
representation of multiple modalities, they fail to incorporate sentimental
knowledge into inter-modality learning. This paper proposes a Multi-channel
Attentive Graph Convolutional Network (MAGCN), consisting of two main
components: cross-modality interactive learning and sentimental feature fusion.
For cross-modality interactive learning, we exploit the self-attention
mechanism combined with densely connected graph convolutional networks to learn
inter-modality dynamics. For sentimental feature fusion, we utilize multi-head
self-attention to merge sentimental knowledge into inter-modality feature
representations. Extensive experiments are conducted on three widely-used
datasets. The experimental results demonstrate that the proposed model achieves
competitive performance on accuracy and F1 scores compared to several
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Long-Form Voice Cloning with Dynamic Convolution Attention. (arXiv:2201.10375v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10375">
<div class="article-summary-box-inner">
<span><p>With recent advancements in voice cloning, the performance of speech
synthesis for a target speaker has been rendered similar to the human level.
However, autoregressive voice cloning systems still suffer from text alignment
failures, resulting in an inability to synthesize long sentences. In this work,
we propose a variant of attention-based text-to-speech system that can
reproduce a target voice from a few seconds of reference speech and generalize
to very long utterances as well. The proposed system is based on three
independently trained components: a speaker encoder, synthesizer and universal
vocoder. Generalization to long utterances is realized using an energy-based
attention mechanism known as Dynamic Convolution Attention, in combination with
a set of modifications proposed for the synthesizer based on Tacotron 2.
Moreover, effective zero-shot speaker adaptation is achieved by conditioning
both the synthesizer and vocoder on a speaker encoder that has been pretrained
on a large corpus of diverse data. We compare several implementations of voice
cloning systems in terms of speech naturalness, speaker similarity, alignment
consistency and ability to synthesize long utterances, and conclude that the
proposed model can produce intelligible synthetic speech for extremely long
utterances, while preserving a high extent of naturalness and similarity for
short texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Multi-level Context for Informational Bias Detection by Contrastive Learning and Sentential Graph Network. (arXiv:2201.10376v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10376">
<div class="article-summary-box-inner">
<span><p>Informational bias is widely present in news articles. It refers to providing
one-sided, selective or suggestive information of specific aspects of certain
entity to guide a specific interpretation, thereby biasing the reader's
opinion. Sentence-level informational bias detection is a very challenging task
in a way that such bias can only be revealed together with the context,
examples include collecting information from various sources or analyzing the
entire article in combination with the background. In this paper, we integrate
three levels of context to detect the sentence-level informational bias in
English news articles: adjacent sentences, whole article, and articles from
other news outlets describing the same event. Our model, MultiCTX (Multi-level
ConTeXt), uses contrastive learning and sentence graphs together with Graph
Attention Network (GAT) to encode these three degrees of context at different
stages by tactically composing contrastive triplets and constructing sentence
graphs within events. Our experiments proved that contrastive learning together
with sentence graphs effectively incorporates context in varying degrees and
significantly outperforms the current SOTA model sentence-wise in informational
bias detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Generation for Broad-Coverage, Explainable Cognitive Systems. (arXiv:2201.10422v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10422">
<div class="article-summary-box-inner">
<span><p>This paper describes recent progress on natural language generation (NLG) for
language-endowed intelligent agents (LEIAs) developed within the OntoAgent
cognitive architecture. The approach draws heavily from past work on natural
language understanding in this paradigm: it uses the same knowledge bases,
theory of computational linguistics, agent architecture, and methodology of
developing broad-coverage capabilities over time while still supporting
near-term applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Quantitative and Qualitative Analysis of Schizophrenia Language. (arXiv:2201.10430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10430">
<div class="article-summary-box-inner">
<span><p>Schizophrenia is one of the most disabling mental health conditions to live
with. Approximately one percent of the population has schizophrenia which makes
it fairly common, and it affects many people and their families. Patients with
schizophrenia suffer different symptoms: formal thought disorder (FTD),
delusions, and emotional flatness. In this paper, we quantitatively and
qualitatively analyze the language of patients with schizophrenia measuring
various linguistic features in two modalities: speech and written text. We
examine the following features: coherence and cohesion of thoughts, emotions,
specificity, level of committed belief (LCB), and personality traits. Our
results show that patients with schizophrenia score high in fear and
neuroticism compared to healthy controls. In addition, they are more committed
to their beliefs, and their writing lacks details. They score lower in most of
the linguistic features of cohesion with significant p-values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distantly supervised end-to-end medical entity extraction from electronic health records with human-level quality. (arXiv:2201.10463v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10463">
<div class="article-summary-box-inner">
<span><p>Medical entity extraction (EE) is a standard procedure used as a first stage
in medical texts processing. Usually Medical EE is a two-step process: named
entity recognition (NER) and named entity normalization (NEN). We propose a
novel method of doing medical EE from electronic health records (EHR) as a
single-step multi-label classification task by fine-tuning a transformer model
pretrained on a large EHR dataset. Our model is trained end-to-end in an
distantly supervised manner using targets automatically extracted from medical
knowledge base. We show that our model learns to generalize for entities that
are present frequently enough, achieving human-level classification quality for
most frequent entities. Our work demonstrates that medical entity extraction
can be done end-to-end without human supervision and with human quality given
the availability of a large enough amount of unlabeled EHR and a medical
knowledge base.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection. (arXiv:2201.10474v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10474">
<div class="article-summary-box-inner">
<span><p>Language models increasingly rely on massive web dumps for diverse text data.
However, these sources are rife with undesirable content. As such, resources
like Wikipedia, books, and newswire often serve as anchors for automatically
selecting web text most suitable for language modeling, a process typically
referred to as quality filtering. Using a new dataset of U.S. high school
newspaper articles -- written by students from across the country -- we
investigate whose language is preferred by the quality filter used for GPT-3.
We find that newspapers from larger schools, located in wealthier, educated,
and urban ZIP codes are more likely to be classified as high quality. We then
demonstrate that the filter's measurement of quality is unaligned with other
sensible metrics, such as factuality or literary acclaim. We argue that
privileging any corpus as high quality entails a language ideology, and more
care is needed to construct training corpora for language models, with better
transparency and justification for the inclusion or exclusion of various texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Suicidal Ideation Detection on Social Media: A Review of Machine Learning Methods. (arXiv:2201.10515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10515">
<div class="article-summary-box-inner">
<span><p>Social media platforms have transformed traditional communication methods by
allowing users worldwide to communicate instantly, openly, and frequently.
People use social media to express their opinion and share their personal
stories and struggles. Negative feelings that express hardship, thoughts of
death, and self-harm are widespread in social media, especially among young
generations. Therefore, using social media to detect and identify suicidal
ideation will help provide proper intervention that will eventually dissuade
others from self-harming and committing suicide and prevent the spread of
suicidal ideations on social media. Many studies have been carried out to
identify suicidal ideation and behaviors in social media. This paper presents a
comprehensive summary of current research efforts to detect suicidal ideation
using machine learning algorithms on social media. This review 24 studies
investigating the feasibility of social media usage for suicidal ideation
detection is intended to facilitate further research in the field and will be a
beneficial resource for researchers engaged in suicidal text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Influential Instances for Distantly Supervised Relation Extraction. (arXiv:2009.09841v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09841">
<div class="article-summary-box-inner">
<span><p>Distant supervision (DS) is a strong way to expand the datasets for enhancing
relation extraction (RE) models but often suffers from high label noise.
Current works based on attention, reinforcement learning, or GAN are black-box
models so they neither provide meaningful interpretation of sample selection in
DS nor stability on different domains. On the contrary, this work proposes a
novel model-agnostic instance sampling method for DS by influence function
(IF), namely REIF. Our method identifies favorable/unfavorable instances in the
bag based on IF, then does dynamic instance sampling. We design a fast
influence sampling algorithm that reduces the computational complexity from
$\mathcal{O}(mn)$ to $\mathcal{O}(1)$, with analyzing its robustness on the
selected sampling function. Experiments show that by simply sampling the
favorable instances during training, REIF is able to win over a series of
baselines that have complicated architectures. We also demonstrate that REIF
can support interpretable instance selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering and Interpreting Biased Concepts in Online Communities. (arXiv:2010.14448v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14448">
<div class="article-summary-box-inner">
<span><p>Language carries implicit human biases, functioning both as a reflection and
a perpetuation of stereotypes that people carry with them. Recently, ML-based
NLP methods such as word embeddings have been shown to learn such language
biases with striking accuracy. This capability of word embeddings has been
successfully exploited as a tool to quantify and study human biases. However,
previous studies only consider a predefined set of biased concepts to attest
(e.g., whether gender is more or less associated with particular jobs), or just
discover biased words without helping to understand their meaning at the
conceptual level. As such, these approaches can be either unable to find biased
concepts that have not been defined in advance, or the biases they find are
difficult to interpret and study. This could make existing approaches
unsuitable to discover and interpret biases in online communities, as such
communities may carry different biases than those in mainstream culture. This
paper improves upon, extends, and evaluates our previous data-driven method to
automatically discover and help interpret biased concepts encoded in word
embeddings. We apply this approach to study the biased concepts present in the
language used in online communities and experimentally show the validity and
stability of our method
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Embarrassingly Simple Model for Dialogue Relation Extraction. (arXiv:2012.13873v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13873">
<div class="article-summary-box-inner">
<span><p>Dialogue relation extraction (RE) is to predict the relation type of two
entities mentioned in a dialogue. In this paper, we propose a simple yet
effective model named SimpleRE for the RE task. SimpleRE captures the
interrelations among multiple relations in a dialogue through a novel input
format named BERT Relation Token Sequence (BRS). In BRS, multiple [CLS] tokens
are used to capture possible relations between different pairs of entities
mentioned in the dialogue. A Relation Refinement Gate (RRG) is then designed to
extract relation-specific semantic representation in an adaptive manner.
Experiments on the DialogRE dataset show that SimpleRE achieves the best
performance, with much shorter training time. Further, SimpleRE outperforms all
direct baselines on sentence-level RE without using external resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ruddit: Norms of Offensiveness for English Reddit Comments. (arXiv:2106.05664v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05664">
<div class="article-summary-box-inner">
<span><p>On social media platforms, hateful and offensive language negatively impact
the mental well-being of users and the participation of people from diverse
backgrounds. Automatic methods to detect offensive language have largely relied
on datasets with categorical labels. However, comments can vary in their degree
of offensiveness. We create the first dataset of English language Reddit
comments that has fine-grained, real-valued scores between -1 (maximally
supportive) and 1 (maximally offensive). The dataset was annotated using
Best--Worst Scaling, a form of comparative annotation that has been shown to
alleviate known biases of using rating scales. We show that the method produces
highly reliable offensiveness scores. Finally, we evaluate the ability of
widely-used neural models to predict offensiveness scores on this new dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07847">
<div class="article-summary-box-inner">
<span><p>While unbiased machine learning models are essential for many applications,
bias is a human-defined concept that can vary across tasks. Given only
input-label pairs, algorithms may lack sufficient information to distinguish
stable (causal) features from unstable (spurious) features. However, related
tasks often share similar biases -- an observation we may leverage to develop
stable classifiers in the transfer setting. In this work, we explicitly inform
the target classifier about unstable features in the source tasks.
Specifically, we derive a representation that encodes the unstable features by
contrasting different data environments in the source task. We achieve
robustness by clustering data of the target task according to this
representation and minimizing the worst-case risk across these clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task for both synthetically generated environments and real-world environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-OCR Document Correction with large Ensembles of Character Sequence-to-Sequence Models. (arXiv:2109.06264v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06264">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel method based on character
sequence-to-sequence models to correct documents already processed with Optical
Character Recognition (OCR) systems. The main contribution of this paper is a
set of strategies to accurately process strings much longer than the ones used
to train the sequence model while being sample- and resource-efficient,
supported by thorough experimentation. The strategy with the best performance
involves splitting the input document in character n-grams and combining their
individual corrections into the final output using a voting scheme that is
equivalent to an ensemble of a large number of sequence models. We further
investigate how to weigh the contributions from each one of the members of this
ensemble. We test our method on nine languages of the ICDAR 2019 competition on
post-OCR text correction and achieve a new state-of-the-art performance in five
of them. Our code for post-OCR correction is shared at
https://github.com/jarobyte91/post_ocr_correction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Primer: Searching for Efficient Transformers for Language Modeling. (arXiv:2109.08668v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08668">
<div class="article-summary-box-inner">
<span><p>Large Transformer models have been central to recent advances in natural
language processing. The training and inference costs of these models, however,
have grown rapidly and become prohibitively expensive. Here we aim to reduce
the costs of Transformers by searching for a more efficient variant. Compared
to previous approaches, our search is performed at a lower level, over the
primitives that define a Transformer TensorFlow program. We identify an
architecture, named Primer, that has a smaller training cost than the original
Transformer and other variants for auto-regressive language modeling. Primer's
improvements can be mostly attributed to two simple modifications: squaring
ReLU activations and adding a depthwise convolution layer after each Q, K, and
V projection in self-attention.
</p>
<p>Experiments show Primer's gains over Transformer increase as compute scale
grows and follow a power law with respect to quality at optimal model sizes. We
also verify empirically that Primer can be dropped into different codebases to
significantly speed up training without additional tuning. For example, at a
500M parameter size, Primer improves the original T5 architecture on C4
auto-regressive language modeling, reducing the training cost by 4X.
Furthermore, the reduced training cost means Primer needs much less compute to
reach a target one-shot performance. For instance, in a 1.9B parameter
configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to
achieve the same one-shot performance as Transformer. We open source our models
and several comparisons in T5 to help with reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopiOCQA: Open-domain Conversational Question Answering with Topic Switching. (arXiv:2110.00768v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00768">
<div class="article-summary-box-inner">
<span><p>In a conversational question answering scenario, a questioner seeks to
extract information about a topic through a series of interdependent questions
and answers. As the conversation progresses, they may switch to related topics,
a phenomenon commonly observed in information-seeking search sessions. However,
current datasets for conversational question answering are limiting in two
ways: 1) they do not contain topic switches; and 2) they assume the reference
text for the conversation is given, i.e., the setting is not open-domain. We
introduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset
with topic switches on Wikipedia. TopiOCQA contains 3,920 conversations with
information-seeking questions and free-form answers. On average, a conversation
in our dataset spans 13 question-answer turns and involves four topics
(documents). TopiOCQA poses a challenging test-bed for models, where efficient
retrieval is required on multiple turns of the same conversation, in
conjunction with constructing valid responses using conversational history. We
evaluate several baselines, by combining state-of-the-art document retrieval
methods with neural reader models. Our best model achieves F1 of 51.9, falling
short of human performance by 18.3 points, indicating the difficulty of our
dataset. Our dataset and code is available at
https://mcgill-nlp.github.io/topiocqa
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Attends the Conversation: Improving Low-Resource Conversational ASR. (arXiv:2110.02267v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02267">
<div class="article-summary-box-inner">
<span><p>We propose new, data-efficient training tasks for BERT models that improve
performance of automatic speech recognition (ASR) systems on conversational
speech. We include past conversational context and fine-tune BERT on transcript
disambiguation without external data to rescore ASR candidates. Our results
show word error rate recoveries up to 37.2%. We test our methods in
low-resource data domains, both in language (Norwegian), tone (spontaneous,
conversational), and topics (parliament proceedings and customer service phone
calls). These techniques are applicable to any ASR system and do not require
any additional data, provided a pre-trained BERT model. We also show how the
performance of our context-augmented rescoring methods strongly depends on the
degree of spontaneity and nature of the conversation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Summarization using Restricted Self-Attention. (arXiv:2110.06263v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06263">
<div class="article-summary-box-inner">
<span><p>Speech summarization is typically performed by using a cascade of speech
recognition and text summarization models. End-to-end modeling of speech
summarization models is challenging due to memory and compute constraints
arising from long input audio sequences. Recent work in document summarization
has inspired methods to reduce the complexity of self-attentions, which enables
transformer models to handle long sequences. In this work, we introduce a
single model optimized end-to-end for speech summarization. We apply the
restricted self-attention technique from text-based models to speech models to
address the memory and compute constraints. We demonstrate that the proposed
model learns to directly summarize speech for the How-2 corpus of instructional
videos. The proposed end-to-end model outperforms the previously proposed
cascaded model by 3 points absolute on ROUGE. Further, we consider the spoken
language understanding task of predicting concepts from speech inputs and show
that the proposed end-to-end model outperforms the cascade model by 4 points
absolute F-1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Masking for Temporal Language Models. (arXiv:2110.06366v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06366">
<div class="article-summary-box-inner">
<span><p>Our world is constantly evolving, and so is the content on the web.
Consequently, our languages, often said to mirror the world, are dynamic in
nature. However, most current contextual language models are static and cannot
adapt to changes over time. In this work, we propose a temporal contextual
language model called TempoBERT, which uses time as an additional context of
texts. Our technique is based on modifying texts with temporal information and
performing time masking - specific masking for the supplementary time
information. We leverage our approach for the tasks of semantic change
detection and sentence time prediction, experimenting on diverse datasets in
terms of time, size, genre, and language. Our extensive evaluation shows that
both tasks benefit from exploiting time masking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SILG: The Multi-environment Symbolic Interactive Language Grounding Benchmark. (arXiv:2110.10661v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10661">
<div class="article-summary-box-inner">
<span><p>Existing work in language grounding typically study single environments. How
do we build unified models that apply across multiple environments? We propose
the multi-environment Symbolic Interactive Language Grounding benchmark (SILG),
which unifies a collection of diverse grounded language learning environments
under a common interface. SILG consists of grid-world environments that require
generalization to new dynamics, entities, and partially observed worlds (RTFM,
Messenger, NetHack), as well as symbolic counterparts of visual worlds that
require interpreting rich natural language with respect to complex scenes
(ALFWorld, Touchdown). Together, these environments provide diverse grounding
challenges in richness of observation space, action space, language
specification, and plan complexity. In addition, we propose the first shared
model architecture for RL on these environments, and evaluate recent advances
such as egocentric local convolution, recurrent state-tracking, entity-centric
attention, and pretrained LM using SILG. Our shared architecture achieves
comparable performance to environment-specific architectures. Moreover, we find
that many recent modelling advances do not result in significant gains on
environments other than the one they were designed for. This highlights the
need for a multi-environment benchmark. Finally, the best models significantly
underperform humans on SILG, which suggests ample room for future work. We hope
SILG enables the community to quickly identify new methodologies for language
grounding that generalize to a diverse set of environments and their associated
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities. (arXiv:2201.06796v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06796">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) offer unprecedented language generation
capabilities and exciting opportunities for interaction design. However, their
highly context-dependent capabilities are difficult to grasp and are often
subjectively interpreted. In this paper, we argue that by curating and
analyzing large interaction datasets, the HCI community can foster more
incisive examinations of LMs' generative capabilities. Exemplifying this
approach, we present CoAuthor, a dataset designed for revealing GPT-3's
capabilities in assisting creative and argumentative writing. CoAuthor captures
rich interactions between 63 writers and four instances of GPT-3 across 1445
writing sessions. We demonstrate that CoAuthor can address questions about
GPT-3's language, ideation, and collaboration capabilities, and reveal its
contribution as a writing "collaborator" under various definitions of good
collaboration. Finally, we discuss how this work may facilitate a more
principled discussion around LMs' promises and pitfalls in relation to
interaction design. The dataset and an interface for replaying the writing
sessions are publicly available at https://coauthor.stanford.edu.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoMC: Automated Model Compression based on Domain Knowledge and Progressive search strategy. (arXiv:2201.09884v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09884">
<div class="article-summary-box-inner">
<span><p>Model compression methods can reduce model complexity on the premise of
maintaining acceptable performance, and thus promote the application of deep
neural networks under resource constrained environments. Despite their great
success, the selection of suitable compression methods and design of details of
the compression scheme are difficult, requiring lots of domain knowledge as
support, which is not friendly to non-expert users. To make more users easily
access to the model compression scheme that best meet their needs, in this
paper, we propose AutoMC, an effective automatic tool for model compression.
AutoMC builds the domain knowledge on model compression to deeply understand
the characteristics and advantages of each compression method under different
settings. In addition, it presents a progressive search strategy to efficiently
explore pareto optimal compression scheme according to the learned prior
knowledge combined with the historical evaluation information. Extensive
experimental results show that AutoMC can provide satisfying compression
schemes within short time, demonstrating the effectiveness of AutoMC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Euclidean and Affine Curve Reconstruction. (arXiv:2201.09929v1 [math.DG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09929">
<div class="article-summary-box-inner">
<span><p>We consider practical aspects of reconstructing planar curves with prescribed
Euclidean or affine curvatures. These curvatures are invariant under the
special Euclidean group and the equi-affine groups, respectively, and play an
important role in computer vision and shape analysis. We discuss and implement
algorithms for such reconstruction, and give estimates on how close
reconstructed curves are relative to the closeness of their curvatures in
appropriate metrics. Several illustrative examples are provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices. (arXiv:2201.09933v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09933">
<div class="article-summary-box-inner">
<span><p>Emotion recognition in smart eyewear devices is highly valuable but
challenging. One key limitation of previous works is that the
expression-related information like facial or eye images is considered as the
only emotional evidence. However, emotional status is not isolated; it is
tightly associated with people's visual perceptions, especially those
sentimental ones. However, little work has examined such associations to better
illustrate the cause of different emotions. In this paper, we study the
emotionship analysis problem in eyewear systems, an ambitious task that
requires not only classifying the user's emotions but also semantically
understanding the potential cause of such emotions. To this end, we devise
EMOShip, a deep-learning-based eyewear system that can automatically detect the
wearer's emotional status and simultaneously analyze its associations with
semantic-level visual perceptions. Experimental studies with 20 participants
demonstrate that, thanks to the emotionship awareness, EMOShip not only
achieves superior emotion recognition accuracy over existing methods (80.2% vs.
69.4%), but also provides a valuable understanding of the cause of emotions.
Pilot studies with 20 participants further motivate the potential use of
EMOShip to empower emotion-aware applications, such as emotionship
self-reflection and emotionship life-logging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is the cost of adding a constraint in linear least squares?. (arXiv:2201.09935v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09935">
<div class="article-summary-box-inner">
<span><p>Although the theory of constrained least squares (CLS) estimation is well
known, it is usually applied with the view that the constraints to be imposed
are unavoidable. However, there are cases in which constraints are optional.
For example, in camera color calibration, one of several possible color
processing systems is obtained if a constraint on the row sums of a desired
color correction matrix is imposed; in this example, it is not clear a priori
whether imposing the constraint leads to better system performance. In this
paper, we derive an exact expression connecting the constraint to the increase
in fitting error obtained from imposing it. As another contribution, we show
how to determine projection matrices that separate the measured data into two
components: the first component drives up the fitting error due to imposing a
constraint, and the second component is unaffected by the constraint. We
demonstrate the use of these results in the color calibration problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Approach for the Detection of COVID-19 from Chest X-Ray Images using Convolutional Neural Networks. (arXiv:2201.09952v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09952">
<div class="article-summary-box-inner">
<span><p>The COVID-19 (coronavirus) is an ongoing pandemic caused by severe acute
respiratory syndrome coronavirus 2 (SARS-CoV-2). The virus was first identified
in mid-December 2019 in the Hubei province of Wuhan, China and by now has
spread throughout the planet with more than 75.5 million confirmed cases and
more than 1.67 million deaths. With limited number of COVID-19 test kits
available in medical facilities, it is important to develop and implement an
automatic detection system as an alternative diagnosis option for COVID-19
detection that can used on a commercial scale. Chest X-ray is the first imaging
technique that plays an important role in the diagnosis of COVID-19 disease.
Computer vision and deep learning techniques can help in determining COVID-19
virus with Chest X-ray Images. Due to the high availability of large-scale
annotated image datasets, great success has been achieved using convolutional
neural network for image analysis and classification. In this research, we have
proposed a deep convolutional neural network trained on five open access
datasets with binary output: Normal and Covid. The performance of the model is
compared with four pre-trained convolutional neural network-based models
(COVID-Net, ResNet18, ResNet and MobileNet-V2) and it has been seen that the
proposed model provides better accuracy on the validation set as compared to
the other four pre-trained models. This research work provides promising
results which can be further improvise and implement on a commercial scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attacks and Defenses for Free-Riders in Multi-Discriminator GAN. (arXiv:2201.09967v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09967">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) are increasingly adopted by the
industry to synthesize realistic images. Due to data not being centrally
available, Multi-Discriminator (MD)-GANs training framework employs multiple
discriminators that have direct access to the real data. Distributedly training
a joint GAN model entails the risk of free-riders, i.e., participants that aim
to benefit from the common model while only pretending to participate in the
training process. In this paper, we conduct the first characterization study of
the impact of free-riders on MD-GAN. Based on two production prototypes of
MD-GAN, we find that free-riders drastically reduce the ability of MD-GANs to
produce images that are indistinguishable from real data, i.e., they increase
the FID score -- the standard measure to assess the quality of generated
images. To mitigate the model degradation, we propose a defense strategy
against free-riders in MD-GAN, termed DFG. DFG distinguishes free-riders and
benign participants through periodic probing and clustering of discriminators'
responses based on a reference response of free-riders, which then allows the
generator to exclude the detected free-riders from the training. Furthermore,
we extend our defense, termed DFG+, to enable discriminators to filter out
free-riders at the variant of MD-GAN that allows peer exchanges of
discriminators networks. Extensive evaluation on various scenarios of
free-riders, MD-GAN architecture, and three datasets show that our defenses
effectively detect free-riders. With 1 to 5 free-riders, DFG and DFG+ averagely
decreases FID by 5.22% to 11.53% for CIFAR10 and 5.79% to 13.22% for CIFAR100
in comparison to an attack without defense. In a shell, the proposed DFG(+) can
effectively defend against free-riders without affecting benign clients at a
negligible computation overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields. (arXiv:2201.09968v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09968">
<div class="article-summary-box-inner">
<span><p>High-resolution optical satellite sensors, in combination with dense stereo
algorithms, have made it possible to reconstruct 3D city models from space.
However, the resulting models are, in practice, rather noisy, and they tend to
miss small geometric features that are clearly visible in the images. We argue
that one reason for the limited DSM quality may be a too early, heuristic
reduction of the triangulated 3D point cloud to an explicit height field or
surface mesh. To make full use of the point cloud and the underlying images, we
introduce ImpliCity, a neural representation of the 3D scene as an implicit,
continuous occupancy field, driven by learned embeddings of the point cloud and
a stereo pair of ortho-photos. We show that this representation enables the
extraction of high-quality DSMs: with image resolution 0.5$\,$m, ImpliCity
reaches a median height error of $\approx\,$0.7$\,$m and outperforms competing
methods, especially w.r.t. building reconstruction, featuring intricate roof
details, smooth surfaces, and straight, regular outlines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Detection Using CT Image Based On YOLOv5 Network. (arXiv:2201.09972v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09972">
<div class="article-summary-box-inner">
<span><p>Computer aided diagnosis (CAD) increases diagnosis efficiency, helping
doctors providing a quick and confident diagnosis, it has played an important
role in the treatment of COVID19. In our task, we solve the problem about
abnormality detection and classification. The dataset provided by Kaggle
platform and we choose YOLOv5 as our model. We introduce some methods on
objective detection in the related work section, the objection detection can be
divided into two streams: onestage and two stage. The representational model
are Faster RCNN and YOLO series. Then we describe the YOLOv5 model in the
detail. Compared Experiments and results are shown in section IV. We choose
mean average precision (mAP) as our experiments' metrics, and the higher (mean)
mAP is, the better result the model will gain. mAP@0.5 of our YOLOv5s is 0.623
which is 0.157 and 0.101 higher than Faster RCNN and EfficientDet respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Vehicle Trajectory Prediction Based on ResNet and EfficientNet Model. (arXiv:2201.09973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09973">
<div class="article-summary-box-inner">
<span><p>At present, a major challenge for the application of automatic driving
technology is the accurate prediction of vehicle trajectory. With the vigorous
development of computer technology and the emergence of convolution depth
neural network, the accuracy of prediction results has been improved. But, the
depth, width of the network and image resolution are still important reasons
that restrict the accuracy of the model and the prediction results. The main
innovation of this paper is the combination of RESNET network and efficient net
network, which not only greatly increases the network depth, but also
comprehensively changes the choice of network width and image resolution, so as
to make the model performance better, but also save computing resources as much
as possible. The experimental results also show that our proposed model obtains
the optimal prediction results. Specifically, the loss value of our method is
separately 4 less and 2.1 less than that of resnet and efficientnet method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Manifold Clustering and Embedding. (arXiv:2201.10000v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10000">
<div class="article-summary-box-inner">
<span><p>Given a union of non-linear manifolds, non-linear subspace clustering or
manifold clustering aims to cluster data points based on manifold structures
and also learn to parameterize each manifold as a linear subspace in a feature
space. Deep neural networks have the potential to achieve this goal under
highly non-linear settings given their large capacity and flexibility. We argue
that achieving manifold clustering with neural networks requires two essential
ingredients: a domain-specific constraint that ensures the identification of
the manifolds, and a learning algorithm for embedding each manifold to a linear
subspace in the feature space. This work shows that many constraints can be
implemented by data augmentation. For subspace feature learning, Maximum Coding
Rate Reduction (MCR$^2$) objective can be used. Putting them together yields
{\em Neural Manifold Clustering and Embedding} (NMCE), a novel method for
general purpose manifold clustering, which significantly outperforms
autoencoder-based deep subspace clustering. Further, on more challenging
natural image datasets, NMCE can also outperform other algorithms specifically
designed for clustering. Qualitatively, we demonstrate that NMCE learns a
meaningful and interpretable feature space. As the formulation of NMCE is
closely related to several important Self-supervised learning (SSL) methods, we
believe this work can help us build a deeper understanding on SSL
representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Recognition and Digital Documentation of Cultural Heritage Hemispherical Domes using Images. (arXiv:2201.10015v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10015">
<div class="article-summary-box-inner">
<span><p>Advancements in optical metrology has enabled documentation of dense 3D point
clouds of cultural heritage sites. For large scale and continuous digital
documentation, processing of dense 3D point clouds becomes computationally
cumbersome, and often requires additional hardware for data management,
increasing the time cost, and complexity of projects. To this end, this
manuscript presents an original approach to generate fast and reliable semantic
digital models of heritage hemispherical domes using only two images. New
closed formulations were derived to establish the relationships between spheres
and their projected ellipses onto images, which fostered the development of a
new automatic framework for as-built generation of spheres. The effectiveness
of the proposed method was evaluated under both laboratory and real-world
datasets. The results revealed that the proposed method achieved as-built
modeling accuracy of around 6mm, while improving the computation time by a
factor of 7, when compared to established point cloud processing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning. (arXiv:2201.10029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10029">
<div class="article-summary-box-inner">
<span><p>State-of-the-art approaches to ObjectGoal navigation rely on reinforcement
learning and typically require significant computational resources and time for
learning. We propose Potential functions for ObjectGoal Navigation with
Interaction-free learning (PONI), a modular approach that disentangles the
skills of `where to look?' for an object and `how to navigate to (x, y)?'. Our
key insight is that `where to look?' can be treated purely as a perception
problem, and learned without environment interactions. To address this, we
propose a network that predicts two complementary potential functions
conditioned on a semantic map and uses them to decide where to look for an
unseen object. We train the potential function network using supervised
learning on a passive dataset of top-down semantic maps, and integrate it into
a modular framework to perform ObjectGoal navigation. Experiments on Gibson and
Matterport3D demonstrate that our method achieves the state-of-the-art for
ObjectGoal navigation while incurring up to 1,600x less computational cost for
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Point Cloud Registration with Deep Versatile Descriptors. (arXiv:2201.10034v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10034">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed an increasing trend toward solving point cloud
registration problems with various deep learning-based algorithms. Compared to
supervised/semi-supervised registration methods, unsupervised methods require
no human annotations. However, unsupervised methods mainly depend on the global
descriptors, which ignore the high-level representations of local geometries.
In this paper, we propose a self-supervised registration scheme with a novel
Deep Versatile Descriptors (DVD), jointly considering global representations
and local representations. The DVD is motivated by a key observation that the
local distinctive geometric structures of the point cloud by two subset points
can be employed to enhance the representation ability of the feature extraction
module. Furthermore, we utilize two additional tasks (reconstruction and normal
estimation) to enhance the transformation awareness of the proposed DVDs.
Lastly, we conduct extensive experiments on synthetic and real-world datasets,
demonstrating that our method achieves state-of-the-art performance against
competing methods over a wide range of experimental settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Commercial Face Detection Models as Biased as Academic Models?. (arXiv:2201.10047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10047">
<div class="article-summary-box-inner">
<span><p>As facial recognition systems are deployed more widely, scholars and
activists have studied their biases and harms. Audits are commonly used to
accomplish this and compare the algorithmic facial recognition systems'
performance against datasets with various metadata labels about the subjects of
the images. Seminal works have found discrepancies in performance by gender
expression, age, perceived race, skin type, etc. These studies and audits often
examine algorithms which fall into two categories: academic models or
commercial models. We present a detailed comparison between academic and
commercial face detection systems, specifically examining robustness to noise.
We find that state-of-the-art academic face detection models exhibit
demographic disparities in their noise robustness, specifically by having
statistically significant decreased performance on older individuals and those
who present their gender in a masculine manner. When we compare the size of
these disparities to that of commercial models, we conclude that commercial
models - in contrast to their relatively larger development budget and
industry-level fairness commitments - are always as biased or more biased than
an academic model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals. (arXiv:2201.10060v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10060">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a surge of significant interest on application of
Deep Learning (DL) models to autonomously perform hand gesture recognition
using surface Electromyogram (sEMG) signals. DL models are, however, mainly
designed to be applied on sparse sEMG signals. Furthermore, due to their
complex structure, typically, we are faced with memory constraints; require
large training times and a large number of training samples, and; there is the
need to resort to data augmentation and/or transfer learning. In this paper,
for the first time (to the best of our knowledge), we investigate and design a
Vision Transformer (ViT) based architecture to perform hand gesture recognition
from High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the
recent breakthrough role of the transformer architecture in tackling different
complex problems together with its potential for employing more input
parallelization via its attention mechanism. The proposed Vision
Transformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the
aforementioned training time problems and can accurately classify a large
number of hand gestures from scratch without any need for data augmentation
and/or transfer learning. The efficiency of the proposed ViT-HGR framework is
evaluated using a recently-released HD-sEMG dataset consisting of 65 isometric
hand gestures. Our experiments with 64-sample (31.25 ms) window size yield
average test accuracy of 84.62 +/- 3.07%, where only 78, 210 number of
parameters is utilized. The compact structure of the proposed ViT-based ViT-HGR
framework (i.e., having significantly reduced number of trainable parameters)
shows great potentials for its practical application for prosthetic control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Splatting-based Synthesis for Video Frame Interpolation. (arXiv:2201.10075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10075">
<div class="article-summary-box-inner">
<span><p>Frame interpolation is an essential video processing technique that adjusts
the temporal resolution of an image sequence. An effective approach to perform
frame interpolation is based on splatting, also known as forward warping.
Specifically, splatting can be used to warp the input images to an arbitrary
temporal location based on an optical flow estimate. A synthesis network, also
sometimes referred to as refinement network, can then be used to generate the
output frame from the warped images. In doing so, it is common to not only warp
the images but also various feature representations which provide rich
contextual cues to the synthesis network. However, while this approach has been
shown to work well and enables arbitrary-time interpolation due to using
splatting, the involved synthesis network is prohibitively slow. In contrast,
we propose to solely rely on splatting to synthesize the output without any
subsequent refinement. This splatting-based synthesis is much faster than
similar approaches, especially for multi-frame interpolation, while enabling
new state-of-the-art results at high resolutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time automatic polyp detection in colonoscopy using feature enhancement module and spatiotemporal similarity correlation unit. (arXiv:2201.10079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10079">
<div class="article-summary-box-inner">
<span><p>Automatic detection of polyps is challenging because different polyps vary
greatly, while the changes between polyps and their analogues are small. The
state-of-the-art methods are based on convolutional neural networks (CNNs).
However, they may fail due to lack of training data, resulting in high rates of
missed detection and false positives (FPs). In order to solve these problems,
our method combines the two-dimensional (2-D) CNN-based real-time object
detector network with spatiotemporal information. Firstly, we use a 2-D
detector network to detect static images and frames, and based on the detector
network, we propose two feature enhancement modules-the FP Relearning Module
(FPRM) to make the detector network learning more about the features of FPs for
higher precision, and the Image Style Transfer Module (ISTM) to enhance the
features of polyps for sensitivity improvement. In video detection, we
integrate spatiotemporal information, which uses Structural Similarity (SSIM)
to measure the similarity between video frames. Finally, we propose the
Inter-frame Similarity Correlation Unit (ISCU) to combine the results obtained
by the detector network and frame similarity to make the final decision. We
verify our method on both private databases and publicly available databases.
Experimental results show that these modules and units provide a performance
improvement compared with the baseline method. Comparison with the
state-of-the-art methods shows that the proposed method outperforms the
existing ones which can meet real-time constraints. It's demonstrated that our
method provides a performance improvement in sensitivity, precision and
specificity, and has great potential to be applied in clinical colonoscopy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting L1 Loss in Super-Resolution: A Probabilistic View and Beyond. (arXiv:2201.10084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10084">
<div class="article-summary-box-inner">
<span><p>Super-resolution as an ill-posed problem has many high-resolution candidates
for a low-resolution input. However, the popular $\ell_1$ loss used to best fit
the given HR image fails to consider this fundamental property of
non-uniqueness in image restoration. In this work, we fix the missing piece in
$\ell_1$ loss by formulating super-resolution with neural networks as a
probabilistic model. It shows that $\ell_1$ loss is equivalent to a degraded
likelihood function that removes the randomness from the learning process. By
introducing a data-adaptive random variable, we present a new objective
function that aims at minimizing the expectation of the reconstruction error
over all plausible solutions. The experimental results show consistent
improvements on mainstream architectures, with no extra parameter or computing
cost at inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Classical Approach to Handcrafted Feature Extraction Techniques for Bangla Handwritten Digit Recognition. (arXiv:2201.10102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10102">
<div class="article-summary-box-inner">
<span><p>Bangla Handwritten Digit recognition is a significant step forward in the
development of Bangla OCR. However, intricate shape, structural likeness and
distinctive composition style of Bangla digits makes it relatively challenging
to distinguish. Thus, in this paper, we benchmarked four rigorous classifiers
to recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector
Machine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)
based on three handcrafted feature extraction techniques: Histogram of Oriented
Gradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly
available Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.
Here, handcrafted feature extraction methods are used to extract features from
the dataset image, which are then utilized to train machine learning
classifiers to identify Bangla handwritten digits. We further fine-tuned the
hyperparameters of the classification algorithms in order to acquire the finest
Bangla handwritten digits recognition performance from these algorithms, and
among all the models we employed, the HOG features combined with SVM model
(HOG+SVM) attained the best performance metrics across all datasets. The
recognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and
BDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well
as we compared the model performance with recent state-of-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARPD: Anchor-free Rotation-aware People Detection using Topview Fisheye Camera. (arXiv:2201.10107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10107">
<div class="article-summary-box-inner">
<span><p>People detection in top-view, fish-eye images is challenging as people in
fish-eye images often appear in arbitrary directions and are distorted
differently. Due to this unique radial geometry, axis-aligned people detectors
often work poorly on fish-eye frames. Recent works account for this variability
by modifying existing anchor-based detectors or relying on complex
pre/post-processing. Anchor-based methods spread a set of pre-defined bounding
boxes on the input image, most of which are invalid. In addition to being
inefficient, this approach could lead to a significant imbalance between the
positive and negative anchor boxes. In this work, we propose ARPD, a
single-stage anchor-free fully convolutional network to detect arbitrarily
rotated people in fish-eye images. Our network uses keypoint estimation to find
the center point of each object and regress the object's other properties
directly. To capture the various orientation of people in fish-eye cameras, in
addition to the center and size, ARPD also predicts the angle of each bounding
box. We also propose a periodic loss function that accounts for angle
periodicity and relieves the difficulty of learning small-angle oscillations.
Experimental results show that our method competes favorably with
state-of-the-art algorithms while running significantly faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Quantum-Classical Algorithm for Robust Fitting. (arXiv:2201.10110v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10110">
<div class="article-summary-box-inner">
<span><p>Fitting geometric models onto outlier contaminated data is provably
intractable. Many computer vision systems rely on random sampling heuristics to
solve robust fitting, which do not provide optimality guarantees and error
bounds. It is therefore critical to develop novel approaches that can bridge
the gap between exact solutions that are costly, and fast heuristics that offer
no quality assurances. In this paper, we propose a hybrid quantum-classical
algorithm for robust fitting. Our core contribution is a novel robust fitting
formulation that solves a sequence of integer programs and terminates with a
global solution or an error bound. The combinatorial subproblems are amenable
to a quantum annealer, which helps to tighten the bound efficiently. While our
usage of quantum computing does not surmount the fundamental intractability of
robust fitting, by providing error bounds our algorithm is a practical
improvement over randomised heuristics. Moreover, our work represents a
concrete application of quantum computing in computer vision. We present
results obtained using an actual quantum computer (D-Wave Advantage) and via
simulation. Source code: https://github.com/dadung/HQC-robust-fitting
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SURDS: Self-Supervised Attention-guided Reconstruction and Dual Triplet Loss for Writer Independent Offline Signature Verification. (arXiv:2201.10138v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10138">
<div class="article-summary-box-inner">
<span><p>Offline Signature Verification (OSV) is a fundamental biometric task across
various forensic, commercial and legal applications. The underlying task at
hand is to carefully model fine-grained features of the signatures to
distinguish between genuine and forged ones, which differ only in minute
deformities. This makes OSV more challenging compared to other verification
problems. In this work, we propose a two-stage deep learning framework that
leverages self-supervised representation learning as well as metric learning
for writer-independent OSV. First, we train an image reconstruction network
using an encoder-decoder architecture that is augmented by a 2D spatial
attention mechanism using signature image patches. Next, the trained encoder
backbone is fine-tuned with a projector head using a supervised metric learning
framework, whose objective is to optimize a novel dual triplet loss by sampling
negative samples from both within the same writer class as well as from other
writers. The intuition behind this is to ensure that a signature sample lies
closer to its positive counterpart compared to negative samples from both
intra-writer and cross-writer sets. This results in robust discriminative
learning of the embedding space. To the best of our knowledge, this is the
first work of using self-supervised learning frameworks for OSV. The proposed
two-stage framework has been evaluated on two publicly available offline
signature datasets and compared with various state-of-the-art methods. It is
noted that the proposed method provided promising results outperforming several
existing pieces of work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSNet: A Deep Multi-scale Submanifold Network for Visual Classification. (arXiv:2201.10145v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10145">
<div class="article-summary-box-inner">
<span><p>The Symmetric Positive Definite (SPD) matrix has received wide attention as a
tool for visual data representation in computer vision. Although there are many
different attempts to develop effective deep architectures for data processing
on the Riemannian manifold of SPD matrices, a very few solutions explicitly
mine the local geometrical information in deep SPD feature representations.
While CNNs have demonstrated the potential of hierarchical local pattern
extraction even for SPD represented data, we argue that it is of utmost
importance to ensure the preservation of local geometric information in the SPD
networks. Accordingly, in this work we propose an SPD network designed with
this objective in mind. In particular, we propose an architecture, referred to
as MSNet, which fuses geometrical multi-scale information. We first analyse the
convolution operator commonly used for mapping the local information in
Euclidean deep networks from the perspective of a higher level of abstraction
afforded by the Category Theory. Based on this analysis, we postulate a
submanifold selection principle to guide the design of our MSNet. In
particular, we use it to design a submanifold fusion block to take advantage of
the rich local geometry encoded in the network layers. The experiments
involving multiple visual tasks show that our algorithm outperforms most
Riemannian SOTA competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TGFuse: An Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network. (arXiv:2201.10147v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10147">
<div class="article-summary-box-inner">
<span><p>The end-to-end image fusion framework has achieved promising performance,
with dedicated convolutional networks aggregating the multi-modal local
appearance. However, long-range dependencies are directly neglected in existing
CNN fusion approaches, impeding balancing the entire image-level perception for
complex scenario fusion. In this paper, therefore, we propose an infrared and
visible image fusion algorithm based on a lightweight transformer module and
adversarial learning. Inspired by the global interaction power, we use the
transformer technique to learn the effective global fusion relations. In
particular, shallow features extracted by CNN are interacted in the proposed
transformer fusion module to refine the fusion relationship within the spatial
scope and across channels simultaneously. Besides, adversarial learning is
designed in the training process to improve the output discrimination via
imposing competitive consistency from the inputs, reflecting the specific
characteristics in infrared and visible images. The experimental performance
demonstrates the effectiveness of the proposed modules, with superior
improvement against the state-of-the-art, generalising a novel paradigm via
transformer and adversarial learning in the fusion task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Image Fusion Method based on Feature Mutual Mapping. (arXiv:2201.10152v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10152">
<div class="article-summary-box-inner">
<span><p>Deep learning-based image fusion approaches have obtained wide attention in
recent years, achieving promising performance in terms of visual perception.
However, the fusion module in the current deep learning-based methods suffers
from two limitations, \textit{i.e.}, manually designed fusion function, and
input-independent network learning. In this paper, we propose an unsupervised
adaptive image fusion method to address the above issues. We propose a feature
mutual mapping fusion module and dual-branch multi-scale autoencoder. More
specifically, we construct a global map to measure the connections of pixels
between the input source images. % The found mapping relationship guides the
image fusion. Besides, we design a dual-branch multi-scale network through
sampling transformation to extract discriminative image features. We further
enrich feature representations of different scales through feature aggregation
in the decoding process. Finally, we propose a modified loss function to train
the network with efficient convergence property. Through sufficient training on
infrared and visible image data sets, our method also shows excellent
generalized performance in multi-focus and medical image fusion. Our method
achieves superior performance in both visual perception and objective
evaluation. Experiments prove that the performance of our proposed method on a
variety of image fusion tasks surpasses other state-of-the-art methods, proving
the effectiveness and versatility of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Video Coding: Instill Static-Dynamic Clues into Structured Bitstream for AI Tasks. (arXiv:2201.10162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10162">
<div class="article-summary-box-inner">
<span><p>Traditional media coding schemes typically encode image/video into a
semantic-unknown binary stream, which fails to directly support downstream
intelligent tasks at the bitstream level. Semantically Structured Image Coding
(SSIC) framework makes the first attempt to enable decoding-free or
partial-decoding image intelligent task analysis via a Semantically Structured
Bitstream (SSB). However, the SSIC only considers image coding and its
generated SSB only contains the static object information. In this paper, we
extend the idea of semantically structured coding from video coding perspective
and propose an advanced Semantically Structured Video Coding (SSVC) framework
to support heterogeneous intelligent applications. Video signals contain more
rich dynamic motion information and exist more redundancy due to the similarity
between adjacent frames. Thus, we present a reformulation of semantically
structured bitstream (SSB) in SSVC which contains both static object
characteristics and dynamic motion clues. Specifically, we introduce optical
flow to encode continuous motion information and reduce cross-frame redundancy
via a predictive coding architecture, then the optical flow and residual
information are reorganized into SSB, which enables the proposed SSVC could
better adaptively support video-based downstream intelligent applications.
Extensive experiments demonstrate that the proposed SSVC framework could
directly support multiple intelligent tasks just depending on a partially
decoded bitstream. This avoids the full bitstream decompression and thus
significantly saves bitrate/bandwidth consumption for intelligent analytics. We
verify this point on the tasks of image object detection, pose estimation,
video action recognition, video object segmentation, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Pixel-Labeling for Reverse-Transfer and Diagnostic Learning on Lung Ultrasound for COVID-19 and Pneumonia Detection. (arXiv:2201.10166v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10166">
<div class="article-summary-box-inner">
<span><p>We propose using a pre-trained segmentation model to perform diagnostic
classification in order to achieve better generalization and interpretability,
terming the technique reverse-transfer learning. We present an architecture to
convert segmentation models to classification models. We compare and contrast
dense vs sparse segmentation labeling and study its impact on diagnostic
classification. We compare the performance of U-Net trained with dense and
sparse labels to segment A-lines, B-lines, and Pleural lines on a custom
dataset of lung ultrasound scans from 4 patients. Our experiments show that
dense labels help reduce false positive detection. We study the classification
capability of the dense and sparse trained U-Net and contrast it with a
non-pretrained U-Net, to detect and differentiate COVID-19 and Pneumonia on a
large ultrasound dataset of about 40k curvilinear and linear probe images. Our
segmentation-based models perform better classification when using pretrained
segmentation weights, with the dense-label pretrained U-Net performing the
best.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore and Match: End-to-End Video Grounding with Transformer. (arXiv:2201.10168v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10168">
<div class="article-summary-box-inner">
<span><p>We present a new paradigm named explore-and-match for video grounding, which
aims to seamlessly unify two streams of video grounding methods: proposal-based
and proposal-free. To achieve this goal, we formulate video grounding as a set
prediction problem and design an end-to-end trainable Video Grounding
Transformer (VidGTR) that can utilize the architectural strengths of rich
contextualization and parallel decoding for set prediction. The overall
training is balanced by two key losses that play different roles, namely span
localization loss and set guidance loss. These two losses force each proposal
to regress the target timespan and identify the target query. Throughout the
training, VidGTR first explores the search space to diversify the initial
proposals and then matches the proposals to the corresponding targets to fit
them in a fine-grained manner. The explore-and-match scheme successfully
combines the strengths of two complementary methods, without encoding prior
knowledge into the pipeline. As a result, VidGTR sets new state-of-the-art
results on two video grounding benchmarks with double the inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RFMask: A Simple Baseline for Human Silhouette Segmentation with Radio Signals. (arXiv:2201.10175v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10175">
<div class="article-summary-box-inner">
<span><p>Human silhouette segmentation, which is originally defined in computer
vision, has achieved promising results for understanding human activities.
However, the physical limitation makes existing systems based on optical
cameras suffer from severe performance degradation under low illumination,
smoke, and/or opaque obstruction conditions. To overcome such limitations, in
this paper, we propose to utilize the radio signals, which can traverse
obstacles and are unaffected by the lighting conditions to achieve silhouette
segmentation. The proposed RFMask framework is composed of three modules. It
first transforms RF signals captured by millimeter wave radar on two planes
into spatial domain and suppress interference with the signal processing
module. Then, it locates human reflections on RF frames and extract features
from surrounding signals with human detection module. Finally, the extracted
features from RF frames are aggregated with an attention based mask generation
module. To verify our proposed framework, we collect a dataset containing
804,760 radio frames and 402,380 camera frames with human activities under
various scenes. Experimental results show that the proposed framework can
achieve impressive human silhouette segmentation even under the challenging
scenarios(such as low light and occlusion scenarios) where traditional
optical-camera-based methods fail. To the best of our knowledge, this is the
first investigation towards segmenting human silhouette based on millimeter
wave signals. We hope that our work can serve as a baseline and inspire further
research that perform vision tasks with radio signals. The dataset and codes
will be made in public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Trained Language Transformers are Universal Image Classifiers. (arXiv:2201.10182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10182">
<div class="article-summary-box-inner">
<span><p>Facial images disclose many hidden personal traits such as age, gender, race,
health, emotion, and psychology. Understanding these traits will help to
classify the people in different attributes. In this paper, we have presented a
novel method for classifying images using a pretrained transformer model. We
apply the pretrained transformer for the binary classification of facial images
in criminal and non-criminal classes. The pretrained transformer of GPT-2 is
trained to generate text and then fine-tuned to classify facial images. During
the finetuning process with images, most of the layers of GT-2 are frozen
during backpropagation and the model is frozen pretrained transformer (FPT).
The FPT acts as a universal image classifier, and this paper shows the
application of FPT on facial images. We also use our FPT on encrypted images
for classification. Our FPT shows high accuracy on both raw facial images and
encrypted images. We hypothesize the meta-learning capacity FPT gained because
of its large size and trained on a large size with theory and experiments. The
GPT-2 trained to generate a single word token at a time, through the
autoregressive process, forced to heavy-tail distribution. Then the FPT uses
the heavy-tail property as its meta-learning capacity for classifying images.
Our work shows one way to avoid bias during the machine classification of
images.The FPT encodes worldly knowledge because of the pretraining of one
text, which it uses during the classification. The statistical error of
classification is reduced because of the added context gained from the text.Our
paper shows the ethical dimension of using encrypted data for
classification.Criminal images are sensitive to share across the boundary but
encrypted largely evades ethical concern.FPT showing good classification
accuracy on encrypted images shows promise for further research on
privacy-preserving machine learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating the Direction and Radius of Pipe from GPR Image by Ellipse Inversion Model. (arXiv:2201.10184v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10184">
<div class="article-summary-box-inner">
<span><p>Ground Penetrating Radar (GPR) is widely used as a non-destructive approach
to estimate buried utilities. When the GPR's detecting direction is
perpendicular to a pipeline, a hyperbolic characteristic would be formed on the
GPR B-scan image. However, in real-world applications, the direction of
pipelines on the existing pipeline map could be inaccurate, and it is hard to
ensure the moving direction of GPR to be actually perpendicular to underground
pipelines. In this paper, a novel model is proposed to estimate the direction
and radius of pipeline and revise the existing pipeline map from GPR B-scan
images. The model consists of two parts: GPR B-scan image processing and
Ellipse Iterative Inversion Algorithm (EIIA). Firstly, the GPR B-scan image is
processed with downward-opening point set extracted. The obtained point set is
then iteratively inverted to the elliptical cross section of the buried
pipeline, which is caused by the angle between the GPR's detecting direction
and the pipeline's direction. By minimizing the sum of the algebraic distances
from the extracted point set to the inverted ellipse, the most likely
pipeline's direction and radius are determined. Experiments on real-world
datasets are conducted, and the results demonstrate the effectiveness of the
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Sketch Based Image Retrieval using Graph Transformer. (arXiv:2201.10185v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10185">
<div class="article-summary-box-inner">
<span><p>The performance of a zero-shot sketch-based image retrieval (ZS-SBIR) task is
primarily affected by two challenges. The substantial domain gap between image
and sketch features needs to be bridged, while at the same time the side
information has to be chosen tactfully. Existing literature has shown that
varying the semantic side information greatly affects the performance of
ZS-SBIR. To this end, we propose a novel graph transformer based zero-shot
sketch-based image retrieval (GTZSR) framework for solving ZS-SBIR tasks which
uses a novel graph transformer to preserve the topology of the classes in the
semantic space and propagates the context-graph of the classes within the
embedding features of the visual space. To bridge the domain gap between the
visual features, we propose minimizing the Wasserstein distance between images
and sketches in a learned domain-shared space. We also propose a novel
compatibility loss that further aligns the two visual domains by bridging the
domain gap of one class with respect to the domain gap of all other classes in
the training set. Experimental results obtained on the extended Sketchy,
TU-Berlin, and QuickDraw datasets exhibit sharp improvements over the existing
state-of-the-art methods in both ZS-SBIR and generalized ZS-SBIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Generative Modeling for Calibration-free Parallel Mr Imaging. (arXiv:2201.10210v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10210">
<div class="article-summary-box-inner">
<span><p>The integration of compressed sensing and parallel imaging (CS-PI) provides a
robust mechanism for accelerating MRI acquisitions. However, most such
strategies require the explicit formation of either coil sensitivity profiles
or a cross-coil correlation operator, and as a result reconstruction
corresponds to solving a challenging bilinear optimization problem. In this
work, we present an unsupervised deep learning framework for calibration-free
parallel MRI, coined universal generative modeling for parallel imaging
(UGM-PI). More precisely, we make use of the merits of both wavelet transform
and the adaptive iteration strategy in a unified framework. We train a powerful
noise conditional score network by forming wavelet tensor as the network input
at the training phase. Experimental results on both physical phantom and in
vivo datasets implied that the proposed method is comparable and even superior
to state-of-the-art CS-PI reconstruction approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Diversity Learning with Sample Dropout for Unsupervised Domain Adaptive Person Re-identification. (arXiv:2201.10212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10212">
<div class="article-summary-box-inner">
<span><p>Clustering-based approach has proved effective in dealing with unsupervised
domain adaptive person re-identification (ReID) tasks. However, existing works
along this approach still suffer from noisy pseudo labels and the unreliable
generalization ability during the whole training process. To solve these
problems, this paper proposes a new approach to learn the feature
representation with better generalization ability through limiting noisy pseudo
labels. At first, we propose a Sample Dropout (SD) method to prevent the
training of the model from falling into the vicious circle caused by samples
that are frequently assigned with noisy pseudo labels. In addition, we put
forward a brand-new method referred as to Feature Diversity Learning (FDL)
under the classic mutual-teaching architecture, which can significantly improve
the generalization ability of the feature representation on the target domain.
Experimental results show that our proposed FDL-SD achieves the
state-of-the-art performance on multiple benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTHA: Video Captioning Evaluation Via Transfer-Learned Human Assessment. (arXiv:2201.10243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10243">
<div class="article-summary-box-inner">
<span><p>Evaluating video captioning systems is a challenging task as there are
multiple factors to consider; for instance: the fluency of the caption,
multiple actions happening in a single scene, and the human bias of what is
considered important. Most metrics try to measure how similar the system
generated captions are to a single or a set of human-annotated captions. This
paper presents a new method based on a deep learning model to evaluate these
systems. The model is based on BERT, which is a language model that has been
shown to work well in multiple NLP tasks. The aim is for the model to learn to
perform an evaluation similar to that of a human. To do so, we use a dataset
that contains human evaluations of system generated captions. The dataset
consists of the human judgments of the captions produce by the system
participating in various years of the TRECVid video to text task. These
annotations will be made publicly available. BERTHA obtain favourable results,
outperforming the commonly used metrics in some setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocEnTr: An End-to-End Document Image Enhancement Transformer. (arXiv:2201.10252v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10252">
<div class="article-summary-box-inner">
<span><p>Document images can be affected by many degradation scenarios, which cause
recognition and processing difficulties. In this age of digitization, it is
important to denoise them for proper usage. To address this challenge, we
present a new encoder-decoder architecture based on vision transformers to
enhance both machine-printed and handwritten document images, in an end-to-end
fashion. The encoder operates directly on the pixel patches with their
positional information without the use of any convolutional layers, while the
decoder reconstructs a clean image from the encoded patches. Conducted
experiments show a superiority of the proposed model compared to the state-of
the-art methods on several DIBCO benchmarks. Code and models will be publicly
available at: \url{https://github.com/dali92002/DocEnTR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Commonsense Reasoning and Knowledge Acquisition to Guide Deep Learning in Robotics. (arXiv:2201.10266v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10266">
<div class="article-summary-box-inner">
<span><p>Algorithms based on deep network models are being used for many pattern
recognition and decision-making tasks in robotics and AI. Training these models
requires a large labeled dataset and considerable computational resources,
which are not readily available in many domains. Also, it is difficult to
explore the internal representations and reasoning mechanisms of these models.
As a step towards addressing the underlying knowledge representation,
reasoning, and learning challenges, the architecture described in this paper
draws inspiration from research in cognitive systems. As a motivating example,
we consider an assistive robot trying to reduce clutter in any given scene by
reasoning about the occlusion of objects and stability of object configurations
in an image of the scene. In this context, our architecture incrementally
learns and revises a grounding of the spatial relations between objects and
uses this grounding to extract spatial information from input images.
Non-monotonic logical reasoning with this information and incomplete
commonsense domain knowledge is used to make decisions about stability and
occlusion. For images that cannot be processed by such reasoning, regions
relevant to the tasks at hand are automatically identified and used to train
deep network models to make the desired decisions. Image regions used to train
the deep networks are also used to incrementally acquire previously unknown
state constraints that are merged with the existing knowledge for subsequent
reasoning. Experimental evaluation performed using simulated and real-world
images indicates that in comparison with baselines based just on deep networks,
our architecture improves reliability of decision making and reduces the effort
involved in training data-driven deep network models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Xformers for Vision. (arXiv:2201.10271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10271">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have found only limited practical use in
processing images, in spite of their state-of-the-art accuracy on certain
benchmarks. The reason for their limited use include their need for larger
training datasets and more computational resources compared to convolutional
neural networks (CNNs), owing to the quadratic complexity of their
self-attention mechanism. We propose a linear attention-convolution hybrid
architecture -- Convolutional X-formers for Vision (CXV) -- to overcome these
limitations. We replace the quadratic attention with linear attention
mechanisms, such as Performer, Nystr\"omformer, and Linear Transformer, to
reduce its GPU usage. Inductive prior for image data is provided by
convolutional sub-layers, thereby eliminating the need for class token and
positional embeddings used by the ViTs. We also propose a new training method
where we use two different optimizers during different phases of training and
show that it improves the top-1 image classification accuracy across different
architectures. CXV outperforms other architectures, token mixers (e.g.
ConvMixer, FNet and MLP Mixer), transformer models (e.g. ViT, CCT, CvT and
hybrid Xformers), and ResNets for image classification in scenarios with
limited data and GPU resources (cores, RAM, power).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">City3D: Large-scale Urban Reconstruction from Airborne Point Clouds. (arXiv:2201.10276v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10276">
<div class="article-summary-box-inner">
<span><p>We present a fully automatic approach for reconstructing compact 3D building
models from large-scale airborne point clouds. A major challenge of urban
reconstruction from airborne point clouds lies in that the vertical walls are
typically missing. Based on the observation that urban buildings typically
consist of planar roofs connected with vertical walls to the ground, we propose
an approach to infer the vertical walls directly from the data. With the planar
segments of both roofs and walls, we hypothesize the faces of the building
surface, and the final model is obtained by using an extended
hypothesis-and-selection-based polygonal surface reconstruction framework.
Specifically, we introduce a new energy term to encourage roof preferences and
two additional hard constraints into the optimization step to ensure correct
topology and enhance detail recovery. Experiments on various large-scale
airborne point clouds have demonstrated that the method is superior to the
state-of-the-art methods in terms of reconstruction accuracy and robustness. In
addition, we have generated a new dataset with our method consisting of the
point clouds and 3D models of 20k real-world buildings. We believe this dataset
can stimulate research in urban reconstruction from airborne point clouds and
the use of 3D city models in urban applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image Enhancement. (arXiv:2201.10294v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10294">
<div class="article-summary-box-inner">
<span><p>Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps
in different energy channels, reflecting energy properties of the scanned
object. Due to the limited photon numbers and the non-ideal detector response
of each energy channel, the reconstructed images usually contain much noise.
With the development of Deep Learning (DL) technique, different kinds of
DL-based models have been proposed for noise reduction. However, most of the
models require clean data set as the training labels, which are not always
available in medical imaging field. Inspiring by the similarities of each
channel's reconstructed image, we proposed a self-supervised learning based
PCCT image enhancement framework via multi-spectral channels (S2MS). In S2MS
framework, both the input and output labels are noisy images. Specifically, one
single channel image was used as output while images of other single channels
and channel-sum image were used as input to train the network, which can fully
use the spectral data information without extra cost. The simulation results
based on the AAPM Low-dose CT Challenge database showed that the proposed S2MS
model can suppress the noise and preserve details more effectively in
comparison with the traditional DL models, which has potential to improve the
image quality of PCCT in clinical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual information neural estimation for unsupervised multi-modal registration of brain images. (arXiv:2201.10305v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10305">
<div class="article-summary-box-inner">
<span><p>Many applications in image-guided surgery and therapy require fast and
reliable non-linear, multi-modal image registration. Recently proposed
unsupervised deep learning-based registration methods have demonstrated
superior performance compared to iterative methods in just a fraction of the
time. Most of the learning-based methods have focused on mono-modal image
registration. The extension to multi-modal registration depends on the use of
an appropriate similarity function, such as the mutual information (MI). We
propose guiding the training of a deep learning-based registration method with
MI estimation between an image-pair in an end-to-end trainable network. Our
results show that a small, 2-layer network produces competitive results in both
mono- and multimodal registration, with sub-second run-times. Comparisons to
both iterative and deep learning-based methods show that our MI-based method
produces topologically and qualitatively superior results with an extremely low
rate of non-diffeomorphic transformations. Real-time clinical application will
benefit from a better visual matching of anatomical structures and less
registration failures/outliers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Addressing the Intra-class Mode Collapse Problem using Adaptive Input Image Normalization in GAN-based X-ray Images. (arXiv:2201.10324v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10324">
<div class="article-summary-box-inner">
<span><p>Biomedical image datasets can be imbalanced due to the rarity of targeted
diseases. Generative Adversarial Networks play a key role in addressing this
imbalance by enabling the generation of synthetic images to augment and balance
datasets. It is important to generate synthetic images that incorporate a
diverse range of features such that they accurately represent the distribution
of features present in the training imagery. Furthermore, the absence of
diverse features in synthetic images can degrade the performance of machine
learning classifiers. The mode collapse problem can impact a Generative
Adversarial Network's capacity to generate diversified images. The mode
collapse comes in two varieties; intra-class and inter-class. In this paper,
the intra-class mode collapse problem is investigated, and its subsequent
impact on the diversity of synthetic X-ray images is evaluated. This work
contributes an empirical demonstration of the benefits of integrating the
adaptive input-image normalization for the Deep Convolutional GAN to alleviate
the intra-class mode collapse problem. Results demonstrate that the DCGAN with
adaptive input-image normalization outperforms DCGAN with un-normalized X-ray
images as evident by the superior diversity scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ShapeFormer: Transformer-based Shape Completion via Sparse Representation. (arXiv:2201.10326v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10326">
<div class="article-summary-box-inner">
<span><p>We present ShapeFormer, a transformer-based network that produces a
distribution of object completions, conditioned on incomplete, and possibly
noisy, point clouds. The resultant distribution can then be sampled to generate
likely completions, each exhibiting plausible shape details while being
faithful to the input. To facilitate the use of transformers for 3D, we
introduce a compact 3D representation, vector quantized deep implicit function,
that utilizes spatial sparsity to represent a close approximation of a 3D shape
by a short sequence of discrete variables. Experiments demonstrate that
ShapeFormer outperforms prior art for shape completion from ambiguous partial
inputs in terms of both completion quality and diversity. We also show that our
approach effectively handles a variety of shape types, incomplete patterns, and
real-world scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ultra Low-Parameter Denoising: Trainable Bilateral Filter Layers in Computed Tomography. (arXiv:2201.10345v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10345">
<div class="article-summary-box-inner">
<span><p>Computed tomography is widely used as an imaging tool to visualize
three-dimensional structures with expressive bone-soft tissue contrast.
However, CT resolution and radiation dose are tightly entangled, highlighting
the importance of low-dose CT combined with sophisticated denoising algorithms.
Most data-driven denoising techniques are based on deep neural networks and,
therefore, contain hundreds of thousands of trainable parameters, making them
incomprehensible and prone to prediction failures. Developing understandable
and robust denoising algorithms achieving state-of-the-art performance helps to
minimize radiation dose while maintaining data integrity. This work presents an
open-source CT denoising framework based on the idea of bilateral filtering. We
propose a bilateral filter that can be incorporated into a deep learning
pipeline and optimized in a purely data-driven way by calculating the gradient
flow toward its hyperparameters and its input. Denoising in pure image-to-image
pipelines and across different domains such as raw detector data and
reconstructed volume, using a differentiable backprojection layer, is
demonstrated. Although only using three spatial parameters and one range
parameter per filter layer, the proposed denoising pipelines can compete with
deep state-of-the-art denoising architectures with several hundred thousand
parameters. Competitive denoising performance is achieved on x-ray microscope
bone data (0.7053 and 33.10) and the 2016 Low Dose CT Grand Challenge dataset
(0.9674 and 43.07) in terms of SSIM and PSNR. Due to the extremely low number
of trainable parameters with well-defined effect, prediction reliance and data
integrity is guaranteed at any time in the proposed pipelines, in contrast to
most other deep learning-based denoising architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-modal Fusion Framework Based on Multi-task Correlation Learning for Cancer Prognosis Prediction. (arXiv:2201.10353v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10353">
<div class="article-summary-box-inner">
<span><p>Morphological attributes from histopathological images and molecular profiles
from genomic data are important information to drive diagnosis, prognosis, and
therapy of cancers. By integrating these heterogeneous but complementary data,
many multi-modal methods are proposed to study the complex mechanisms of
cancers, and most of them achieve comparable or better results from previous
single-modal methods. However, these multi-modal methods are restricted to a
single task (e.g., survival analysis or grade classification), and thus neglect
the correlation between different tasks. In this study, we present a
multi-modal fusion framework based on multi-task correlation learning
(MultiCoFusion) for survival analysis and cancer grade classification, which
combines the power of multiple modalities and multiple tasks. Specifically, a
pre-trained ResNet-152 and a sparse graph convolutional network (SGCN) are used
to learn the representations of histopathological images and mRNA expression
data respectively. Then these representations are fused by a fully connected
neural network (FCNN), which is also a multi-task shared network. Finally, the
results of survival analysis and cancer grade classification output
simultaneously. The framework is trained by an alternate scheme. We
systematically evaluate our framework using glioma datasets from The Cancer
Genome Atlas (TCGA). Results demonstrate that MultiCoFusion learns better
representations than traditional feature extraction methods. With the help of
multi-task alternating learning, even simple multi-modal concatenation can
achieve better performance than other deep learning and traditional methods.
Multi-task learning can improve the performance of multiple tasks not just one
of them, and it is effective in both single-modal and multi-modal data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resource-efficient Deep Neural Networks for Automotive Radar Interference Mitigation. (arXiv:2201.10360v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10360">
<div class="article-summary-box-inner">
<span><p>Radar sensors are crucial for environment perception of driver assistance
systems as well as autonomous vehicles. With a rising number of radar sensors
and the so far unregulated automotive radar frequency band, mutual interference
is inevitable and must be dealt with. Algorithms and models operating on radar
data are required to run the early processing steps on specialized radar sensor
hardware. This specialized hardware typically has strict resource-constraints,
i.e. a low memory capacity and low computational power. Convolutional Neural
Network (CNN)-based approaches for denoising and interference mitigation yield
promising results for radar processing in terms of performance. Regarding
resource-constraints, however, CNNs typically exceed the hardware's capacities
by far.
</p>
<p>In this paper we investigate quantization techniques for CNN-based denoising
and interference mitigation of radar signals. We analyze the quantization of
(i) weights and (ii) activations of different CNN-based model architectures.
This quantization results in reduced memory requirements for model storage and
during inference. We compare models with fixed and learned bit-widths and
contrast two different methodologies for training quantized CNNs, i.e. the
straight-through gradient estimator and training distributions over discrete
weights. We illustrate the importance of structurally small real-valued base
models for quantization and show that learned bit-widths yield the smallest
models. We achieve a memory reduction of around 80\% compared to the
real-valued baseline. Due to practical reasons, however, we recommend the use
of 8 bits for weights and activations, which results in models that require
only 0.2 megabytes of memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADAPT: An Open-Source sUAS Payload for Real-Time Disaster Prediction and Response with AI. (arXiv:2201.10366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10366">
<div class="article-summary-box-inner">
<span><p>Small unmanned aircraft systems (sUAS) are becoming prominent components of
many humanitarian assistance and disaster response (HADR) operations. Pairing
sUAS with onboard artificial intelligence (AI) substantially extends their
utility in covering larger areas with fewer support personnel. A variety of
missions, such as search and rescue, assessing structural damage, and
monitoring forest fires, floods, and chemical spills, can be supported simply
by deploying the appropriate AI models. However, adoption by
resource-constrained groups, such as local municipalities, regulatory agencies,
and researchers, has been hampered by the lack of a cost-effective,
readily-accessible baseline platform that can be adapted to their unique
missions. To fill this gap, we have developed the free and open-source ADAPT
multi-mission payload for deploying real-time AI and computer vision onboard a
sUAS during local and beyond-line-of-site missions. We have emphasized a
modular design with low-cost, readily-available components, open-source
software, and thorough documentation (https://kitware.github.io/adapt/). The
system integrates an inertial navigation system, high-resolution color camera,
computer, and wireless downlink to process imagery and broadcast georegistered
analytics back to a ground station. Our goal is to make it easy for the HADR
community to build their own copies of the ADAPT payload and leverage the
thousands of hours of engineering we have devoted to developing and testing. In
this paper, we detail the development and testing of the ADAPT payload. We
demonstrate the example mission of real-time, in-flight ice segmentation to
monitor river ice state and provide timely predictions of catastrophic flooding
events. We deploy a novel active learning workflow to annotate river ice
imagery, train a real-time deep neural network for ice segmentation, and
demonstrate operation in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Winograd Convolution for Deep Neural Networks: Efficient Point Selection. (arXiv:2201.10369v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10369">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have dramatically improved the accuracy
of tasks such as object recognition, image segmentation and interactive speech
systems. CNNs require large amounts of computing resources because
ofcomputationally intensive convolution layers. Fast convolution algorithms
such as Winograd convolution can greatly reduce the computational cost of these
layers at a cost of poor numeric properties, such that greater savings in
computation exponentially increase floating point errors.
</p>
<p>A defining feature of each Winograd convolution algorithm is a set of
real-value points where polynomials are sampled. The choice of points impacts
the numeric accuracy of the algorithm, but the optimal set of points for small
convolutions remains unknown. Existing work considers only small integers and
simple fractions as candidate points. In this work, we propose a novel approach
to point selection using points of the form {-1/c , -c, c, 1/c } using the full
range of real-valued numbers for c. We show that groups of this form cause
cancellations in the Winograd transform matrices that reduce numeric error. We
find empirically that the error for different values of c forms a rough curve
across the range of real-value numbers helping to localize the values of c that
reduce error and that lower errors can be achieved with non-obvious real-valued
evaluation points instead of integers or simple fractions. We study a range of
sizes for small convolutions and achieve reduction in error ranging from 2% to
around 59% for both 1D and 2D convolution. Furthermore, we identify patterns in
cases when we select a subset of our proposed points which will always lead to
a lower error. Finally we implement a complete Winograd convolution layer and
use it to run deep convolution neural networks on real datasets and show that
our proposed points reduce error, ranging from 22% to 63%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLDNet: A Semi-supervised Change Detection Building Damage Framework using Graph Convolutional Networks and Urban Domain Knowledge. (arXiv:2201.10389v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10389">
<div class="article-summary-box-inner">
<span><p>Change detection is instrumental to localize damage and understand
destruction in disaster informatics. While convolutional neural networks are at
the core of recent change detection solutions, we present in this work, BLDNet,
a novel graph formulation for building damage change detection and enable
learning relationships and representations from both local patterns and
non-stationary neighborhoods. More specifically, we use graph convolutional
networks to efficiently learn these features in a semi-supervised framework
with few annotated data. Additionally, BLDNet formulation allows for the
injection of additional contextual building meta-features. We train and
benchmark on the xBD dataset to validate the effectiveness of our approach. We
also demonstrate on urban data from the 2020 Beirut Port Explosion that
performance is improved by incorporating domain knowledge building
meta-features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition. (arXiv:2201.10394v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10394">
<div class="article-summary-box-inner">
<span><p>We address the problem of capturing temporal information for video
classification in 2D networks, without increasing computational cost. Existing
approaches focus on modifying the architecture of 2D networks (e.g. by
including filters in the temporal dimension to turn them into 3D networks, or
using optical flow, etc.), which increases computation cost. Instead, we
propose a novel sampling strategy, where we re-order the channels of the input
video, to capture short-term frame-to-frame changes. We observe that without
bells and whistles, the proposed sampling strategy improves performance on
multiple architectures (e.g. TSN, TRN, and TSM) and datasets (CATER,
Something-Something-V1 and V2), up to 24% over the baseline of using the
standard video input. In addition, our sampling strategies do not require
training from scratch and do not increase the computational cost of training
and testing. Given the generality of the results and the flexibility of the
approach, we hope this can be widely useful to the video understanding
community. Code is available at https://github.com/kiyoon/PyVideoAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Cross-Disaster Building Damage Assessment with Graph Convolutional Networks. (arXiv:2201.10395v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10395">
<div class="article-summary-box-inner">
<span><p>In the aftermath of disasters, building damage maps are obtained using change
detection to plan rescue operations. Current convolutional neural network
approaches do not consider the similarities between neighboring buildings for
predicting the damage. We present a novel graph-based building damage detection
solution to capture these relationships. Our proposed model architecture learns
from both local and neighborhood features to predict building damage.
Specifically, we adopt the sample and aggregate graph convolution strategy to
learn aggregation functions that generalize to unseen graphs which is essential
for alleviating the time needed to obtain predictions for new disasters. Our
experiments on the xBD dataset and comparisons with a classical convolutional
neural network reveal that while our approach is handicapped by class
imbalance, it presents a promising and distinct advantage when it comes to
cross-disaster generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of Evaluation Metrics for Landmark Detection in CMR Images. (arXiv:2201.10410v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10410">
<div class="article-summary-box-inner">
<span><p>Cardiac Magnetic Resonance (CMR) images are widely used for cardiac diagnosis
and ventricular assessment. Extracting specific landmarks like the right
ventricular insertion points is of importance for spatial alignment and 3D
modeling. The automatic detection of such landmarks has been tackled by
multiple groups using Deep Learning, but relatively little attention has been
paid to the failure cases of evaluation metrics in this field. In this work, we
extended the public ACDC dataset with additional labels of the right
ventricular insertion points and compare different variants of a heatmap-based
landmark detection pipeline. In this comparison, we demonstrate very likely
pitfalls of apparently simple detection and localisation metrics which
highlights the importance of a clear detection strategy and the definition of
an upper limit for localisation-based metrics. Our preliminary results indicate
that a combination of different metrics is necessary, as they yield different
winners for method comparison. Additionally, they highlight the need of a
comprehensive metric description and evaluation standardisation, especially for
the error cases where no metrics could be computed or where no lower/upper
boundary of a metric exists. Code and labels:
https://github.com/Cardio-AI/rvip_landmark_detection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rayleigh EigenDirections (REDs): GAN latent space traversals for multidimensional features. (arXiv:2201.10423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10423">
<div class="article-summary-box-inner">
<span><p>We present a method for finding paths in a deep generative model's latent
space that can maximally vary one set of image features while holding others
constant. Crucially, unlike past traversal approaches, ours can manipulate
multidimensional features of an image such as facial identity and pixels within
a specified region. Our method is principled and conceptually simple: optimal
traversal directions are chosen by maximizing differential changes to one
feature set such that changes to another set are negligible. We show that this
problem is nearly equivalent to one of Rayleigh quotient maximization, and
provide a closed-form solution to it based on solving a generalized eigenvalue
equation. We use repeated computations of the corresponding optimal directions,
which we call Rayleigh EigenDirections (REDs), to generate appropriately curved
paths in latent space. We empirically evaluate our method using StyleGAN2 on
two image domains: faces and living rooms. We show that our method is capable
of controlling various multidimensional features out of the scope of previous
latent space traversal methods: face identity, spatial frequency bands, pixels
within a region, and the appearance and position of an object. Our work
suggests that a wealth of opportunities lies in the local analysis of the
geometry and semantics of latent spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plaque segmentation via masking of the artery wall. (arXiv:2201.10424v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10424">
<div class="article-summary-box-inner">
<span><p>The presence of plaques in the coronary arteries are a major risk to the
patients' life. In particular, non-calcified plaques pose a great challenge, as
they are harder to detect and more likely to rupture than calcified plaques.
While current deep learning techniques allow precise segmentation of regular
images, the performance in medical images is still low, caused mostly by
blurriness and ambiguous voxel intensities of unrelated parts that fall on the
same range. In this paper, we propose a novel methodology for segmenting
calcified and non-calcified plaques in CCTA-CPR scans of coronary arteries. The
input slices are masked so only the voxels within the wall vessel are
considered for segmentation. We also provide an exhaustive evaluation by
applying different types of masks, in order to validate the potential of vessel
masking for plaque segmentation. Our methodology results in a prominent boost
in segmentation performance, in both quantitative and qualitative evaluation,
achieving accurate plaque shapes even for the challenging non-calcified
plaques. We believe our findings can lead the future research for
high-performance plaque segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Main Product Detection with Graph Networks for Fashion. (arXiv:2201.10431v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10431">
<div class="article-summary-box-inner">
<span><p>Computer vision has established a foothold in the online fashion retail
industry. Main product detection is a crucial step of vision-based fashion
product feed parsing pipelines, focused in identifying the bounding boxes that
contain the product being sold in the gallery of images of the product page.
The current state-of-the-art approach does not leverage the relations between
regions in the image, and treats images of the same product independently,
therefore not fully exploiting visual and product contextual information. In
this paper we propose a model that incorporates Graph Convolutional Networks
(GCN) that jointly represent all detected bounding boxes in the gallery as
nodes. We show that the proposed method is better than the state-of-the-art,
especially, when we consider the scenario where title-input is missing at
inference time and for cross-dataset evaluation, our method outperforms
previous approaches by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition. (arXiv:2201.10439v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10439">
<div class="article-summary-box-inner">
<span><p>Audio-visual automatic speech recognition (AV-ASR) extends the speech
recognition by introducing the video modality. In particular, the information
contained in the motion of the speaker's mouth is used to augment the audio
features. The video modality is traditionally processed with a 3D convolutional
neural network (e.g. 3D version of VGG). Recently, image transformer networks
<a href="/abs/2010.11929">arXiv:2010.11929</a> demonstrated the ability to extract rich visual features for
the image classification task. In this work, we propose to replace the 3D
convolution with a video transformer video feature extractor. We train our
baselines and the proposed model on a large scale corpus of the YouTube videos.
Then we evaluate the performance on a labeled subset of YouTube as well as on
the public corpus LRS3-TED. Our best model video-only model achieves the
performance of 34.9% WER on YTDEV18 and 19.3% on LRS3-TED which is a 10% and 9%
relative improvements over the convolutional baseline. We achieve the state of
the art performance of the audio-visual recognition on the LRS3-TED after
fine-tuning our model (1.6% WER).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AggMatch: Aggregating Pseudo Labels for Semi-Supervised Learning. (arXiv:2201.10444v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10444">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) has recently proven to be an effective
paradigm for leveraging a huge amount of unlabeled data while mitigating the
reliance on large labeled data. Conventional methods focused on extracting a
pseudo label from individual unlabeled data sample and thus they mostly
struggled to handle inaccurate or noisy pseudo labels, which degenerate
performance.
</p>
<p>In this paper, we address this limitation with a novel SSL framework for
aggregating pseudo labels, called AggMatch, which refines initial pseudo labels
by using different confident instances. Specifically, we introduce an
aggregation module for consistency regularization framework that aggregates the
initial pseudo labels based on the similarity between the instances. To enlarge
the aggregation candidates beyond the mini-batch, we present a class-balanced
confidence-aware queue built with the momentum model, encouraging to provide
more stable and consistent aggregation. We also propose a novel
uncertainty-based confidence measure for the pseudo label by considering the
consensus among multiple hypotheses with different subsets of the queue. We
conduct experiments to demonstrate the effectiveness of AggMatch over the
latest methods on standard benchmarks and provide extensive analyses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Low Can We Go? Pixel Annotation for Semantic Segmentation. (arXiv:2201.10448v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10448">
<div class="article-summary-box-inner">
<span><p>How many labeled pixels are needed to segment an image, without any prior
knowledge? We conduct an experiment to answer this question.
</p>
<p>In our experiment, an Oracle is using Active Learning to train a network from
scratch. The Oracle has access to the entire label map of the image, but the
goal is to reveal as little pixel labels to the network as possible. We find
that, on average, the Oracle needs to reveal (i.e., annotate) less than $0.1\%$
of the pixels in order to train a network. The network can then label all
pixels in the image at an accuracy of more than $98\%$.
</p>
<p>Based on this single-image-annotation experiment, we design an experiment to
quickly annotate an entire data set. In the data set level experiment the
Oracle trains a new network for each image from scratch. The network can then
be used to create pseudo-labels, which are the network predicted labels of the
unlabeled pixels, for the entire image. Only then, a data set level network is
trained from scratch on all the pseudo-labeled images at once.
</p>
<p>We repeat both image level and data set level experiments on two, very
different, real-world data sets, and find that it is possible to reach the
performance of a fully annotated data set using a fraction of the annotation
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sphere2Vec: Multi-Scale Representation Learning over a Spherical Surface for Geospatial Predictions. (arXiv:2201.10489v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10489">
<div class="article-summary-box-inner">
<span><p>Generating learning-friendly representations for points in a 2D space is a
fundamental and long-standing problem in machine learning. Recently,
multi-scale encoding schemes (such as Space2Vec) were proposed to directly
encode any point in 2D space as a high-dimensional vector, and has been
successfully applied to various (geo)spatial prediction tasks. However, a map
projection distortion problem rises when applying location encoding models to
large-scale real-world GPS coordinate datasets (e.g., species images taken all
over the world) - all current location encoding models are designed for
encoding points in a 2D (Euclidean) space but not on a spherical surface, e.g.,
earth surface. To solve this problem, we propose a multi-scale location
encoding model called Sphere2V ec which directly encodes point coordinates on a
spherical surface while avoiding the mapprojection distortion problem. We
provide theoretical proof that the Sphere2Vec encoding preserves the spherical
surface distance between any two points. We also developed a unified view of
distance-reserving encoding on spheres based on the Double Fourier Sphere
(DFS). We apply Sphere2V ec to the geo-aware image classification task. Our
analysis shows that Sphere2V ec outperforms other 2D space location encoder
models especially on the polar regions and data-sparse areas for image
classification tasks because of its nature for spherical surface distance
preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Initial Investigations Towards Non-invasive Monitoring of Chronic Wound Healing Using Deep Learning and Ultrasound Imaging. (arXiv:2201.10511v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10511">
<div class="article-summary-box-inner">
<span><p>Chronic wounds including diabetic and arterial/venous insufficiency injuries
have become a major burden for healthcare systems worldwide. Demographic
changes suggest that wound care will play an even bigger role in the coming
decades. Predicting and monitoring response to therapy in wound care is
currently largely based on visual inspection with little information on the
underlying tissue. Thus, there is an urgent unmet need for innovative
approaches that facilitate personalized diagnostics and treatments at the
point-of-care. It has been recently shown that ultrasound imaging can monitor
response to therapy in wound care, but this work required onerous manual image
annotations. In this study, we present initial results of a deep learning-based
automatic segmentation of cross-sectional wound size in ultrasound images and
identify requirements and challenges for future research on this application.
Evaluation of the segmentation results underscores the potential of the
proposed deep learning approach to complement non-invasive imaging with Dice
scores of 0.34 (U-Net, FCN) and 0.27 (ResNet-U-Net) but also highlights the
need for improving robustness further. We conclude that deep learning-supported
analysis of non-invasive ultrasound images is a promising area of research to
automatically extract cross-sectional wound size and depth information with
potential value in monitoring response to therapy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Activation-based Structured Pruning. (arXiv:2201.10520v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10520">
<div class="article-summary-box-inner">
<span><p>Pruning is a promising approach to compress complex deep learning models in
order to deploy them on resource-constrained edge devices. However, many
existing pruning solutions are based on unstructured pruning, which yield
models that cannot efficiently run on commodity hardware, and require users to
manually explore and tune the pruning process, which is time consuming and
often leads to sub-optimal results. To address these limitations, this paper
presents an adaptive, activation-based, structured pruning approach to
automatically and efficiently generate small, accurate, and hardware-efficient
models that meet user requirements. First, it proposes iterative structured
pruning using activation-based attention feature maps to effectively identify
and prune unimportant filters. Then, it proposes adaptive pruning policies for
automatically meeting the pruning objectives of accuracy-critical,
memory-constrained, and latency-sensitive tasks. A comprehensive evaluation
shows that the proposed method can substantially outperform the
state-of-the-art structured pruning works on CIFAR-10 and ImageNet datasets.
For example, on ResNet-56 with CIFAR-10, without any accuracy drop, our method
achieves the largest parameter reduction (79.11%), outperforming the related
works by 22.81% to 66.07%, and the largest FLOPs reduction (70.13%),
outperforming the related works by 14.13% to 26.53%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Deep Learning Based Image Super-resolution Techniques. (arXiv:2201.10521v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10521">
<div class="article-summary-box-inner">
<span><p>Image super-resolution technology is the process of obtaining high-resolution
images from one or more low-resolution images. With the development of deep
learning, image super-resolution technology based on deep learning method is
emerging. This paper reviews the research progress of the application of depth
learning method in the field of image super-resolution, introduces this kind of
super-resolution work from several aspects, and looks forward to the further
application of depth learning method in the field of image super-resolution. By
collecting and counting the relevant literature on the application of depth
learning in the field of image super-resolution, we preliminarily summarizes
the application results of depth learning method in the field of image
super-resolution, and reports the latest progress of image super-resolution
technology based on depth learning method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind Image Deblurring: a Review. (arXiv:2201.10522v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10522">
<div class="article-summary-box-inner">
<span><p>This is a review on blind image deblurring. First, we formulate the blind
image deblurring problem and explain why it is challenging. Next, we bring some
psychological and cognitive studies on the way our human vision system deblurs.
Then, relying on several previous reviews, we discuss the topic of metrics and
datasets, which is non-trivial to blind deblurring. Finally, we introduce some
typical optimization-based methods and learning-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretability in Convolutional Neural Networks for Building Damage Classification in Satellite Imagery. (arXiv:2201.10523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10523">
<div class="article-summary-box-inner">
<span><p>Natural disasters ravage the world's cities, valleys, and shores on a regular
basis. Deploying precise and efficient computational mechanisms for assessing
infrastructure damage is essential to channel resources and minimize the loss
of life. Using a dataset that includes labeled pre- and post- disaster
satellite imagery, we take a machine learning-based remote sensing approach and
train multiple convolutional neural networks (CNNs) to assess building damage
on a per-building basis. We present a novel methodology of interpretable deep
learning that seeks to explicitly investigate the most useful modalities of
information in the training data to create an accurate classification model. We
also investigate which loss functions best optimize these models. Our findings
include that ordinal-cross entropy loss is the most optimal criterion for
optimization to use and that including the type of disaster that caused the
damage in combination with pre- and post-disaster training data most accurately
predicts the level of damage caused. Further, we make progress in the
qualitative representation of which parts of the images that the model is using
to predict damage levels, through gradient-weighted class activation mapping
(Grad-CAM). Our research seeks to computationally contribute to aiding in this
ongoing and growing humanitarian crisis, heightened by anthropogenic climate
change.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonarchNet: Differentiating Monarch Butterflies from Butterflies Species with Similar Phenotypes. (arXiv:2201.10526v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10526">
<div class="article-summary-box-inner">
<span><p>In recent years, the monarch butterfly's iconic migration patterns have come
under threat from a number of factors, from climate change to pesticide use. To
track trends in their populations, scientists as well as citizen scientists
must identify individuals accurately. This is uniquely key for the study of
monarch butterflies because there exist other species of butterfly, such as
viceroy butterflies, that are "look-alikes" (coined by the Convention on
International Trade in Endangered Species of Wild Fauna and Flora), having
similar phenotypes. To tackle this problem and to aid in more efficient
identification, we present MonarchNet, the first comprehensive dataset
consisting of butterfly imagery for monarchs and five look-alike species. We
train a baseline deep-learning classification model to serve as a tool for
differentiating monarch butterflies and its various look-alikes. We seek to
contribute to the study of biodiversity and butterfly ecology by providing a
novel method for computational classification of these particular butterfly
species. The ultimate aim is to help scientists track monarch butterfly
population and migration trends in the most precise and efficient manner
possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Representation Distillation. (arXiv:1910.10699v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.10699">
<div class="article-summary-box-inner">
<span><p>Often we wish to transfer representational knowledge from one neural network
to another. Examples include distilling a large network into a smaller one,
transferring knowledge from one sensory modality to a second, or ensembling a
collection of models into a single estimator. Knowledge distillation, the
standard approach to these problems, minimizes the KL divergence between the
probabilistic outputs of a teacher and student network. We demonstrate that
this objective ignores important structural knowledge of the teacher network.
This motivates an alternative objective by which we train a student to capture
significantly more information in the teacher's representation of the data. We
formulate this objective as contrastive learning. Experiments demonstrate that
our resulting new objective outperforms knowledge distillation and other
cutting-edge distillers on a variety of knowledge transfer tasks, including
single model compression, ensemble distillation, and cross-modal transfer. Our
method sets a new state-of-the-art in many transfer tasks, and sometimes even
outperforms the teacher network when combined with knowledge distillation.
Code: <a href="http://github.com/HobbitLong/RepDistiller.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Posterior Adaptation With New Priors. (arXiv:2007.01386v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01386">
<div class="article-summary-box-inner">
<span><p>Classification approaches based on the direct estimation and analysis of
posterior probabilities will degrade if the original class priors begin to
change. We prove that a unique (up to scale) solution is possible to recover
the data likelihoods for a test example from its original class posteriors and
dataset priors. Given the recovered likelihoods and a set of new priors, the
posteriors can be re-computed using Bayes' Rule to reflect the influence of the
new priors. The method is simple to compute and allows a dynamic update of the
original posteriors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vid2CAD: CAD Model Alignment using Multi-View Constraints from Videos. (arXiv:2012.04641v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04641">
<div class="article-summary-box-inner">
<span><p>We address the task of aligning CAD models to a video sequence of a complex
scene containing multiple objects. Our method can process arbitrary videos and
fully automatically recover the 9 DoF pose for each object appearing in it,
thus aligning them in a common 3D coordinate frame. The core idea of our method
is to integrate neural network predictions from individual frames with a
temporally global, multi-view constraint optimization formulation. This
integration process resolves the scale and depth ambiguities in the per-frame
predictions, and generally improves the estimate of all pose parameters. By
leveraging multi-view constraints, our method also resolves occlusions and
handles objects that are out of view in individual frames, thus reconstructing
all objects into a single globally consistent CAD representation of the scene.
In comparison to the state-of-the-art single-frame method Mask2CAD that we
build on, we achieve substantial improvements on the Scan2CAD dataset (from
11.6% to 30.7% class average accuracy).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polarization Guided Specular Reflection Separation. (arXiv:2103.11652v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11652">
<div class="article-summary-box-inner">
<span><p>Since specular reflection often exists in the real captured images and causes
deviation between the recorded color and intrinsic color, specular reflection
separation can bring advantages to multiple applications that require
consistent object surface appearance. However, due to the color of an object is
significantly influenced by the color of the illumination, the existing
researches still suffer from the near-duplicate challenge, that is, the
separation becomes unstable when the illumination color is close to the surface
color. In this paper, we derive a polarization guided model to incorporate the
polarization information into a designed iteration optimization separation
strategy to separate the specular reflection. Based on the analysis of
polarization, we propose a polarization guided model to generate a polarization
chromaticity image, which is able to reveal the geometrical profile of the
input image in complex scenarios, such as diversity of illumination. The
polarization chromaticity image can accurately cluster the pixels with similar
diffuse color. We further use the specular separation of all these clusters as
an implicit prior to ensure that the diffuse components will not be mistakenly
separated as the specular components. With the polarization guided model, we
reformulate the specular reflection separation into a unified optimization
function which can be solved by the ADMM strategy. The specular reflection will
be detected and separated jointly by RGB and polarimetric information. Both
qualitative and quantitative experimental results have shown that our method
can faithfully separate the specular reflection, especially in some challenging
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using Spatial and Temporal Transformers. (arXiv:2103.14829v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14829">
<div class="article-summary-box-inner">
<span><p>Tracking a time-varying indefinite number of objects in a video sequence over
time remains a challenge despite recent advances in the field. Ignoring
long-term temporal information, most existing approaches are not able to
properly handle multi-object tracking challenges such as occlusion. To address
these shortcomings, we present MO3TR: a truly end-to-end Transformer-based
online multi-object tracking (MOT) framework that learns to handle occlusions,
track initiation and termination without the need for an explicit data
association module or any heuristics/post-processing. MO3TR encodes object
interactions into long-term temporal embeddings using a combination of spatial
and temporal Transformers, and recursively uses the information jointly with
the input data to estimate the states of all tracked objects over time. The
spatial attention mechanism enables our framework to learn implicit
representations between all the objects and the objects to the measurements,
while the temporal attention mechanism focuses on specific parts of past
information, allowing our approach to resolve occlusions over multiple frames.
Our experiments demonstrate the potential of this new approach, reaching new
state-of-the-art results on multiple MOT metrics for two popular multi-object
tracking benchmarks. Our code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hierarchical Approach to Remote Sensing Scene Classification. (arXiv:2103.15463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15463">
<div class="article-summary-box-inner">
<span><p>Remote sensing scene classification deals with the problem of classifying
land use/cover of a region from images. To predict the development and
socioeconomic structures of cities, the status of land use in regions is
tracked by the national mapping agencies of countries. Many of these agencies
use land-use types that are arranged in multiple levels. In this paper, we
examined the efficiency of a hierarchically designed Convolutional Neural
Network (CNN) based framework that is suitable for such arrangements. We use
the NWPU-RESISC45 dataset for our experiments and arranged this data set in a
two-level nested hierarchy. Each node in the designed hierarchy is trained
using DenseNet-121 architectures. We provide detailed empirical analysis to
compare the performances of this hierarchical scheme and its non-hierarchical
counterpart, together with the individual model performances. We also evaluated
the performance of the hierarchical structure statistically to validate the
presented empirical results. The results of our experiments show that although
individual classifiers for different sub-categories in the hierarchical scheme
perform considerably well, the accumulation of the classification errors in the
cascaded structure prevents its classification performance from exceeding that
of the non-hierarchical deep model
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category-Adaptive Domain Adaptation for Semantic Segmentation. (arXiv:2103.15467v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15467">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) becomes more and more popular in
tackling real-world problems without ground truth of the target domain. Though
tedious annotation work is not required, UDA unavoidably faces two problems: 1)
how to narrow the domain discrepancy to boost the transferring performance; 2)
how to improve pseudo annotation producing mechanism for self-supervised
learning (SSL). In this paper, we focus on UDA for semantic segmentation task.
Firstly, we introduce adversarial learning into style gap bridging mechanism to
keep the style information from two domains in the similar space. Secondly, to
keep the balance of pseudo labels on each category, we propose a
category-adaptive threshold mechanism to choose category-wise pseudo labels for
SSL. The experiments are conducted using GTA5 as the source domain, Cityscapes
as the target domain. The results show that our model outperforms the
state-of-the-arts with a noticeable gain on cross-domain adaptation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement. (arXiv:2104.09958v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09958">
<div class="article-summary-box-inner">
<span><p>Advances in unsupervised learning of object-representations have culminated
in the development of a broad range of methods for unsupervised object
segmentation and interpretable object-centric scene generation. These methods,
however, are limited to simulated and real-world datasets with limited visual
complexity. Moreover, object representations are often inferred using RNNs
which do not scale well to large images or iterative refinement which avoids
imposing an unnatural ordering on objects in an image but requires the a priori
initialisation of a fixed number of object representations. In contrast to
established paradigms, this work proposes an embedding-based approach in which
embeddings of pixels are clustered in a differentiable fashion using a
stochastic stick-breaking process. Similar to iterative refinement, this
clustering procedure also leads to randomly ordered object representations, but
without the need of initialising a fixed number of clusters a priori. This is
used to develop a new model, GENESIS-v2, which can infer a variable number of
object representations without using RNNs or iterative refinement. We show that
GENESIS-v2 performs strongly in comparison to recent baselines in terms of
unsupervised image segmentation and object-centric scene generation on
established synthetic datasets as well as more complex real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heuristic Weakly Supervised 3D Human Pose Estimation in Novel Contexts without Any 3D Pose Ground Truth. (arXiv:2105.10996v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10996">
<div class="article-summary-box-inner">
<span><p>Monocular 3D human pose estimation from a single RGB image has received a lot
attentions in the past few year. Pose inference models with competitive
performance however require supervision with 3D pose ground truth data or at
least known pose priors in their target domain. Yet, these data requirements in
many real-world applications with data collection constraints may not be
attainable. In this paper, we present a heuristic weakly supervised human pose
(HW-HuP) solution to estimate 3D human pose in contexts that no ground truth 3D
pose data is accessible, even for fine-tuning. HW-HuP learns partial pose
priors from public 3D human pose datasets and uses easy-to-access observations
from the target domain to iteratively estimate 3D human pose and shape in an
optimization and regression hybrid cycle. In our design, depth data as an
auxiliary information is employed as weak supervision during training, yet it
is not needed for the inference. HW-HuP shows comparable performance on public
benchmarks to the state-of-the-art approaches which benefit from full 3D pose
supervision. In this paper, we focus on two practical applications of 3D pose
estimation for individuals while in bed as well as infants, where no reliable
3D pose data exists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroWaste Dataset: Towards Deformable Object Segmentation in Cluttered Scenes. (arXiv:2106.02740v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02740">
<div class="article-summary-box-inner">
<span><p>Less than 35% of recyclable waste is being actually recycled in the US, which
leads to increased soil and sea pollution and is one of the major concerns of
environmental researchers as well as the common public. At the heart of the
problem are the inefficiencies of the waste sorting process (separating paper,
plastic, metal, glass, etc.) due to the extremely complex and cluttered nature
of the waste stream. Recyclable waste detection poses a unique computer vision
challenge as it requires detection of highly deformable and often translucent
objects in cluttered scenes without the kind of context information usually
present in human-centric datasets. This challenging computer vision task
currently lacks suitable datasets or methods in the available literature. In
this paper, we take a step towards computer-aided waste detection and present
the first in-the-wild industrial-grade waste detection and segmentation
dataset, ZeroWaste. We believe that ZeroWaste will catalyze research in object
detection and semantic segmentation in extreme clutter as well as applications
in the recycling domain. Our project page can be found at
<a href="http://ai.bu.edu/zerowaste/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07847">
<div class="article-summary-box-inner">
<span><p>While unbiased machine learning models are essential for many applications,
bias is a human-defined concept that can vary across tasks. Given only
input-label pairs, algorithms may lack sufficient information to distinguish
stable (causal) features from unstable (spurious) features. However, related
tasks often share similar biases -- an observation we may leverage to develop
stable classifiers in the transfer setting. In this work, we explicitly inform
the target classifier about unstable features in the source tasks.
Specifically, we derive a representation that encodes the unstable features by
contrasting different data environments in the source task. We achieve
robustness by clustering data of the target task according to this
representation and minimizing the worst-case risk across these clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task for both synthetically generated environments and real-world environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data. (arXiv:2107.06777v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06777">
<div class="article-summary-box-inner">
<span><p>One of the most pressing problems in the automated analysis of historical
documents is the availability of annotated training data. The problem is that
labeling samples is a time-consuming task because it requires human expertise
and thus, cannot be automated well. In this work, we propose a novel method to
construct synthetic labeled datasets for historical documents where no
annotations are available. We train a StyleGAN model to synthesize document
images that capture the core features of the original documents. While
originally, the StyleGAN architecture was not intended to produce labels, it
indirectly learns the underlying semantics to generate realistic images. Using
our approach, we can extract the semantic information from the intermediate
feature maps and use it to generate ground truth labels. To investigate if our
synthetic dataset can be used to segment the text in historical documents, we
use it to train multiple supervised segmentation models and evaluate their
performance. We also train these models on another dataset created by a
state-of-the-art synthesis approach to show that the models trained on our
dataset achieve better results while requiring even less human annotation
effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applications of Artificial Neural Networks in Microorganism Image Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to Popular Convolutional Neural Network and Potential Visual Transformer. (arXiv:2108.00358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00358">
<div class="article-summary-box-inner">
<span><p>Microorganisms are widely distributed in the human daily living environment.
They play an essential role in environmental pollution control, disease
prevention and treatment, and food and drug production. The analysis of
microorganisms is the basic step for make full use of different microorganisms.
The conventional analysis methods are laborious and time-consuming. Therefore,
the automatic image analysis based on artificial neural networks is introduced
to optimize it. However, the automatic microorganism image analysis faces many
challenges, such as the requirement of robust algorithm caused by various
application occasions, insignificant features and easy undersegmentation caused
by the image characteristic, and various analysis tasks. Therefore, we conduct
this review to comprehensively discuss the characteristics of microorganism
image analysis based on artificial neural networks. In this review, the
background and motivation are introduced first. Then, the development of
artificial neural networks and representative networks are presented. After
that, the papers related to microorganism image analysis based on classical and
deep neural networks are reviewed from the perspectives of different tasks. In
the end, the methodology analysis and potential direction are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Observer Visual Problem-Solving Methods are Dynamically Hypothesized, Deployed and Tested. (arXiv:2108.08145v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08145">
<div class="article-summary-box-inner">
<span><p>The STAR architecture was designed to test the value of the full Selective
Tuning model of visual attention for complex real-world visuospatial tasks and
behaviors. However, knowledge of how humans solve such tasks in 3D as active
observers is lean. We thus devised a novel experimental setup and examined such
behavior. We discovered that humans exhibit a variety of problem-solving
strategies whose breadth and complexity are surprising and not easily handled
by current methodologies. It is apparent that solution methods are dynamically
composed by hypothesizing sequences of actions, testing them, and if they fail,
trying different ones. The importance of active observation is striking as is
the lack of any learning effect. These results inform our Cognitive Program
representation of STAR extending its relevance to real-world tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer. (arXiv:2109.04335v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04335">
<div class="article-summary-box-inner">
<span><p>Most recent semantic segmentation methods adopt a U-Net framework with an
encoder-decoder architecture. It is still challenging for U-Net with a simple
skip connection scheme to model the global multi-scale context: 1) Not each
skip connection setting is effective due to the issue of incompatible feature
sets of encoder and decoder stage, even some skip connection negatively
influence the segmentation performance; 2) The original U-Net is worse than the
one without any skip connection on some datasets. Based on our findings, we
propose a new segmentation framework, named UCTransNet (with a proposed CTrans
module in U-Net), from the channel perspective with attention mechanism.
Specifically, the CTrans module is an alternate of the U-Net skip connections,
which consists of a sub-module to conduct the multi-scale Channel Cross fusion
with Transformer (named CCT) and a sub-module Channel-wise Cross-Attention
(named CCA) to guide the fused multi-scale channel-wise information to
effectively connect to the decoder features for eliminating the ambiguity.
Hence, the proposed connection consisting of the CCT and CCA is able to replace
the original skip connection to solve the semantic gaps for an accurate
automatic medical image segmentation. The experimental results suggest that our
UCTransNet produces more precise segmentation performance and achieves
consistent improvements over the state-of-the-art for semantic segmentation
across different datasets and conventional architectures involving transformer
or U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAE-Transformer: Transformer-based Model to Predict Invasiveness of Lung Adenocarcinoma Subsolid Nodules from Non-thin Section 3D CT Scans. (arXiv:2110.08721v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08721">
<div class="article-summary-box-inner">
<span><p>Lung cancer is the leading cause of mortality from cancer worldwide and has
various histologic types, among which Lung Adenocarcinoma (LUAC) has recently
been the most prevalent one. The current approach to determine the invasiveness
of LUACs is surgical resection, which is not a viable solution to fight lung
cancer in a timely fashion. An alternative approach is to analyze chest
Computed Tomography (CT) scans. The radiologists' analysis based on CT images,
however, is subjective and might result in a low accuracy. In this paper, a
transformer-based framework, referred to as the "CAE-Transformer", is developed
to efficiently classify LUACs using whole CT images instead of finely annotated
nodules. The proposed CAE-Transformer can achieve high accuracy over a small
dataset and requires minor supervision from radiologists. The CAE Transformer
utilizes an encoder to automatically extract informative features from CT
slices, which are then fed to a modified transformer to capture global
inter-slice relations and provide classification labels. Experimental results
on our in-house dataset of 114 pathologically proven Sub-Solid Nodules (SSNs)
demonstrate the superiority of the CAE-Transformer over its counterparts,
achieving an accuracy of 87.73%, sensitivity of 88.67%, specificity of 86.33%,
and AUC of 0.913, using a 10-fold cross-validation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Primal-Dual Deep Unrolling. (arXiv:2110.10093v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10093">
<div class="article-summary-box-inner">
<span><p>We propose a new type of efficient deep-unrolling networks for solving
imaging inverse problems. Conventional deep-unrolling methods require full
forward operator and its adjoint across each layer, and hence can be
significantly more expensive computationally as compared with other end-to-end
methods that are based on post-processing of model-based reconstructions,
especially for 3D image reconstruction tasks. We develop a stochastic
(ordered-subsets) variant of the classical learned primal-dual (LPD), which is
a state-of-the-art unrolling network for tomographic image reconstruction. The
proposed learned stochastic primal-dual (LSPD) network only uses subsets of the
forward and adjoint operators and offers considerable computational efficiency.
We provide theoretical analysis of a special case of our LSPD framework,
suggesting that it has the potential to achieve image reconstruction quality
competitive with the full-batch LPD while requiring only a fraction of the
computation. The numerical results for two different X-ray computed tomography
(CT) imaging tasks (namely, low-dose and sparse-view CT) corroborate this
theoretical finding, demonstrating the promise of LSPD networks for large-scale
imaging problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Event-based Spatio-Temporal Feature Descriptors via Local Synaptic Plasticity: A Biologically-Plausible Perspective of Computer Vision. (arXiv:2111.00791v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00791">
<div class="article-summary-box-inner">
<span><p>We present an optimization-based theory describing spiking cortical ensembles
equipped with Spike-Timing-Dependent Plasticity (STDP) learning, as empirically
observed in the visual cortex. Using our methods, we build a class of
fully-connected, convolutional and action-based feature descriptors for
event-based camera that we respectively assess on N-MNIST, challenging
CIFAR10-DVS and on the IBM DVS128 gesture dataset. We report significant
accuracy improvements compared to conventional state-of-the-art event-based
feature descriptors (+8% on CIFAR10-DVS). We report large improvements in
accuracy compared to state-of-the-art STDP-based systems (+10% on N-MNIST,
+7.74% on IBM DVS128 Gesture). In addition to ultra-low-power learning in
neuromorphic edge devices, our work helps paving the way towards a
biologically-realistic, optimization-based theory of cortical vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MQBench: Towards Reproducible and Deployable Model Quantization Benchmark. (arXiv:2111.03759v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03759">
<div class="article-summary-box-inner">
<span><p>Model quantization has emerged as an indispensable technique to accelerate
deep learning inference. While researchers continue to push the frontier of
quantization algorithms, existing quantization work is often unreproducible and
undeployable. This is because researchers do not choose consistent training
pipelines and ignore the requirements for hardware deployments. In this work,
we propose Model Quantization Benchmark (MQBench), a first attempt to evaluate,
analyze, and benchmark the reproducibility and deployability for model
quantization algorithms. We choose multiple different platforms for real-world
deployments, including CPU, GPU, ASIC, DSP, and evaluate extensive
state-of-the-art quantization algorithms under a unified training pipeline.
MQBench acts like a bridge to connect the algorithm and the hardware. We
conduct a comprehensive analysis and find considerable intuitive or
counter-intuitive insights. By aligning the training settings, we find existing
algorithms have about the same performance on the conventional academic track.
While for the hardware-deployable quantization, there is a huge accuracy gap
which remains unsettled. Surprisingly, no existing algorithm wins every
challenge in MQBench, and we hope this work could inspire future research
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic tumour segmentation in H&E-stained whole-slide images of the pancreas. (arXiv:2112.01533v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01533">
<div class="article-summary-box-inner">
<span><p>Pancreatic cancer will soon be the second leading cause of cancer-related
death in Western society. Imaging techniques such as CT, MRI and ultrasound
typically help providing the initial diagnosis, but histopathological
assessment is still the gold standard for final confirmation of disease
presence and prognosis. In recent years machine learning approaches and
pathomics pipelines have shown potential in improving diagnostics and
prognostics in other cancerous entities, such as breast and prostate cancer. A
crucial first step in these pipelines is typically identification and
segmentation of the tumour area. Ideally this step is done automatically to
prevent time consuming manual annotation. We propose a multi-task convolutional
neural network to balance disease detection and segmentation accuracy. We
validated our approach on a dataset of 29 patients (for a total of 58 slides)
at different resolutions. The best single task segmentation network achieved a
median Dice of 0.885 (0.122) IQR at a resolution of 15.56 $\mu$m. Our
multi-task network improved on that with a median Dice score of 0.934 (0.077)
IQR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COROLLA: An Efficient Multi-Modality Fusion Framework with Supervised Contrastive Learning for Glaucoma Grading. (arXiv:2201.03795v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03795">
<div class="article-summary-box-inner">
<span><p>Glaucoma is one of the ophthalmic diseases that may cause blindness, for
which early detection and treatment are very important. Fundus images and
optical coherence tomography (OCT) images are both widely-used modalities in
diagnosing glaucoma. However, existing glaucoma grading approaches mainly
utilize a single modality, ignoring the complementary information between
fundus and OCT. In this paper, we propose an efficient multi-modality
supervised contrastive learning framework, named COROLLA, for glaucoma grading.
Through layer segmentation as well as thickness calculation and projection,
retinal thickness maps are extracted from the original OCT volumes and used as
a replacing modality, resulting in more efficient calculations with less memory
usage. Given the high structure and distribution similarities across medical
image samples, we employ supervised contrastive learning to increase our
models' discriminative power with better convergence. Moreover, feature-level
fusion of paired fundus image and thickness map is conducted for enhanced
diagnosis accuracy. On the GAMMA dataset, our COROLLA framework achieves
overwhelming glaucoma grading performance compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Lightweight Neural Animation : Exploration of Neural Network Pruning in Mixture of Experts-based Animation Models. (arXiv:2201.04042v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04042">
<div class="article-summary-box-inner">
<span><p>In the past few years, neural character animation has emerged and offered an
automatic method for animating virtual characters. Their motion is synthesized
by a neural network. Controlling this movement in real time with a user-defined
control signal is also an important task in video games for example. Solutions
based on fully-connected layers (MLPs) and Mixture-of-Experts (MoE) have given
impressive results in generating and controlling various movements with
close-range interactions between the environment and the virtual character.
However, a major shortcoming of fully-connected layers is their computational
and memory cost which may lead to sub-optimized solution. In this work, we
apply pruning algorithms to compress an MLP- MoE neural network in the context
of interactive character animation, which reduces its number of parameters and
accelerates its computation time with a trade-off between this acceleration and
the synthesized motion quality. This work demonstrates that, with the same
number of experts and parameters, the pruned model produces less motion
artifacts than the dense model and the learned high-level motion features are
similar for both
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth. (arXiv:2201.07436v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07436">
<div class="article-summary-box-inner">
<span><p>Depth estimation from a single image is an important task that can be applied
to various fields in computer vision, and has grown rapidly with the
development of convolutional neural networks. In this paper, we propose a novel
structure and training strategy for monocular depth estimation to further
improve the prediction accuracy of the network. We deploy a hierarchical
transformer encoder to capture and convey the global context, and design a
lightweight yet powerful decoder to generate an estimated depth map while
considering local connectivity. By constructing connected paths between
multi-scale local features and the global decoding stream with our proposed
selective feature fusion module, the network can integrate both representations
and recover fine details. In addition, the proposed decoder shows better
performance than the previously proposed decoders, with considerably less
computational complexity. Furthermore, we improve the depth-specific
augmentation method by utilizing an important observation in depth estimation
to enhance the model. Our network achieves state-of-the-art performance over
the challenging depth dataset NYU Depth V2. Extensive experiments have been
conducted to validate and show the effectiveness of the proposed approach.
Finally, our model shows better generalisation ability and robustness than
other comparative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DCNGAN: A Deformable Convolutional-Based GAN with QP Adaptation for Perceptual Quality Enhancement of Compressed Video. (arXiv:2201.08944v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08944">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a deformable convolution-based generative
adversarial network (DCNGAN) for perceptual quality enhancement of compressed
videos. DCNGAN is also adaptive to the quantization parameters (QPs). Compared
with optical flows, deformable convolutions are more effective and efficient to
align frames. Deformable convolutions can operate on multiple frames, thus
leveraging more temporal information, which is beneficial for enhancing the
perceptual quality of compressed videos. Instead of aligning frames in a
pairwise manner, the deformable convolution can process multiple frames
simultaneously, which leads to lower computational complexity. Experimental
results demonstrate that the proposed DCNGAN outperforms other state-of-the-art
compressed video quality enhancement algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Semantics for Visual Place Recognition through Multi-Scale Attention. (arXiv:2201.09701v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09701">
<div class="article-summary-box-inner">
<span><p>In this paper we address the task of visual place recognition (VPR), where
the goal is to retrieve the correct GPS coordinates of a given query image
against a huge geotagged gallery. While recent works have shown that building
descriptors incorporating semantic and appearance information is beneficial,
current state-of-the-art methods opt for a top down definition of the
significant semantic content. Here we present the first VPR algorithm that
learns robust global embeddings from both visual appearance and semantic
content of the data, with the segmentation process being dynamically guided by
the recognition of places through a multi-scale attention module. Experiments
on various scenarios validate this new approach and demonstrate its performance
against state-of-the-art methods. Finally, we propose the first synthetic-world
dataset suited for both place recognition and segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symmetry Perception by Deep Networks: Inadequacy of Feed-Forward Architectures and Improvements with Recurrent Connections. (arXiv:2112.04162v2 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04162">
<div class="article-summary-box-inner">
<span><p>Symmetry is omnipresent in nature and perceived by the visual system of many
species, as it facilitates detecting ecologically important classes of objects
in our environment. Symmetry perception requires abstraction of long-range
spatial dependencies between image regions, and its underlying neural
mechanisms remain elusive. In this paper, we evaluate Deep Neural Network (DNN)
architectures on the task of learning symmetry perception from examples. We
demonstrate that feed-forward DNNs that excel at modelling human performance on
object recognition tasks, are unable to acquire a general notion of symmetry.
This is the case even when the DNNs are architected to capture long-range
spatial dependencies, such as through `dilated' convolutions and the recently
introduced `transformers' design. By contrast, we find that recurrent
architectures are capable of learning to perceive symmetry by decomposing the
long-range spatial dependencies into a sequence of local operations, that are
reusable for novel images. These results suggest that recurrent connections
likely play an important role in symmetry perception in artificial systems, and
possibly, biological ones too.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-26 23:07:16.438399547 UTC">2022-01-26 23:07:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>