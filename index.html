<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-08T01:30:00Z">09-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of "Subjectivity" and "Identity Terms". (arXiv:2109.02691v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02691">
<div class="article-summary-box-inner">
<span><p>Toxic comment classification models are often found biased toward identity
terms which are terms characterizing a specific group of people such as
"Muslim" and "black". Such bias is commonly reflected in false-positive
predictions, i.e. non-toxic comments with identity terms. In this work, we
propose a novel approach to tackle such bias in toxic comment classification,
leveraging the notion of subjectivity level of a comment and the presence of
identity terms. We hypothesize that when a comment is made about a group of
people that is characterized by an identity term, the likelihood of that
comment being toxic is associated with the subjectivity level of the comment,
i.e. the extent to which the comment conveys personal feelings and opinions.
Building upon the BERT model, we propose a new structure that is able to
leverage these features, and thoroughly evaluate our model on 4 datasets of
varying sizes and representing different social media platforms. The results
show that our model can consistently outperform BERT and a SOTA model devised
to address identity term bias in a different way, with a maximum improvement in
F1 of 2.43% and 1.91% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-to-Table: A New Way of Information Extraction. (arXiv:2109.02707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02707">
<div class="article-summary-box-inner">
<span><p>We study a new problem setting of information extraction (IE), referred to as
text-to-table, which can be viewed as an inverse problem of the well-studied
table-to-text. In text-to-table, given a text, one creates a table or several
tables expressing the main content of the text, while the model is learned from
text-table pair data. The problem setting differs from those of the existing
methods for IE. First, the extraction can be carried out from long texts to
large tables with complex structures. Second, the extraction is entirely
data-driven, and there is no need to explicitly define the schemas. As far as
we know, there has been no previous work that studies the problem. In this
work, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.
We first employ a seq2seq model fine-tuned from a pre-trained language model to
perform the task. We also develop a new method within the seq2seq approach,
exploiting two additional techniques in table generation: table constraint and
table relation embeddings. We make use of four existing table-to-text datasets
in our experiments on text-to-table. Experimental results show that the vanilla
seq2seq model can outperform the baseline methods of using relation extraction
and named entity extraction. The results also show that our method can further
boost the performances of the vanilla seq2seq model. We further discuss the
main challenges of the proposed task. The code and data will be made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Inspiring Content on Social Media. (arXiv:2109.02734v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02734">
<div class="article-summary-box-inner">
<span><p>Inspiration moves a person to see new possibilities and transforms the way
they perceive their own potential. Inspiration has received little attention in
psychology, and has not been researched before in the NLP community. To the
best of our knowledge, this work is the first to study inspiration through
machine learning methods. We aim to automatically detect inspiring content from
social media data. To this end, we analyze social media posts to tease out what
makes a post inspiring and what topics are inspiring. We release a dataset of
5,800 inspiring and 5,800 non-inspiring English-language public post unique ids
collected from a dump of Reddit public posts made available by a third party
and use linguistic heuristics to automatically detect which social media
English-language posts are inspiring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica. (arXiv:2109.02738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02738">
<div class="article-summary-box-inner">
<span><p>People convey their intention and attitude through linguistic styles of the
text that they write. In this study, we investigate lexicon usages across
styles throughout two lenses: human perception and machine word importance,
since words differ in the strength of the stylistic cues that they provide. To
collect labels of human perception, we curate a new dataset, Hummingbird, on
top of benchmarking style datasets. We have crowd workers highlight the
representative words in the text that makes them think the text has the
following styles: politeness, sentiment, offensiveness, and five emotion types.
We then compare these human word labels with word importance derived from a
popular fine-tuned style classifier like BERT. Our results show that the BERT
often finds content words not relevant to the target style as important words
used in style prediction, but humans do not perceive the same way even though
for some styles (e.g., positive sentiment and joy) human- and
machine-identified words share significant overlap for some styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02747">
<div class="article-summary-box-inner">
<span><p>We aim to automatically identify human action reasons in online videos. We
focus on the widespread genre of lifestyle vlogs, in which people perform
actions while verbally describing them. We introduce and make publicly
available the {\sc WhyAct} dataset, consisting of 1,077 visual actions manually
annotated with their reasons. We describe a multimodal model that leverages
visual and textual information to automatically infer the reasons corresponding
to an action presented in the video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Neural Information Status Classification. (arXiv:2109.02753v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02753">
<div class="article-summary-box-inner">
<span><p>Most previous studies on information status (IS) classification and bridging
anaphora recognition assume that the gold mention or syntactic tree information
is given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio,
2020). In this paper, we propose an end-to-end neural approach for information
status classification. Our approach consists of a mention extraction component
and an information status assignment component. During the inference time, our
system takes a raw text as the input and generates mentions together with their
information status. On the ISNotes corpus (Markert et al., 2012), we show that
our information status assignment component achieves new state-of-the-art
results on fine-grained IS classification based on gold mentions. Furthermore,
our system performs significantly better than other baselines for both mention
extraction and fine-grained IS classification in the end-to-end setting.
Finally, we apply our system on BASHI (Roesiger, 2018) and SciCorp (Roesiger,
2016) to recognize referential bridging anaphora. We find that our end-to-end
system trained on ISNotes achieves competitive results on bridging anaphora
recognition compared to the previous state-of-the-art system that relies on
syntactic information and is trained on the in-domain datasets (Yu and Poesio,
2020).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02789">
<div class="article-summary-box-inner">
<span><p>Pretrained contextualized representations offer great success for many
downstream tasks, including document ranking. The multilingual versions of such
pretrained representations provide a possibility of jointly learning many
languages with the same model. Although it is expected to gain big with such
joint training, in the case of cross lingual information retrieval (CLIR), the
models under a multilingual setting are not achieving the same level of
performance as those under a monolingual setting. We hypothesize that the
performance drop is due to the translation gap between query and documents. In
the monolingual retrieval task, because of the same lexical inputs, it is
easier for model to identify the query terms that occurred in documents.
However, in the multilingual pretrained models that the words in different
languages are projected into the same hyperspace, the model tends to translate
query terms into related terms, i.e., terms that appear in a similar context,
in addition to or sometimes rather than synonyms in the target language. This
property is creating difficulties for the model to connect terms that cooccur
in both query and document. To address this issue, we propose a novel Mixed
Attention Transformer (MAT) that incorporates external word level knowledge,
such as a dictionary or translation table. We design a sandwich like
architecture to embed MAT into the recent transformer based deep neural models.
By encoding the translation knowledge into an attention matrix, the model with
MAT is able to focus on the mutually translated words in the input sequence.
Experimental results demonstrate the effectiveness of the external knowledge
and the significant improvement of MAT embedded neural reranking model on CLIR
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02797">
<div class="article-summary-box-inner">
<span><p>The application of Generative Pre-trained Transformer (GPT-2) to learn
text-archived game notation provides a model environment for exploring sparse
reward gameplay. The transformer architecture proves amenable to training on
solved text archives describing mazes, Rubik's Cube, and Sudoku solvers. The
method benefits from fine-tuning the transformer architecture to visualize
plausible strategies derived outside any guidance from human heuristics or
domain expertise. The large search space ($&gt;10^{19}$) for the games provides a
puzzle environment in which the solution has few intermediate rewards and a
final move that solves the challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02808">
<div class="article-summary-box-inner">
<span><p>FDA has been promoting enrollment practices that could enhance the diversity
of clinical trial populations, through broadening eligibility criteria.
However, how to broaden eligibility remains a significant challenge. We propose
an AI approach to Cohort Optimization (AICO) through transformer-based natural
language processing of the eligibility criteria and evaluation of the criteria
using real-world data. The method can extract common eligibility criteria
variables from a large set of relevant trials and measure the generalizability
of trial designs to real-world patients. It overcomes the scalability limits of
existing manual methods and enables rapid simulation of eligibility criteria
design for a disease of interest. A case study on breast cancer trial design
demonstrates the utility of the method in improving trial generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models. (arXiv:2109.02837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02837">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning benchmarks have been largely solved by fine-tuning
language models. The downside is that fine-tuning may cause models to overfit
to task-specific data and thereby forget their knowledge gained during
pre-training. Recent works only propose lightweight model updates as models may
already possess useful knowledge from past experience, but a challenge remains
in understanding what parts and to what extent models should be refined for a
given task. In this paper, we investigate what models learn from commonsense
reasoning datasets. We measure the impact of three different adaptation methods
on the generalization and accuracy of models. Our experiments with two models
show that fine-tuning performs best, by learning both the content and the
structure of the task, but suffers from overfitting and limited generalization
to novel answers. We observe that alternative adaptation methods like
prefix-tuning have comparable accuracy, but generalize better to unseen answers
and are more robust to adversarial splits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Datasets: A Community Library for Natural Language Processing. (arXiv:2109.02846v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02846">
<div class="article-summary-box-inner">
<span><p>The scale, variety, and quantity of publicly-available NLP datasets has grown
rapidly as researchers propose new tasks, larger models, and novel benchmarks.
Datasets is a community library for contemporary NLP designed to support this
ecosystem. Datasets aims to standardize end-user interfaces, versioning, and
documentation, while providing a lightweight front-end that behaves similarly
for small datasets as for internet-scale corpora. The design of the library
incorporates a distributed, community-driven approach to adding datasets and
documenting usage. After a year of development, the library now includes more
than 650 unique datasets, has more than 250 contributors, and has helped
support a variety of novel cross-dataset research projects and shared tasks.
The library is available at https://github.com/huggingface/datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Regular Expressions with Neural Networks via DFA. (arXiv:2109.02882v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02882">
<div class="article-summary-box-inner">
<span><p>Human-designed rules are widely used to build industry applications. However,
it is infeasible to maintain thousands of such hand-crafted rules. So it is
very important to integrate the rule knowledge into neural networks to build a
hybrid model that achieves better performance. Specifically, the human-designed
rules are formulated as Regular Expressions (REs), from which the equivalent
Minimal Deterministic Finite Automatons (MDFAs) are constructed. We propose to
use the MDFA as an intermediate model to capture the matched RE patterns as
rule-based features for each input sentence and introduce these additional
features into neural networks. We evaluate the proposed method on the ATIS
intent classification task. The experiment results show that the proposed
method achieves the best performance compared to neural networks and four other
methods that combine REs and neural networks when the training dataset is
relatively small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages. (arXiv:2109.02903v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02903">
<div class="article-summary-box-inner">
<span><p>In this paper we present IndicBART, a multilingual, sequence-to-sequence
pre-trained model focusing on 11 Indic languages and English. Different from
existing pre-trained models, IndicBART utilizes the orthographic similarity
between Indic scripts to improve transfer learning between similar Indic
languages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation
(NMT) and extreme summarization. Our experiments on NMT for 12 language pairs
and extreme summarization for 7 languages using multilingual fine-tuning show
that IndicBART is competitive with or better than mBART50 despite containing
significantly fewer parameters. Our analyses focus on identifying the impact of
script unification (to Devanagari), corpora size as well as multilingualism on
the final performance. The IndicBART model is available under the MIT license
at https://indicnlp.ai4bharat.org/indic-bart .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Reasoning Chains for Multi-hop Science Question Answering. (arXiv:2109.02905v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02905">
<div class="article-summary-box-inner">
<span><p>We propose a novel Chain Guided Retriever-reader ({\tt CGR}) framework to
model the reasoning chain for multi-hop Science Question Answering. Our
framework is capable of performing explainable reasoning without the need of
any corpus-specific annotations, such as the ground-truth reasoning chain, or
human-annotated entity mentions. Specifically, we first generate reasoning
chains from a semantic graph constructed by Abstract Meaning Representation of
retrieved evidence facts. A \textit{Chain-aware loss}, concerning both local
and global chain information, is also designed to enable the generated chains
to serve as distant supervision signals for training the retriever, where
reinforcement learning is also adopted to maximize the utility of the reasoning
chains. Our framework allows the retriever to capture step-by-step clues of the
entire reasoning process, which is not only shown to be effective on two
challenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge,
but also favors explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Driven Content Creation using Statistical and Natural Language Processing Techniques for Financial Domain. (arXiv:2109.02935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02935">
<div class="article-summary-box-inner">
<span><p>Over the years customers' expectation of getting information instantaneously
has given rise to the increased usage of channels like virtual assistants.
Typically, customers try to get their questions answered by low-touch channels
like search and virtual assistant first, before getting in touch with a live
chat agent or the phone representative. Higher usage of these low-touch systems
is a win-win for both customers and the organization since it enables
organizations to attain a low cost of service while customers get served
without delay. In this paper, we propose a two-part framework where the first
part describes methods to combine the information from different interaction
channels like call, search, and chat. We do this by summarizing (using a
stacked Bi-LSTM network) the high-touch interaction channel data such as call
and chat into short searchquery like customer intents and then creating an
organically grown intent taxonomy from interaction data (using Hierarchical
Agglomerative Clustering). The second part of the framework focuses on
extracting customer questions by analyzing interaction data sources. It
calculates similarity scores using TF-IDF and BERT(Devlin et al., 2019). It
also maps these identified questions to the output of the first part of the
framework using syntactic and semantic similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues using BERT. (arXiv:2109.02938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02938">
<div class="article-summary-box-inner">
<span><p>This paper presents an automatic method to evaluate the naturalness of
natural language generation in dialogue systems. While this task was previously
rendered through expensive and time-consuming human labor, we present this
novel task of automatic naturalness evaluation of generated language. By
fine-tuning the BERT model, our proposed naturalness evaluation method shows
robust results and outperforms the baselines: support vector machines,
bi-directional LSTMs, and BLEURT. In addition, the training speed and
evaluation performance of naturalness model are improved by transfer learning
from quality and informativeness linguistic knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02941">
<div class="article-summary-box-inner">
<span><p>Online hate speech has caught everyone's attention from the news related to
the COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -
an umbrella term for online hateful behavior, manifests itself in forms such as
online hate speech. Hate speech is a deliberate attack directed towards an
individual or a group motivated by the targeted entity's identity or opinions.
The rising mass communication through social media further exacerbates the
harmful consequences of online hate speech. While there has been significant
research on hate-speech identification using Natural Language Processing (NLP),
the work on utilizing NLP for prevention and intervention of online hate speech
lacks relatively. This paper presents a holistic conceptual framework on
hate-speech NLP countering methods along with a thorough survey on the current
progress of NLP for countering online hate speech. It classifies the countering
techniques based on their time of action, and identifies potential future
research areas on this topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrase Generation as Unsupervised Machine Translation. (arXiv:2109.02950v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02950">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new paradigm for paraphrase generation by
treating the task as unsupervised machine translation (UMT) based on the
assumption that there must be pairs of sentences expressing the same meaning in
a large-scale unlabeled monolingual corpus. The proposed paradigm first splits
a large unlabeled corpus into multiple clusters, and trains multiple UMT models
using pairs of these clusters. Then based on the paraphrase pairs produced by
these UMT models, a unified surrogate model can be trained to serve as the
final Seq2Seq model to generate paraphrases, which can be directly used for
test in the unsupervised setup, or be finetuned on labeled datasets in the
supervised setup. The proposed method offers merits over
machine-translation-based paraphrase generation methods, as it avoids reliance
on bilingual sentence pairs. It also allows human intervene with the model so
that more diverse paraphrases can be generated using different filtering
criteria. Extensive experiments on existing paraphrase dataset for both the
supervised and unsupervised setups demonstrate the effectiveness the proposed
paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FH-SWF SG at GermEval 2021: Using Transformer-Based Language Models to Identify Toxic, Engaging, & Fact-Claiming Comments. (arXiv:2109.02966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02966">
<div class="article-summary-box-inner">
<span><p>In this paper we describe the methods we used for our submissions to the
GermEval 2021 shared task on the identification of toxic, engaging, and
fact-claiming comments. For all three subtasks we fine-tuned freely available
transformer-based models from the Huggingface model hub. We evaluated the
performance of various pre-trained models after fine-tuning on 80% of the
training data with different hyperparameters and submitted predictions of the
two best performing resulting models. We found that this approach worked best
for subtask 3, for which we achieved an F1-score of 0.736.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02972">
<div class="article-summary-box-inner">
<span><p>Despite constant improvements in machine translation quality, automatic
poetry translation remains a challenging problem due to the lack of
open-sourced parallel poetic corpora, and to the intrinsic complexities
involved in preserving the semantics, style, and figurative nature of poetry.
We present an empirical investigation for poetry translation along several
dimensions: 1) size and style of training data (poetic vs. non-poetic),
including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)
language-family-specific models vs. mixed-multilingual models. To accomplish
this, we contribute a parallel dataset of poetry translations for several
language pairs. Our results show that multilingual fine-tuning on poetic text
significantly outperforms multilingual fine-tuning on non-poetic text that is
35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and
human evaluation metrics such as faithfulness (meaning and poetic style).
Moreover, multilingual fine-tuning on poetic data outperforms \emph{bilingual}
fine-tuning on poetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Context Choices for Context-aware Machine Translation. (arXiv:2109.02995v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02995">
<div class="article-summary-box-inner">
<span><p>One of the most popular methods for context-aware machine translation (MT) is
to use separate encoders for the source sentence and context as multiple
sources for one target sentence. Recent work has cast doubt on whether these
models actually learn useful signals from the context or are improvements in
automatic evaluation metrics just a side-effect. We show that multi-source
transformer models improve MT over standard transformer-base models even with
empty lines provided as context, but the translation quality improves
significantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is
provided. We also show that even though randomly shuffling in-domain context
can also improve over baselines, the correct context further improves
translation quality and random out-of-domain context further degrades it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge. (arXiv:2109.03004v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03004">
<div class="article-summary-box-inner">
<span><p>One challenge for dialogue agents is to recognize feelings of the
conversation partner and respond accordingly. In this work, RoBERTa-GPT2 is
proposed for empathetic dialogue generation, where the pre-trained
auto-encoding RoBERTa is utilised as encoder and the pre-trained
auto-regressive GPT-2 as decoder. With the combination of the pre-trained
RoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.
To enable the empathetic ability of RoBERTa-GPT2 model, we propose a
commonsense knowledge and emotional concepts extractor, in which the
commonsensible and emotional concepts of dialogue context are extracted for the
GPT-2 decoder. The experiment results demonstrate that the empathetic dialogue
generation benefits from both pre-trained encoder-decoder architecture and
external knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Attention Module for Natural Language Processing. (arXiv:2109.03009v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03009">
<div class="article-summary-box-inner">
<span><p>Recently, large pre-trained neural language models have attained remarkable
performance on many downstream natural language processing (NLP) applications
via fine-tuning. In this paper, we target at how to further improve the token
representations on the language models. We, therefore, propose a simple yet
effective plug-and-play module, Sequential Attention Module (SAM), on the token
embeddings learned from a pre-trained language model. Our proposed SAM consists
of two main attention modules deployed sequentially: Feature-wise Attention
Module (FAM) and Token-wise Attention Module (TAM). More specifically, FAM can
effectively identify the importance of features at each dimension and promote
the effect via dot-product on the original token embeddings for downstream NLP
applications. Meanwhile, TAM can further re-weight the features at the
token-wise level. Moreover, we propose an adaptive filter on FAM to prevent
noise impact and increase information absorption. Finally, we conduct extensive
experiments to demonstrate the advantages and properties of our proposed SAM.
We first show how SAM plays a primary role in the champion solution of two
subtasks of SemEval'21 Task 7. After that, we apply SAM on sentiment analysis
and three popular NLP tasks and demonstrate that SAM consistently outperforms
the state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generate & Rank: A Multi-task Framework for Math Word Problems. (arXiv:2109.03034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03034">
<div class="article-summary-box-inner">
<span><p>Math word problem (MWP) is a challenging and critical task in natural
language processing. Many recent studies formalize MWP as a generation task and
have adopted sequence-to-sequence models to transform problem descriptions to
mathematical expressions. However, mathematical expressions are prone to minor
mistakes while the generation objective does not explicitly handle such
mistakes. To address this limitation, we devise a new ranking task for MWP and
propose Generate &amp; Rank, a multi-task framework based on a generative
pre-trained language model. By joint training with generation and ranking, the
model learns from its own mistakes and is able to distinguish between correct
and incorrect expressions. Meanwhile, we perform tree-based disturbance
specially designed for MWP and an online update to boost the ranker. We
demonstrate the effectiveness of our proposed method on the benchmark and the
results show that our method consistently outperforms baselines in all
datasets. Particularly, in the classical Math23k, our method is 7% (78.4%
$\rightarrow$ 85.4%) higher than the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03039">
<div class="article-summary-box-inner">
<span><p>Conversational search systems, such as Google Assistant and Microsoft
Cortana, provide a new search paradigm where users are allowed, via natural
language dialogues, to communicate with search systems. Evaluating such systems
is very challenging since search results are presented in the format of natural
language sentences. Given the unlimited number of possible responses,
collecting relevance assessments for all the possible responses is infeasible.
In this paper, we propose POSSCORE, a simple yet effective automatic evaluation
method for conversational search. The proposed embedding-based metric takes the
influence of part of speech (POS) of the terms in the response into account. To
the best knowledge, our work is the first to systematically demonstrate the
importance of incorporating syntactic information, such as POS labels, for
conversational search evaluation. Experimental results demonstrate that our
metrics can correlate with human preference, achieving significant improvements
over state-of-the-art baseline metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning. (arXiv:2109.03062v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03062">
<div class="article-summary-box-inner">
<span><p>Multitask deep learning has been applied to patient outcome prediction from
text, taking clinical notes as input and training deep neural networks with a
joint loss function of multiple tasks. However, the joint training scheme of
multitask learning suffers from inter-task interference, and diagnosis
prediction among the multiple tasks has the generalizability issue due to rare
diseases or unseen diagnoses. To solve these challenges, we propose a
hypernetwork-based approach that generates task-conditioned parameters and
coefficients of multitask prediction heads to learn task-specific prediction
and balance the multitask learning. We also incorporate semantic task
information to improves the generalizability of our task-conditioned multitask
model. Experiments on early and discharge notes extracted from the real-world
MIMIC database show our method can achieve better performance on multitask
patient outcome prediction than strong baselines in most cases. Besides, our
method can effectively handle the scenario with limited information and improve
zero-shot prediction on unseen diagnosis categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation. (arXiv:2109.03079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03079">
<div class="article-summary-box-inner">
<span><p>Practical dialogue systems require robust methods of detecting out-of-scope
(OOS) utterances to avoid conversational breakdowns and related failure modes.
Directly training a model with labeled OOS examples yields reasonable
performance, but obtaining such data is a resource-intensive process. To tackle
this limited-data problem, previous methods focus on better modeling the
distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal
technique that augments existing data to train better OOS detectors operating
in low-data regimes. GOLD generates pseudo-labeled candidates using samples
from an auxiliary dataset and keeps only the most beneficial candidates for
training through a novel filtering mechanism. In experiments across three
target benchmarks, the top GOLD model outperforms all existing methods on all
key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median
baseline performance. We also analyze the unique properties of OOS data to
identify key factors for optimally applying our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning grounded word meaning representations on similarity graphs. (arXiv:2109.03084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03084">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel approach to learn visually grounded meaning
representations of words as low-dimensional node embeddings on an underlying
graph hierarchy. The lower level of the hierarchy models modality-specific word
representations through dedicated but communicating graphs, while the higher
level puts these representations together on a single graph to learn a
representation jointly from both modalities. The topology of each graph models
similarity relations among words, and is estimated jointly with the graph
embedding. The assumption underlying this model is that words sharing similar
meaning correspond to communities in an underlying similarity graph in a
low-dimensional space. We named this model Hierarchical Multi-Modal Similarity
Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE
to simulate human similarity judgements and concept categorization,
outperforming the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning. (arXiv:2109.03094v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03094">
<div class="article-summary-box-inner">
<span><p>The availability of language representations learned by large pretrained
neural network models (such as BERT and ELECTRA) has led to improvements in
many downstream Natural Language Processing tasks in recent years. Pretrained
models usually differ in pretraining objectives, architectures, and datasets
they are trained on which can affect downstream performance. In this
contribution, we fine-tuned German BERT and German ELECTRA models to identify
toxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3)
in Facebook data provided by the GermEval 2021 competition. We created
ensembles of these models and investigated whether and how classification
performance depends on the number of ensemble members and their composition. On
out-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for
all subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infusing Future Information into Monotonic Attention Through Language Models. (arXiv:2109.03121v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03121">
<div class="article-summary-box-inner">
<span><p>Simultaneous neural machine translation(SNMT) models start emitting the
target sequence before they have processed the source sequence. The recent
adaptive policies for SNMT use monotonic attention to perform read/write
decisions based on the partial source and target sequences. The lack of
sufficient information might cause the monotonic attention to take poor
read/write decisions, which in turn negatively affects the performance of the
SNMT model. On the other hand, human translators make better read/write
decisions since they can anticipate the immediate future words using linguistic
information and domain knowledge.Motivated by human translators, in this work,
we propose a framework to aid monotonic attention with an external language
model to improve its decisions.We conduct experiments on the MuST-C
English-German and English-French speech-to-text translation tasks to show the
effectiveness of the proposed framework.The proposed SNMT method improves the
quality-latency trade-off over the state-of-the-art monotonic multihead
attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rare Words Degenerate All Words. (arXiv:2109.03127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03127">
<div class="article-summary-box-inner">
<span><p>Despite advances in neural network language model, the representation
degeneration problem of embeddings is still challenging. Recent studies have
found that the learned output embeddings are degenerated into a narrow-cone
distribution which makes the similarity between each embeddings positive. They
analyzed the cause of the degeneration problem has been demonstrated as common
to most embeddings. However, we found that the degeneration problem is
especially originated from the training of embeddings of rare words. In this
study, we analyze the intrinsic mechanism of the degeneration of rare word
embeddings with respect of their gradient about the negative log-likelihood
loss function. Furthermore, we theoretically and empirically demonstrate that
the degeneration of rare word embeddings causes the degeneration of non-rare
word embeddings, and that the overall degeneration problem can be alleviated by
preventing the degeneration of rare word embeddings. Based on our analyses, we
propose a novel method, Adaptive Gradient Partial Scaling(AGPS), to address the
degeneration problem. Experimental results demonstrate the effectiveness of the
proposed method qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03137">
<div class="article-summary-box-inner">
<span><p>Existing generative pre-trained language models (e.g., GPT) focus on modeling
the language structure and semantics of general texts. However, those models do
not consider the numerical properties of numbers and cannot perform robustly on
numerical reasoning tasks (e.g., math word problems and measurement
estimation). In this paper, we propose NumGPT, a generative pre-trained model
that explicitly models the numerical properties of numbers in texts.
Specifically, it leverages a prototype-based numeral embedding to encode the
mantissa of the number and an individual embedding to encode the exponent of
the number. A numeral-aware loss function is designed to integrate numerals
into the pre-training objective of NumGPT. We conduct extensive experiments on
four different datasets to evaluate the numeracy ability of NumGPT. The
experiment results show that NumGPT outperforms baseline models (e.g., GPT and
GPT with DICE) on a range of numerical reasoning tasks such as measurement
estimation, number comparison, math word problems, and magnitude
classification. Ablation studies are also conducted to evaluate the impact of
pre-training and model hyperparameters on the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03155">
<div class="article-summary-box-inner">
<span><p>Sentence embedding refers to a set of effective and versatile techniques for
converting raw text into numerical vector representations that can be used in a
wide range of natural language processing (NLP) applications. The majority of
these techniques are either supervised or unsupervised. Compared to the
unsupervised methods, the supervised ones make less assumptions about
optimization objectives and usually achieve better results. However, the
training requires a large amount of labeled sentence pairs, which is not
available in many industrial scenarios. To that end, we propose a generic and
end-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence
Embedding), capable of learning high-quality sentence embeddings from a
partially labeled dataset. We experimentally show that PAUSE achieves, and
sometimes surpasses, state-of-the-art results using only a small fraction of
labeled sentence pairs on various benchmark tasks. When applied to a real
industrial use case where labeled samples are scarce, PAUSE encourages us to
extend our dataset without the liability of extensive manual annotation work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03158">
<div class="article-summary-box-inner">
<span><p>An individual's variation in writing style is often a function of both social
and personal attributes. While structured social variation has been extensively
studied, e.g., gender based variation, far less is known about how to
characterize individual styles due to their idiosyncratic nature. We introduce
a new approach to studying idiolects through a massive cross-author comparison
to identify and encode stylistic features. The neural model achieves strong
performance at authorship identification on short texts and through an
analogy-based probing task, showing that the learned representations exhibit
surprising regularities that encode qualitative and quantitative shifts of
idiolectal styles. Through text perturbation, we quantify the relative
contributions of different linguistic elements to idiolectal variation.
Furthermore, we provide a description of idiolects through measuring inter- and
intra-author variation, showing that variation in idiolects is often
distinctive yet consistent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How much pretraining data do language models need to learn syntax?. (arXiv:2109.03160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03160">
<div class="article-summary-box-inner">
<span><p>Transformers-based pretrained language models achieve outstanding results in
many well-known NLU benchmarks. However, while pretraining methods are very
convenient, they are expensive in terms of time and resources. This calls for a
study of the impact of pretraining data size on the knowledge of the models. We
explore this impact on the syntactic capabilities of RoBERTa, using models
trained on incremental sizes of raw text data. First, we use syntactic
structural probes to determine whether models pretrained on more data encode a
higher amount of syntactic information. Second, we perform a targeted syntactic
evaluation to analyze the impact of pretraining data size on the syntactic
generalization performance of the models. Third, we compare the performance of
the different models on three downstream applications: part-of-speech tagging,
dependency parsing and paraphrase identification. We complement our study with
an analysis of the cost-benefit trade-off of training such models. Our
experiments show that while models pretrained on more data encode more
syntactic knowledge and perform better on downstream applications, they do not
always offer a better performance across the different syntactic phenomena and
come at a higher financial and environmental cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-Controllable Opinion Summarization. (arXiv:2109.03171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03171">
<div class="article-summary-box-inner">
<span><p>Recent work on opinion summarization produces general summaries based on a
set of input reviews and the popularity of opinions expressed in them. In this
paper, we propose an approach that allows the generation of customized
summaries based on aspect queries (e.g., describing the location and room of a
hotel). Using a review corpus, we create a synthetic training dataset of
(review, summary) pairs enriched with aspect controllers which are induced by a
multi-instance learning model that predicts the aspects of a document at
different levels of granularity. We fine-tune a pretrained model using our
synthetic dataset and generate aspect-specific summaries by modifying the
aspect controllers. Experiments on two benchmarks show that our model
outperforms the previous state of the art and generates personalized summaries
by controlling the number of aspects discussed in them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When differential privacy meets NLP: The devil is in the detail. (arXiv:2109.03175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03175">
<div class="article-summary-box-inner">
<span><p>Differential privacy provides a formal approach to privacy of individuals.
Applications of differential privacy in various scenarios, such as protecting
users' original utterances, must satisfy certain mathematical properties. Our
contribution is a formal analysis of ADePT, a differentially private
auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising
results on downstream tasks while providing tight privacy guarantees. Our proof
reveals that ADePT is not differentially private, thus rendering the
experimental results unsubstantiated. We also quantify the impact of the error
in its private mechanism, showing that the true sensitivity is higher by at
least factor 6 in an optimistic case of a very small encoder's dimension and
that the amount of utterances that are not privatized could easily reach 100%
of the entire dataset. Our intention is neither to criticize the authors, nor
the peer-reviewing process, but rather point out that if differential privacy
applications in NLP rely on formal guarantees, these should be outlined in full
and put under detailed scrutiny.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Conversation Disentanglement through Co-Training. (arXiv:2109.03199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03199">
<div class="article-summary-box-inner">
<span><p>Conversation disentanglement aims to separate intermingled messages into
detached sessions, which is a fundamental task in understanding multi-party
conversations. Existing work on conversation disentanglement relies heavily
upon human-annotated datasets, which are expensive to obtain in practice. In
this work, we explore to train a conversation disentanglement model without
referencing any human annotations. Our method is built upon a deep co-training
algorithm, which consists of two neural networks: a message-pair classifier and
a session classifier. The former is responsible for retrieving local relations
between two messages while the latter categorizes a message to a session by
capturing context-aware information. Both networks are initialized respectively
with pseudo data built from an unannotated corpus. During the deep co-training
process, we use the session classifier as a reinforcement learning component to
learn a session assigning policy by maximizing the local rewards given by the
message-pair classifier. For the message-pair classifier, we enrich its
training data by retrieving message pairs with high confidence from the
disentangled sessions predicted by the session classifier. Experimental results
on the large Movie Dialogue Dataset demonstrate that our proposed approach
achieves competitive performance compared to the previous supervised methods.
Further experiments show that the predicted disentangled conversations can
promote the performance on the downstream task of multi-party response
selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03200">
<div class="article-summary-box-inner">
<span><p>The increasing use of social media sites in countries like India has given
rise to large volumes of code-mixed data. Sentiment analysis of this data can
provide integral insights into people's perspectives and opinions. Developing
robust explainability techniques which explain why models make their
predictions becomes essential. In this paper, we propose an adequate
methodology to integrate explainable approaches into code-mixed sentiment
analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint model for intent and entity recognition. (arXiv:2109.03221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03221">
<div class="article-summary-box-inner">
<span><p>The semantic understanding of natural dialogues composes of several parts.
Some of them, like intent classification and entity detection, have a crucial
role in deciding the next steps in handling user input. Handling each task as
an individual problem can be wasting of training resources, and also each
problem can benefit from each other. This paper tackles these problems as one.
Our new model, which combine intent and entity recognition into one system, is
achieving better metrics in both tasks with lower training requirements than
solving each task separately. We also optimize the model based on the inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03228">
<div class="article-summary-box-inner">
<span><p>Recent studies on compression of pretrained language models (e.g., BERT)
usually use preserved accuracy as the metric for evaluation. In this paper, we
propose two new metrics, label loyalty and probability loyalty that measure how
closely a compressed model (i.e., student) mimics the original model (i.e.,
teacher). We also explore the effect of compression with regard to robustness
under adversarial attacks. We benchmark quantization, pruning, knowledge
distillation and progressive module replacing with loyalty and robustness. By
combining multiple compression techniques, we provide a practical strategy to
achieve better accuracy, loyalty and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06438">
<div class="article-summary-box-inner">
<span><p>Advertising is an important revenue source for many companies. However, it is
expensive to manually create advertisements that meet the needs of various
queries for massive items. In this paper, we propose the query-variant
advertisement text generation task that aims to generate candidate
advertisements for different queries with various needs given the item
keywords. In this task, for many different queries there is only one general
purposed advertisement with no predefined query-advertisement pair, which would
discourage traditional End-to-End models from generating query-variant
advertisements for different queries with different needs. To deal with the
problem, we propose a query-variant advertisement text generation model that
takes keywords and associated external knowledge as input during training and
adds different queries during inference. Adding external knowledge helps the
model adapted to the information besides the item keywords during training,
which makes the transition between training and inference more smoothing when
the query is added during inference. Both automatic and human evaluation show
that our model can generate more attractive and query-focused advertisements
than the strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12835">
<div class="article-summary-box-inner">
<span><p>This paper shows that, modern word embeddings contain information that
distinguishes synonyms and antonyms despite small cosine similarities between
corresponding vectors. This information is encoded in the geometry of the
embeddings and could be extracted with a straight-forward and intuitive
manifold learning procedure or a contrasting map. Such a map is trained on a
small labeled subset of the data and can produce new embeddings that explicitly
highlight specific semantic attributes of the word. The new embeddings produced
by the map are shown to improve the performance on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13401">
<div class="article-summary-box-inner">
<span><p>Recent successes in deep generative modeling have led to significant advances
in natural language generation (NLG). Incorporating entities into neural
generation models has demonstrated great improvements by assisting to infer the
summary topic and to generate coherent content. To enhance the role of entity
in NLG, in this paper, we aim to model the entity type in the decoding phase to
generate contextual words accurately. We develop a novel NLG model to produce a
target sequence based on a given list of entities. Our model has a multi-step
decoder that injects the entity types into the process of entity mention
generation. Experiments on two public news datasets demonstrate type injection
performs better than existing type embedding concatenation baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Synthetic Data Improves Neural Machine Translation withKnowledge Distillation. (arXiv:2012.15455v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15455">
<div class="article-summary-box-inner">
<span><p>This paper explores augmenting monolingual data for knowledge distillation in
neural machine translation. Source language monolingual text can be
incorporated as a forward translation. Interestingly, we find the best way to
incorporate target language monolingual text is to translate it to the source
language and round-trip translate it back to the target language, resulting in
a fully synthetic corpus. We find that combining monolingual data from both
source and target languages yields better performance than a corpus twice as
large only in one language. Moreover, experiments reveal that the improvement
depends upon the provenance of the test set. If the test set was originally in
the source language (with the target side written by translators), then forward
translating source monolingual data matters. If the test set was originally in
the target language (with the source written by translators), then
incorporating target monolingual data matters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00234">
<div class="article-summary-box-inner">
<span><p>Transformers have shown improved performance when compared to previous
architectures for sequence processing such as RNNs. Despite their sizeable
performance gains, as recently suggested, the model is computationally
expensive to train and with a high parameter budget. In light of this, we
explore parameter-sharing methods in Transformers with a specific focus on
generative models. We perform an analysis of different parameter
sharing/reduction methods and develop the Subformer. Our model combines
sandwich-style parameter sharing, which overcomes naive cross-layer parameter
sharing in generative models, and self-attentive embedding factorization
(SAFE). Experiments on machine translation, abstractive summarization and
language modeling show that the Subformer can outperform the Transformer even
when using significantly fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Improving Coherence and Diversity of Slogan Generation. (arXiv:2102.05924v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05924">
<div class="article-summary-box-inner">
<span><p>Previous work in slogan generation focused on utilising slogan skeletons
mined from existing slogans. While some generated slogans can be catchy, they
are often not coherent with the company's focus or style across their marketing
communications because the skeletons are mined from other companies' slogans.
We propose a sequence-to-sequence (seq2seq) transformer model to generate
slogans from a brief company description. A naive seq2seq model fine-tuned for
slogan generation is prone to introducing false information. We use company
name delexicalisation and entity masking to alleviate this problem and improve
the generated slogans' quality and truthfulness. Furthermore, we apply
conditional training based on the first words' POS tag to generate
syntactically diverse slogans. Our best model achieved a ROUGE-1/-2/-L F1 score
of 35.58/18.47/33.32. Besides, automatic and human evaluations indicate that
our method generates significantly more factual, diverse and catchy slogans
than strong LSTM and transformer seq2seq baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings. (arXiv:2103.03598v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03598">
<div class="article-summary-box-inner">
<span><p>Intersectional bias is a bias caused by an overlap of multiple social factors
like gender, sexuality, race, disability, religion, etc. A recent study has
shown that word embedding models can be laden with biases against
intersectional groups like African American females, etc. The first step
towards tackling such intersectional biases is to identify them. However,
discovering biases against different intersectional groups remains a
challenging task. In this work, we present WordBias, an interactive visual tool
designed to explore biases against intersectional groups encoded in static word
embeddings. Given a pretrained static word embedding, WordBias computes the
association of each word along different groups based on race, age, etc. and
then visualizes them using a novel interactive interface. Using a case study,
we demonstrate how WordBias can help uncover biases against intersectional
groups like Black Muslim Males, Poor Females, etc. encoded in word embedding.
In addition, we also evaluate our tool using qualitative feedback from expert
interviews. The source code for this tool can be publicly accessed for
reproducibility at github.com/bhavyaghai/WordBias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-Permuted Paragraph Generation. (arXiv:2104.07228v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07228">
<div class="article-summary-box-inner">
<span><p>Generating paragraphs of diverse contents is important in many applications.
Existing generation models produce similar contents from homogenized contexts
due to the fixed left-to-right sentence order. Our idea is permuting the
sentence orders to improve the content diversity of multi-sentence paragraph.
We propose a novel framework PermGen whose objective is to maximize the
expected log-likelihood of output paragraph distributions with respect to all
possible sentence orders. PermGen uses hierarchical positional embedding and
designs new procedures for training, decoding, and candidate ranking in the
sentence-permuted generation. Experiments on three paragraph generation
benchmarks demonstrate PermGen generates more diverse outputs with a higher
quality than existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation. (arXiv:2104.07555v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07555">
<div class="article-summary-box-inner">
<span><p>QuestEval is a reference-less metric used in text-to-text tasks, that
compares the generated summaries directly to the source text, by automatically
asking and answering questions. Its adaptation to Data-to-Text tasks is not
straightforward, as it requires multimodal Question Generation and Answering
systems on the considered tasks, which are seldom available. To this purpose,
we propose a method to build synthetic multimodal corpora enabling to train
multimodal components for a data-QuestEval metric. The resulting metric is
reference-less and multimodal; it obtains state-of-the-art correlations with
human judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's
code and models available for reproducibility purpose, as part of the QuestEval
project.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03861">
<div class="article-summary-box-inner">
<span><p>Identifying political perspective in news media has become an important task
due to the rapid growth of political commentary and the increasingly polarized
ideologies. Previous approaches only focus on leveraging the semantic
information and leaves out the rich social and political context that helps
individuals understand political stances. In this paper, we propose a
perspective detection method that incorporates external knowledge of real-world
politics. Specifically, we construct a contemporary political knowledge graph
with 1,071 entities and 10,703 triples. We then build a heterogeneous
information network for each news document that jointly models article
semantics and external knowledge in knowledge graphs. Finally, we apply gated
relational graph convolutional networks and conduct political perspective
detection as graph-level classification. Extensive experiments show that our
method achieves the best performance and outperforms state-of-the-art methods
by 5.49%. Numerous ablation studies further bear out the necessity of external
knowledge and the effectiveness of our graph-based approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03881">
<div class="article-summary-box-inner">
<span><p>Political stance detection has become an important task due to the
increasingly polarized political ideologies. Most existing works focus on
identifying perspectives in news articles or social media posts, while social
entities, such as individuals and organizations, produce these texts and
actually take stances. In this paper, we propose the novel task of entity
stance prediction, which aims to predict entities' stances given their social
and political context. Specifically, we retrieve facts from Wikipedia about
social entities regarding contemporary U.S. politics. We then annotate social
entities' stances towards political ideologies with the help of domain experts.
After defining the task of entity stance prediction, we propose a graph-based
solution, which constructs a heterogeneous information network from collected
facts and adopts gated relational graph convolutional networks for
representation learning. Our model is then trained with a combination of
supervised, self-supervised and unsupervised loss functions, which are
motivated by multiple social and political phenomenons. We conduct extensive
experiments to compare our method with existing text and graph analysis
baselines. Our model achieves highest stance detection accuracy and yields
inspiring insights regarding social entity stances. We further conduct ablation
study and parameter analysis to study the mechanism and effectiveness of our
proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13140">
<div class="article-summary-box-inner">
<span><p>While deep learning models have greatly improved the performance of most
artificial intelligence tasks, they are often criticized to be untrustworthy
due to the black-box problem. Consequently, many works have been proposed to
study the trustworthiness of deep learning. However, as most open datasets are
designed for evaluating the accuracy of model outputs, there is still a lack of
appropriate datasets for evaluating the inner workings of neural networks. The
lack of datasets obviously hinders the development of trustworthiness research.
Therefore, in order to systematically evaluate the factors for building
trustworthy systems, we propose a novel and well-annotated sentiment analysis
dataset to evaluate robustness and interpretability. To evaluate these factors,
our dataset contains diverse annotations about the challenging distribution of
instances, manual adversarial instances and sentiment explanations. Several
evaluation metrics are further proposed for interpretability and robustness.
Based on the dataset and metrics, we conduct comprehensive comparisons for the
trustworthiness of three typical models, and also study the relations between
accuracy, robustness and interpretability. We release this trustworthiness
evaluation dataset at \url{https://github/xyz} and hope our work can facilitate
the progress on building more trustworthy systems for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00122">
<div class="article-summary-box-inner">
<span><p>The sheer volume of financial statements makes it difficult for humans to
access and analyze a business's financials. Robust numerical reasoning likewise
faces unique challenges in this domain. In this work, we focus on answering
deep questions over financial data, aiming to automate the analysis of a large
corpus of financial documents. In contrast to existing tasks on general domain,
the finance domain includes complex numerical reasoning and understanding of
heterogeneous representations. To facilitate analytical progress, we propose a
new large-scale dataset, FinQA, with Question-Answering pairs over Financial
reports, written by financial experts. We also annotate the gold reasoning
programs to ensure full explainability. We further introduce baselines and
conduct comprehensive experiments in our dataset. The results demonstrate that
popular, large, pre-trained models fall far short of expert humans in acquiring
finance knowledge and in complex multi-step numerical reasoning on that
knowledge. Our dataset -- the first of its kind -- should therefore enable
significant, new community research into complex application domains. The
dataset and code are publicly available\url{https://github.com/czyssrs/FinQA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. (arXiv:2109.00904v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00904">
<div class="article-summary-box-inner">
<span><p>We introduce MULTI-EURLEX, a new multilingual dataset for topic
classification of legal documents. The dataset comprises 65k European Union
(EU) laws, officially translated in 23 languages, annotated with multiple
labels from the EUROVOC taxonomy. We highlight the effect of temporal concept
drift and the importance of chronological, instead of random splits. We use the
dataset as a testbed for zero-shot cross-lingual transfer, where we exploit
annotated training documents in one language (source) to classify documents in
another language (target). We find that fine-tuning a multilingually pretrained
model (XLM-ROBERTA, MT5) in a single source language leads to catastrophic
forgetting of multilingual knowledge and, consequently, poor zero-shot transfer
to other languages. Adaptation strategies, namely partial fine-tuning,
adapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new
end-tasks, help retain multilingual knowledge from pretraining, substantially
improving zero-shot cross-lingual transfer, but their impact also depends on
the pretrained model used and the size of the label set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction. (arXiv:2109.02403v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02403">
<div class="article-summary-box-inner">
<span><p>Aspect-level sentiment classification (ALSC) aims at identifying the
sentiment polarity of a specified aspect in a sentence. ALSC is a practical
setting in aspect-based sentiment analysis due to no opinion term labeling
needed, but it fails to interpret why a sentiment polarity is derived for the
aspect. To address this problem, recent works fine-tune pre-trained Transformer
encoders for ALSC to extract an aspect-centric dependency tree that can locate
the opinion words. However, the induced opinion words only provide an intuitive
cue far below human-level interpretability. Besides, the pre-trained encoder
tends to internalize an aspect's intrinsic sentiment, causing sentiment bias
and thus affecting model performance. In this paper, we propose a span-based
anti-bias aspect representation learning framework. It first eliminates the
sentiment bias in the aspect embedding by adversarial learning against aspects'
prior sentiment. Then, it aligns the distilled opinion candidates with the
aspect by span-based dependency modeling to highlight the interpretable opinion
terms. Our method achieves new state-of-the-art performance on five benchmarks,
with the capability of unsupervised opinion extraction.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02639">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection and lossless compression constitute two
problems that can be solved by the training of probabilistic models on a first
dataset with subsequent likelihood evaluation on a second dataset, where data
distributions differ. By defining the generalization of probabilistic models in
terms of likelihood we show that, in the case of image models, the OOD
generalization ability is dominated by local features. This motivates our
proposal of a Local Autoregressive model that exclusively models local image
features towards improving OOD performance. We apply the proposed model to OOD
detection tasks and achieve state-of-the-art unsupervised OOD detection
performance without the introduction of additional data. Additionally, we
employ our model to build a new lossless image compressor: NeLLoC (Neural Local
Lossless Compressor) and report state-of-the-art compression rates and model
size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End to end hyperspectral imaging system with coded compression imaging process. (arXiv:2109.02643v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02643">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images (HSIs) can provide rich spatial and spectral information
with extensive application prospects. Recently, several methods using
convolutional neural networks (CNNs) to reconstruct HSIs have been developed.
However, most deep learning methods fit a brute-force mapping relationship
between the compressive and standard HSIs. Thus, the learned mapping would be
invalid when the observation data deviate from the training data. To recover
the three-dimensional HSIs from two-dimensional compressive images, we present
dual-camera equipment with a physics-informed self-supervising CNN method based
on a coded aperture snapshot spectral imaging system. Our method effectively
exploits the spatial-spectral relativization from the coded spectral
information and forms a self-supervising system based on the camera quantum
effect model. The experimental results show that our method can be adapted to a
wide imaging environment with good performance. In addition, compared with most
of the network-based methods, our system does not require a dedicated dataset
for pre-training. Therefore, it has greater scenario adaptability and better
generalization ability. Meanwhile, our system can be constantly fine-tuned and
self-improved in real-life scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Crowdsourcing Annotation: Partial Annotation with Salient Labels for Multi-Label Image Classification. (arXiv:2109.02688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02688">
<div class="article-summary-box-inner">
<span><p>Annotated images are required for both supervised model training and
evaluation in image classification. Manually annotating images is arduous and
expensive, especially for multi-labeled images. A recent trend for conducting
such laboursome annotation tasks is through crowdsourcing, where images are
annotated by volunteers or paid workers online (e.g., workers of Amazon
Mechanical Turk) from scratch. However, the quality of crowdsourcing image
annotations cannot be guaranteed, and incompleteness and incorrectness are two
major concerns for crowdsourcing annotations. To address such concerns, we have
a rethinking of crowdsourcing annotations: Our simple hypothesis is that if the
annotators only partially annotate multi-label images with salient labels they
are confident in, there will be fewer annotation errors and annotators will
spend less time on uncertain labels. As a pleasant surprise, with the same
annotation budget, we show a multi-label image classifier supervised by images
with salient annotations can outperform models supervised by fully annotated
images. Our method contributions are 2-fold: An active learning way is proposed
to acquire salient labels for multi-label images; and a novel Adaptive
Temperature Associated Model (ATAM) specifically using partial annotations is
proposed for multi-label image classification. We conduct experiments on
practical crowdsourcing data, the Open Street Map (OSM) dataset and benchmark
dataset COCO 2014. When compared with state-of-the-art classification methods
trained on fully annotated images, the proposed ATAM can achieve higher
accuracy. The proposed idea is promising for crowdsourcing data annotation. Our
code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Transferability of Domain Adaptation Networks Through Domain Alignment Layers. (arXiv:2109.02693v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02693">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) has been the primary approach used in various computer
vision tasks due to its relevant results achieved on many tasks. However, on
real-world scenarios with partially or no labeled data, DL methods are also
prone to the well-known domain shift problem. Multi-source unsupervised domain
adaptation (MSDA) aims at learning a predictor for an unlabeled domain by
assigning weak knowledge from a bag of source models. However, most works
conduct domain adaptation leveraging only the extracted features and reducing
their domain shift from the perspective of loss function designs. In this
paper, we argue that it is not sufficient to handle domain shift only based on
domain-level features, but it is also essential to align such information on
the feature space. Unlike previous works, we focus on the network design and
propose to embed Multi-Source version of DomaIn Alignment Layers (MS-DIAL) at
different levels of the predictor. These layers are designed to match the
feature distributions between different domains and can be easily applied to
various MSDA methods. To show the robustness of our approach, we conducted an
extensive experimental evaluation considering two challenging scenarios: digit
recognition and object classification. The experimental results indicated that
our approach can improve state-of-the-art MSDA methods, yielding relative gains
of up to +30.64% on their classification accuracies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intelligent Motion Planning for a Cost-effective Object Follower Mobile Robotic System with Obstacle Avoidance. (arXiv:2109.02700v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02700">
<div class="article-summary-box-inner">
<span><p>There are few industries which use manually controlled robots for carrying
material and this cannot be used all the time in all the places. So, it is very
tranquil to have robots which can follow a specific human by following the
unique coloured object held by that person. So, we propose a robotic system
which uses robot vision and deep learning to get the required linear and
angular velocities which are {\nu} and {\omega}, respectively. Which in turn
makes the robot to avoid obstacles when following the unique coloured object
held by the human. The novel methodology that we are proposing is accurate in
detecting the position of the unique coloured object in any kind of lighting
and tells us the horizontal pixel value where the robot is present and also
tells if the object is close to or far from the robot. Moreover, the artificial
neural networks that we have used in this problem gave us a meagre error in
linear and angular velocity prediction and the PI controller which was used to
control the linear and angular velocities, which in turn controls the position
of the robot gave us impressive results and this methodology outperforms all
other methodologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crash Report Data Analysis for Creating Scenario-Wise, Spatio-Temporal Attention Guidance to Support Computer Vision-based Perception of Fatal Crash Risks. (arXiv:2109.02710v1 [stat.AP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02710">
<div class="article-summary-box-inner">
<span><p>Reducing traffic fatalities and serious injuries is a top priority of the US
Department of Transportation. The computer vision (CV)-based crash anticipation
in the near-crash phase is receiving growing attention. The ability to perceive
fatal crash risks earlier is also critical because it will improve the
reliability of crash anticipation. Yet, annotated image data for training a
reliable AI model for the early visual perception of crash risks are not
abundant. The Fatality Analysis Reporting System contains big data of fatal
crashes. It is a reliable data source for learning the relationship between
driving scene characteristics and fatal crashes to compensate for the
limitation of CV. Therefore, this paper develops a data analytics model, named
scenario-wise, Spatio-temporal attention guidance, from fatal crash report
data, which can estimate the relevance of detected objects to fatal crashes
from their environment and context information. First, the paper identifies
five sparse variables that allow for decomposing the 5-year fatal crash dataset
to develop scenario-wise attention guidance. Then, exploratory analysis of
location- and time-related variables of the crash report data suggests reducing
fatal crashes to spatially defined groups. The group's temporal pattern is an
indicator of the similarity of fatal crashes in the group. Hierarchical
clustering and K-means clustering merge the spatially defined groups into six
clusters according to the similarity of their temporal patterns. After that,
association rule mining discovers the statistical relationship between the
temporal information of driving scenes with crash features, for each cluster.
The paper shows how the developed attention guidance supports the design and
implementation of a preliminary CV model that can identify objects of a
possibility to involve in fatal crashes from their environment and context
information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Attention Layer Evolves Semantic Segmentation for Road Pothole Detection: A Benchmark and Algorithms. (arXiv:2109.02711v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02711">
<div class="article-summary-box-inner">
<span><p>Existing road pothole detection approaches can be classified as computer
vision-based or machine learning-based. The former approaches typically employ
2-D image analysis/understanding or 3-D point cloud modeling and segmentation
algorithms to detect road potholes from vision sensor data. The latter
approaches generally address road pothole detection using convolutional neural
networks (CNNs) in an end-to-end manner. However, road potholes are not
necessarily ubiquitous and it is challenging to prepare a large well-annotated
dataset for CNN training. In this regard, while computer vision-based methods
were the mainstream research trend in the past decade, machine learning-based
methods were merely discussed. Recently, we published the first stereo
vision-based road pothole detection dataset and a novel disparity
transformation algorithm, whereby the damaged and undamaged road areas can be
highly distinguished. However, there are no benchmarks currently available for
state-of-the-art (SoTA) CNNs trained using either disparity images or
transformed disparity images. Therefore, in this paper, we first discuss the
SoTA CNNs designed for semantic segmentation and evaluate their performance for
road pothole detection with extensive experiments. Additionally, inspired by
graph neural network (GNN), we propose a novel CNN layer, referred to as graph
attention layer (GAL), which can be easily deployed in any existing CNN to
optimize image feature representations for semantic segmentation. Our
experiments compare GAL-DeepLabv3+, our best-performing implementation, with
nine SoTA CNNs on three modalities of training data: RGB images, disparity
images, and transformed disparity images. The experimental results suggest that
our proposed GAL-DeepLabv3+ achieves the best overall pothole detection
accuracy on all training data modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images. (arXiv:2109.02716v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02716">
<div class="article-summary-box-inner">
<span><p>Crop and weed monitoring is an important challenge for agriculture and food
production nowadays. Thanks to recent advances in data acquisition and
computation technologies, agriculture is evolving to a more smart and precision
farming to meet with the high yield and high quality crop production.
Classification and recognition in Unmanned Aerial Vehicles (UAV) images are
important phases for crop monitoring. Advances in deep learning models relying
on Convolutional Neural Network (CNN) have achieved high performances in image
classification in the agricultural domain. Despite the success of this
architecture, CNN still faces many challenges such as high computation cost,
the need of large labelled datasets, ... Natural language processing's
transformer architecture can be an alternative approach to deal with CNN's
limitations. Making use of the self-attention paradigm, Vision Transformer
(ViT) models can achieve competitive or better results without applying any
convolution operations. In this paper, we adopt the self-attention mechanism
via the ViT models for plant classification of weeds and crops: red beet,
off-type beet (green leaves), parsley and spinach. Our experiments show that
with small set of labelled training data, ViT models perform better compared to
state-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy
of 99.8\% achieved by the ViT model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Landmarks Correspondence Detection in Medical Images with an Application to Deformable Image Registration. (arXiv:2109.02722v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02722">
<div class="article-summary-box-inner">
<span><p>Deformable Image Registration (DIR) can benefit from additional guidance
using corresponding landmarks in the images. However, the benefits thereof are
largely understudied, especially due to the lack of automatic detection methods
for corresponding landmarks in three-dimensional (3D) medical images. In this
work, we present a Deep Convolutional Neural Network (DCNN), called DCNN-Match,
that learns to predict landmark correspondences in 3D images in a
self-supervised manner. We explored five variants of DCNN-Match that use
different loss functions and tested DCNN-Match separately as well as in
combination with the open-source registration software Elastix to assess its
impact on a common DIR approach. We employed lower-abdominal Computed
Tomography (CT) scans from cervical cancer patients: 121 pelvic CT scan pairs
containing simulated elastic transformations and 11 pairs demonstrating
clinical deformations. Our results show significant improvement in DIR
performance when landmark correspondences predicted by DCNN-Match were used in
case of simulated as well as clinical deformations. We also observed that the
spatial distribution of the automatically identified landmarks and the
associated matching errors affect the extent of improvement in DIR. Finally,
DCNN-Match was found to generalize well to Magnetic Resonance Imaging (MRI)
scans without requiring retraining, indicating easy applicability to other
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Dietary Assessment Via Integrated Hierarchy Food Classification. (arXiv:2109.02736v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02736">
<div class="article-summary-box-inner">
<span><p>Image-based dietary assessment refers to the process of determining what
someone eats and how much energy and nutrients are consumed from visual data.
Food classification is the first and most crucial step. Existing methods focus
on improving accuracy measured by the rate of correct classification based on
visual information alone, which is very challenging due to the high complexity
and inter-class similarity of foods. Further, accuracy in food classification
is conceptual as description of a food can always be improved. In this work, we
introduce a new food classification framework to improve the quality of
predictions by integrating the information from multiple domains while
maintaining the classification accuracy. We apply a multi-task network based on
a hierarchical structure that uses both visual and nutrition domain specific
information to cluster similar foods. Our method is validated on the modified
VIPER-FoodNet (VFN) food image dataset by including associated energy and
nutrient information. We achieve comparable classification accuracy with
existing methods that use visual information only, but with less error in terms
of energy and nutrient values for the wrong predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications. (arXiv:2109.02740v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02740">
<div class="article-summary-box-inner">
<span><p>We address the problem of estimating the shape of a person's head, defined as
the geometry of the complete head surface, from a video taken with a single
moving camera, and determining the alignment of the fitted 3D head for all
video frames, irrespective of the person's pose. 3D head reconstructions
commonly tend to focus on perfecting the face reconstruction, leaving the scalp
to a statistical approximation. Our goal is to reconstruct the head model of
each person to enable future mixed reality applications. To do this, we recover
a dense 3D reconstruction and camera information via structure-from-motion and
multi-view stereo. These are then used in a new two-stage fitting process to
recover the 3D head shape by iteratively fitting a 3D morphable model of the
head with the dense reconstruction in canonical space and fitting it to each
person's head, using both traditional facial landmarks and scalp features
extracted from the head's segmentation mask. Our approach recovers consistent
geometry for varying head shapes, from videos taken by different people, with
different smartphones, and in a variety of environments from living rooms to
outdoor spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02747">
<div class="article-summary-box-inner">
<span><p>We aim to automatically identify human action reasons in online videos. We
focus on the widespread genre of lifestyle vlogs, in which people perform
actions while verbally describing them. We introduce and make publicly
available the {\sc WhyAct} dataset, consisting of 1,077 visual actions manually
annotated with their reasons. We describe a multimodal model that leverages
visual and textual information to automatically infer the reasons corresponding
to an action presented in the video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02748">
<div class="article-summary-box-inner">
<span><p>In a regular open set detection problem, samples of known classes (also
called closed set classes) are used to train a special classifier. In testing,
the classifier can (1) classify the test samples of known classes to their
respective classes and (2) also detect samples that do not belong to any of the
known classes (we say they belong to some unknown or open set classes). This
paper studies the problem of zero-shot open-set detection, which still performs
the same two tasks in testing but has no training except using the given known
class names. This paper proposes a novel and yet simple method (called ZO-CLIP)
to solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot
classification through multi-modal representation learning. It first extends
the pre-trained multi-modal model CLIP by training a text-based image
description generator on top of CLIP. In testing, it uses the extended model to
generate some candidate unknown class names for each test sample and computes a
confidence score based on both the known class names and candidate unknown
class names for zero-shot open set detection. Experimental results on 5
benchmark datasets for open set detection confirm that ZO-CLIP outperforms the
baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pano3D: A Holistic Benchmark and a Solid Baseline for $360^o$ Depth Estimation. (arXiv:2109.02749v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02749">
<div class="article-summary-box-inner">
<span><p>Pano3D is a new benchmark for depth estimation from spherical panoramas. It
aims to assess performance across all depth estimation traits, the primary
direct depth estimation performance targeting precision and accuracy, and also
the secondary traits, boundary preservation, and smoothness. Moreover, Pano3D
moves beyond typical intra-dataset evaluation to inter-dataset performance
assessment. By disentangling the capacity to generalize to unseen data into
different test splits, Pano3D represents a holistic benchmark for $360^o$ depth
estimation. We use it as a basis for an extended analysis seeking to offer
insights into classical choices for depth estimation. This results in a solid
baseline for panoramic depth that follow-up works can build upon to steer
future progress.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond. (arXiv:2109.02752v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02752">
<div class="article-summary-box-inner">
<span><p>Training deep neural networks may be challenging in real world data. Using
models as black-boxes, even with transfer learning, can result in poor
generalization or inconclusive results when it comes to small datasets or
specific applications. This tutorial covers the basic steps as well as more
recent options to improve models, in particular, but not restricted to,
supervised learning. It can be particularly useful in datasets that are not as
well-prepared as those in challenges, and also under scarce annotation and/or
small data. We describe basic procedures: as data preparation, optimization and
transfer learning, but also recent architectural choices such as use of
transformer modules, alternative convolutional layers, activation functions,
wide and deep networks, as well as training procedures including as curriculum,
contrastive and self-supervised learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STRIVE: Scene Text Replacement In Videos. (arXiv:2109.02762v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02762">
<div class="article-summary-box-inner">
<span><p>We propose replacing scene text in videos using deep style transfer and
learned photometric transformations.Building on recent progress on still image
text replacement,we present extensions that alter text while preserving the
appearance and motion characteristics of the original video.Compared to the
problem of still image text replacement,our method addresses additional
challenges introduced by video, namely effects induced by changing lighting,
motion blur, diverse variations in camera-object pose over time,and
preservation of temporal consistency. We parse the problem into three steps.
First, the text in all frames is normalized to a frontal pose using a
spatio-temporal trans-former network. Second, the text is replaced in a single
reference frame using a state-of-art still-image text replacement method.
Finally, the new text is transferred from the reference to remaining frames
using a novel learned image transformation network that captures lighting and
blur effects in a temporally consistent manner. Results on synthetic and
challenging real videos show realistic text trans-fer, competitive quantitative
and qualitative performance,and superior inference speed relative to
alternatives. We introduce new synthetic and real-world datasets with paired
text objects. To the best of our knowledge this is the first attempt at deep
video text replacement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds. (arXiv:2109.02763v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02763">
<div class="article-summary-box-inner">
<span><p>Humans can robustly recognize and localize objects by using visual and/or
auditory cues. While machines are able to do the same with visual data already,
less work has been done with sounds. This work develops an approach for scene
understanding purely based on binaural sounds. The considered tasks include
predicting the semantic masks of sound-making objects, the motion of
sound-making objects, and the depth map of the scene. To this aim, we propose a
novel sensor setup and record a new audio-visual dataset of street scenes with
eight professional binaural microphones and a 360-degree camera. The
co-existence of visual and audio cues is leveraged for supervision transfer. In
particular, we employ a cross-modal distillation framework that consists of
multiple vision teacher methods and a sound student method -- the student
method is trained to generate the same results as the teacher methods do. This
way, the auditory system can be trained without using human annotations. To
further boost the performance, we propose another novel auxiliary task, coined
Spatial Sound Super-Resolution, to increase the directional resolution of
sounds. We then formulate the four tasks into one end-to-end trainable
multi-tasking network aiming to boost the overall performance. Experimental
results show that 1) our method achieves good results for all four tasks, 2)
the four tasks are mutually beneficial -- training them together achieves the
best performance, 3) the number and orientation of microphones are both
important, and 4) features learned from the standard spectrogram and features
obtained by the classic signal processing pipeline are complementary for
auditory perception tasks. The data and code are released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness and Generalization via Generative Adversarial Training. (arXiv:2109.02765v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02765">
<div class="article-summary-box-inner">
<span><p>While deep neural networks have achieved remarkable success in various
computer vision tasks, they often fail to generalize to new domains and subtle
variations of input images. Several defenses have been proposed to improve the
robustness against these variations. However, current defenses can only
withstand the specific attack used in training, and the models often remain
vulnerable to other input variations. Moreover, these methods often degrade
performance of the model on clean images and do not generalize to out-of-domain
samples. In this paper we present Generative Adversarial Training, an approach
to simultaneously improve the model's generalization to the test set and
out-of-domain samples as well as its robustness to unseen adversarial attacks.
Instead of altering a low-level pre-defined aspect of images, we generate a
spectrum of low-level, mid-level and high-level changes using generative models
with a disentangled latent space. Adversarial training with these examples
enable the model to withstand a wide range of attacks by observing a variety of
input alterations during training. We show that our approach not only improves
performance of the model on clean images and out-of-domain samples but also
makes it robust against unforeseen attacks and outperforms prior work. We
validate effectiveness of our method by demonstrating results on various tasks
such as classification, segmentation and object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of MRI Biomarkers for Brain Cancer Survival Prediction. (arXiv:2109.02785v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02785">
<div class="article-summary-box-inner">
<span><p>Prediction of Overall Survival (OS) of brain cancer patients from multi-modal
MRI is a challenging field of research. Most of the existing literature on
survival prediction is based on Radiomic features, which does not consider
either non-biological factors or the functional neurological status of the
patient(s). Besides, the selection of an appropriate cut-off for survival and
the presence of censored data create further problems. Application of deep
learning models for OS prediction is also limited due to the lack of large
annotated publicly available datasets. In this scenario we analyse the
potential of two novel neuroimaging feature families, extracted from brain
parcellation atlases and spatial habitats, along with classical radiomic and
geometric features; to study their combined predictive power for analysing
overall survival. A cross validation strategy with grid search is proposed to
simultaneously select and evaluate the most predictive feature subset based on
its predictive power. A Cox Proportional Hazard (CoxPH) model is employed for
univariate feature selection, followed by the prediction of patient-specific
survival functions by three multivariate parsimonious models viz. Coxnet,
Random survival forests (RSF) and Survival SVM (SSVM). The brain cancer MRI
data used for this research was taken from two open-access collections TCGA-GBM
and TCGA-LGG available from The Cancer Imaging Archive (TCIA). Corresponding
survival data for each patient was downloaded from The Cancer Genome Atlas
(TCGA). A high cross validation $C-index$ score of $0.82\pm.10$ was achieved
using RSF with the best $24$ selected features. Age was found to be the most
important biological predictor. There were $9$, $6$, $6$ and $2$ features
selected from the parcellation, habitat, radiomic and region-based feature
groups respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep SIMBAD: Active Landmark-based Self-localization Using Ranking -based Scene Descriptor. (arXiv:2109.02786v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02786">
<div class="article-summary-box-inner">
<span><p>Landmark-based robot self-localization has recently garnered interest as a
highly-compressive domain-invariant approach for performing visual place
recognition (VPR) across domains (e.g., time of day, weather, and season).
However, landmark-based self-localization can be an ill-posed problem for a
passive observer (e.g., manual robot control), as many viewpoints may not
provide an effective landmark view. In this study, we consider an active
self-localization task by an active observer and present a novel reinforcement
learning (RL)-based next-best-view (NBV) planner. Our contributions are as
follows. (1) SIMBAD-based VPR: We formulate the problem of landmark-based
compact scene description as SIMBAD (similarity-based pattern recognition) and
further present its deep learning extension. (2) VPR-to-NBV knowledge transfer:
We address the challenge of RL under uncertainty (i.e., active
self-localization) by transferring the state recognition ability of VPR to the
NBV. (3) NNQL-based NBV: We regard the available VPR as the experience database
by adapting nearest-neighbor approximation of Q-learning (NNQL). The result
shows an extremely compact data structure that compresses both the VPR and NBV
into a single incremental inverted index. Experiments using the public NCLT
dataset validated the effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Collaborative Multi-Modal Learning for Unsupervised Kinship Estimation. (arXiv:2109.02804v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02804">
<div class="article-summary-box-inner">
<span><p>Kinship verification is a long-standing research challenge in computer
vision. The visual differences presented to the face have a significant effect
on the recognition capabilities of the kinship systems. We argue that
aggregating multiple visual knowledge can better describe the characteristics
of the subject for precise kinship identification. Typically, the age-invariant
features can represent more natural facial details. Such age-related
transformations are essential for face recognition due to the biological
effects of aging. However, the existing methods mainly focus on employing the
single-view image features for kinship identification, while more meaningful
visual properties such as race and age are directly ignored in the feature
learning step. To this end, we propose a novel deep collaborative multi-modal
learning (DCML) to integrate the underlying information presented in facial
properties in an adaptive manner to strengthen the facial details for effective
unsupervised kinship verification. Specifically, we construct a well-designed
adaptive feature fusion mechanism, which can jointly leverage the complementary
properties from different visual perspectives to produce composite features and
draw greater attention to the most informative components of spatial feature
maps. Particularly, an adaptive weighting strategy is developed based on a
novel attention mechanism, which can enhance the dependencies between different
properties by decreasing the information redundancy in channels in a
self-adaptive manner. To validate the effectiveness of the proposed method,
extensive experimental evaluations conducted on four widely-used datasets show
that our DCML method is always superior to some state-of-the-art kinship
verification methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kinship Verification Based on Cross-Generation Feature Interaction Learning. (arXiv:2109.02809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02809">
<div class="article-summary-box-inner">
<span><p>Kinship verification from facial images has been recognized as an emerging
yet challenging technique in many potential computer vision applications. In
this paper, we propose a novel cross-generation feature interaction learning
(CFIL) framework for robust kinship verification. Particularly, an effective
collaborative weighting strategy is constructed to explore the characteristics
of cross-generation relations by corporately extracting features of both
parents and children image pairs. Specifically, we take parents and children as
a whole to extract the expressive local and non-local features. Different from
the traditional works measuring similarity by distance, we interpolate the
similarity calculations as the interior auxiliary weights into the deep CNN
architecture to learn the whole and natural features. These similarity weights
not only involve corresponding single points but also excavate the multiple
relationships cross points, where local and non-local features are calculated
by using these two kinds of distance measurements. Importantly, instead of
separately conducting similarity computation and feature extraction, we
integrate similarity learning and feature extraction into one unified learning
process. The integrated representations deduced from local and non-local
features can comprehensively express the informative semantics embedded in
images and preserve abundant correlation knowledge from image pairs. Extensive
experiments demonstrate the efficiency and superiority of the proposed model
compared to some state-of-the-art kinship verification methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Learning via Dependency Maximization and Instance Discriminant Analysis. (arXiv:2109.02820v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02820">
<div class="article-summary-box-inner">
<span><p>We study the few-shot learning (FSL) problem, where a model learns to
recognize new objects with extremely few labeled training data per category.
Most of previous FSL approaches resort to the meta-learning paradigm, where the
model accumulates inductive bias through learning many training tasks so as to
solve a new unseen few-shot task. In contrast, we propose a simple approach to
exploit unlabeled data accompanying the few-shot task for improving few-shot
performance. Firstly, we propose a Dependency Maximization method based on the
Hilbert-Schmidt norm of the cross-covariance operator, which maximizes the
statistical dependency between the embedded feature of those unlabeled data and
their label predictions, together with the supervised loss over the support
set. We then use the obtained model to infer the pseudo-labels for those
unlabeled data. Furthermore, we propose anInstance Discriminant Analysis to
evaluate the credibility of each pseudo-labeled example and select the most
faithful ones into an augmented support set to retrain the model as in the
first step. We iterate the above process until the pseudo-labels for the
unlabeled data becomes stable. Following the standard transductive and
semi-supervised FSL setting, our experiments show that the proposed method
out-performs previous state-of-the-art methods on four widely used benchmarks,
including mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIM: Class-Irrelevant Mapping for Few-Shot Classification. (arXiv:2109.02840v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02840">
<div class="article-summary-box-inner">
<span><p>Few-shot classification (FSC) is one of the most concerned hot issues in
recent years. The general setting consists of two phases: (1) Pre-train a
feature extraction model (FEM) with base data (has large amounts of labeled
samples). (2) Use the FEM to extract the features of novel data (with few
labeled samples and totally different categories from base data), then classify
them with the to-be-designed classifier. The adaptability of pre-trained FEM to
novel data determines the accuracy of novel features, thereby affecting the
final classification performances. To this end, how to appraise the pre-trained
FEM is the most crucial focus in the FSC community. It sounds like traditional
Class Activate Mapping (CAM) based methods can achieve this by overlaying
weighted feature maps. However, due to the particularity of FSC (e.g., there is
no backpropagation when using the pre-trained FEM to extract novel features),
we cannot activate the feature map with the novel classes. To address this
challenge, we propose a simple, flexible method, dubbed as Class-Irrelevant
Mapping (CIM). Specifically, first, we introduce dictionary learning theory and
view the channels of the feature map as the bases in a dictionary. Then we
utilize the feature map to fit the feature vector of an image to achieve the
corresponding channel weights. Finally, we overlap the weighted feature map for
visualization to appraise the ability of pre-trained FEM on novel data. For
fair use of CIM in evaluating different models, we propose a new measurement
index, called Feature Localization Accuracy (FLA). In experiments, we first
compare our CIM with CAM in regular tasks and achieve outstanding performances.
Next, we use our CIM to appraise several classical FSC frameworks without
considering the classification results and discuss them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCsT: Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02860">
<div class="article-summary-box-inner">
<span><p>Graph convolutional networks (GCNs) achieve promising performance for
skeleton-based action recognition. However, in most GCN-based methods, the
spatial-temporal graph convolution is strictly restricted by the graph topology
while only captures the short-term temporal context, thus lacking the
flexibility of feature extraction. In this work, we present a novel
architecture, named Graph Convolutional skeleton Transformer (GCsT), which
addresses limitations in GCNs by introducing Transformer. Our GCsT employs all
the benefits of Transformer (i.e. dynamical attention and global context) while
keeps the advantages of GCNs (i.e. hierarchy and local topology structure). In
GCsT, the spatial-temporal GCN forces the capture of local dependencies while
Transformer dynamically extracts global spatial-temporal relationships.
Furthermore, the proposed GCsT shows stronger expressive capability by adding
additional information present in skeleton sequences. Incorporating the
Transformer allows that information to be introduced into the model almost
effortlessly. We validate the proposed GCsT by conducting extensive
experiments, which achieves the state-of-the-art performance on NTU RGB+D, NTU
RGB+D 120 and Northwestern-UCLA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02862">
<div class="article-summary-box-inner">
<span><p>Image classification is a major application domain for conventional deep
learning (DL). Quantum machine learning (QML) has the potential to
revolutionize image classification. In any typical DL-based image
classification, we use convolutional neural network (CNN) to extract features
from the image and multi-layer perceptron network (MLP) to create the actual
decision boundaries. On one hand, QML models can be useful in both of these
tasks. Convolution with parameterized quantum circuits (Quanvolution) can
extract rich features from the images. On the other hand, quantum neural
network (QNN) models can create complex decision boundaries. Therefore,
Quanvolution and QNN can be used to create an end-to-end QML model for image
classification. Alternatively, we can extract image features separately using
classical dimension reduction techniques such as, Principal Components Analysis
(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to
train a QNN. We review two proposals on quantum-classical hybrid ML models for
image classification namely, Quanvolutional Neural Network and dimension
reduction using a classical algorithm followed by QNN. Particularly, we make a
case for trainable filters in Quanvolution and CAE-based feature extraction for
image datasets (instead of dimension reduction using linear transformations
such as, PCA). We discuss various design choices, potential opportunities, and
drawbacks of these models. We also release a Python-based framework to create
and explore these hybrid models with a variety of design choices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Journalistic Guidelines Aware News Image Captioning. (arXiv:2109.02865v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02865">
<div class="article-summary-box-inner">
<span><p>The task of news article image captioning aims to generate descriptive and
informative captions for news article images. Unlike conventional image
captions that simply describe the content of the image in general terms, news
image captions follow journalistic guidelines and rely heavily on named
entities to describe the image content, often drawing context from the whole
article they are associated with. In this work, we propose a new approach to
this task, motivated by caption guidelines that journalists follow. Our
approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC),
leverages the structure of captions to improve the generation quality and guide
our representation design. Experimental results, including detailed ablation
studies, on two large-scale publicly available datasets show that JoGANIC
substantially outperforms state-of-the-art methods both on caption generation
and named entity related metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning. (arXiv:2109.02874v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02874">
<div class="article-summary-box-inner">
<span><p>The rapid advancement in deep learning makes the differentiation of authentic
and manipulated facial images and video clips unprecedentedly harder. The
underlying technology of manipulating facial appearances through deep
generative approaches, enunciated as DeepFake that have emerged recently by
promoting a vast number of malicious face manipulation applications.
Subsequently, the need of other sort of techniques that can assess the
integrity of digital visual content is indisputable to reduce the impact of the
creations of DeepFake. A large body of research that are performed on DeepFake
creation and detection create a scope of pushing each other beyond the current
status. This study presents challenges, research trends, and directions related
to DeepFake creation and detection techniques by reviewing the notable research
in the DeepFake domain to facilitate the development of more robust approaches
that could deal with the more advance DeepFake in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Hand Gesture Recognition in Multi-viewpoint Hand Hygiene. (arXiv:2109.02917v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02917">
<div class="article-summary-box-inner">
<span><p>This paper contributes a new high-quality dataset for hand gesture
recognition in hand hygiene systems, named "MFH". Generally, current datasets
are not focused on: (i) fine-grained actions; and (ii) data mismatch between
different viewpoints, which are available under realistic settings. To address
the aforementioned issues, the MFH dataset is proposed to contain a total of
731147 samples obtained by different camera views in 6 non-overlapping
locations. Additionally, each sample belongs to one of seven steps introduced
by the World Health Organization (WHO). As a minor contribution, inspired by
advances in fine-grained image recognition and distribution adaptation, this
paper recommends using the self-supervised learning method to handle these
preceding problems. The extensive experiments on the benchmarking MFH dataset
show that the introduced method yields competitive performance in both the
Accuracy and the Macro F1-score. The code and the MFH dataset are available at
https://github.com/willogy-team/hand-gesture-recognition-smc2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FDA: Feature Decomposition and Aggregation for Robust Airway Segmentation. (arXiv:2109.02920v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02920">
<div class="article-summary-box-inner">
<span><p>3D Convolutional Neural Networks (CNNs) have been widely adopted for airway
segmentation. The performance of 3D CNNs is greatly influenced by the dataset
while the public airway datasets are mainly clean CT scans with coarse
annotation, thus difficult to be generalized to noisy CT scans (e.g. COVID-19
CT scans). In this work, we proposed a new dual-stream network to address the
variability between the clean domain and noisy domain, which utilizes the clean
CT scans and a small amount of labeled noisy CT scans for airway segmentation.
We designed two different encoders to extract the transferable clean features
and the unique noisy features separately, followed by two independent decoders.
Further on, the transferable features are refined by the channel-wise feature
recalibration and Signed Distance Map (SDM) regression. The feature
recalibration module emphasizes critical features and the SDM pays more
attention to the bronchi, which is beneficial to extracting the transferable
topological features robust to the coarse labels. Extensive experimental
results demonstrated the obvious improvement brought by our proposed method.
Compared to other state-of-the-art transfer learning methods, our method
accurately segmented more bronchi in the noisy CT scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Combine the Modalities of Language and Video for Temporal Moment Localization. (arXiv:2109.02925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02925">
<div class="article-summary-box-inner">
<span><p>Temporal moment localization aims to retrieve the best video segment matching
a moment specified by a query. The existing methods generate the visual and
semantic embeddings independently and fuse them without full consideration of
the long-term temporal relationship between them. To address these
shortcomings, we introduce a novel recurrent unit, cross-modal long short-term
memory (CM-LSTM), by mimicking the human cognitive process of localizing
temporal moments that focuses on the part of a video segment related to the
part of a query, and accumulates the contextual information across the entire
video recurrently. In addition, we devise a two-stream attention mechanism for
both attended and unattended video features by the input query to prevent
necessary visual information from being neglected. To obtain more precise
boundaries, we propose a two-stream attentive cross-modal interaction network
(TACI) that generates two 2D proposal maps obtained globally from the
integrated contextual features, which are generated by using CM-LSTM, and
locally from boundary score sequences and then combines them into a final 2D
map in an end-to-end manner. On the TML benchmark dataset,
ActivityNet-Captions, the TACI outperform state-of-the-art TML methods with R@1
of 45.50% and 27.23% for IoU@0.5 and IoU@0.7, respectively. In addition, we
show that the revised state-of-the-arts methods by replacing the original LSTM
with our CM-LSTM achieve performance gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network. (arXiv:2109.02929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02929">
<div class="article-summary-box-inner">
<span><p>In this paper we present our solution to extract albedo of branded labels for
e-commerce products. To this end, we generate a large-scale photo-realistic
synthetic data set for albedo extraction followed by training a generative
model to translate images with diverse lighting conditions to albedo. We
performed an extensive evaluation to test the generalisation of our method to
in-the-wild images. From the experimental results, we observe that our solution
generalises well compared to the existing method both in the unseen rendered
images as well as in the wild image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02934">
<div class="article-summary-box-inner">
<span><p>Learning robust models that generalize well under changes in the data
distribution is critical for real-world applications. To this end, there has
been a growing surge of interest to learn simultaneously from multiple training
domains - while enforcing different types of invariance across those domains.
Yet, all existing approaches fail to show systematic benefits under fair
evaluation protocols. In this paper, we propose a new learning scheme to
enforce domain invariance in the space of the gradients of the loss function:
specifically, we introduce a regularization term that matches the domain-level
variances of gradients across training domains. Critically, our strategy, named
Fishr, exhibits close relations with the Fisher Information and the Hessian of
the loss. We show that forcing domain-level gradient covariances to be similar
during the learning procedure eventually aligns the domain-level loss
landscapes locally around the final weights. Extensive experiments demonstrate
the effectiveness of Fishr for out-of-distribution generalization. In
particular, Fishr improves the state of the art on the DomainBed benchmark and
performs significantly better than Empirical Risk Minimization. The code is
released at https://github.com/alexrame/fishr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention. (arXiv:2109.02955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02955">
<div class="article-summary-box-inner">
<span><p>Automatically describing video, or video captioning, has been widely studied
in the multimedia field. This paper proposes a new task of sensor-augmented
egocentric-video captioning, a newly constructed dataset for it called MMAC
Captions, and a method for the newly proposed task that effectively utilizes
multi-modal data of video and motion sensors, or inertial measurement units
(IMUs). While conventional video captioning tasks have difficulty in dealing
with detailed descriptions of human activities due to the limited view of a
fixed camera, egocentric vision has greater potential to be used for generating
the finer-grained descriptions of human activities on the basis of a much
closer view. In addition, we utilize wearable-sensor data as auxiliary
information to mitigate the inherent problems in egocentric vision: motion
blur, self-occlusion, and out-of-camera-range activities. We propose a method
for effectively utilizing the sensor data in combination with the video data on
the basis of an attention mechanism that dynamically determines the modality
that requires more attention, taking the contextual information into account.
We compared the proposed sensor-fusion method with strong baselines on the MMAC
Captions dataset and found that using sensor data as supplementary information
to the egocentric-video data was beneficial, and that our proposed method
outperformed the strong baselines, demonstrating the effectiveness of the
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CovarianceNet: Conditional Generative Model for Correct Covariance Prediction in Human Motion Prediction. (arXiv:2109.02965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02965">
<div class="article-summary-box-inner">
<span><p>The correct characterization of uncertainty when predicting human motion is
equally important as the accuracy of this prediction. We present a new method
to correctly predict the uncertainty associated with the predicted distribution
of future trajectories. Our approach, CovariaceNet, is based on a Conditional
Generative Model with Gaussian latent variables in order to predict the
parameters of a bi-variate Gaussian distribution. The combination of
CovarianceNet with a motion prediction model results in a hybrid approach that
outputs a uni-modal distribution. We will show how some state of the art
methods in motion prediction become overconfident when predicting uncertainty,
according to our proposed metric and validated in the ETH data-set
\cite{pellegrini2009you}. CovarianceNet correctly predicts uncertainty, which
makes our method suitable for applications that use predicted distributions,
e.g., planning or decision making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient ADMM-based Algorithms for Convolutional Sparse Coding. (arXiv:2109.02969v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02969">
<div class="article-summary-box-inner">
<span><p>Convolutional sparse coding improves on the standard sparse approximation by
incorporating a global shift-invariant model. The most efficient convolutional
sparse coding methods are based on the alternating direction method of
multipliers and the convolution theorem. The only major difference between
these methods is how they approach a convolutional least-squares fitting
subproblem. This letter presents a solution to this subproblem, which improves
the efficiency of the state-of-the-art algorithms. We also use the same
approach for developing an efficient convolutional dictionary learning method.
Furthermore, we propose a novel algorithm for convolutional sparse coding with
a constraint on the approximation error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Adversarial Learning for Single Image Deraining with Rain-Space Contrastive Constraints. (arXiv:2109.02973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02973">
<div class="article-summary-box-inner">
<span><p>Deep learning-based single image deraining (SID) with unpaired information is
of immense importance, as relying on paired synthetic data often limits their
generality and scalability in real-world applications. However, we noticed that
direct employ of unpaired adversarial learning and cycle-consistency
constraints in the SID task is insufficient to learn the underlying
relationship from rainy input to clean outputs, since the domain knowledge
between rainy and rain-free images is asymmetrical. To address such limitation,
we develop an effective unpaired SID method which explores mutual properties of
the unpaired exemplars by a contrastive learning manner in a GAN framework,
named as CDR-GAN. The proposed method mainly consists of two cooperative
branches: Bidirectional Translation Branch (BTB) and Contrastive Guidance
Branch (CGB). Specifically, BTB takes full advantage of the circulatory
architecture of adversarial consistency to exploit latent feature distributions
and guide transfer ability between two domains by equipping it with
bidirectional mapping. Simultaneously, CGB implicitly constrains the embeddings
of different exemplars in rain space by encouraging the similar feature
distributions closer while pushing the dissimilar further away, in order to
better help rain removal and image restoration. During training, we explore
several loss functions to further constrain the proposed CDR-GAN. Extensive
experiments show that our method performs favorably against existing unpaired
deraining approaches on both synthetic and real-world datasets, even
outperforms several fully-supervised or semi-supervised models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting. (arXiv:2109.02974v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02974">
<div class="article-summary-box-inner">
<span><p>Transformer, as a strong and flexible architecture for modelling long-range
relations, has been widely explored in vision tasks. However, when used in
video inpainting that requires fine-grained representation, existed method
still suffers from yielding blurry edges in detail due to the hard patch
splitting. Here we aim to tackle this problem by proposing FuseFormer, a
Transformer model designed for video inpainting via fine-grained feature fusion
based on novel Soft Split and Soft Composition operations. The soft split
divides feature map into many patches with given overlapping interval. On the
contrary, the soft composition operates by stitching different patches into a
whole feature map where pixels in overlapping regions are summed up. These two
modules are first used in tokenization before Transformer layers and
de-tokenization after Transformer layers, for effective mapping between tokens
and features. Therefore, sub-patch level information interaction is enabled for
more effective feature propagation between neighboring patches, resulting in
synthesizing vivid content for hole regions in videos. Moreover, in FuseFormer,
we elaborately insert the soft composition and soft split into the feed-forward
network, enabling the 1D linear layers to have the capability of modelling 2D
structure. And, the sub-patch level feature fusion ability is further enhanced.
In both quantitative and qualitative evaluations, our proposed FuseFormer
surpasses state-of-the-art methods. We also conduct detailed analysis to
examine its superiority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grassmannian Graph-attentional Landmark Selection for Domain Adaptation. (arXiv:2109.02990v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02990">
<div class="article-summary-box-inner">
<span><p>Domain adaptation aims to leverage information from the source domain to
improve the classification performance in the target domain. It mainly utilizes
two schemes: sample reweighting and feature matching. While the first scheme
allocates different weights to individual samples, the second scheme matches
the feature of two domains using global structural statistics. The two schemes
are complementary with each other, which are expected to jointly work for
robust domain adaptation. Several methods combine the two schemes, but the
underlying relationship of samples is insufficiently analyzed due to the
neglect of the hierarchy of samples and the geometric properties between
samples. To better combine the advantages of the two schemes, we propose a
Grassmannian graph-attentional landmark selection (GGLS) framework for domain
adaptation. GGLS presents a landmark selection scheme using attention-induced
neighbors of the graphical structure of samples and performs distribution
adaptation and knowledge adaptation over Grassmann manifold. the former treats
the landmarks of each sample differently, and the latter avoids feature
distortion and achieves better geometric properties. Experimental results on
different real-world cross-domain visual recognition tasks demonstrate that
GGLS provides better classification accuracies compared with state-of-the-art
domain adaptation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors. (arXiv:2109.02993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02993">
<div class="article-summary-box-inner">
<span><p>Significant advancements made in the generation of deepfakes have caused
security and privacy issues. Attackers can easily impersonate a person's
identity in an image by replacing his face with the target person's face.
Moreover, a new domain of cloning human voices using deep-learning technologies
is also emerging. Now, an attacker can generate realistic cloned voices of
humans using only a few seconds of audio of the target person. With the
emerging threat of potential harm deepfakes can cause, researchers have
proposed deepfake detection methods. However, they only focus on detecting a
single modality, i.e., either video or audio. On the other hand, to develop a
good deepfake detector that can cope with the recent advancements in deepfake
generation, we need to have a detector that can detect deepfakes of multiple
modalities, i.e., videos and audios. To build such a detector, we need a
dataset that contains video and respective audio deepfakes. We were able to
find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection
Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized
fake audios as well. We used this multimodal deepfake dataset and performed
detailed baseline experiments using state-of-the-art unimodal, ensemble-based,
and multimodal detection methods to evaluate it. We conclude through detailed
experimentation that unimodals, addressing only a single modality, video or
audio, do not perform well compared to ensemble-based methods. Whereas purely
multimodal-based baselines provide the worst performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical analysis of locally parameterized shapes. (arXiv:2109.03027v1 [stat.ME])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03027">
<div class="article-summary-box-inner">
<span><p>The alignment of shapes has been a crucial step in statistical shape
analysis, for example, in calculating mean shape, detecting locational
differences between two shape populations, and classification. Procrustes
alignment is the most commonly used method and state of the art. In this work,
we uncover that alignment might seriously affect the statistical analysis. For
example, alignment can induce false shape differences and lead to misleading
results and interpretations. We propose a novel hierarchical shape
parameterization based on local coordinate systems. The local parameterized
shapes are translation and rotation invariant. Thus, the inherent alignment
problems from the commonly used global coordinate system for shape
representation can be avoided using this parameterization. The new
parameterization is also superior for shape deformation and simulation. The
method's power is demonstrated on the hypothesis testing of simulated data as
well as the left hippocampi of patients with Parkinson's disease and controls.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution. (arXiv:2109.03075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03075">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is an effective framework that aims to transfer
meaningful information from a large teacher to a smaller student. Generally, KD
often involves how to define and transfer knowledge. Previous KD methods often
focus on mining various forms of knowledge, for example, feature maps and
refined information. However, the knowledge is derived from the primary
supervised task and thus is highly task-specific. Motivated by the recent
success of self-supervised representation learning, we propose an auxiliary
self-supervision augmented task to guide networks to learn more meaningful
features. Therefore, we can derive soft self-supervision augmented
distributions as richer dark knowledge from this task for KD. Unlike previous
knowledge, this distribution encodes joint knowledge from supervised and
self-supervised feature learning. Beyond knowledge exploration, another crucial
aspect is how to learn and distill our proposed knowledge effectively. To fully
take advantage of hierarchical feature maps, we propose to append several
auxiliary branches at various hidden layers. Each auxiliary branch is guided to
learn self-supervision augmented task and distill this distribution from
teacher to student. Thus we call our KD method as Hierarchical Self-Supervision
Augmented Knowledge Distillation (HSSAKD). Experiments on standard image
classification show that both offline and online HSSAKD achieves
state-of-the-art performance in the field of KD. Further transfer experiments
on object detection further verify that HSSAKD can guide the network to learn
better features, which can be attributed to learn and distill an auxiliary
self-supervision augmented task effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Support Vector Machine for Handwritten Character Recognition. (arXiv:2109.03081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03081">
<div class="article-summary-box-inner">
<span><p>Handwriting recognition has been one of the most fascinating and challenging
research areas in field of image processing and pattern recognition. It
contributes enormously to the improvement of automation process. In this paper,
a system for recognition of unconstrained handwritten Malayalam characters is
proposed. A database of 10,000 character samples of 44 basic Malayalam
characters is used in this work. A discriminate feature set of 64 local and 4
global features are used to train and test SVM classifier and achieved 92.24%
accuracy
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03082">
<div class="article-summary-box-inner">
<span><p>This paper proposes a Perceptual Learned Video Compression (PLVC) approach
with recurrent conditional generative adversarial network. In our approach, the
recurrent auto-encoder-based generator learns to fully explore the temporal
correlation for compressing video. More importantly, we propose a recurrent
conditional discriminator, which judges raw and compressed video conditioned on
both spatial and temporal information, including the latent representation,
temporal motion and hidden states in recurrent cells. This way, in the
adversarial training, it pushes the generated video to be not only spatially
photo-realistic but also temporally consistent with groundtruth and coherent
among video frames. Therefore, the proposed PLVC model learns to compress video
towards good perceptual quality at low bit-rate. The experimental results show
that our PLVC approach outperforms the previous traditional and learned
approaches on several perceptual quality metrics. The user study further
validates the outstanding perceptual performance of PLVC in comparison with the
latest learned video compression approaches and the official HEVC test model
(HM 16.20). The codes will be released at https://github.com/RenYang-home/PLVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics of Functional Connectivity. (arXiv:2109.03115v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03115">
<div class="article-summary-box-inner">
<span><p>The study of functional brain connectivity (FC) is important for
understanding the underlying mechanisms of many psychiatric disorders. Many
recent analyses adopt graph convolutional networks, to study non-linear
interactions between functionally-correlated states. However, although patterns
of brain activation are known to be hierarchically organised in both space and
time, many methods have failed to extract powerful spatio-temporal features. To
overcome those challenges, and improve understanding of long-range functional
dynamics, we translate an approach, from the domain of skeleton-based action
recognition, designed to model interactions across space and time. We evaluate
this approach using the Human Connectome Project (HCP) dataset on sex
classification and fluid intelligence prediction. To account for subject
topographic variability of functional organisation, we modelled functional
connectomes using multi-resolution dual-regressed (subject-specific) ICA nodes.
Results show a prediction accuracy of 94.4% for sex classification (an increase
of 6.2% compared to other methods), and an improvement of correlation with
fluid intelligence of 0.325 vs 0.144, relative to a baseline model that encodes
space and time separately. Results suggest that explicit encoding of
spatio-temporal dynamics of brain functional activity may improve the precision
with which behavioural and cognitive phenotypes may be predicted in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart Traffic Monitoring System using Computer Vision and Edge Computing. (arXiv:2109.03141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03141">
<div class="article-summary-box-inner">
<span><p>Traffic management systems capture tremendous video data and leverage
advances in video processing to detect and monitor traffic incidents. The
collected data are traditionally forwarded to the traffic management center
(TMC) for in-depth analysis and may thus exacerbate the network paths to the
TMC. To alleviate such bottlenecks, we propose to utilize edge computing by
equipping edge nodes that are close to cameras with computing resources (e.g.
cloudlets). A cloudlet, with limited computing resources as compared to TMC,
provides limited video processing capabilities. In this paper, we focus on two
common traffic monitoring tasks, congestion detection, and speed detection, and
propose a two-tier edge computing based model that takes into account of both
the limited computing capability in cloudlets and the unstable network
condition to the TMC. Our solution utilizes two algorithms for each task, one
implemented at the edge and the other one at the TMC, which are designed with
the consideration of different computing resources. While the TMC provides
strong computation power, the video quality it receives depends on the
underlying network conditions. On the other hand, the edge processes very
high-quality video but with limited computing resources. Our model captures
this trade-off. We evaluate the performance of the proposed two-tier model as
well as the traffic monitoring algorithms via test-bed experiments under
different weather as well as network conditions and show that our proposed
hybrid edge-cloud solution outperforms both the cloud-only and edge-only
solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System. (arXiv:2109.03144v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03144">
<div class="article-summary-box-inner">
<span><p>Optical Character Recognition (OCR) systems have been widely used in various
of application scenarios. Designing an OCR system is still a challenging task.
In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR)
to balance the accuracy against the efficiency. In order to improve the
accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more
robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better
text detector and a better text recognizer, which include Collaborative Mutual
Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual
Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the
precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost.
It is also comparable to the server models of the PP-OCR which uses ResNet
series as backbones. All of the above mentioned models are open-sourced and the
code is available in the GitHub repository PaddleOCR which is powered by
PaddlePaddle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair Comparison: Quantifying Variance in Resultsfor Fine-grained Visual Categorization. (arXiv:2109.03156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03156">
<div class="article-summary-box-inner">
<span><p>For the task of image classification, researchers work arduously to develop
the next state-of-the-art (SOTA) model, each bench-marking their own
performance against that of their predecessors and of their peers.
Unfortunately, the metric used most frequently to describe a model's
performance, average categorization accuracy, is often used in isolation. As
the number of classes increases, such as in fine-grained visual categorization
(FGVC), the amount of information conveyed by average accuracy alone dwindles.
While its most glaring weakness is its failure to describe the model's
performance on a class-by-class basis, average accuracy also fails to describe
how performance may vary from one trained model of the same architecture, on
the same dataset, to another (both averaged across all categories and at the
per-class level). We first demonstrate the magnitude of these variations across
models and across class distributions based on attributes of the data,
comparing results on different visual domains and different per-class image
distributions, including long-tailed distributions and few-shot subsets. We
then analyze the impact various FGVC methods have on overall and per-class
variance. From this analysis, we both highlight the importance of reporting and
comparing methods based on information beyond overall accuracy, as well as
point out techniques that mitigate variance in FGVC results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03201">
<div class="article-summary-box-inner">
<span><p>Transformers, the default model of choices in natural language processing,
have drawn scant attention from the medical imaging community. Given the
ability to exploit long-term dependencies, transformers are promising to help
atypical convolutional neural networks (convnets) to overcome its inherent
shortcomings of spatial inductive bias. However, most of recently proposed
transformer-based segmentation approaches simply treated transformers as
assisted modules to help encode global context into convolutional
representations without investigating how to optimally combine self-attention
(i.e., the core of transformers) with convolution. To address this issue, in
this paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful
segmentation model with an interleaved architecture based on empirical
combination of self-attention and convolution. In practice, nnFormer learns
volumetric representations from 3D local volumes. Compared to the naive
voxel-level self-attention implementation, such volume-based operations help to
reduce the computational complexity by approximate 98% and 99.5% on Synapse and
ACDC datasets, respectively. In comparison to prior-art network configurations,
nnFormer achieves tremendous improvements over previous transformer-based
methods on two commonly used datasets Synapse and ACDC. For instance, nnFormer
outperforms Swin-UNet by over 7 percents on Synapse. Even when compared to
nnUNet, currently the best performing fully-convolutional medical segmentation
network, nnFormer still provides slightly better performance on Synapse and
ACDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Fast Sample Re-weighting Without Reward Data. (arXiv:2109.03216v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03216">
<div class="article-summary-box-inner">
<span><p>Training sample re-weighting is an effective approach for tackling data
biases such as imbalanced and corrupted labels. Recent methods develop
learning-based algorithms to learn sample re-weighting strategies jointly with
model training based on the frameworks of reinforcement learning and meta
learning. However, depending on additional unbiased reward data is limiting
their general applicability. Furthermore, existing learning-based sample
re-weighting methods require nested optimizations of models and weighting
parameters, which requires expensive second-order computation. This paper
addresses these two problems and presents a novel learning-based fast sample
re-weighting (FSR) method that does not require additional reward data. The
method is based on two key ideas: learning from history to build proxy reward
data and feature sharing to reduce the optimization cost. Our experiments show
the proposed method achieves competitive results compared to state of the arts
on label noise robustness and long-tailed recognition, and does so while
achieving significantly improved training efficiency. The source code is
publicly available at
https://github.com/google-research/google-research/tree/master/ieg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos. (arXiv:2109.03223v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03223">
<div class="article-summary-box-inner">
<span><p>Out of all existing frameworks for surgical workflow analysis in endoscopic
videos, action triplet recognition stands out as the only one aiming to provide
truly fine-grained and comprehensive information on surgical activities. This
information, presented as &lt;instrument, verb, target&gt; combinations, is highly
challenging to be accurately identified. Triplet components can be difficult to
recognize individually; in this task, it requires not only performing
recognition simultaneously for all three triplet components, but also correctly
establishing the data association between them. To achieve this task, we
introduce our new model, the Rendezvous (RDV), which recognizes triplets
directly from surgical videos by leveraging attention at two different levels.
We first introduce a new form of spatial attention to capture individual action
triplet components in a scene; called the Class Activation Guided Attention
Mechanism (CAGAM). This technique focuses on the recognition of verbs and
targets using activations resulting from instruments. To solve the association
problem, our RDV model adds a new form of semantic attention inspired by
Transformer networks. Using multiple heads of cross and self attentions, RDV is
able to effectively capture relationships between instruments, verbs, and
targets. We also introduce CholecT50 - a dataset of 50 endoscopic videos in
which every frame has been annotated with labels from 100 triplet classes. Our
proposed RDV model significantly improves the triplet prediction mAP by over 9%
compared to the state-of-the-art methods on this dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets. (arXiv:2109.03229v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03229">
<div class="article-summary-box-inner">
<span><p>Many existing works have made great strides towards reducing racial bias in
face recognition. However, most of these methods attempt to rectify bias that
manifests in models during training instead of directly addressing a major
source of the bias, the dataset itself. Exceptions to this are
BUPT-Balancedface/RFW and Fairface, but these works assume that primarily
training on a single race or not racially balancing the dataset are inherently
disadvantageous. We demonstrate that these assumptions are not necessarily
valid. In our experiments, training on only African faces induced less bias
than training on a balanced distribution of faces and distributions skewed to
include more African faces produced more equitable models. We additionally
notice that adding more images of existing identities to a dataset in place of
adding new identities can lead to accuracy boosts across racial categories. Our
code is available at
https://github.com/j-alex-hanson/rethinking-race-face-datasets
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Tumor Segmentation through Layer Decomposition. (arXiv:2109.03230v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03230">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a self-supervised approach for tumor segmentation.
Specifically, we advocate a zero-shot setting, where models from
self-supervised learning should be directly applicable for the downstream task,
without using any manual annotations whatsoever. We make the following
contributions. First, with careful examination on existing self-supervised
learning approaches, we reveal the surprising result that, given suitable data
augmentation, models trained from scratch in fact achieve comparable
performance to those pre-trained with self-supervised learning. Second,
inspired by the fact that tumors tend to be characterized independently to the
contexts, we propose a scalable pipeline for generating synthetic tumor data,
and train a self-supervised model that minimises the generalisation gap with
the downstream task. Third, we conduct extensive ablation studies on different
downstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for
liver tumor segmentation. While evaluating the model transferability for tumor
segmentation under a low-annotation regime, including an extreme case of
zero-shot segmentation, the proposed approach demonstrates state-of-the-art
performance, substantially outperforming all existing self-supervised
approaches, and opening up the usage of self-supervised learning in practical
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of Dense Non-rigid Optical Flow. (arXiv:1812.01946v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.01946">
<div class="article-summary-box-inner">
<span><p>There hardly exists any large-scale datasets with dense optical flow of
non-rigid motion from real-world imagery as of today. The reason lies mainly in
the required setup to derive ground truth optical flows: a series of images
with known camera poses along its trajectory, and an accurate 3D model from a
textured scene. Human annotation is not only too tedious for large databases,
it can simply hardly contribute to accurate optical flow. To circumvent the
need for manual annotation, we propose a framework to automatically generate
optical flow from real-world videos. The method extracts and matches objects
from video frames to compute initial constraints, and applies a deformation
over the objects of interest to obtain dense optical flow fields. We propose
several ways to augment the optical flow variations. Extensive experimental
results show that training on our automatically generated optical flow
outperforms methods that are trained on rigid synthetic data using FlowNet-S,
LiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow
generation framework are released at https://github.com/lhoangan/arap_flow
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization. (arXiv:1911.12990v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.12990">
<div class="article-summary-box-inner">
<span><p>Network quantization, which aims to reduce the bit-lengths of the network
weights and activations, has emerged as one of the key ingredients to reduce
the size of neural networks for their deployments to resource-limited devices.
In order to overcome the nature of transforming continuous activations and
weights to discrete ones, recent study called Relaxed Quantization (RQ)
[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that
allows this transformation with efficient gradient-based optimization. However,
RQ with this Gumbel-Softmax relaxation still suffers from bias-variance
trade-off depending on the temperature parameter of Gumbel-Softmax. To resolve
the issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses
multi-class straight-through estimator to effectively reduce the bias and
variance, along with a new regularization technique, DropBits that replaces
dropout regularization to randomly drop the bits instead of neurons to further
reduce the bias of the multi-class straight-through estimator in SRQ. As a
natural extension of DropBits, we further introduce the way of learning
heterogeneous quantization levels to find proper bit-length for each layer
using DropBits. We experimentally validate our method on various benchmark
datasets and network architectures, and also support the quantized lottery
ticket hypothesis: learning heterogeneous quantization levels outperforms the
case using the same but fixed quantization levels from scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks Against Deep Learning Systems in the Physical World. (arXiv:2006.14580v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14580">
<div class="article-summary-box-inner">
<span><p>Backdoor attacks embed hidden malicious behaviors into deep learning models,
which only activate and cause misclassifications on model inputs containing a
specific trigger. Existing works on backdoor attacks and defenses, however,
mostly focus on digital attacks that use digitally generated patterns as
triggers. A critical question remains unanswered: can backdoor attacks succeed
using physical objects as triggers, thus making them a credible threat against
deep learning systems in the real world? We conduct a detailed empirical study
to explore this question for facial recognition, a critical deep learning task.
Using seven physical objects as triggers, we collect a custom dataset of 3205
images of ten volunteers and use it to study the feasibility of physical
backdoor attacks under a variety of real-world conditions. Our study reveals
two key findings. First, physical backdoor attacks can be highly successful if
they are carefully configured to overcome the constraints imposed by physical
objects. In particular, the placement of successful triggers is largely
constrained by the target model's dependence on key facial features. Second,
four of today's state-of-the-art defenses against (digital) backdoors are
ineffective against physical backdoors, because the use of physical objects
breaks core assumptions used to construct these defenses. Our study confirms
that (physical) backdoor attacks are not a hypothetical phenomenon but rather
pose a serious real-world threat to critical classification tasks. We need new
and more robust defenses against backdoors in the physical world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00596">
<div class="article-summary-box-inner">
<span><p>Previous versions of sparse principal component analysis (PCA) have presumed
that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We
propose a method that presumes the $p \times k$ matrix becomes approximately
sparse after a $k \times k$ rotation. The simplest version of the algorithm
initializes with the leading $k$ principal components. Then, the principal
components are rotated with an $k \times k$ orthogonal rotation to make them
approximately sparse. Finally, soft-thresholding is applied to the rotated
principal components. This approach differs from prior approaches because it
uses an orthogonal rotation to approximate a sparse basis. One consequence is
that a sparse component need not to be a leading eigenvector, but rather a
mixture of them. In this way, we propose a new (rotated) basis for sparse PCA.
In addition, our approach avoids "deflation" and multiple tuning parameters
required for that. Our sparse PCA framework is versatile; for example, it
extends naturally to a two-way analysis of a data matrix for simultaneous
dimensionality reduction of rows and columns. We provide evidence showing that
for the same level of sparsity, the proposed sparse PCA method is more stable
and can explain more variance compared to alternative methods. Through three
applications -- sparse coding of images, analysis of transcriptome sequencing
data, and large-scale clustering of social networks, we demonstrate the modern
usefulness of sparse PCA in exploring multivariate data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.02394">
<div class="article-summary-box-inner">
<span><p>Deep learning based semi-supervised learning (SSL) algorithms have led to
promising results in recent years. However, they tend to introduce multiple
tunable hyper-parameters, making them less practical in real SSL scenarios
where the labeled data is scarce for extensive hyper-parameter search. In this
paper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that
requires tuning only one additional hyper-parameter, compared with a standard
supervised deep learning algorithm, to achieve competitive performance under
various conditions of SSL. We start by defining a meta optimization problem
that minimizes the loss on labeled data through dynamically reweighting the
loss on unlabeled samples, which are associated with soft pseudo labels during
training. As the meta problem is computationally intensive to solve directly,
we propose an efficient algorithm to dynamically obtain the approximate
solutions. We show theoretically that Meta-Semi converges to the stationary
point of the loss function on labeled data under mild conditions. Empirically,
Meta-Semi outperforms state-of-the-art SSL algorithms significantly on the
challenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves
competitive performance on CIFAR-10 and SVHN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experimental Quantum Generative Adversarial Networks for Image Generation. (arXiv:2010.06201v3 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06201">
<div class="article-summary-box-inner">
<span><p>Quantum machine learning is expected to be one of the first practical
applications of near-term quantum devices. Pioneer theoretical works suggest
that quantum generative adversarial networks (GANs) may exhibit a potential
exponential advantage over classical GANs, thus attracting widespread
attention. However, it remains elusive whether quantum GANs implemented on
near-term quantum devices can actually solve real-world learning tasks. Here,
we devise a flexible quantum GAN scheme to narrow this knowledge gap, which
could accomplish image generation with arbitrarily high-dimensional features,
and could also take advantage of quantum superposition to train multiple
examples in parallel. For the first time, we experimentally achieve the
learning and generation of real-world hand-written digit images on a
superconducting quantum processor. Moreover, we utilize a gray-scale bar
dataset to exhibit the competitive performance between quantum GANs and the
classical GANs based on multilayer perceptron and convolutional neural network
architectures, respectively, benchmarked by the Fr\'echet Distance score. Our
work provides guidance for developing advanced quantum generative models on
near-term quantum devices and opens up an avenue for exploring quantum
advantages in various GAN-related learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Infer Shape Programs Using Self Training. (arXiv:2011.13045v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13045">
<div class="article-summary-box-inner">
<span><p>Inferring programs which generate 2D and 3D shapes is important for reverse
engineering, editing, and more. Training such inference models is challenging
due to the lack of paired (shape, program) data in most domains. A popular
approach is to pre-train a model on synthetic data and then fine-tune on real
shapes using slow, unstable reinforcement learning. In this paper, we argue
that self-training is a viable alternative for fine-tuning such models.
Self-training is a semi-supervised learning paradigm where a model assigns
pseudo-labels to unlabeled data, and then retrains with (data, pseudo-label)
pairs as the new ground truth. We show that for constructive solid geometry and
assembly-based modeling, self-training outperforms state-of-the-art
reinforcement learning approaches. Additionally, shape program inference has a
unique property that circumvents a potential downside of self-training
(incorrect pseudo-label assignment): inferred programs are executable. For a
given shape from our distribution of interest $\mathbf{x}^*$ and its predicted
program $\mathbf{z}$, one can execute $\mathbf{z}$ to obtain a shape
$\mathbf{x}$ and train on $(\mathbf{z}, \mathbf{x})$ pairs, rather than
$(\mathbf{z}, \mathbf{x}^*)$ pairs. We term this procedure latent execution
self training (LEST). We demonstrate that self training infers shape programs
with higher shape reconstruction accuracy and converges significantly faster
than reinforcement learning approaches, and in some domains, LEST can further
improve this performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01338">
<div class="article-summary-box-inner">
<span><p>Training deep learning models in technical domains is often accompanied by
the challenge that although the task is clear, insufficient data for training
is available. In this work, we propose a novel approach based on the
combination of Siamese networks and radial basis function networks to perform
data-efficient classification without pretraining by measuring the distance
between images in semantic space in a data-efficient manner. We develop the
models using three technical datasets, the NEU dataset, the BSD dataset, and
the TEX dataset. In addition to the technical domain, we show the general
applicability to classical datasets (cifar10 and MNIST) as well. The approach
is tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise
reduction of the number of samples available for training. The authors show
that the proposed approach outperforms the state-of-the-art models in the low
data regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DS-Net: Dynamic Spatiotemporal Network for Video Salient Object Detection. (arXiv:2012.04886v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04886">
<div class="article-summary-box-inner">
<span><p>As moving objects always draw more attention of human eyes, the temporal
motive information is always exploited complementarily with spatial information
to detect salient objects in videos. Although efficient tools such as optical
flow have been proposed to extract temporal motive information, it often
encounters difficulties when used for saliency detection due to the movement of
camera or the partial movement of salient objects. In this paper, we
investigate the complimentary roles of spatial and temporal information and
propose a novel dynamic spatiotemporal network (DS-Net) for more effective
fusion of spatiotemporal information. We construct a symmetric two-bypass
network to explicitly extract spatial and temporal features. A dynamic weight
generator (DWG) is designed to automatically learn the reliability of
corresponding saliency branch. And a top-down cross attentive aggregation (CAA)
procedure is designed so as to facilitate dynamic complementary aggregation of
spatiotemporal features. Finally, the features are modified by spatial
attention with the guidance of coarse saliency map and then go through decoder
part for final saliency map. Experimental results on five benchmarks VOS,
DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method
achieves superior performance than state-of-the-art algorithms. The source code
is available at https://github.com/TJUMMG/DS-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Self-Guided Loss for Salient Object Detection. (arXiv:2101.02412v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02412">
<div class="article-summary-box-inner">
<span><p>We present a simple yet effective progressive self-guided loss function to
facilitate deep learning-based salient object detection (SOD) in images. The
saliency maps produced by the most relevant works still suffer from incomplete
predictions due to the internal complexity of salient objects. Our proposed
progressive self-guided loss simulates a morphological closing operation on the
model predictions for progressively creating auxiliary training supervisions to
step-wisely guide the training process. We demonstrate that this new loss
function can guide the SOD model to highlight more complete salient objects
step-by-step and meanwhile help to uncover the spatial dependencies of the
salient object pixels in a region growing manner. Moreover, a new feature
aggregation module is proposed to capture multi-scale features and aggregate
them adaptively by a branch-wise attention mechanism. Benefiting from this
module, our SOD framework takes advantage of adaptively aggregated multi-scale
features to locate and detect salient objects effectively. Experimental results
on several benchmark datasets show that our loss function not only advances the
performance of existing SOD models without architecture modification but also
helps our proposed framework to achieve state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvNets for Counting: Object Detection of Transient Phenomena in Steelpan Drums. (arXiv:2102.00632v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00632">
<div class="article-summary-box-inner">
<span><p>We train an object detector built from convolutional neural networks to count
interference fringes in elliptical antinode regions in frames of high-speed
video recordings of transient oscillations in Caribbean steelpan drums
illuminated by electronic speckle pattern interferometry (ESPI). The
annotations provided by our model aim to contribute to the understanding of
time-dependent behavior in such drums by tracking the development of
sympathetic vibration modes. The system is trained on a dataset of crowdsourced
human-annotated images obtained from the Zooniverse Steelpan Vibrations
Project. Due to the small number of human-annotated images and the ambiguity of
the annotation task, we also evaluate the model on a large corpus of synthetic
images whose properties have been matched to the real images by style transfer
using a Generative Adversarial Network. Applying the model to thousands of
unlabeled video frames, we measure oscillations consistent with audio
recordings of these drum strikes. One unanticipated result is that sympathetic
oscillations of higher-octave notes significantly precede the rise in sound
intensity of the corresponding second harmonic tones; the mechanism responsible
for this remains unidentified. This paper primarily concerns the development of
the predictive model; further exploration of the steelpan images and deeper
physical insights await its further application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLAM: a Posit Logarithm-Approximate Multiplier. (arXiv:2102.09262v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09262">
<div class="article-summary-box-inner">
<span><p>The Posit Number System was introduced in 2017 as a replacement for
floating-point numbers. Since then, the community has explored its application
in Neural Network related tasks and produced some unit designs which are still
far from being competitive with their floating-point counterparts. This paper
proposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to
significantly reduce the complexity of posit multipliers, the most power-hungry
units within Deep Neural Network architectures. When comparing with
state-of-the-art posit multipliers, experiments show that the proposed
technique reduces the area, power, and delay of hardware multipliers up to
72.86%, 81.79%, and 17.01%, respectively, without accuracy degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Material Measurement Units for a Circular Economy: Foundations through a Survey. (arXiv:2103.01997v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01997">
<div class="article-summary-box-inner">
<span><p>Long-term availability of minerals and industrial materials is a necessary
condition for sustainable development as they are the constituents of any
manufacturing product. To enhance the efficiency of material management, we
define a computer-vision-enabled material measurement system and provide a
survey of works relevant to its development with particular emphasis on the
foundations. A network of such systems for wide-area material stock monitoring
is also covered. Finally, challenges and future research directions are
discussed. As the first article bridging industrial ecology and advanced
computer vision, this survey is intended to support both research communities
towards more sustainable manufacturing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03102">
<div class="article-summary-box-inner">
<span><p>The accuracy of DL classifiers is unstable in that it often changes
significantly when retested on adversarial images, imperfect images, or
perturbed images. This paper adds to the small but fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. Unlike
existed single-factor digital perturbation work, we provide state-of-the-art
two-factor perturbation that provides two natural perturbations on images
applied in different sequences. The two-factor perturbation includes (1) two
digital perturbations (Salt &amp; pepper noise and Gaussian noise) applied in both
sequences. (2) one digital perturbation (salt &amp; pepper noise) and a geometric
perturbation (rotation) applied in different sequences. To measure robust DL
classifiers, previous scientists provided 15 types of single-factor corruption.
We created 69 benchmarking image sets, including a clean set, sets with single
factor perturbations, and sets with two-factor perturbation conditions. To be
best of our knowledge, this is the first report that two-factor perturbed
images improves both robustness and accuracy of DL classifiers. Previous
research evaluating deep learning (DL) classifiers has often used top-1/top-5
accuracy, so researchers have usually offered tables, line diagrams, and bar
charts to display accuracy of DL classifiers. But these existed approaches
cannot quantitively evaluate robustness of DL classifiers. We innovate a new
two-dimensional, statistical visualization tool, including mean accuracy and
coefficient of variation (CV), to benchmark the robustness of DL classifiers.
All source codes and related image sets are shared on websites
(<a href="http://cslinux.semo.edu/david/data.html">this http URL</a> or
https://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to
support future academic research and industry projects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative-Adversarial-Networks-based Ghost Recognition. (arXiv:2103.13858v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13858">
<div class="article-summary-box-inner">
<span><p>Nowadays, target recognition technique plays an important role in many
fields. However, the current target image information based methods suffer from
the influence of image quality and the time cost of image reconstruction. In
this paper, we propose a novel imaging-free target recognition method combining
ghost imaging (GI) and generative adversarial networks (GAN). Based on the
mechanism of GI, a set of random speckles sequence is employed to illuminate
target, and a bucket detector without resolution is utilized to receive echo
signal. The bucket signal sequence formed after continuous detections is
constructed into a bucket signal array, which is regarded as the sample of GAN.
Then, conditional GAN is used to map bucket signal array and target category.
In practical application, the speckles sequence in training step is employed to
illuminate target, and the bucket signal array is input GAN for recognition.
The proposed method can improve the problems caused by conventional recognition
methods that based on target image information, and provide a certain
turbulence-free ability. Extensive experiments show that the proposed method
achieves promising performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Attentive 3D Human Pose and Shape Estimation from Videos. (arXiv:2103.14182v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14182">
<div class="article-summary-box-inner">
<span><p>We consider the task of estimating 3D human pose and shape from videos. While
existing frame-based approaches have made significant progress, these methods
are independently applied to each image, thereby often leading to inconsistent
predictions. In this work, we present a video-based learning algorithm for 3D
human pose and shape estimation. The key insights of our method are two-fold.
First, to address the inconsistent temporal prediction issue, we exploit
temporal information in videos and propose a self-attention module that jointly
considers short-range and long-range dependencies across frames, resulting in
temporally coherent estimations. Second, we model human motion with a
forecasting module that allows the transition between adjacent frames to be
smooth. We evaluate our method on the 3DPW, MPI-INF-3DHP, and Human3.6M
datasets. Extensive experimental results show that our algorithm performs
favorably against the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal RGB-D Scene Recognition Across Domains. (arXiv:2103.14672v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14672">
<div class="article-summary-box-inner">
<span><p>Scene recognition is one of the basic problems in computer vision research
with extensive applications in robotics. When available, depth images provide
helpful geometric cues that complement the RGB texture information and help to
identify discriminative scene image features. Depth sensing technology
developed fast in the last years and a great variety of 3D cameras have been
introduced, each with different acquisition properties. However, those
properties are often neglected when targeting big data collections, so
multi-modal images are gathered disregarding their original nature. In this
work, we put under the spotlight the existence of a possibly severe domain
shift issue within multi-modality scene recognition datasets. As a consequence,
a scene classification model trained on one camera may not generalize on data
from a different camera, only providing a low recognition performance. Starting
from the well-known SUN RGB-D dataset, we designed an experimental testbed to
study this problem and we use it to benchmark the performance of existing
methods. Finally, we introduce a novel adaptive scene recognition approach that
leverages self-supervised translation between modalities. Indeed, learning to
go from RGB to depth and vice-versa is an unsupervised procedure that can be
trained jointly on data of multiple cameras and may help to bridge the gap
among the extracted feature distributions. Our experimental results confirm the
effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02265">
<div class="article-summary-box-inner">
<span><p>Employing clustering strategy to assign unlabeled target images with pseudo
labels has become a trend for person re-identification (re-ID) algorithms in
domain adaptation. A potential limitation of these clustering-based methods is
that they always tend to introduce noisy labels, which will undoubtedly hamper
the performance of our re-ID system. To handle this limitation, an intuitive
solution is to utilize collaborative training to purify the pseudo label
quality. However, there exists a challenge that the complementarity of two
networks, which inevitably share a high similarity, becomes weakened gradually
as training process goes on; worse still, these approaches typically ignore to
consider the self-discrepancy of intra-class relations. To address this issue,
in this paper, we propose a multiple co-teaching framework for domain adaptive
person re-ID, opening up a promising direction about self-discrepancy problem
under unsupervised condition. On top of that, a mean-teaching mechanism is
leveraged to enlarge the difference and discover more complementary features.
Comprehensive experiments conducted on several large-scale datasets show that
our method achieves competitive performance compared with the
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiScene: A Large-scale Dataset and Benchmark for Multi-scene Recognition in Single Aerial Images. (arXiv:2104.02846v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02846">
<div class="article-summary-box-inner">
<span><p>Aerial scene recognition is a fundamental research problem in interpreting
high-resolution aerial imagery. Over the past few years, most studies focus on
classifying an image into one scene category, while in real-world scenarios, it
is more often that a single image contains multiple scenes. Therefore, in this
paper, we investigate a more practical yet underexplored task -- multi-scene
recognition in single images. To this end, we create a large-scale dataset,
called MultiScene, composed of 100,000 unconstrained high-resolution aerial
images. Considering that manually labeling such images is extremely arduous, we
resort to low-cost annotations from crowdsourcing platforms, e.g.,
OpenStreetMap (OSM). However, OSM data might suffer from incompleteness and
incorrectness, which introduce noise into image labels. To address this issue,
we visually inspect 14,000 images and correct their scene labels, yielding a
subset of cleanly-annotated images, named MultiScene-Clean. With it, we can
develop and evaluate deep networks for multi-scene recognition using clean
data. Moreover, we provide crowdsourced annotations of all images for the
purpose of studying network learning with noisy labels. We conduct experiments
with extensive baseline models on both MultiScene-Clean and MultiScene to offer
benchmarks for multi-scene recognition in single images and learning from noisy
labels for this task, respectively. To facilitate progress, we make our dataset
and trained models available on
https://gitlab.lrz.de/ai4eo/reasoning/multiscene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPR-CAE: Capsule Autoencoder with Dynamic Part Representation for Image Parsing. (arXiv:2104.14735v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14735">
<div class="article-summary-box-inner">
<span><p>Parsing an image into a hierarchy of objects, parts, and relations is
important and also challenging in many computer vision tasks. This paper
proposes a simple and effective capsule autoencoder to address this issue,
called DPR-CAE. In our approach, the encoder parses the input into a set of
part capsules, including pose, intensity, and dynamic vector. The decoder
introduces a novel dynamic part representation (DPR) by combining the dynamic
vector and a shared template bank. These part representations are then
regulated by corresponding capsules to composite the final output in an
interpretable way. Besides, an extra translation-invariant module is proposed
to avoid directly learning the uncertain scene-part relationship in our
DPR-CAE, which makes the resulting method achieves a promising performance gain
on $rm$-MNIST and $rm$-Fashion-MNIST. % to model the scene-object relationship
DPR-CAE can be easily combined with the existing stacked capsule autoencoder
and experimental results show it significantly improves performance in terms of
unsupervised object classification. Our code is available in the Appendix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model. (arXiv:2105.11312v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11312">
<div class="article-summary-box-inner">
<span><p>3D action recognition is referred to as the classification of action
sequences which consist of 3D skeleton joints. While many research work are
devoted to 3D action recognition, it mainly suffers from three problems: highly
complicated articulation, a great amount of noise, and a low implementation
efficiency. To tackle all these problems, we propose a real-time 3D action
recognition framework by integrating the locally aggregated kinematic-guided
skeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first
define the skeletonlet as a few combinations of joint offsets grouped in terms
of kinematic principle, and then represent an action sequence using LAKS, which
consists of a denoising phase and a locally aggregating phase. The denoising
phase detects the noisy action data and adjust it by replacing all the features
within it with the features of the corresponding previous frame, while the
locally aggregating phase sums the difference between an offset feature of the
skeletonlet and its cluster center together over all the offset features of the
sequence. Finally, the SHA model which combines sparse representation with a
hashing model, aiming at promoting the recognition accuracy while maintaining a
high efficiency. Experimental results on MSRAction3D, UTKinectAction3D and
Florence3DAction datasets demonstrate that the proposed method outperforms
state-of-the-art methods in both recognition accuracy and implementation
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03932">
<div class="article-summary-box-inner">
<span><p>Successful active speaker detection requires a three-stage pipeline: (i)
audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation
modeling between a reference speaker and the background speakers within each
frame, and (iii) temporal modeling for the reference speaker. Each stage of
this pipeline plays an important role for the final performance of the created
architecture. Based on a series of controlled experiments, this work presents
several practical guidelines for audio-visual active speaker detection.
Correspondingly, we present a new architecture called ASDNet, which achieves a
new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%
outperforming the second best with a large margin of 4.7%. Our code and
pretrained models are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Animatable Neural Radiance Fields from Monocular RGB Videos. (arXiv:2106.13629v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13629">
<div class="article-summary-box-inner">
<span><p>We present animatable neural radiance fields (animatable NeRF) for detailed
human avatar creation from monocular videos. Our approach extends neural
radiance fields (NeRF) to the dynamic scenes with human movements via
introducing explicit pose-guided deformation while learning the scene
representation network. In particular, we estimate the human pose for each
frame and learn a constant canonical space for the detailed human template,
which enables natural shape deformation from the observation space to the
canonical space under the explicit control of the pose parameters. To
compensate for inaccurate pose estimation, we introduce the pose refinement
strategy that updates the initial pose during the learning process, which not
only helps to learn more accurate human reconstruction but also accelerates the
convergence. In experiments we show that the proposed approach achieves 1)
implicit human geometry and appearance reconstruction with high-quality
details, 2) photo-realistic rendering of the human from novel views, and 3)
animation of the human with novel poses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15753">
<div class="article-summary-box-inner">
<span><p>Robust and accurate nuclei centroid detection is important for the
understanding of biological structures in fluorescence microscopy images.
Existing automated nuclei localization methods face three main challenges: (1)
Most of object detection methods work only on 2D images and are difficult to
extend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes
but it is computational expensive for large microscopy volumes and they have
difficulty distinguishing different instances of objects; (3) Hand annotated
ground truth is limited for 3D microscopy volumes. To address these issues, we
present a scalable approach for nuclei centroid detection of 3D microscopy
volumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each
slice of the volume from different directions and 3D agglomerative hierarchical
clustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.
The model was trained with the synthetic microscopy data generated using
Spatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and
tested on different types of real 3D microscopy data. Extensive experimental
results demonstrate that our proposed method can accurately count and detect
the nuclei centroids in a 3D microscopy volume.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01378">
<div class="article-summary-box-inner">
<span><p>This paper studies the model compression problem of vision transformers.
Benefit from the self-attention module, transformer architectures have shown
extraordinary performance on many computer vision tasks. Although the network
performance is boosted, transformers are often required more computational
resources including memory usage and the inference complexity. Compared with
the existing knowledge distillation approaches, we propose to excavate useful
information from the teacher transformer through the relationship between
images and the divided patches. We then explore an efficient fine-grained
manifold distillation approach that simultaneously calculates cross-images,
cross-patch, and random-selected manifolds in teacher and student models.
Experimental results conducted on several benchmarks demonstrate the
superiority of the proposed algorithm for distilling portable transformer
models with higher performance. For example, our approach achieves 75.06% Top-1
accuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which
outperforms other ViT distillation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07791">
<div class="article-summary-box-inner">
<span><p>We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08323">
<div class="article-summary-box-inner">
<span><p>Temporal action proposal generation is an essential and challenging task that
aims at localizing temporal intervals containing human actions in untrimmed
videos. Most of existing approaches are unable to follow the human cognitive
process of understanding the video context due to lack of attention mechanism
to express the concept of an action or an agent who performs the action or the
interaction between the agent and the environment. Based on the action
definition that a human, known as an agent, interacts with the environment and
performs an action that affects the environment, we propose a contextual
Agent-Environment Network. Our proposed contextual AEN involves (i) agent
pathway, operating at a local level to tell about which humans/agents are
acting and (ii) environment pathway operating at a global level to tell about
how the agents interact with the environment. Comprehensive evaluations on
20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different
backbone networks, i.e C3D and SlowFast, show that our method robustly exhibits
outperformance against state-of-the-art methods regardless of the employed
backbone network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Go Wider Instead of Deeper. (arXiv:2107.11817v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11817">
<div class="article-summary-box-inner">
<span><p>More transformer blocks with residual connections have recently achieved
impressive results on various tasks. To achieve better performance with fewer
trainable parameters, recent methods are proposed to go shallower by parameter
sharing or model compressing along with the depth. However, weak modeling
capacity limits their performance. Contrastively, going wider by inducing more
trainable matrixes and parameters would produce a huge model requiring advanced
parallelism to train and inference.
</p>
<p>In this paper, we propose a parameter-efficient framework, going wider
instead of deeper. Specially, following existing works, we adapt parameter
sharing to compress along depth. But, such deployment would limit the
performance. To maximize modeling capacity, we scale along model width by
replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across
transformer blocks, instead of sharing normalization layers, we propose to use
individual layernorms to transform various semantic representations in a more
parameter-efficient way. To evaluate our plug-and-run framework, we design
WideNet and conduct comprehensive experiments on popular computer vision and
natural language processing benchmarks. On ImageNet-1K, our best model
outperforms Vision Transformer (ViT) by $1.5\%$ with $0.72 \times$ trainable
parameters. Using $0.46 \times$ and $0.13 \times$ parameters, our WideNet can
still surpass ViT and ViT-MoE by $0.8\%$ and $2.1\%$, respectively. On four
natural language processing datasets, WideNet outperforms ALBERT by $1.8\%$ on
average and surpass BERT using factorized embedding parameterization by $0.8\%$
with fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00784">
<div class="article-summary-box-inner">
<span><p>According to recent studies, commonly used computer vision datasets contain
about 4% of label errors. For example, the COCO dataset is known for its high
level of noise in data labels, which limits its use for training robust neural
deep architectures in a real-world scenario. To model such a noise, in this
paper we have proposed the homoscedastic aleatoric uncertainty estimation, and
present a series of novel loss functions to address the problem of image object
detection at scale. Specifically, the proposed functions are based on Bayesian
inference and we have incorporated them into the common community-adopted
object detection deep learning architecture RetinaNet. We have also shown that
modeling of homoscedastic aleatoric uncertainty using our novel functions
allows to increase the model interpretability and to improve the object
detection performance being evaluated on the COCO dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-mask Matters in Weakly-supervised Semantic Segmentation. (arXiv:2108.12995v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12995">
<div class="article-summary-box-inner">
<span><p>Most weakly supervised semantic segmentation (WSSS) methods follow the
pipeline that generates pseudo-masks initially and trains the segmentation
model with the pseudo-masks in fully supervised manner after. However, we find
some matters related to the pseudo-masks, including high quality pseudo-masks
generation from class activation maps (CAMs), and training with noisy
pseudo-mask supervision. For these matters, we propose the following designs to
push the performance to new state-of-art: (i) Coefficient of Variation
Smoothing to smooth the CAMs adaptively; (ii) Proportional Pseudo-mask
Generation to project the expanded CAMs to pseudo-mask based on a new metric
indicating the importance of each class on each location, instead of the scores
trained from binary classifiers. (iii) Pretended Under-Fitting strategy to
suppress the influence of noise in pseudo-mask; (iv) Cyclic Pseudo-mask to
boost the pseudo-masks during training of fully supervised semantic
segmentation (FSSS). Experiments based on our methods achieve new state-of-art
results on two changeling weakly supervised semantic segmentation datasets,
pushing the mIoU to 70.0% and 40.2% on PAS-CAL VOC 2012 and MS COCO 2014
respectively. Codes including segmentation framework are released at
https://github.com/Eli-YiLi/PMM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies. (arXiv:2108.13459v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13459">
<div class="article-summary-box-inner">
<span><p>Generative models for 3D shapes represented by hierarchies of parts can
generate realistic and diverse sets of outputs. However, existing models suffer
from the key practical limitation of modelling shapes holistically and thus
cannot perform conditional sampling, i.e. they are not able to generate
variants on individual parts of generated shapes without modifying the rest of
the shape. This is limiting for applications such as 3D CAD design that involve
adjusting created shapes at multiple levels of detail. To address this, we
introduce LSD-StructureNet, an augmentation to the StructureNet architecture
that enables re-generation of parts situated at arbitrary positions in the
hierarchies of its outputs. We achieve this by learning individual,
probabilistic conditional decoders for each hierarchy depth. We evaluate
LSD-StructureNet on the PartNet dataset, the largest dataset of 3D shapes
represented by hierarchies of parts. Our results show that contrarily to
existing methods, LSD-StructureNet can perform conditional sampling without
impacting inference speed or the realism and diversity of its outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrouSPI-Net: Spatio-temporal attention on parallel atrous convolutions and U-GRUs for skeletal pedestrian crossing prediction. (arXiv:2109.00953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00953">
<div class="article-summary-box-inner">
<span><p>Understanding the behaviors and intentions of pedestrians is still one of the
main challenges for vehicle autonomy, as accurate predictions of their
intentions can guarantee their safety and driving comfort of vehicles. In this
paper, we address pedestrian crossing prediction in urban traffic environments
by linking the dynamics of a pedestrian's skeleton to a binary crossing
intention. We introduce TrouSPI-Net: a context-free, lightweight, multi-branch
predictor. TrouSPI-Net extracts spatio-temporal features for different time
resolutions by encoding pseudo-images sequences of skeletal joints' positions
and processes them with parallel attention modules and atrous convolutions. The
proposed approach is then enhanced by processing features such as relative
distances of skeletal joints, bounding box positions, or ego-vehicle speed with
U-GRUs. Using the newly proposed evaluation procedures for two large public
naturalistic data sets for studying pedestrian behavior in traffic: JAAD and
PIE, we evaluate TrouSPI-Net and analyze its performance. Experimental results
show that TrouSPI-Net achieved 0.76 F1 score on JAAD and 0.80 F1 score on PIE,
therefore outperforming current state-of-the-art while being lightweight and
context-free.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatiotemporal Inconsistency Learning for DeepFake Video Detection. (arXiv:2109.01860v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01860">
<div class="article-summary-box-inner">
<span><p>The rapid development of facial manipulation techniques has aroused public
concerns in recent years. Following the success of deep learning, existing
methods always formulate DeepFake video detection as a binary classification
problem and develop frame-based and video-based solutions. However, little
attention has been paid to capturing the spatial-temporal inconsistency in
forged videos. To address this issue, we term this task as a Spatial-Temporal
Inconsistency Learning (STIL) process and instantiate it into a novel STIL
block, which consists of a Spatial Inconsistency Module (SIM), a Temporal
Inconsistency Module (TIM), and an Information Supplement Module (ISM).
Specifically, we present a novel temporal modeling paradigm in TIM by
exploiting the temporal difference over adjacent frames along with both
horizontal and vertical directions. And the ISM simultaneously utilizes the
spatial information from SIM and temporal information from TIM to establish a
more comprehensive spatial-temporal representation. Moreover, our STIL block is
flexible and could be plugged into existing 2D CNNs. Extensive experiments and
visualizations are presented to demonstrate the effectiveness of our method
against the state-of-the-art competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigational Path-Planning For All-Terrain Autonomous Agricultural Robot. (arXiv:2109.02015v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02015">
<div class="article-summary-box-inner">
<span><p>The shortage of workforce and increasing cost of maintenance has forced many
farm industrialists to shift towards automated and mechanized approaches. The
key component for autonomous systems is the path planning techniques used.
Coverage path planning (CPP) algorithm is used for navigating over farmlands to
perform various agricultural operations such as seeding, ploughing, or spraying
pesticides and fertilizers. This report paper compares novel algorithms for
autonomous navigation of farmlands. For reduction of navigational constraints,
a high-resolution grid map representation is taken into consideration specific
to Indian environments. The free space is covered by distinguishing the grid
cells as covered, unexplored, partially explored and presence of an obstacle.
The performance of the compared algorithms is evaluated with metrics such as
time efficiency, space efficiency, accuracy, and robustness to changes in the
environment. Robotic Operating System (ROS), Dassault Systemes Experience
Platform (3DS Experience), MATLAB along Python were used for the simulation of
the compared algorithms. The results proved the applicability of the algorithms
for autonomous field navigation and feasibility with robotic path planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02119">
<div class="article-summary-box-inner">
<span><p>The use of mobiles phones when driving have been a major factor when it comes
to road traffic incidents and the process of capturing such violations can be a
laborious task. Advancements in both modern object detection frameworks and
high-performance hardware has paved the way for a more automated approach when
it comes to video surveillance. In this work, we propose a custom-trained
state-of-the-art object detector to work with roadside cameras to capture
driver phone usage without the need for human intervention. The proposed
approach also addresses the issues caused by windscreen glare and introduces
the steps required to remedy this. Twelve pre-trained models are fine-tuned
with our custom dataset using four popular object detection methods: YOLO, SSD,
Faster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO
yields the highest accuracy levels of up to 96% (AP10) and frame rates of up to
~30 FPS. DeepSort object tracking algorithm is also integrated into the
best-performing model to collect records of only the unique violations, and
enable the proposed approach to count the number of vehicles. The proposed
automated system will collect the output images of the identified violations,
timestamps of each violation, and total vehicle count. Data can be accessed via
a purpose-built user interface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Realistic Single-View 3D Object Reconstruction with Unsupervised Learning from Multiple Images. (arXiv:2109.02288v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02288">
<div class="article-summary-box-inner">
<span><p>Recovering the 3D structure of an object from a single image is a challenging
task due to its ill-posed nature. One approach is to utilize the plentiful
photos of the same object category to learn a strong 3D shape prior for the
object. This approach has successfully been demonstrated by a recent work of Wu
et al. (2020), which obtained impressive 3D reconstruction networks with
unsupervised learning. However, their algorithm is only applicable to symmetric
objects. In this paper, we eliminate the symmetry requirement with a novel
unsupervised algorithm that can learn a 3D reconstruction network from a
multi-image dataset. Our algorithm is more general and covers the
symmetry-required scenario as a special case. Besides, we employ a novel albedo
loss that improves the reconstructed details and realisticity. Our method
surpasses the previous work in both quality and robustness, as shown in
experiments on datasets of various structures, including single-view,
multi-view, image-collection, and video sets.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-08 23:02:25.192939813 UTC">2021-09-08 23:02:25 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>