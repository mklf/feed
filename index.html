<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-25T01:30:00Z">04-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction. (arXiv:2204.10360v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10360">
<div class="article-summary-box-inner">
<span><p>Relation extraction is a core problem for natural language processing in the
biomedical domain. Recent research on relation extraction showed that
prompt-based learning improves the performance on both fine-tuning on full
training set and few-shot training. However, less effort has been made on
domain-specific tasks where good prompt design can be even harder. In this
paper, we investigate prompting for biomedical relation extraction, with
experiments on the ChemProt dataset. We present a simple yet effective method
to systematically generate comprehensive prompts that reformulate the relation
extraction task as a cloze-test task under a simple prompt formulation. In
particular, we experiment with different ranking scores for prompt selection.
With BioMed-RoBERTa-base, our results show that prompting-based fine-tuning
obtains gains by 14.21 F1 over its regular fine-tuning baseline, and 1.14 F1
over SciFive-Large, the current state-of-the-art on ChemProt. Besides, we find
prompt-based learning requires fewer training examples to make reasonable
predictions. The results demonstrate the potential of our methods in such a
domain-specific relation extraction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias. (arXiv:2204.10365v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10365">
<div class="article-summary-box-inner">
<span><p>The remarkable progress in Natural Language Processing (NLP) brought about by
deep learning, particularly with the recent advent of large pre-trained neural
language models, is brought into scrutiny as several studies began to discuss
and report potential biases in NLP applications. Bias in NLP is found to
originate from latent historical biases encoded by humans into textual data
which gets perpetuated or even amplified by NLP algorithm. We present a survey
to comprehend bias in large pre-trained language models, analyze the stages at
which they occur in these models, and various ways in which these biases could
be quantified and mitigated. Considering wide applicability of textual
affective computing based downstream tasks in real-world systems such as
business, healthcare, education, etc., we give a special emphasis on
investigating bias in the context of affect (emotion) i.e., Affective Bias, in
large pre-trained language models. We present a summary of various bias
evaluation corpora that help to aid future research and discuss challenges in
the research on bias in pre-trained language models. We believe that our
attempt to draw a comprehensive view of bias in pre-trained language models,
and especially the exploration of affective bias will be highly beneficial to
researchers interested in this evolving field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICDBigBird: A Contextual Embedding Model for ICD Code Classification. (arXiv:2204.10408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10408">
<div class="article-summary-box-inner">
<span><p>The International Classification of Diseases (ICD) system is the
international standard for classifying diseases and procedures during a
healthcare encounter and is widely used for healthcare reporting and management
purposes. Assigning correct codes for clinical procedures is important for
clinical, operational, and financial decision-making in healthcare. Contextual
word embedding models have achieved state-of-the-art results in multiple NLP
tasks. However, these models have yet to achieve state-of-the-art results in
the ICD classification task since one of their main disadvantages is that they
can only process documents that contain a small number of tokens which is
rarely the case with real patient notes. In this paper, we introduce ICDBigBird
a BigBird-based model which can integrate a Graph Convolutional Network (GCN),
that takes advantage of the relations between ICD codes in order to create
'enriched' representations of their embeddings, with a BigBird contextual model
that can process larger documents. Our experiments on a real-world clinical
dataset demonstrate the effectiveness of our BigBird-based model on the ICD
classification task as it outperforms the previous state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">German Parliamentary Corpus (GerParCor). (arXiv:2204.10422v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10422">
<div class="article-summary-box-inner">
<span><p>Parliamentary debates represent a large and partly unexploited treasure trove
of publicly accessible texts. In the German-speaking area, there is a certain
deficit of uniformly accessible and annotated corpora covering all
German-speaking parliaments at the national and federal level. To address this
gap, we introduce the German Parliament Corpus (GerParCor). GerParCor is a
genre-specific corpus of (predominantly historical) German-language
parliamentary protocols from three centuries and four countries, including
state and federal level data. In addition, GerParCor contains conversions of
scanned protocols and, in particular, of protocols in Fraktur converted via an
OCR process based on Tesseract. All protocols were preprocessed by means of the
NLP pipeline of spaCy3 and automatically annotated with metadata regarding
their session date. GerParCor is made available in the XMI format of the UIMA
project. In this way, GerParCor can be used as a large corpus of historical
texts in the field of political communication for various tasks in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires. (arXiv:2204.10432v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10432">
<div class="article-summary-box-inner">
<span><p>Automated methods have been widely used to identify and analyze mental health
conditions (e.g., depression) from various sources of information, including
social media. Yet, deployment of such models in real-world healthcare
applications faces challenges including poor out-of-domain generalization and
lack of trust in black box models. In this work, we propose approaches for
depression detection that are constrained to different degrees by the presence
of symptoms described in PHQ9, a questionnaire used by clinicians in the
depression screening process. In dataset-transfer experiments on three social
media datasets, we find that grounding the model in PHQ9's symptoms
substantially improves its ability to generalize to out-of-distribution data
compared to a standard BERT-based approach. Furthermore, this approach can
still perform competitively on in-domain data. These results and our
qualitative analyses suggest that grounding model predictions in
clinically-relevant symptoms can improve generalizability while producing a
model that is easier to inspect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering. (arXiv:2204.10448v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10448">
<div class="article-summary-box-inner">
<span><p>Knowledge-based visual question answering (QA) aims to answer a question
which requires visually-grounded external knowledge beyond image content
itself. Answering complex questions that require multi-hop reasoning under weak
supervision is considered as a challenging problem since i) no supervision is
given to the reasoning process and ii) high-order semantics of multi-hop
knowledge facts need to be captured. In this paper, we introduce a concept of
hypergraph to encode high-level semantics of a question and a knowledge base,
and to learn high-order associations between them. The proposed model,
Hypergraph Transformer, constructs a question hypergraph and a query-aware
knowledge hypergraph, and infers an answer by encoding inter-associations
between two hypergraphs and intra-associations in both hypergraph itself.
Extensive experiments on two knowledge-based visual QA and two knowledge-based
textual QA demonstrate the effectiveness of our method, especially for
multi-hop reasoning problem. Our source code is available at
https://github.com/yujungheo/kbvqa-public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WaBERT: A Low-resource End-to-end Model for Spoken Language Understanding and Speech-to-BERT Alignment. (arXiv:2204.10461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10461">
<div class="article-summary-box-inner">
<span><p>Historically lower-level tasks such as automatic speech recognition (ASR) and
speaker identification are the main focus in the speech field. Interest has
been growing in higher-level spoken language understanding (SLU) tasks
recently, like sentiment analysis (SA). However, improving performances on SLU
tasks remains a big challenge. Basically, there are two main methods for SLU
tasks: (1) Two-stage method, which uses a speech model to transfer speech to
text, then uses a language model to get the results of downstream tasks; (2)
One-stage method, which just fine-tunes a pre-trained speech model to fit in
the downstream tasks. The first method loses emotional cues such as intonation,
and causes recognition errors during ASR process, and the second one lacks
necessary language knowledge. In this paper, we propose the Wave BERT (WaBERT),
a novel end-to-end model combining the speech model and the language model for
SLU tasks. WaBERT is based on the pre-trained speech and language model, hence
training from scratch is not needed. We also set most parameters of WaBERT
frozen during training. By introducing WaBERT, audio-specific information and
language knowledge are integrated in the short-time and low-resource training
process to improve results on the dev dataset of SLUE SA tasks by 1.15% of
recall score and 0.82% of F1 score. Additionally, we modify the serial
Continuous Integrate-and-Fire (CIF) mechanism to achieve the monotonic
alignment between the speech and text modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Contrastive Clustering: Fully Unsupervised Bias Reduction for Sentiment Classification. (arXiv:2204.10467v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10467">
<div class="article-summary-box-inner">
<span><p>Background: Neural networks produce biased classification results due to
correlation bias (they learn correlations between their inputs and outputs to
classify samples, even when those correlations do not represent
cause-and-effect relationships).
</p>
<p>Objective: This study introduces a fully unsupervised method of mitigating
correlation bias, demonstrated with sentiment classification on COVID-19 social
media data.
</p>
<p>Methods: Correlation bias in sentiment classification often arises in
conversations about controversial topics. Therefore, this study uses
adversarial learning to contrast clusters based on sentiment classification
labels, with clusters produced by unsupervised topic modeling. This discourages
the neural network from learning topic-related features that produce biased
classification results.
</p>
<p>Results: Compared to a baseline classifier, neural contrastive clustering
approximately doubles accuracy on bias-prone sentences for human-labeled
COVID-19 social media data, without adversely affecting the classifier's
overall F1 score. Despite being a fully unsupervised approach, neural
contrastive clustering achieves a larger improvement in accuracy on bias-prone
sentences than a supervised masking approach.
</p>
<p>Conclusions: Neural contrastive clustering reduces correlation bias in
sentiment text classification. Further research is needed to explore
generalizing this technique to other neural network architectures and
application domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP Based Anomaly Detection for Categorical Time Series. (arXiv:2204.10483v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10483">
<div class="article-summary-box-inner">
<span><p>Identifying anomalies in large multi-dimensional time series is a crucial and
difficult task across multiple domains. Few methods exist in the literature
that address this task when some of the variables are categorical in nature. We
formalize an analogy between categorical time series and classical Natural
Language Processing and demonstrate the strength of this analogy for anomaly
detection and root cause investigation by implementing and testing three
different machine learning anomaly detection and root cause investigation
models based upon it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks. (arXiv:2204.10496v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10496">
<div class="article-summary-box-inner">
<span><p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with
carefully curated vision-language datasets. While these datasets reach an order
of 10 million samples, the labor cost is prohibitive to scale further.
Conversely, unimodal encoders are pretrained with simpler annotations that are
less cost-prohibitive, achieving scales of hundreds of millions to billions. As
a result, unimodal encoders have achieved state-of-art (SOTA) on many
downstream tasks. However, challenges remain when applying to VL tasks. The
pretraining data is not optimal for cross-modal architectures and requires
heavy computational resources. In addition, unimodal architectures lack
cross-modal interactions that have demonstrated significant benefits for VL
tasks. Therefore, how to best leverage pretrained unimodal encoders for VL
tasks is still an area of active research. In this work, we propose a method to
leverage unimodal vision and text encoders for VL tasks that augment existing
VL approaches while conserving computational complexity. Specifically, we
propose Multimodal Adaptive Distillation (MAD), which adaptively distills
useful knowledge from pretrained encoders to cross-modal VL encoders. Second,
to better capture nuanced impacts on VL task performance, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data constraints and conditions of domain shift. Experiments demonstrate that
MAD leads to consistent gains in the low-shot, domain-shifted, and
fully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA
performance on VCR compared to other single models pretrained with image-text
data. Finally, MAD outperforms concurrent works utilizing pretrained vision
encoder from CLIP. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Multi-Turn Empathetic Dialogs with Positive Emotion Elicitation. (arXiv:2204.10509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10509">
<div class="article-summary-box-inner">
<span><p>Emotional support is a crucial skill for many real-world scenarios, including
caring for the elderly, mental health support, and customer service chats. This
paper presents a novel task of empathetic dialog generation with positive
emotion elicitation to promote users' positive emotions, similar to that of
emotional support between humans. In this task, the agent conducts empathetic
responses along with the target of eliciting the user's positive emotions in
the multi-turn dialog. To facilitate the study of this task, we collect a
large-scale emotional dialog dataset with positive emotion elicitation, called
PosEmoDial (about 820k dialogs, 3M utterances). In these dialogs, the agent
tries to guide the user from any possible initial emotional state, e.g.,
sadness, to a positive emotional state. Then we present a
positive-emotion-guided dialog generation model with a novel loss function
design. This loss function encourages the dialog model to not only elicit
positive emotions from users but also ensure smooth emotional transitions along
with the whole dialog. Finally, we establish benchmark results on PosEmoDial,
and we will release this dataset and related source code to facilitate future
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taygete at SemEval-2022 Task 4: RoBERTa based models for detecting Patronising and Condescending Language. (arXiv:2204.10519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10519">
<div class="article-summary-box-inner">
<span><p>This work describes the development of different models to detect patronising
and condescending language within extracts of news articles as part of the
SemEval 2022 competition (Task-4). This work explores different models based on
the pre-trained RoBERTa language model coupled with LSTM and CNN layers. The
best models achieved 15$^{th}$ rank with an F1-score of 0.5924 for subtask-A
and 12$^{th}$ in subtask-B with a macro-F1 score of 0.3763.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem. (arXiv:2204.10521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10521">
<div class="article-summary-box-inner">
<span><p>We introduce the task of implicit offensive text detection in dialogues,
where a statement may have either an offensive or non-offensive interpretation,
depending on the listener and context. We argue that reasoning is crucial for
understanding this broader class of offensive utterances and release SLIGHT, a
dataset to support research on this task. Experiments using the data show that
state-of-the-art methods of offense detection perform poorly when asked to
detect implicitly offensive statements, achieving only ${\sim} 11\%$ accuracy.
</p>
<p>In contrast to existing offensive text detection datasets, SLIGHT features
human-annotated chains of reasoning which describe the mental process by which
an offensive interpretation can be reached from each ambiguous statement. We
explore the potential for a multi-hop reasoning approach by utilizing existing
entailment models to score the probability of these chains and show that even
naive reasoning models can yield improved performance in most situations.
Furthermore, analysis of the chains provides insight into the human
interpretation process and emphasizes the importance of incorporating
additional commonsense knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero and Few-shot Learning for Author Profiling. (arXiv:2204.10543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10543">
<div class="article-summary-box-inner">
<span><p>Author profiling classifies author characteristics by analyzing how language
is shared among people. In this work, we study that task from a low-resource
viewpoint: using little or no training data. We explore different zero and
few-shot models based on entailment and evaluate our systems on several
profiling tasks in Spanish and English. In addition, we study the effect of
both the entailment hypothesis and the size of the few-shot training sample. We
find that entailment-based models out-perform supervised text classifiers based
on roberta-XLM and that we can reach 80% of the accuracy of previous approaches
using less than 50\% of the training data on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KALA: Knowledge-Augmented Language Model Adaptation. (arXiv:2204.10555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10555">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have achieved remarkable success on
various natural language understanding tasks. Simple fine-tuning of PLMs, on
the other hand, might be suboptimal for domain-specific tasks because they
cannot possibly cover knowledge from all domains. While adaptive pre-training
of PLMs can help them obtain domain-specific knowledge, it requires a large
training cost. Moreover, adaptive pre-training can harm the PLM's performance
on the downstream task by causing catastrophic forgetting of its general
knowledge. To overcome such limitations of adaptive pre-training for PLM
adaption, we propose a novel domain adaption framework for PLMs coined as
Knowledge-Augmented Language model Adaptation (KALA), which modulates the
intermediate hidden representations of PLMs with domain knowledge, consisting
of entities and their relational facts. We validate the performance of our KALA
on question answering and named entity recognition tasks on multiple datasets
across various domains. The results show that, despite being computationally
efficient, our KALA largely outperforms adaptive pre-training. Code is
available at: https://github.com/Nardien/KALA/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues. (arXiv:2204.10558v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10558">
<div class="article-summary-box-inner">
<span><p>Ranking responses for a given dialogue context is a popular benchmark in
which the setup is to re-rank the ground-truth response over a limited set of
$n$ responses, where $n$ is typically 10. The predominance of this setup in
conversation response ranking has lead to a great deal of attention to building
neural re-rankers, while the first-stage retrieval step has been overlooked.
Since the correct answer is always available in the candidate list of $n$
responses, this artificial evaluation setup assumes that there is a first-stage
retrieval step which is always able to rank the correct response in its top-$n$
list. In this paper we focus on the more realistic task of full-rank retrieval
of responses, where $n$ can be up to millions of responses. We investigate both
dialogue context and response expansion techniques for sparse retrieval, as
well as zero-shot and fine-tuned dense retrieval approaches. Our findings based
on three different information-seeking dialogue datasets reveal that a learned
response expansion technique is a solid baseline for sparse retrieval. We find
the best performing method overall to be dense retrieval with intermediate
training, i.e. a step after the language model pre-training where sentence
representations are learned, followed by fine-tuning on the target
conversational data. We also investigate the intriguing phenomena that harder
negatives sampling techniques lead to worse results for the fine-tuned dense
retrieval models. The code and datasets are available at
https://github.com/Guzpenha/transformer_rankers/tree/full_rank_retrieval_dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Training of Neural Transducer for Speech Recognition. (arXiv:2204.10586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10586">
<div class="article-summary-box-inner">
<span><p>As one of the most popular sequence-to-sequence modeling approaches for
speech recognition, the RNN-Transducer has achieved evolving performance with
more and more sophisticated neural network models of growing size and
increasing training epochs. While strong computation resources seem to be the
prerequisite of training superior models, we try to overcome it by carefully
designing a more efficient training pipeline. In this work, we propose an
efficient 3-stage progressive training pipeline to build highly-performing
neural transducer models from scratch with very limited computation resources
in a reasonable short time period. The effectiveness of each stage is
experimentally verified on both Librispeech and Switchboard corpora. The
proposed pipeline is able to train transducer models approaching
state-of-the-art performance with a single GPU in just 2-3 weeks. Our best
conformer transducer achieves 4.1% WER on Librispeech test-other with only 35
epochs of training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Communication for Understanding Human Language Evolution: What's Missing?. (arXiv:2204.10590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10590">
<div class="article-summary-box-inner">
<span><p>Emergent communication protocols among humans and artificial neural network
agents do not yet share the same properties and show some critical mismatches
in results. We describe three important phenomena with respect to the emergence
and benefits of compositionality: ease-of-learning, generalization, and group
size effects (i.e., larger groups create more systematic languages). The latter
two are not fully replicated with neural agents, which hinders the use of
neural emergent communication for language evolution research. We argue that
one possible reason for these mismatches is that key cognitive and
communicative constraints of humans are not yet integrated. Specifically, in
humans, memory constraints and the alternation between the roles of speaker and
listener underlie the emergence of linguistic structure, yet these constraints
are typically absent in neural simulations. We suggest that introducing such
communicative and cognitive constraints would promote more linguistically
plausible behaviors with neural agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues. (arXiv:2204.10591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10591">
<div class="article-summary-box-inner">
<span><p>Dialogue systems are usually categorized into two types, open-domain and
task-oriented. The first one focuses on chatting with users and making them
engage in the conversations, where selecting a proper topic to fit the dialogue
context is essential for a successful dialogue. The other one focuses on a
specific task instead of casual talks, e.g., finding a movie on Friday night,
or playing a song. These two directions have been studied separately due to
their different purposes. However, how smoothly transitioning from social
chatting to task-oriented dialogues is important for triggering business
opportunities, and there is no public data focusing on such scenarios. Hence,
this paper focuses on investigating the conversations starting from open-domain
social chatting and then gradually transitioning to task-oriented purposes, and
releases a large-scale dataset with detailed annotations for encouraging this
research direction. To achieve this goal, this paper proposes a framework to
automatically generate many dialogues without human involvement, in which any
powerful open-domain dialogue generation model can be easily leveraged. The
human evaluation shows that our generated dialogue data has a natural flow at a
reasonable quality, showing that our released data has a great potential of
guiding future research directions and commercial activities. Furthermore, the
released models allow researchers to automatically generate unlimited dialogues
in the target scenarios, which can greatly benefit semi-supervised and
unsupervised approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LibriS2S: A German-English Speech-to-Speech Translation Corpus. (arXiv:2204.10593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10593">
<div class="article-summary-box-inner">
<span><p>Recently, we have seen an increasing interest in the area of speech-to-text
translation. This has led to astonishing improvements in this area. In
contrast, the activities in the area of speech-to-speech translation is still
limited, although it is essential to overcome the language barrier. We believe
that one of the limiting factors is the availability of appropriate training
data. We address this issue by creating LibriS2S, to our knowledge the first
publicly available speech-to-speech training corpus between German and English.
For this corpus, we used independently created audio for German and English
leading to an unbiased pronunciation of the text in both languages. This allows
the creation of a new text-to-speech and speech-to-speech translation model
that directly learns to generate the speech signal based on the pronunciation
of the source language. Using this created corpus, we propose Text-to-Speech
models based on the example of the recently proposed FastSpeech 2 model that
integrates source language information. We do this by adapting the model to
take information such as the pitch, energy or transcript from the source speech
as additional input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks. (arXiv:2204.10615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10615">
<div class="article-summary-box-inner">
<span><p>Logical approaches to representing language have developed and evaluated
computational models of quantifier words since the 19th century, but today's
NLU models still struggle to capture their semantics. We rely on Generalized
Quantifier Theory for language-independent representations of the semantics of
quantifier words, to quantify their contribution to the errors of NLU models.
We find that quantifiers are pervasive in NLU benchmarks, and their occurrence
at test time is associated with performance drops. Multilingual models also
exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse
for non-English languages. To facilitate directly-targeted probing, we present
an adversarial generalized quantifier NLI task (GQNLI) and show that
pre-trained language models have a clear lack of robustness in generalized
quantifier reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out-of-Domain Evaluation of Finnish Dependency Parsing. (arXiv:2204.10621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10621">
<div class="article-summary-box-inner">
<span><p>The prevailing practice in the academia is to evaluate the model performance
on in-domain evaluation data typically set aside from the training corpus.
However, in many real world applications the data on which the model is applied
may very substantially differ from the characteristics of the training data. In
this paper, we focus on Finnish out-of-domain parsing by introducing a novel UD
Finnish-OOD out-of-domain treebank including five very distinct data sources
(web documents, clinical, online discussions, tweets, and poetry), and a total
of 19,382 syntactic words in 2,122 sentences released under the Universal
Dependencies framework. Together with the new treebank, we present extensive
out-of-domain parsing evaluation utilizing the available section-level
information from three different Finnish UD treebanks (TDT, PUD, OOD). Compared
to the previously existing treebanks, the new Finnish-OOD is shown include
sections more challenging for the general parser, creating an interesting
evaluation setting and yielding valuable information for those applying the
parser outside of its training domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Functional Distributional Semantics with Visual Data. (arXiv:2204.10624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10624">
<div class="article-summary-box-inner">
<span><p>Functional Distributional Semantics is a recently proposed framework for
learning distributional semantics that provides linguistic interpretability. It
models the meaning of a word as a binary classifier rather than a numerical
vector. In this work, we propose a method to train a Functional Distributional
Semantics model with grounded visual data. We train it on the Visual Genome
dataset, which is closer to the kind of data encountered in human language
acquisition than a large text corpus. On four external evaluation datasets, our
model outperforms previous work on learning semantics from Visual Genome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive Search Engines: Generating Substrings as Document Identifiers. (arXiv:2204.10628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10628">
<div class="article-summary-box-inner">
<span><p>Knowledge-intensive language tasks require NLP systems to both provide the
correct answer and retrieve supporting evidence for it in a given corpus.
Autoregressive language models are emerging as the de-facto standard for
generating answers, with newer and more powerful systems emerging at an
astonishing pace. In this paper we argue that all this (and future) progress
can be directly applied to the retrieval problem with minimal intervention to
the models' architecture. Previous work has explored ways to partition the
search space into hierarchical structures and retrieve documents by
autoregressively generating their unique identifier. In this work we propose an
alternative that doesn't force any structure in the search space: using all
ngrams in a passage as its possible identifiers. This setup allows us to use an
autoregressive model to generate and score distinctive ngrams, that are then
mapped to full passages through an efficient data structure. Empirically, we
show this not only outperforms prior autoregressive approaches but also leads
to an average improvement of at least 10 points over more established retrieval
solutions for passage-level retrieval on the KILT benchmark, establishing new
state-of-the-art downstream performance on some datasets, while using a
considerably lighter memory footprint than competing systems. Code and
pre-trained models at https://github.com/facebookresearch/SEAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEKER: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering. (arXiv:2204.10629v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10629">
<div class="article-summary-box-inner">
<span><p>Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG
embedding contains concise data used in NLP tasks requiring implicit
information about the real world. Furthermore, the size of KGs that may be
useful in actual NLP assignments is enormous, and creating embedding over it
has memory cost issues. We represent KG as a 3rd-order binary tensor and move
beyond the standard CP decomposition by using a data-specific generalized
version of it. The generalization of the standard CP-ALS algorithm allows
obtaining optimization gradients without a backpropagation mechanism. It
reduces the memory needed in training while providing computational benefits.
We propose a MEKER, a memory-efficient KG embedding model, which yields
SOTA-comparable performance on link prediction tasks and KG-based Question
Answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persona-Guided Planning for Controlling the Protagonist's Persona in Story Generation. (arXiv:2204.10703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10703">
<div class="article-summary-box-inner">
<span><p>Endowing the protagonist with a specific personality is essential for writing
an engaging story. In this paper, we aim to control the protagonist's persona
in story generation, i.e., generating a story from a leading context and a
persona description, where the protagonist should exhibit the specified
personality through a coherent event sequence. Considering that personas are
usually embodied implicitly and sparsely in stories, we propose a
planning-based generation model named CONPER to explicitly model the
relationship between personas and events. CONPER first plans events of the
protagonist's behavior which are motivated by the specified persona through
predicting one target sentence, then plans the plot as a sequence of keywords
with the guidance of the predicted persona-related events and commonsense
knowledge, and finally generates the whole story. Both automatic and manual
evaluation results demonstrate that CONPER outperforms state-of-the-art
baselines for generating more coherent and persona-controllable stories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Chinese Opinion Expressions with Extremely-Noisy Crowdsourcing Annotations. (arXiv:2204.10714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10714">
<div class="article-summary-box-inner">
<span><p>Recent works of opinion expression identification (OEI) rely heavily on the
quality and scale of the manually-constructed training corpus, which could be
extremely difficult to satisfy. Crowdsourcing is one practical solution for
this problem, aiming to create a large-scale but quality-unguaranteed corpus.
In this work, we investigate Chinese OEI with extremely-noisy crowdsourcing
annotations, constructing a dataset at a very low cost. Following zhang et al.
(2021), we train the annotator-adapter model by regarding all annotations as
gold-standard in terms of crowd annotators, and test the model by using a
synthetic expert, which is a mixture of all annotators. As this
annotator-mixture for testing is never modeled explicitly in the training
phase, we propose to generate synthetic training samples by a pertinent mixup
strategy to make the training and testing highly consistent. The simulation
experiments on our constructed dataset show that crowdsourcing is highly
promising for OEI, and our proposed annotator-mixup can further enhance the
crowdsourcing modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Label-wise Attention Transformer Model for Explainable ICD Coding. (arXiv:2204.10716v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10716">
<div class="article-summary-box-inner">
<span><p>International Classification of Diseases (ICD) coding plays an important role
in systematically classifying morbidity and mortality data. In this study, we
propose a hierarchical label-wise attention Transformer model (HiLAT) for the
explainable prediction of ICD codes from clinical documents. HiLAT firstly
fine-tunes a pretrained Transformer model to represent the tokens of clinical
documents. We subsequently employ a two-level hierarchical label-wise attention
mechanism that creates label-specific document representations. These
representations are in turn used by a feed-forward neural network to predict
whether a specific ICD code is assigned to the input clinical document of
interest. We evaluate HiLAT using hospital discharge summaries and their
corresponding ICD-9 codes from the MIMIC-III database. To investigate the
performance of different types of Transformer models, we develop
ClinicalplusXLNet, which conducts continual pretraining from XLNet-Base using
all the MIMIC-III clinical notes. The experiment results show that the F1
scores of the HiLAT+ClinicalplusXLNet outperform the previous state-of-the-art
models for the top-50 most frequent ICD-9 codes from MIMIC-III. Visualisations
of attention weights present a potential explainability tool for checking the
face validity of ICD code predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Summary of the ALQAC 2021 Competition. (arXiv:2204.10717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10717">
<div class="article-summary-box-inner">
<span><p>We summarize the evaluation of the first Automated Legal Question Answering
Competition (ALQAC 2021). The competition this year contains three tasks, which
aims at processing the statute law document, which are Legal Text Information
Retrieval (Task 1), Legal Text Entailment Prediction (Task 2), and Legal Text
Question Answering (Task 3). The final goal of these tasks is to build a system
that can automatically determine whether a particular statement is lawful.
There is no limit to the approaches of the participating teams. This year,
there are 5 teams participating in Task 1, 6 teams participating in Task 2, and
5 teams participating in Task 3. There are in total 36 runs submitted to the
organizer. In this paper, we summarize each team's approaches, official
results, and some discussion about the competition. Only results of the teams
who successfully submit their approach description paper are reported in this
paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pathways through Conspiracy: The Evolution of Conspiracy Radicalization through Engagement in Online Conspiracy Discussions. (arXiv:2204.10729v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10729">
<div class="article-summary-box-inner">
<span><p>The disruptive offline mobilization of participants in online conspiracy
theory (CT) discussions has highlighted the importance of understanding how
online users may form radicalized conspiracy beliefs. While prior work
researched the factors leading up to joining online CT discussions and provided
theories of how conspiracy beliefs form, we have little understanding of how
conspiracy radicalization evolves after users join CT discussion communities.
In this paper, we provide the empirical modeling of various radicalization
phases in online CT discussion participants. To unpack how conspiracy
engagement is related to radicalization, we first characterize the users'
journey through CT discussions via conspiracy engagement pathways.
Specifically, by studying 36K Reddit users through their 169M contributions, we
uncover four distinct pathways of conspiracy engagement: steady high,
increasing, decreasing, and steady low. We further model three successive
stages of radicalization guided by prior theoretical works. Specific
sub-populations of users, namely those on steady high and increasing conspiracy
engagement pathways, progress successively through various radicalization
stages. In contrast, users on the decreasing engagement pathway show distinct
behavior: they limit their CT discussions to specialized topics, participate in
diverse discussion groups, and show reduced conformity with conspiracy
subreddits. By examining users who disengage from online CT discussions, this
paper provides promising insights about conspiracy recovery process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR. (arXiv:2204.10749v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10749">
<div class="article-summary-box-inner">
<span><p>Improving the performance of end-to-end ASR models on long utterances ranging
from minutes to hours in length is an ongoing challenge in speech recognition.
A common solution is to segment the audio in advance using a separate voice
activity detector (VAD) that decides segment boundary locations based purely on
acoustic speech/non-speech information. VAD segmenters, however, may be
sub-optimal for real-world speech where, e.g., a complete sentence that should
be taken as a whole may contain hesitations in the middle ("set an alarm for...
5 o'clock").
</p>
<p>We propose to replace the VAD with an end-to-end ASR model capable of
predicting segment boundaries in a streaming fashion, allowing the segmentation
decision to be conditioned not only on better acoustic features but also on
semantic features from the decoded text with negligible extra computation. In
experiments on real world long-form audio (YouTube) with lengths of up to 30
minutes, we demonstrate 8.5% relative WER improvement and 250 ms reduction in
median end-of-segment latency compared to the VAD segmenter baseline on a
state-of-the-art Conformer RNN-T model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaithDial: A Faithful Benchmark for Information-Seeking Dialogue. (arXiv:2204.10757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10757">
<div class="article-summary-box-inner">
<span><p>The goal of information-seeking dialogue is to respond to seeker queries with
natural language utterances that are grounded on knowledge sources. However,
dialogue systems often produce unsupported utterances, a phenomenon known as
hallucination. Dziri et al. (2022)'s investigation of hallucinations has
revealed that existing knowledge-grounded benchmarks are contaminated with
hallucinated responses at an alarming level (&gt;60% of the responses) and models
trained on this data amplify hallucinations even further (&gt;80% of the
responses). To mitigate this behavior, we adopt a data-centric solution and
create FaithDial, a new benchmark for hallucination-free dialogues, by editing
hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe
that FaithDial is more faithful than WoW while also maintaining engaging
conversations. We show that FaithDial can serve as a training signal for: i) a
hallucination critic, which discriminates whether an utterance is faithful or
not, and boosts the performance by 21.1 F1 score on the BEGIN benchmark
compared to existing datasets for dialogue coherence; ii) high-quality dialogue
generation. We benchmark a series of state-of-the-art models and propose an
auxiliary contrastive objective that achieves the highest level of faithfulness
and abstractiveness based on several automated metrics. Further, we find that
the benefits of FaithDial generalize to zero-shot transfer on other datasets,
such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that
responses generated by models trained on FaithDial are perceived as more
interpretable, cooperative, and engaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review. (arXiv:2204.10805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10805">
<div class="article-summary-box-inner">
<span><p>Peer review is a key component of the publishing process in most fields of
science. The increasing submission rates put a strain on reviewing quality and
efficiency, motivating the development of applications to support the reviewing
and editorial work. While existing NLP studies focus on the analysis of
individual texts, editorial assistance often requires modeling interactions
between pairs of texts -- yet general frameworks and datasets to support this
scenario are missing. Relationships between texts are the core object of the
intertextuality theory -- a family of approaches in literary studies not yet
operationalized in NLP. Inspired by prior theoretical work, we propose the
first intertextual model of text-based collaboration, which encompasses three
major phenomena that make up a full iteration of the review-revise-and-resubmit
cycle: pragmatic tagging, linking and long-document version alignment. While
peer review is used across the fields of science and publication formats,
existing datasets solely focus on conference-style review in computer science.
Addressing this, we instantiate our proposed model in the first annotated
multi-domain corpus in journal-style post-publication open peer review, and
provide detailed insights into the practical aspects of intertextual
annotation. Our resource is a major step towards multi-domain, fine-grained
applications of NLP in editorial support for peer review, and our intertextual
framework paves the path for general-purpose modeling of text-based
collaboration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Scaffold: Optimizing Model Explanations for Teaching. (arXiv:2204.10810v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10810">
<div class="article-summary-box-inner">
<span><p>Modern machine learning models are opaque, and as a result there is a
burgeoning academic subfield on methods that explain these models' behavior.
However, what is the precise goal of providing such explanations, and how can
we demonstrate that explanations achieve this goal? Some research argues that
explanations should help teach a student (either human or machine) to simulate
the model being explained, and that the quality of explanations can be measured
by the simulation accuracy of students on unexplained examples. In this work,
leveraging meta-learning techniques, we extend this idea to improve the quality
of the explanations themselves, specifically by optimizing explanations such
that student models more effectively learn to simulate the original model. We
train models on three natural language processing and computer vision tasks,
and find that students trained with explanations extracted with our framework
are able to simulate the teacher significantly more effectively than ones
produced with previous methods. Through human annotations and a user study, we
further find that these learned explanations more closely align with how humans
would explain the required decisions in these tasks. Our code is available at
https://github.com/coderpat/learning-scaffold
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning. (arXiv:2204.10815v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10815">
<div class="article-summary-box-inner">
<span><p>Subword tokenization is a commonly used input pre-processing step in most
recent NLP models. However, it limits the models' ability to leverage
end-to-end task learning. Its frequency-based vocabulary creation compromises
tokenization in low-resource languages, leading models to produce suboptimal
representations. Additionally, the dependency on a fixed vocabulary limits the
subword models' adaptability across languages and domains. In this work, we
propose a vocabulary-free neural tokenizer by distilling segmentation
information from heuristic-based subword tokenization. We pre-train our
character-based tokenizer by processing unique words from multilingual corpus,
thereby extensively increasing word diversity across languages. Unlike the
predefined and fixed vocabularies in subword methods, our tokenizer allows
end-to-end task learning, resulting in optimal task-specific tokenization. The
experimental results show that replacing the subword tokenizer with our neural
tokenizer consistently improves performance on multilingual (NLI) and
code-switching (sentiment analysis) tasks, with larger gains in low-resource
languages. Additionally, our neural tokenizer exhibits a robust performance on
downstream tasks when adversarial noise is present (typos and misspelling),
further increasing the initial improvements over statistical subword
tokenizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances. (arXiv:2204.10825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10825">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider mimicking fictional characters as a promising
direction for building engaging conversation models. To this end, we present a
new practical task where only a few utterances of each fictional character are
available to generate responses mimicking them. Furthermore, we propose a new
method named Pseudo Dialog Prompting (PDP) that generates responses by
leveraging the power of large-scale language models with prompts containing the
target character's utterances. To better reflect the style of the character,
PDP builds the prompts in the form of dialog that includes the character's
utterances as dialog history. Since only utterances of the characters are
available in the proposed task, PDP matches each utterance with an appropriate
pseudo-context from a predefined set of context candidates using a retrieval
model. Through human and automatic evaluation, we show that PDP generates
responses that better reflect the style of fictional characters than baseline
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting early signs of depression in the conversational domain: The role of transfer learning in low-resource scenarios. (arXiv:2204.10841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10841">
<div class="article-summary-box-inner">
<span><p>The high prevalence of depression in society has given rise to the need for
new digital tools to assist in its early detection. To this end, existing
research has mainly focused on detecting depression in the domain of social
media, where there is a sufficient amount of data. However, with the rise of
conversational agents like Siri or Alexa, the conversational domain is becoming
more critical. Unfortunately, there is a lack of data in the conversational
domain. We perform a study focusing on domain adaptation from social media to
the conversational domain. Our approach mainly exploits the linguistic
information preserved in the vector representation of text. We describe
transfer learning techniques to classify users who suffer from early signs of
depression with high recall. We achieve state-of-the-art results on a commonly
used conversational dataset, and we highlight how the method can easily be used
in conversational agents. We publicly release all source code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metric Learning and Adaptive Boundary for Out-of-Domain Detection. (arXiv:2204.10849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10849">
<div class="article-summary-box-inner">
<span><p>Conversational agents are usually designed for closed-world environments.
Unfortunately, users can behave unexpectedly. Based on the open-world
environment, we often encounter the situation that the training and test data
are sampled from different distributions. Then, data from different
distributions are called out-of-domain (OOD). A robust conversational agent
needs to react to these OOD utterances adequately. Thus, the importance of
robust OOD detection is emphasized. Unfortunately, collecting OOD data is a
challenging task. We have designed an OOD detection algorithm independent of
OOD data that outperforms a wide range of current state-of-the-art algorithms
on publicly available datasets. Our algorithm is based on a simple but
efficient approach of combining metric learning with adaptive decision
boundary. Furthermore, compared to other algorithms, we have found that our
proposed algorithm has significantly improved OOD performance in a scenario
with a lower number of classes while preserving the accuracy for in-domain
(IND) classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study into Patient Similarity through Representation Learning from Medical Records. (arXiv:2108.10682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10682">
<div class="article-summary-box-inner">
<span><p>Patient similarity assessment, which identifies patients similar to a given
patient, can help improve medical care. The assessment can be performed using
Electronic Medical Records (EMRs). Patient similarity measurement requires
converting heterogeneous EMRs into comparable formats to calculate their
distance. While versatile document representation learning methods have been
developed in recent years, it is still unclear how complex EMR data should be
processed to create the most useful patient representations. This study
presents a new data representation method for EMRs that takes the information
in clinical narratives into account. To address the limitations of previous
approaches in handling complex parts of EMR data, an unsupervised method is
proposed for building a patient representation, which integrates unstructured
data with structured data extracted from patients' EMRs. In order to model the
extracted data, we employed a tree structure that captures the temporal
relations of multiple medical events from EMR. We processed clinical notes to
extract symptoms, signs, and diseases using different tools such as medspaCy,
MetaMap, and scispaCy and mapped entities to the Unified Medical Language
System (UMLS). After creating a tree data structure, we utilized two novel
relabeling methods for the non-leaf nodes of the tree to capture two temporal
aspects of the extracted events. By traversing the tree, we generated a
sequence that could create an embedding vector for each patient. The
comprehensive evaluation of the proposed method for patient similarity and
mortality prediction tasks demonstrated that our proposed model leads to lower
mean squared error (MSE), higher precision, and normalized discounted
cumulative gain (NDCG) relative to baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regionalized models for Spanish language variations based on Twitter. (arXiv:2110.06128v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06128">
<div class="article-summary-box-inner">
<span><p>Spanish is one of the most spoken languages in the globe, but not necessarily
Spanish is written and spoken in the same way in different countries.
Understanding local language variations can help to improve model performances
on regional tasks, both understanding local structures and also improving the
message's content. For instance, think about a machine learning engineer who
automatizes some language classification task on a particular region or a
social scientist trying to understand a regional event with echoes on social
media; both can take advantage of dialect-based language models to understand
what is happening with more contextual information hence more precision.
</p>
<p>This manuscript presents and describes a set of regionalized resources for
the Spanish language built on four-year Twitter public messages geotagged in 26
Spanish-speaking countries. We introduce word embeddings based on FastText,
language models based on BERT, and per-region sample corpora. We also provide a
broad comparison among regions covering lexical and semantical similarities; as
well as examples of using regional resources on message classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction. (arXiv:2203.10316v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10316">
<div class="article-summary-box-inner">
<span><p>Solving math word problems requires deductive reasoning over the quantities
in the text. Various recent research efforts mostly relied on
sequence-to-sequence or sequence-to-tree models to generate mathematical
expressions without explicitly performing relational reasoning between
quantities in the given context. While empirically effective, such approaches
typically do not provide explanations for the generated expressions. In this
work, we view the task as a complex relation extraction problem, proposing a
novel approach that presents explainable deductive reasoning steps to
iteratively construct target expressions, where each step involves a primitive
operation over two quantities defining their relation. Through extensive
experiments on four benchmark datasets, we show that the proposed model
significantly outperforms existing strong baselines. We further demonstrate
that the deductive procedure not only presents more explainable steps but also
enables us to make more accurate predictions on questions that require more
complex reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model. (arXiv:2204.03905v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03905">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have served as important backbones for natural
language processing. Recently, in-domain pretraining has been shown to benefit
various domain-specific downstream tasks. In the biomedical domain, natural
language generation (NLG) tasks are of critical importance, while understudied.
Approaching natural language understanding (NLU) tasks as NLG achieves
satisfying performance in the general domain through constrained language
generation or language prompting. We emphasize the lack of in-domain generative
language models and the unsystematic generative downstream benchmarks in the
biomedical domain, hindering the development of the research community. In this
work, we introduce the generative language model BioBART that adapts BART to
the biomedical domain. We collate various biomedical language generation tasks
including dialogue, summarization, entity linking, and named entity
recognition. BioBART pretrained on PubMed abstracts has enhanced performance
compared to BART and set strong baselines on several tasks. Furthermore, we
conduct ablation studies on the pretraining tasks for BioBART and find that
sentence permutation has negative effects on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GERE: Generative Evidence Retrieval for Fact Verification. (arXiv:2204.05511v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05511">
<div class="article-summary-box-inner">
<span><p>Fact verification (FV) is a challenging task which aims to verify a claim
using multiple evidential sentences from trustworthy corpora, e.g., Wikipedia.
Most existing approaches follow a three-step pipeline framework, including
document retrieval, sentence retrieval and claim verification. High-quality
evidences provided by the first two steps are the foundation of the effective
reasoning in the last step. Despite being important, high-quality evidences are
rarely studied by existing works for FV, which often adopt the off-the-shelf
models to retrieve relevant documents and sentences in an
"index-retrieve-then-rank" fashion. This classical approach has clear drawbacks
as follows: i) a large document index as well as a complicated search process
is required, leading to considerable memory and computational overhead; ii)
independent scoring paradigms fail to capture the interactions among documents
and sentences in ranking; iii) a fixed number of sentences are selected to form
the final evidence set. In this work, we propose GERE, the first system that
retrieves evidences in a generative fashion, i.e., generating the document
titles as well as evidence sentence identifiers. This enables us to mitigate
the aforementioned technical issues since: i) the memory and computational cost
is greatly reduced because the document index is eliminated and the heavy
ranking process is replaced by a light generative process; ii) the dependency
between documents and that between sentences could be captured via sequential
generation process; iii) the generative formulation allows us to dynamically
select a precise set of relevant evidences for each claim. The experimental
results on the FEVER dataset show that GERE achieves significant improvements
over the state-of-the-art baselines, with both time-efficiency and
memory-efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imagination-Augmented Natural Language Understanding. (arXiv:2204.08535v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08535">
<div class="article-summary-box-inner">
<span><p>Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Situational Perception Guided Image Matting. (arXiv:2204.09276v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09276">
<div class="article-summary-box-inner">
<span><p>Most automatic matting methods try to separate the salient foreground from
the background. However, the insufficient quantity and subjective bias of the
current existing matting datasets make it difficult to fully explore the
semantic association between object-to-object and object-to-environment in a
given image. In this paper, we propose a Situational Perception Guided Image
Matting (SPG-IM) method that mitigates subjective bias of matting annotations
and captures sufficient situational perception information for better global
saliency distilled from the visual-to-textual task. SPG-IM can better associate
inter-objects and object-to-environment saliency, and compensate the subjective
nature of image matting and its expensive annotation. We also introduce a
textual Semantic Transformation (TST) module that can effectively transform and
integrate the semantic feature stream to guide the visual representations. In
addition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed
to adaptively switch multi-scale receptive fields and focal points to enhance
both global and local details. Extensive experiments demonstrate the
effectiveness of situational perception guidance from the visual-to-textual
tasks on image matting, and our model outperforms the state-of-the-art methods.
We also analyze the significance of different components in our model. The code
will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Language Modeling for Goal-Oriented Dialogue Systems. (arXiv:2204.10198v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10198">
<div class="article-summary-box-inner">
<span><p>Goal-oriented dialogue systems face a trade-off between fluent language
generation and task-specific control. While supervised learning with large
language models is capable of producing realistic text, how to steer such
responses towards completing a specific task without sacrificing language
quality remains an open question. In this work, we formulate goal-oriented
dialogue as a partially observed Markov decision process, interpreting the
language model as a representation of both the dynamics and the policy. This
view allows us to extend techniques from learning-based control, such as task
relabeling, to derive a simple and effective method to finetune language models
in a goal-aware way, leading to significantly improved task performance. We
additionally introduce a number of training strategies that serve to better
focus the model on the task at hand. We evaluate our method, Context-Aware
Language Models (CALM), on a practical flight-booking task using AirDialogue.
Empirically, CALM outperforms the state-of-the-art method by 7% in terms of
task success, matching human-level task performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Topic Modeling of Psychotherapy Sessions. (arXiv:2204.10189v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10189">
<div class="article-summary-box-inner">
<span><p>In this work, we compare different neural topic modeling methods in learning
the topical propensities of different psychiatric conditions from the
psychotherapy session transcripts parsed from speech recordings. We also
incorporate temporal modeling to put this additional interpretability to action
by parsing out topic similarities as a time series in a turn-level resolution.
We believe this topic modeling framework can offer interpretable insights for
the therapist to optimally decide his or her strategy and improve the
psychotherapy effectiveness.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Segmentation and Visualization for Tiny Objects in Multi-megapixel Images. (arXiv:2204.10356v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10356">
<div class="article-summary-box-inner">
<span><p>We introduce an interactive image segmentation and visualization framework
for identifying, inspecting, and editing tiny objects (just a few pixels wide)
in large multi-megapixel high-dynamic-range (HDR) images. Detecting cosmic rays
(CRs) in astronomical observations is a cumbersome workflow that requires
multiple tools, so we developed an interactive toolkit that unifies model
inference, HDR image visualization, segmentation mask inspection and editing
into a single graphical user interface. The feature set, initially designed for
astronomical data, makes this work a useful research-supporting tool for
human-in-the-loop tiny-object segmentation in scientific areas like
biomedicine, materials science, remote sensing, etc., as well as computer
vision. Our interface features mouse-controlled, synchronized, dual-window
visualization of the image and the segmentation mask, a critical feature for
locating tiny objects in multi-megapixel images. The browser-based tool can be
readily hosted on the web to provide multi-user access and GPU acceleration for
any device. The toolkit can also be used as a high-precision annotation tool,
or adapted as the frontend for an interactive machine learning framework. Our
open-source dataset, CR detection model, and visualization toolkit are
available at https://github.com/cy-xu/cosmic-conn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Test-Time Adaptation. (arXiv:2204.10377v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10377">
<div class="article-summary-box-inner">
<span><p>Test-time adaptation is a special setting of unsupervised domain adaptation
where a trained model on the source domain has to adapt to the target domain
without accessing source data. We propose a novel way to leverage
self-supervised contrastive learning to facilitate target feature learning,
along with an online pseudo labeling scheme with refinement that significantly
denoises pseudo labels. The contrastive learning task is applied jointly with
pseudo labeling, contrasting positive and negative pairs constructed similarly
as MoCo but with source-initialized encoder, and excluding same-class negative
pairs indicated by pseudo labels. Meanwhile, we produce pseudo labels online
and refine them via soft voting among their nearest neighbors in the target
feature space, enabled by maintaining a memory queue. Our method, AdaContrast,
achieves state-of-the-art performance on major benchmarks while having several
desirable properties compared to existing works, including memory efficiency,
insensitivity to hyper-parameters, and better model calibration. Project page:
sites.google.com/view/adacontrast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 6th AI City Challenge. (arXiv:2204.10380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10380">
<div class="article-summary-box-inner">
<span><p>The 6th edition of the AI City Challenge specifically focuses on problems in
two domains where there is tremendous unlocked potential at the intersection of
computer vision and artificial intelligence: Intelligent Traffic Systems (ITS),
and brick and mortar retail businesses. The four challenge tracks of the 2022
AI City Challenge received participation requests from 254 teams across 27
countries. Track 1 addressed city-scale multi-target multi-camera (MTMC)
vehicle tracking. Track 2 addressed natural-language-based vehicle track
retrieval. Track 3 was a brand new track for naturalistic driving analysis,
where the data were captured by several cameras mounted inside the vehicle
focusing on driver safety, and the task was to classify driver actions. Track 4
was another new track aiming to achieve retail store automated checkout using
only a single view camera. We released two leader boards for submissions based
on different methods, including a public leader board for the contest, where no
use of external data is allowed, and a general leader board for all submitted
results. The top performance of participating teams established strong
baselines and even outperformed the state-of-the-art in the proposed challenge
tracks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Depth Estimation Using Cues Inspired by Biological Vision Systems. (arXiv:2204.10384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10384">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation (MDE) aims to transform an RGB image of a scene
into a pixelwise depth map from the same camera view. It is fundamentally
ill-posed due to missing information: any single image can have been taken from
many possible 3D scenes. Part of the MDE task is, therefore, to learn which
visual cues in the image can be used for depth estimation, and how. With
training data limited by cost of annotation or network capacity limited by
computational power, this is challenging. In this work we demonstrate that
explicitly injecting visual cue information into the model is beneficial for
depth estimation. Following research into biological vision systems, we focus
on semantic information and prior knowledge of object sizes and their
relations, to emulate the biological cues of relative size, familiar size, and
absolute size. We use state-of-the-art semantic and instance segmentation
models to provide external information, and exploit language embeddings to
encode relational information between classes. We also provide a prior on the
average real-world size of objects. This external information overcomes the
limitation in data availability, and ensures that the limited capacity of a
given network is focused on known-helpful cues, therefore improving
performance. We experimentally validate our hypothesis and evaluate the
proposed model on the widely used NYUD2 indoor depth estimation benchmark. The
results show improvements in depth prediction when the semantic information,
size prior and instance size are explicitly provided along with the RGB images,
and our method can be easily adapted to any depth estimation system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Sequential Latent Variable Models from Multimodal Time Series Data. (arXiv:2204.10419v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10419">
<div class="article-summary-box-inner">
<span><p>Sequential modelling of high-dimensional data is an important problem that
appears in many domains including model-based reinforcement learning and
dynamics identification for control. Latent variable models applied to
sequential data (i.e., latent dynamics models) have been shown to be a
particularly effective probabilistic approach to solve this problem, especially
when dealing with images. However, in many application areas (e.g., robotics),
information from multiple sensing modalities is available -- existing latent
dynamics methods have not yet been extended to effectively make use of such
multimodal sequential data. Multimodal sensor streams can be correlated in a
useful manner and often contain complementary information across modalities. In
this work, we present a self-supervised generative modelling framework to
jointly learn a probabilistic latent state representation of multimodal data
and the respective dynamics. Using synthetic and real-world datasets from a
multimodal robotic planar pushing task, we demonstrate that our approach leads
to significant improvements in prediction and representation quality.
Furthermore, we compare to the common learning baseline of concatenating each
modality in the latent space and show that our principled probabilistic
formulation performs better. Finally, despite being fully self-supervised, we
demonstrate that our method is nearly as effective as an existing supervised
approach that relies on ground truth labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PreTraM: Self-Supervised Pre-training via Connecting Trajectory and Map. (arXiv:2204.10435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10435">
<div class="article-summary-box-inner">
<span><p>Deep learning has recently achieved significant progress in trajectory
forecasting. However, the scarcity of trajectory data inhibits the data-hungry
deep-learning models from learning good representations. While mature
representation learning methods exist in computer vision and natural language
processing, these pre-training methods require large-scale data. It is hard to
replicate these approaches in trajectory forecasting due to the lack of
adequate trajectory data (e.g., 34K samples in the nuScenes dataset). To work
around the scarcity of trajectory data, we resort to another data modality
closely related to trajectories-HD-maps, which is abundantly provided in
existing datasets. In this paper, we propose PreTraM, a self-supervised
pre-training scheme via connecting trajectories and maps for trajectory
forecasting. Specifically, PreTraM consists of two parts: 1) Trajectory-Map
Contrastive Learning, where we project trajectories and maps to a shared
embedding space with cross-modal contrastive learning, and 2) Map Contrastive
Learning, where we enhance map representation with contrastive learning on
large quantities of HD-maps. On top of popular baselines such as AgentFormer
and Trajectron++, PreTraM boosts their performance by 5.5% and 6.9% relatively
in FDE-10 on the challenging nuScenes dataset. We show that PreTraM improves
data efficiency and scales well with model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-Equivariant Unrolled Neural Networks for Data-Efficient Accelerated MRI Reconstruction. (arXiv:2204.10436v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10436">
<div class="article-summary-box-inner">
<span><p>Unrolled neural networks have enabled state-of-the-art reconstruction
performance and fast inference times for the accelerated magnetic resonance
imaging (MRI) reconstruction task. However, these approaches depend on
fully-sampled scans as ground truth data which is either costly or not possible
to acquire in many clinical medical imaging applications; hence, reducing
dependence on data is desirable. In this work, we propose modeling the proximal
operators of unrolled neural networks with scale-equivariant convolutional
neural networks in order to improve the data-efficiency and robustness to
drifts in scale of the images that might stem from the variability of patient
anatomies or change in field-of-view across different MRI scanners. Our
approach demonstrates strong improvements over the state-of-the-art unrolled
neural networks under the same memory constraints both with and without data
augmentations on both in-distribution and out-of-distribution scaled images
without significantly increasing the train or inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis. (arXiv:2204.10437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10437">
<div class="article-summary-box-inner">
<span><p>Discriminative learning, restorative learning, and adversarial learning have
proven beneficial for self-supervised learning schemes in computer vision and
medical imaging. Existing efforts, however, omit their synergistic effects on
each other in a ternary setup, which, we envision, can significantly benefit
deep semantic representation learning. To realize this vision, we have
developed DiRA, the first framework that unites discriminative, restorative,
and adversarial learning in a unified manner to collaboratively glean
complementary visual information from unlabeled medical images for fine-grained
semantic representation learning. Our extensive experiments demonstrate that
DiRA (1) encourages collaborative learning among three learning ingredients,
resulting in more generalizable representation across organs, diseases, and
modalities; (2) outperforms fully supervised ImageNet models and increases
robustness in small data regimes, reducing annotation cost across multiple
medical imaging applications; (3) learns fine-grained semantic representation,
facilitating accurate lesion localization with only image-level annotation; and
(4) enhances state-of-the-art restorative approaches, revealing that DiRA is a
general mechanism for united representation learning. All code and pre-trained
models are available at https: //github.com/JLiangLab/DiRA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering. (arXiv:2204.10448v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10448">
<div class="article-summary-box-inner">
<span><p>Knowledge-based visual question answering (QA) aims to answer a question
which requires visually-grounded external knowledge beyond image content
itself. Answering complex questions that require multi-hop reasoning under weak
supervision is considered as a challenging problem since i) no supervision is
given to the reasoning process and ii) high-order semantics of multi-hop
knowledge facts need to be captured. In this paper, we introduce a concept of
hypergraph to encode high-level semantics of a question and a knowledge base,
and to learn high-order associations between them. The proposed model,
Hypergraph Transformer, constructs a question hypergraph and a query-aware
knowledge hypergraph, and infers an answer by encoding inter-associations
between two hypergraphs and intra-associations in both hypergraph itself.
Extensive experiments on two knowledge-based visual QA and two knowledge-based
textual QA demonstrate the effectiveness of our method, especially for
multi-hop reasoning problem. Our source code is available at
https://github.com/yujungheo/kbvqa-public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dynamic View Synthesis With Few RGBD Cameras. (arXiv:2204.10477v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10477">
<div class="article-summary-box-inner">
<span><p>There have been significant advancements in dynamic novel view synthesis in
recent years. However, current deep learning models often require (1) prior
models (e.g., SMPL human models), (2) heavy pre-processing, or (3) per-scene
optimization. We propose to utilize RGBD cameras to remove these limitations
and synthesize free-viewpoint videos of dynamic indoor scenes. We generate
feature point clouds from RGBD frames and then render them into free-viewpoint
videos via a neural renderer. However, the inaccurate, unstable, and incomplete
depth measurements induce severe distortions, flickering, and ghosting
artifacts. We enforce spatial-temporal consistency via the proposed Cycle
Reconstruction Consistency and Temporal Stabilization module to reduce these
artifacts. We introduce a simple Regional Depth-Inpainting module that
adaptively inpaints missing depth values to render complete novel views.
Additionally, we present a Human-Things Interactions dataset to validate our
approach and facilitate future research. The dataset consists of 43 multi-view
RGBD video sequences of everyday activities, capturing complex interactions
between human subjects and their surroundings. Experiments on the HTI dataset
show that our method outperforms the baseline per-frame image fidelity and
spatial-temporal consistency. We will release our code, and the dataset on the
website soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Affine Transformation for Text-to-image Synthesis. (arXiv:2204.10482v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10482">
<div class="article-summary-box-inner">
<span><p>Text-to-image synthesis aims to generate natural images conditioned on text
descriptions. The main difficulty of this task lies in effectively fusing text
information into the image synthesis process. Existing methods usually
adaptively fuse suitable text information into the synthesis process with
multiple isolated fusion blocks (e.g., Conditional
</p>
<p>Batch Normalization and Instance Normalization). However, isolated fusion
blocks not only conflict with each other but also increase the difficulty of
training (see first page of the supplementary). To address these issues, we
propose a Recurrent Affine Transformation (RAT) for Generative Adversarial
Networks that connects all the fusion blocks with a recurrent neural network to
model their long-term dependency. Besides, to improve semantic consistency
between texts and synthesized images, we incorporate a spatial attention model
in the discriminator. Being aware of matching image regions, text descriptions
supervise the generator to synthesize more relevant image contents. Extensive
experiments on the CUB, Oxford-102 and COCO datasets demonstrate the
superiority of the proposed model in comparison to state-of-the-art models
\footnote{https://github.com/senmaoy/Recurrent-Affine-Transformation-for-Text-to-image-Synthesis.git}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SE-GAN: Skeleton Enhanced GAN-based Model for Brush Handwriting Font Generation. (arXiv:2204.10484v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10484">
<div class="article-summary-box-inner">
<span><p>Previous works on font generation mainly focus on the standard print fonts
where character's shape is stable and strokes are clearly separated. There is
rare research on brush handwriting font generation, which involves holistic
structure changes and complex strokes transfer. To address this issue, we
propose a novel GAN-based image translation model by integrating the skeleton
information. We first extract the skeleton from training images, then design an
image encoder and a skeleton encoder to extract corresponding features. A
self-attentive refined attention module is devised to guide the model to learn
distinctive features between different domains. A skeleton discriminator is
involved to first synthesize the skeleton image from the generated image with a
pre-trained generator, then to judge its realness to the target one. We also
contribute a large-scale brush handwriting font image dataset with six styles
and 15,000 high-resolution images. Both quantitative and qualitative
experimental results demonstrate the competitiveness of our proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentions Help CNNs See Better: Attention-based Hybrid Image Quality Assessment Network. (arXiv:2204.10485v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10485">
<div class="article-summary-box-inner">
<span><p>Image quality assessment (IQA) algorithm aims to quantify the human
perception of image quality. Unfortunately, there is a performance drop when
assessing the distortion images generated by generative adversarial network
(GAN) with seemingly realistic texture. In this work, we conjecture that this
maladaptation lies in the backbone of IQA models, where patch-level prediction
methods use independent image patches as input to calculate their scores
separately, but lack spatial relationship modeling among image patches.
Therefore, we propose an Attention-based Hybrid Image Quality Assessment
Network (AHIQ) to deal with the challenge and get better performance on the
GAN-based IQA task. Firstly, we adopt a two-branch architecture, including a
vision transformer (ViT) branch and a convolutional neural network (CNN) branch
for feature extraction. The hybrid architecture combines interaction
information among image patches captured by ViT and local texture details from
CNN. To make the features from shallow CNN more focused on the visually salient
region, a deformable convolution is applied with the help of semantic
information from the ViT branch. Finally, we use a patch-wise score prediction
module to obtain the final score. The experiments show that our model
outperforms the state-of-the-art methods on four standard IQA datasets and AHIQ
ranked first on the Full Reference (FR) track of the NTIRE 2022 Perceptual
Image Quality Assessment Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks. (arXiv:2204.10496v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10496">
<div class="article-summary-box-inner">
<span><p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with
carefully curated vision-language datasets. While these datasets reach an order
of 10 million samples, the labor cost is prohibitive to scale further.
Conversely, unimodal encoders are pretrained with simpler annotations that are
less cost-prohibitive, achieving scales of hundreds of millions to billions. As
a result, unimodal encoders have achieved state-of-art (SOTA) on many
downstream tasks. However, challenges remain when applying to VL tasks. The
pretraining data is not optimal for cross-modal architectures and requires
heavy computational resources. In addition, unimodal architectures lack
cross-modal interactions that have demonstrated significant benefits for VL
tasks. Therefore, how to best leverage pretrained unimodal encoders for VL
tasks is still an area of active research. In this work, we propose a method to
leverage unimodal vision and text encoders for VL tasks that augment existing
VL approaches while conserving computational complexity. Specifically, we
propose Multimodal Adaptive Distillation (MAD), which adaptively distills
useful knowledge from pretrained encoders to cross-modal VL encoders. Second,
to better capture nuanced impacts on VL task performance, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data constraints and conditions of domain shift. Experiments demonstrate that
MAD leads to consistent gains in the low-shot, domain-shifted, and
fully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA
performance on VCR compared to other single models pretrained with image-text
data. Finally, MAD outperforms concurrent works utilizing pretrained vision
encoder from CLIP. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keypoint based Sign Language Translation without Glosses. (arXiv:2204.10511v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10511">
<div class="article-summary-box-inner">
<span><p>Sign Language Translation (SLT) is a task that has not been studied
relatively much compared to the study of Sign Language Recognition (SLR).
However, the SLR is a study that recognizes the unique grammar of sign
language, which is different from the spoken language and has a problem that
non-disabled people cannot easily interpret. So, we're going to solve the
problem of translating directly spoken language in sign language video. To this
end, we propose a new keypoint normalization method for performing translation
based on the skeleton point of the signer and robustly normalizing these points
in sign language translation. It contributed to performance improvement by a
customized normalization method depending on the body parts. In addition, we
propose a stochastic frame selection method that enables frame augmentation and
sampling at the same time. Finally, it is translated into the spoken language
through an Attention-based translation model. Our method can be applied to
various datasets in a way that can be applied to datasets without glosses. In
addition, quantitative experimental evaluation proved the excellence of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIPR:Automatic Annotation of Medical Images with Pixel Rearrangement. (arXiv:2204.10513v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10513">
<div class="article-summary-box-inner">
<span><p>Most of the state-of-the-art semantic segmentation reported in recent years
is based on fully supervised deep learning in the medical domain. How?ever, the
high-quality annotated datasets require intense labor and domain knowledge,
consuming enormous time and cost. Previous works that adopt semi?supervised and
unsupervised learning are proposed to address the lack of anno?tated data
through assisted training with unlabeled data and achieve good perfor?mance.
Still, these methods can not directly get the image annotation as doctors do.
In this paper, inspired by self-training of semi-supervised learning, we
pro?pose a novel approach to solve the lack of annotated data from another
angle, called medical image pixel rearrangement (short in MIPR). The MIPR
combines image-editing and pseudo-label technology to obtain labeled data. As
the number of iterations increases, the edited image is similar to the original
image, and the labeled result is similar to the doctor annotation. Therefore,
the MIPR is to get labeled pairs of data directly from amounts of unlabled data
with pixel rearrange?ment, which is implemented with a designed conditional
Generative Adversarial Networks and a segmentation network. Experiments on the
ISIC18 show that the effect of the data annotated by our method for
segmentation task is is equal to or even better than that of doctors
annotations
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Object Detection with Proposal Balance Refinement. (arXiv:2204.10527v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10527">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection has gained significant attention in recent years as
it has the potential to greatly reduce the reliance on large amounts of
manually annotated bounding boxes. While most existing few-shot object
detection literature primarily focuses on bounding box classification by
obtaining as discriminative feature embeddings as possible, we emphasize the
necessity of handling the lack of intersection-over-union (IoU) variations
induced by a biased distribution of novel samples. In this paper, we analyze
the IoU imbalance that is caused by the relatively high number of low-quality
region proposals, and reveal that it plays a critical role in improving
few-shot learning capabilities. The well-known two stage fine-tuning technique
causes insufficient quality and quantity of the novel positive samples, which
hinders the effective object detection of unseen novel classes. To alleviate
this issue, we present a few-shot object detection model with proposal balance
refinement, a simple yet effective approach in learning object proposals using
an auxiliary sequential bounding box refinement process. This process enables
the detector to be optimized on the various IoU scores through additional novel
class samples. To fully exploit our sequential stage architecture, we revise
the fine-tuning strategy and expose the Region Proposal Network to the novel
classes in order to provide increased learning opportunities for the
region-of-interest (RoI) classifiers and regressors. Our extensive assessments
on PASCAL VOC and COCO demonstrate that our framework substantially outperforms
other existing few-shot object detection approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fourier Imager Network (FIN): A deep neural network for hologram reconstruction with superior external generalization. (arXiv:2204.10533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10533">
<div class="article-summary-box-inner">
<span><p>Deep learning-based image reconstruction methods have achieved remarkable
success in phase recovery and holographic imaging. However, the generalization
of their image reconstruction performance to new types of samples never seen by
the network remains a challenge. Here we introduce a deep learning framework,
termed Fourier Imager Network (FIN), that can perform end-to-end phase recovery
and image reconstruction from raw holograms of new types of samples, exhibiting
unprecedented success in external generalization. FIN architecture is based on
spatial Fourier transform modules that process the spatial frequencies of its
inputs using learnable filters and a global receptive field. Compared with
existing convolutional deep neural networks used for hologram reconstruction,
FIN exhibits superior generalization to new types of samples, while also being
much faster in its image inference speed, completing the hologram
reconstruction task in ~0.04 s per 1 mm^2 of the sample area. We experimentally
validated the performance of FIN by training it using human lung tissue samples
and blindly testing it on human prostate, salivary gland tissue and Pap smear
samples, proving its superior external generalization and image reconstruction
speed. Beyond holographic microscopy and quantitative phase imaging, FIN and
the underlying neural network architecture might open up various new
opportunities to design broadly generalizable deep learning models in
computational imaging and machine vision fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alleviating Representational Shift for Continual Fine-tuning. (arXiv:2204.10535v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10535">
<div class="article-summary-box-inner">
<span><p>We study a practical setting of continual learning: fine-tuning on a
pre-trained model continually. Previous work has found that, when training on
new tasks, the features (penultimate layer representations) of previous data
will change, called representational shift. Besides the shift of features, we
reveal that the intermediate layers' representational shift (IRS) also matters
since it disrupts batch normalization, which is another crucial cause of
catastrophic forgetting. Motivated by this, we propose ConFiT, a fine-tuning
method incorporating two components, cross-convolution batch normalization
(Xconv BN) and hierarchical fine-tuning. Xconv BN maintains pre-convolution
running means instead of post-convolution, and recovers post-convolution ones
before testing, which corrects the inaccurate estimates of means under IRS.
Hierarchical fine-tuning leverages a multi-stage strategy to fine-tune the
pre-trained network, preventing massive changes in Conv layers and thus
alleviating IRS. Experimental results on four datasets show that our method
remarkably outperforms several state-of-the-art methods with lower storage
overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth Pruning with Auxiliary Networks for TinyML. (arXiv:2204.10546v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10546">
<div class="article-summary-box-inner">
<span><p>Pruning is a neural network optimization technique that sacrifices accuracy
in exchange for lower computational requirements. Pruning has been useful when
working with extremely constrained environments in tinyML. Unfortunately,
special hardware requirements and limited study on its effectiveness on already
compact models prevent its wider adoption. Depth pruning is a form of pruning
that requires no specialized hardware but suffers from a large accuracy
falloff. To improve this, we propose a modification that utilizes a highly
efficient auxiliary network as an effective interpreter of intermediate feature
maps. Our results show a parameter reduction of 93% on the MLPerfTiny Visual
Wakewords (VWW) task and 28% on the Keyword Spotting (KWS) task with accuracy
cost of 0.65% and 1.06% respectively. When evaluated on a Cortex-M0
microcontroller, our proposed method reduces the VWW model size by 4.7x and
latency by 1.6x while counter intuitively gaining 1% accuracy. KWS model size
on Cortex-M0 was also reduced by 1.2x and latency by 1.2x at the cost of 2.21%
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JIFF: Jointly-aligned Implicit Face Function for High Quality Single View Clothed Human Reconstruction. (arXiv:2204.10549v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10549">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of single view 3D human reconstruction.
Recent implicit function based methods have shown impressive results, but they
fail to recover fine face details in their reconstructions. This largely
degrades user experience in applications like 3D telepresence. In this paper,
we focus on improving the quality of face in the reconstruction and propose a
novel Jointly-aligned Implicit Face Function (JIFF) that combines the merits of
the implicit function based approach and model based approach. We employ a 3D
morphable face model as our shape prior and compute space-aligned 3D features
that capture detailed face geometry information. Such space-aligned 3D features
are combined with pixel-aligned 2D features to jointly predict an implicit face
function for high quality face reconstruction. We further extend our pipeline
and introduce a coarse-to-fine architecture to predict high quality texture for
our detailed face model. Extensive evaluations have been carried out on public
datasets and our proposed JIFF has demonstrates superior performance (both
quantitatively and qualitatively) over existing state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of an algorithm for medical image segmentation of bone tissue in interaction with metallic implants. (arXiv:2204.10560v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10560">
<div class="article-summary-box-inner">
<span><p>This preliminary study focuses on the development of a medical image
segmentation algorithm based on artificial intelligence for calculating bone
growth in contact with metallic implants. %as a result of the problem of
estimating the growth of new bone tissue due to artifacts. %the presence of
various types of distortions and errors, known as artifacts.
</p>
<p>Two databases consisting of computerized microtomography images have been
used throughout this work: 100 images for training and 196 images for testing.
Both bone and implant tissue were manually segmented in the training data set.
The type of network constructed follows the U-Net architecture, a convolutional
neural network explicitly used for medical image segmentation.
</p>
<p>In terms of network accuracy, the model reached around 98\%. Once the
prediction was obtained from the new data set (test set), the total number of
pixels belonging to bone tissue was calculated. This volume is around 15\% of
the volume estimated by conventional techniques, which are usually
overestimated. This method has shown its good performance and results, although
it has a wide margin for improvement, modifying various parameters of the
networks or using larger databases to improve training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Clustering as an Emergent Consensus of Autonomous Agents. (arXiv:2204.10585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10585">
<div class="article-summary-box-inner">
<span><p>We present a data segmentation method based on a first-order density-induced
consensus protocol. We provide a mathematically rigorous analysis of the
consensus model leading to the stopping criteria of the data segmentation
algorithm. To illustrate our method, the algorithm is applied to
two-dimensional shape datasets and selected images from Berkeley Segmentation
Dataset. The method can be seen as an augmentation of classical clustering
techniques for multimodal feature space, such as DBSCAN. It showcases a curious
connection between data clustering and collective behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Note on the Regularity of Images Generated by Convolutional Neural Networks. (arXiv:2204.10588v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10588">
<div class="article-summary-box-inner">
<span><p>The regularity of images generated by convolutional neural networks, such as
the U-net, generative adversarial networks, or the deep image prior, is
analyzed. In a resolution-independent, infinite dimensional setting, it is
shown that such images, represented as functions, are always continuous and, in
some circumstances, even continuously differentiable, contradicting the widely
accepted modeling of sharp edges in images via jump discontinuities. While such
statements require an infinite dimensional setting, the connection to
(discretized) neural networks used in practice is made by considering the limit
as the resolution approaches infinity. As practical consequence, the results of
this paper suggest to refrain from basic L2 regularization of network weights
in case of images being the network output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spacing Loss for Discovering Novel Categories. (arXiv:2204.10595v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10595">
<div class="article-summary-box-inner">
<span><p>Novel Class Discovery (NCD) is a learning paradigm, where a machine learning
model is tasked to semantically group instances from unlabeled data, by
utilizing labeled instances from a disjoint set of classes. In this work, we
first characterize existing NCD approaches into single-stage and two-stage
methods based on whether they require access to labeled and unlabeled data
together while discovering new classes. Next, we devise a simple yet powerful
loss function that enforces separability in the latent space using cues from
multi-dimensional scaling, which we refer to as Spacing Loss. Our proposed
formulation can either operate as a standalone method or can be plugged into
existing methods to enhance them. We validate the efficacy of Spacing Loss with
thorough experimental evaluation across multiple settings on CIFAR-10 and
CIFAR-100 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Expert Utilization in Mixture-of-Experts Layers Embedded in CNNs. (arXiv:2204.10598v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10598">
<div class="article-summary-box-inner">
<span><p>This work addresses the problem of unbalanced expert utilization in
sparsely-gated Mixture of Expert (MoE) layers, embedded directly into
convolutional neural networks. To enable a stable training process, we present
both soft and hard constraint-based approaches. With hard constraints, the
weights of certain experts are allowed to become zero, while soft constraints
balance the contribution of experts with an additional auxiliary loss. As a
result, soft constraints handle expert utilization better and support the
expert specialization process, hard constraints mostly maintain generalized
experts and increase the model performance for many applications. Our findings
demonstrate that even with a single dataset and end-to-end training, experts
can implicitly focus on individual sub-domains of the input space. Experts in
the proposed models with MoE embeddings implicitly focus on distinct domains,
even without suitable predefined datasets. As an example, experts trained for
CIFAR-100 image classification specialize in recognizing different domains such
as sea animals or flowers without previous data clustering. Experiments with
RetinaNet and the COCO dataset further indicate that object detection experts
can also specialize in detecting objects of distinct sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstructing Surfaces for Sparse Point Clouds with On-Surface Priors. (arXiv:2204.10603v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10603">
<div class="article-summary-box-inner">
<span><p>It is an important task to reconstruct surfaces from 3D point clouds. Current
methods are able to reconstruct surfaces by learning Signed Distance Functions
(SDFs) from single point clouds without ground truth signed distances or point
normals. However, they require the point clouds to be dense, which dramatically
limits their performance in real applications. To resolve this issue, we
propose to reconstruct highly accurate surfaces from sparse point clouds with
an on-surface prior. We train a neural network to learn SDFs via projecting
queries onto the surface represented by the sparse point cloud. Our key idea is
to infer signed distances by pushing both the query projections to be on the
surface and the projection distance to be the minimum. To achieve this, we
train a neural network to capture the on-surface prior to determine whether a
point is on a sparse point cloud or not, and then leverage it as a
differentiable function to learn SDFs from unseen sparse point cloud. Our
method can learn SDFs from a single sparse point cloud without ground truth
signed distances or point normals. Our numerical evaluation under widely used
benchmarks demonstrates that our method achieves state-of-the-art
reconstruction accuracy, especially for sparse point clouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing the Transferability via Feature-Momentum Adversarial Attack. (arXiv:2204.10606v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10606">
<div class="article-summary-box-inner">
<span><p>Transferable adversarial attack has drawn increasing attention due to their
practical threaten to real-world applications. In particular, the feature-level
adversarial attack is one recent branch that can enhance the transferability
via disturbing the intermediate features. The existing methods usually create a
guidance map for features, where the value indicates the importance of the
corresponding feature element and then employs an iterative algorithm to
disrupt the features accordingly. However, the guidance map is fixed in
existing methods, which can not consistently reflect the behavior of networks
as the image is changed during iteration. In this paper, we describe a new
method called Feature-Momentum Adversarial Attack (FMAA) to further improve
transferability. The key idea of our method is that we estimate a guidance map
dynamically at each iteration using momentum to effectively disturb the
category-relevant features. Extensive experiments demonstrate that our method
significantly outperforms other state-of-the-art methods by a large margin on
different target models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time HOG+SVM based object detection using SoC FPGA for a UHD video stream. (arXiv:2204.10619v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10619">
<div class="article-summary-box-inner">
<span><p>Object detection is an essential component of many vision systems. For
example, pedestrian detection is used in advanced driver assistance systems
(ADAS) and advanced video surveillance systems (AVSS). Currently, most
detectors use deep convolutional neural networks (e.g., the YOLO -- You Only
Look Once -- family), which, however, due to their high computational
complexity, are not able to process a very high-resolution video stream in
real-time, especially within a limited energy budget. In this paper we present
a hardware implementation of the well-known pedestrian detector with HOG
(Histogram of Oriented Gradients) feature extraction and SVM (Support Vector
Machine) classification. Our system running on AMD Xilinx Zynq UltraScale+
MPSoC (Multiprocessor System on Chip) device allows real-time processing of 4K
resolution (UHD -- Ultra High Definition, 3840 x 2160 pixels) video for 60
frames per second. The system is capable of detecting a pedestrian in a single
scale. The results obtained confirm the high suitability of reprogrammable
devices in the real-time implementation of embedded vision systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Prototype Convolution Network for Few-Shot Semantic Segmentation. (arXiv:2204.10638v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10638">
<div class="article-summary-box-inner">
<span><p>The key challenge for few-shot semantic segmentation (FSS) is how to tailor a
desirable interaction among support and query features and/or their prototypes,
under the episodic training scenario. Most existing FSS methods implement such
support-query interactions by solely leveraging plain operations - e.g., cosine
similarity and feature concatenation - for segmenting the query objects.
However, these interaction approaches usually cannot well capture the intrinsic
object details in the query images that are widely encountered in FSS, e.g., if
the query object to be segmented has holes and slots, inaccurate segmentation
almost always happens. To this end, we propose a dynamic prototype convolution
network (DPCN) to fully capture the aforementioned intrinsic details for
accurate FSS. Specifically, in DPCN, a dynamic convolution module (DCM) is
firstly proposed to generate dynamic kernels from support foreground, then
information interaction is achieved by convolution operations over query
features using these kernels. Moreover, we equip DPCN with a support activation
module (SAM) and a feature filtering module (FFM) to generate pseudo mask and
filter out background information for the query images, respectively. SAM and
FFM together can mine enriched context information from the query features. Our
DPCN is also flexible and efficient under the k-shot FSS setting. Extensive
experiments on PASCAL-5i and COCO-20i show that DPCN yields superior
performances under both 1-shot and 5-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposure Correction Model to Enhance Image Quality. (arXiv:2204.10648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10648">
<div class="article-summary-box-inner">
<span><p>Exposure errors in an image cause a degradation in the contrast and low
visibility in the content. In this paper, we address this problem and propose
an end-to-end exposure correction model in order to handle both under- and
overexposure errors with a single model. Our model contains an image encoder,
consecutive residual blocks, and image decoder to synthesize the corrected
image. We utilize perceptual loss, feature matching loss, and multi-scale
discriminator to increase the quality of the generated image as well as to make
the training more stable. The experimental results indicate the effectiveness
of proposed model. We achieve the state-of-the-art result on a large-scale
exposure dataset. Besides, we investigate the effect of exposure setting of the
image on the portrait matting task. We find that under- and overexposed images
cause severe degradation in the performance of the portrait matting models. We
show that after applying exposure correction with the proposed model, the
portrait matting quality increases significantly.
https://github.com/yamand16/ExposureCorrection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFAM-DETR: Deformable feature based attention mechanism DETR on slender object detection. (arXiv:2204.10667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10667">
<div class="article-summary-box-inner">
<span><p>Object detection is one of the most significant aspects of computer vision,
and it has achieved substantial results in a variety of domains. It is worth
noting that there are few studies focusing on slender object detection. CNNs
are widely employed in object detection, however it performs poorly on slender
object detection due to the fixed geometric structure and sampling points. In
comparison, Deformable DETR has the ability to obtain global to specific
features. Even though it outperforms the CNNs in slender objects detection
accuracy and efficiency, the results are still not satisfactory. Therefore, we
propose Deformable Feature based Attention Mechanism (DFAM) to increase the
slender object detection accuracy and efficiency of Deformable DETR. The DFAM
has adaptive sampling points of deformable convolution and attention mechanism
that aggregate information from the entire input sequence in the backbone
network. This improved detector is named as Deformable Feature based Attention
Mechanism DETR (DFAM- DETR). Results indicate that DFAM-DETR achieves
outstanding detection performance on slender objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unknown Face Presentation Attack Detection via Localised Learning of Multiple Kernels. (arXiv:2204.10675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10675">
<div class="article-summary-box-inner">
<span><p>The paper studies face spoofing, a.k.a. presentation attack detection (PAD)
in the demanding scenarios of unknown types of attack. While earlier studies
have revealed the benefits of ensemble methods, and in particular, a multiple
kernel learning approach to the problem, one limitation of such techniques is
that they typically treat the entire observation space similarly and ignore any
variability and local structure inherent to the data. This work studies this
aspect of the face presentation attack detection problem in relation to
multiple kernel learning in a one-class setting to benefit from intrinsic local
structure in bona fide face samples. More concretely, inspired by the success
of the one-class Fisher null formalism, we formulate a convex localised
multiple kernel learning algorithm by imposing a joint matrix-norm constraint
on the collection of local kernel weights and infer locally adaptive weights
for zero-shot one-class unseen attack detection.
</p>
<p>We present a theoretical study of the proposed localised MKL algorithm using
Rademacher complexities to characterise its generalisation capability and
demonstrate the advantages of the proposed technique over some other options.
An assessment of the proposed approach on general object image datasets
illustrates its efficacy for abnormality and novelty detection while the
results of the experiments on face PAD datasets verifies its potential in
detecting unknown/unseen face presentation attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving tracking with a tracklet associator. (arXiv:2204.10677v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10677">
<div class="article-summary-box-inner">
<span><p>Multiple object tracking (MOT) is a task in computer vision that aims to
detect the position of various objects in videos and to associate them to a
unique identity. We propose an approach based on Constraint Programming (CP)
whose goal is to be grafted to any existing tracker in order to improve its
object association results. We developed a modular algorithm divided into three
independent phases. The first phase consists in recovering the tracklets
provided by a base tracker and to cut them at the places where uncertain
associations are spotted, for example, when tracklets overlap, which may cause
identity switches. In the second phase, we associate the previously constructed
tracklets using a Belief Propagation Constraint Programming algorithm, where we
propose various constraints that assign scores to each of the tracklets based
on multiple characteristics, such as their dynamics or the distance between
them in time and space. Finally, the third phase is a rudimentary interpolation
model to fill in the remaining holes in the trajectories we built. Experiments
show that our model leads to improvements in the results for all three of the
state-of-the-art trackers on which we tested it (3 to 4 points gained on HOTA
and IDF1).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds. (arXiv:2204.10688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10688">
<div class="article-summary-box-inner">
<span><p>Dense captioning in 3D point clouds is an emerging vision-and-language task
involving object-level 3D scene understanding. Apart from coarse semantic class
prediction and bounding box regression as in traditional 3D object detection,
3D dense captioning aims at producing a further and finer instance-level label
of natural language description on visual appearance and spatial relations for
each scene object of interest. To detect and describe objects in a scene,
following the spirit of neural machine translation, we propose a
transformer-based encoder-decoder architecture, namely SpaCap3D, to transform
objects into descriptions, where we especially investigate the relative
spatiality of objects in 3D scenes and design a spatiality-guided encoder via a
token-to-token spatial relation learning objective and an object-centric
decoder for precise and spatiality-enhanced object caption generation.
Evaluated on two benchmark datasets, ScanRefer and ReferIt3D, our proposed
SpaCap3D outperforms the baseline method Scan2Cap by 4.94% and 9.61% in
CIDEr@0.5IoU, respectively. Our project page with source code and supplementary
files is available at https://SpaCap3D.github.io/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcing Generated Images via Meta-learning for One-Shot Fine-Grained Visual Recognition. (arXiv:2204.10689v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10689">
<div class="article-summary-box-inner">
<span><p>One-shot fine-grained visual recognition often suffers from the problem of
having few training examples for new fine-grained classes. To alleviate this
problem, off-the-shelf image generation techniques based on Generative
Adversarial Networks (GANs) can potentially create additional training images.
However, these GAN-generated images are often not helpful for actually
improving the accuracy of one-shot fine-grained recognition. In this paper, we
propose a meta-learning framework to combine generated images with original
images, so that the resulting "hybrid" training images improve one-shot
learning. Specifically, the generic image generator is updated by a few
training instances of novel classes, and a Meta Image Reinforcing Network
(MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as
image reinforcement. Our experiments demonstrate consistent improvement over
baselines on one-shot fine-grained image classification benchmarks.
Furthermore, our analysis shows that the reinforced images have more diversity
compared to the original and GAN-generated images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUES-200: A Multi-height Multi-scene Cross-view Image Benchmark Across Drone and Satellite. (arXiv:2204.10704v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10704">
<div class="article-summary-box-inner">
<span><p>The purpose of cross-view image matching is to match images acquired from the
different platforms of the same target scene and then help positioning system
to infer the location of the target scene. With the rapid development of drone
technology, how to help Drone positioning or navigation through cross-view
matching technology has become a challenging research topic. However, the
accuracy of current cross-view matching models is still low, mainly because the
existing public datasets do not include the differences in images obtained by
drones at different heights, and the types of scenes are relatively
homogeneous, which makes the models unable to adapt to complex and changing
scenes. We propose a new cross-view dataset, SUES-200, to address these
issues.SUES-200 contains images acquired by the drone at four flight heights
and the corresponding satellite view images under the same target scene. To our
knowledge, SUES-200 is the first dataset that considers the differences
generated by aerial photography of drones at different flight heights. In
addition, we build a pipeline for efficient training testing and evaluation of
cross-view matching models. Then, we comprehensively evaluate the performance
of feature extractors with different CNN architectures on SUES-200 through an
evaluation system for cross-view matching models and propose a robust baseline
model. The experimental results show that SUES-200 can help the model learn
features with high discrimination at different heights. Evaluating indicators
of the matching system improves as the drone flight height gets higher because
the drone camera pose and the surrounding environment have less influence on
aerial photography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmbedTrack -- Simultaneous Cell Segmentation and Tracking Through Learning Offsets and Clustering Bandwidths. (arXiv:2204.10713v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10713">
<div class="article-summary-box-inner">
<span><p>A systematic analysis of the cell behavior requires automated approaches for
cell segmentation and tracking. While deep learning has been successfully
applied for the task of cell segmentation, there are few approaches for
simultaneous cell segmentation and tracking using deep learning. Here, we
present EmbedTrack, a single convolutional neural network for simultaneous cell
segmentation and tracking which predicts easy to interpret embeddings. As
embeddings, offsets of cell pixels to their cell center and bandwidths are
learned. We benchmark our approach on nine 2D data sets from the Cell Tracking
Challenge, where our approach performs on seven out of nine data sets within
the top 3 contestants including three top 1 performances. The source code is
publicly available at https://git.scc.kit.edu/kit-loe-ge/embedtrack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Instance Discovery: Vision-Transformer for Instance-Aware Multi-Label Image Recognition. (arXiv:2204.10731v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10731">
<div class="article-summary-box-inner">
<span><p>Previous works on multi-label image recognition (MLIR) usually use CNNs as a
starting point for research. In this paper, we take pure Vision Transformer
(ViT) as the research base and make full use of the advantages of Transformer
with long-range dependency modeling to circumvent the disadvantages of CNNs
limited to local receptive field. However, for multi-label images containing
multiple objects from different categories, scales, and spatial relations, it
is not optimal to use global information alone. Our goal is to leverage ViT's
patch tokens and self-attention mechanism to mine rich instances in multi-label
images, named diverse instance discovery (DiD). To this end, we propose a
semantic category-aware module and a spatial relationship-aware module,
respectively, and then combine the two by a re-constraint strategy to obtain
instance-aware attention maps. Finally, we propose a weakly supervised object
localization-based approach to extract multi-scale local features, to form a
multi-view pipeline. Our method requires only weakly supervised information at
the label level, no additional knowledge injection or other strongly supervised
information is required. Experiments on three benchmark datasets show that our
method significantly outperforms previous works and achieves state-of-the-art
results under fair experimental comparisons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Deepfakes to Close the Domain Gap between Real and Synthetic Images in Facial Capture Pipelines. (arXiv:2204.10746v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10746">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end pipeline for both building and tracking 3D facial
models from personalized in-the-wild (cellphone, webcam, youtube clips, etc.)
video data. First, we present a method for automatic data curation and
retrieval based on a hierarchical clustering framework typical of collision
detection algorithms in traditional computer graphics pipelines. Subsequently,
we utilize synthetic turntables and leverage deepfake technology in order to
build a synthetic multi-view stereo pipeline for appearance capture that is
robust to imperfect synthetic geometry and image misalignment. The resulting
model is fit with an animation rig, which is then used to track facial
performances. Notably, our novel use of deepfake technology enables us to
perform robust tracking of in-the-wild data using differentiable renderers
despite a significant synthetic-to-real domain gap. Finally, we outline how we
train a motion capture regressor, leveraging the aforementioned techniques to
avoid the need for real-world ground truth data and/or a high-end calibrated
camera capture setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PU-EVA: An Edge Vector based Approximation Solution for Flexible-scale Point Cloud Upsampling. (arXiv:2204.10750v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10750">
<div class="article-summary-box-inner">
<span><p>High-quality point clouds have practical significance for point-based
rendering, semantic understanding, and surface reconstruction. Upsampling
sparse, noisy and nonuniform point clouds for a denser and more regular
approximation of target objects is a desirable but challenging task. Most
existing methods duplicate point features for upsampling, constraining the
upsampling scales at a fixed rate. In this work, the flexible upsampling rates
are achieved via edge vector based affine combinations, and a novel design of
Edge Vector based Approximation for Flexible-scale Point clouds Upsampling
(PU-EVA) is proposed. The edge vector based approximation encodes the
neighboring connectivity via affine combinations based on edge vectors, and
restricts the approximation error within the second-order term of Taylor's
Expansion. The EVA upsampling decouples the upsampling scales with network
architecture, achieving the flexible upsampling rates in one-time training.
Qualitative and quantitative evaluations demonstrate that the proposed PU-EVA
outperforms the state-of-the-art in terms of proximity-to-surface, distribution
uniformity, and geometric details preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iCAR: Bridging Image Classification and Image-text Alignment for Visual Recognition. (arXiv:2204.10760v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10760">
<div class="article-summary-box-inner">
<span><p>Image classification, which classifies images by pre-defined categories, has
been the dominant approach to visual representation learning over the last
decade. Visual learning through image-text alignment, however, has emerged to
show promising performance, especially for zero-shot recognition. We believe
that these two learning tasks are complementary, and suggest combining them for
better visual learning. We propose a deep fusion method with three adaptations
that effectively bridge two learning tasks, rather than shallow fusion through
naive multi-task learning. First, we modify the previous common practice in
image classification, a linear classifier, with a cosine classifier which shows
comparable performance. Second, we convert the image classification problem
from learning parametric category classifier weights to learning a text encoder
as a meta network to generate category classifier weights. The learnt text
encoder is shared between image classification and image-text alignment. Third,
we enrich each class name with a description to avoid confusion between classes
and make the classification method closer to the image-text alignment. We prove
that this deep fusion approach performs better on a variety of visual
recognition tasks and setups than the individual learning or shallow fusion
approach, from zero-shot/few-shot image classification, such as the Kornblith
12-dataset benchmark, to downstream tasks of action recognition, semantic
segmentation, and object detection in fine-tuning and open-vocabulary settings.
The code will be available at https://github.com/weiyx16/iCAR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose Estimation. (arXiv:2204.10762v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10762">
<div class="article-summary-box-inner">
<span><p>A high-resolution network exhibits remarkable capability in extracting
multi-scale features for human pose estimation, but fails to capture long-range
interactions between joints and has high computational complexity. To address
these problems, we present a Dynamic lightweight High-Resolution Network
(Dite-HRNet), which can efficiently extract multi-scale contextual information
and model long-range spatial dependency for human pose estimation.
Specifically, we propose two methods, dynamic split convolution and adaptive
context modeling, and embed them into two novel lightweight blocks, which are
named dynamic multi-scale context block and dynamic global context block. These
two blocks, as the basic component units of our Dite-HRNet, are specially
designed for the high-resolution networks to make full use of the parallel
multi-resolution architecture. Experimental results show that the proposed
network achieves superior performance on both COCO and MPII human pose
estimation datasets, surpassing the state-of-the-art lightweight networks. Code
is available at: \url{https://github.com/ZiyiZhang27/Dite-HRNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tag-Based Attention Guided Bottom-Up Approach for Video Instance Segmentation. (arXiv:2204.10765v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10765">
<div class="article-summary-box-inner">
<span><p>Video Instance Segmentation is a fundamental computer vision task that deals
with segmenting and tracking object instances across a video sequence. Most
existing methods typically accomplish this task by employing a multi-stage
top-down approach that usually involves separate networks to detect and segment
objects in each frame, followed by associating these detections in consecutive
frames using a learned tracking head. In this work, however, we introduce a
simple end-to-end trainable bottom-up approach to achieve instance mask
predictions at the pixel-level granularity, instead of the typical
region-proposals-based approach. Unlike contemporary frame-based models, our
network pipeline processes an input video clip as a single 3D volume to
incorporate temporal information. The central idea of our formulation is to
solve the video instance segmentation task as a tag assignment problem, such
that generating distinct tag values essentially separates individual object
instances across the video sequence (here each tag could be any arbitrary value
between 0 and 1). To this end, we propose a novel spatio-temporal tagging loss
that allows for sufficient separation of different objects as well as necessary
identification of different instances of the same object. Furthermore, we
present a tag-based attention module that improves instance tags, while
concurrently learning instance propagation within a video. Evaluations
demonstrate that our method provides competitive results on YouTube-VIS and
DAVIS-19 datasets, and has minimum run-time compared to other state-of-the-art
performance methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoising of Three-Dimensional Fast Spin Echo Magnetic Resonance Images of Knee Joints using Spatial-Variant Noise-Relevant Residual Learning of Convolution Neural Network. (arXiv:2204.10773v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10773">
<div class="article-summary-box-inner">
<span><p>Two-dimensional (2D) fast spin echo (FSE) techniques play a central role in
the clinical magnetic resonance imaging (MRI) of knee joints. Moreover,
three-dimensional (3D) FSE provides high-isotropic-resolution magnetic
resonance (MR) images of knee joints, but it has a reduced signal-to-noise
ratio compared to 2D FSE. Deep-learning denoising methods are a promising
approach for denoising MR images, but they are often trained using synthetic
noise due to challenges in obtaining true noise distributions for MR images. In
this study, inherent true noise information from 2-NEX acquisition was used to
develop a deep-learning model based on residual learning of convolutional
neural network (CNN), and this model was used to suppress the noise in 3D FSE
MR images of knee joints. The proposed CNN used two-step residual learning over
parallel transporting and residual blocks and was designed to comprehensively
learn real noise features from 2-NEX training data. The results of an ablation
study validated the network design. The new method achieved improved denoising
performance of 3D FSE knee MR images compared with current state-of-the-art
methods, based on the peak signal-to-noise ratio and structural similarity
index measure. The improved image quality after denoising using the new method
was verified by radiological evaluation. A deep CNN using the inherent
spatial-varying noise information in 2-NEX acquisitions was developed. This
method showed promise for clinical MRI assessments of the knee, and has
potential applications for the assessment of other anatomical structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images. (arXiv:2204.10776v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10776">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a generalizable model-free 6-DoF object pose
estimator called Gen6D. Existing generalizable pose estimators either need
high-quality object models or require additional depth maps or object masks in
test time, which significantly limits their application scope. In contrast, our
pose estimator only requires some posed images of the unseen object and is able
to accurately predict the poses of the object in arbitrary environments. Gen6D
consists of an object detector, a viewpoint selector and a pose refiner, all of
which do not require the 3D object model and can generalize to unseen objects.
Experiments show that Gen6D achieves state-of-the-art results on two model-free
datasets: the MOPED dataset and a new GenMOP dataset collected by us. In
addition, on the LINEMOD dataset, Gen6D achieves competitive results compared
with instance-specific pose estimators. Project page:
https://liuyuan-pal.github.io/Gen6D/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer. (arXiv:2204.10777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10777">
<div class="article-summary-box-inner">
<span><p>The problem of multimodal intent and trajectory prediction for human-driven
vehicles in parking lots is addressed in this paper. Using models designed with
CNN and Transformer networks, we extract temporal-spatial and contextual
information from trajectory history and local bird's eye view (BEV) semantic
images, and generate predictions about intent distribution and future
trajectory sequences. Our methods outperforms existing models in accuracy,
while allowing an arbitrary number of modes, encoding complex multi-agent
scenarios, and adapting to different parking maps. In addition, we present the
first public human driving dataset in parking lot with high resolution and rich
traffic scenarios for relevant research in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Centralized Adversarial Learning for Robust Deep Hashing. (arXiv:2204.10779v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10779">
<div class="article-summary-box-inner">
<span><p>Deep hashing has been extensively utilized in massive image retrieval because
of its efficiency and effectiveness. Recently, it becomes a hot issue to study
adversarial examples which poses a security challenge to deep hashing models.
However, there is still a critical bottleneck: how to find a superior and exact
semantic representative as the guide to further enhance the adversarial attack
and defense in deep hashing based retrieval. We, for the first time, attempt to
design an effective adversarial learning with the min-max paradigm to improve
the robustness of hashing networks by using the generated adversarial samples.
Specifically, we obtain the optimal solution (called center code) through a
proved Continuous Hash Center Method (CHCM), which preserves the semantic
similarity with positive samples and dissimilarity with negative samples. On
one hand, we propose the Deep Hashing Central Attack (DHCA) for efficient
attack on hashing retrieval by maximizing the Hamming distance between the hash
code of adversarial example and the center code. On the other hand, we present
the Deep Hashing Central Adversarial Training (DHCAT) to optimize the hashing
networks for defense, by minimizing the Hamming distance to the center code.
Extensive experiments on the benchmark datasets verify that our attack method
can achieve better performance than the state-of-the-arts, and our defense
algorithm can effectively mitigate the effects of adversarial perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pay "Attention" to Adverse Weather: Weather-aware Attention-based Object Detection. (arXiv:2204.10803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10803">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances of deep neural networks, object detection for
adverse weather remains challenging due to the poor perception of some sensors
in adverse weather. Instead of relying on one single sensor, multimodal fusion
has been one promising approach to provide redundant detection information
based on multiple sensors. However, most existing multimodal fusion approaches
are ineffective in adjusting the focus of different sensors under varying
detection environments in dynamic adverse weather conditions. Moreover, it is
critical to simultaneously observe local and global information under complex
weather conditions, which has been neglected in most early or late-stage
multimodal fusion works. In view of these, this paper proposes a Global-Local
Attention (GLA) framework to adaptively fuse the multi-modality sensing
streams, i.e., camera, gated camera, and lidar data, at two fusion stages.
Specifically, GLA integrates an early-stage fusion via a local attention
network and a late-stage fusion via a global attention network to deal with
both local and global information, which automatically allocates higher weights
to the modality with better detection features at the late-stage fusion to cope
with the specific weather condition adaptively. Experimental results
demonstrate the superior performance of the proposed GLA compared with
state-of-the-art fusion approaches under various adverse weather conditions,
such as light fog, dense fog, and snow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Scaffold: Optimizing Model Explanations for Teaching. (arXiv:2204.10810v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10810">
<div class="article-summary-box-inner">
<span><p>Modern machine learning models are opaque, and as a result there is a
burgeoning academic subfield on methods that explain these models' behavior.
However, what is the precise goal of providing such explanations, and how can
we demonstrate that explanations achieve this goal? Some research argues that
explanations should help teach a student (either human or machine) to simulate
the model being explained, and that the quality of explanations can be measured
by the simulation accuracy of students on unexplained examples. In this work,
leveraging meta-learning techniques, we extend this idea to improve the quality
of the explanations themselves, specifically by optimizing explanations such
that student models more effectively learn to simulate the original model. We
train models on three natural language processing and computer vision tasks,
and find that students trained with explanations extracted with our framework
are able to simulate the teacher significantly more effectively than ones
produced with previous methods. Through human annotations and a user study, we
further find that these learned explanations more closely align with how humans
would explain the required decisions in these tasks. Our code is available at
https://github.com/coderpat/learning-scaffold
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Video Object Segmentation via Cutout Prediction and Tagging. (arXiv:2204.10846v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10846">
<div class="article-summary-box-inner">
<span><p>We propose a novel self-supervised Video Object Segmentation (VOS) approach
that strives to achieve better object-background discriminability for accurate
object segmentation. Distinct from previous self-supervised VOS methods, our
approach is based on a discriminative learning loss formulation that takes into
account both object and background information to ensure object-background
discriminability, rather than using only object appearance. The discriminative
learning loss comprises cutout-based reconstruction (cutout region represents
part of a frame, whose pixels are replaced with some constant values) and tag
prediction loss terms. The cutout-based reconstruction term utilizes a simple
cutout scheme to learn the pixel-wise correspondence between the current and
previous frames in order to reconstruct the original current frame with added
cutout region in it. The introduced cutout patch guides the model to focus as
much on the significant features of the object of interest as the less
significant ones, thereby implicitly equipping the model to address
occlusion-based scenarios. Next, the tag prediction term encourages
object-background separability by grouping tags of all pixels in the cutout
region that are similar, while separating them from the tags of the rest of the
reconstructed frame pixels. Additionally, we introduce a zoom-in scheme that
addresses the problem of small object segmentation by capturing fine structural
information at multiple scales. Our proposed approach, termed CT-VOS, achieves
state-of-the-art results on two challenging benchmarks: DAVIS-2017 and
Youtube-VOS. A detailed ablation showcases the importance of the proposed loss
formulation to effectively capture object-background discriminability and the
impact of our zoom-in scheme to accurately segment small-sized objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Control-NeRF: Editable Feature Volumes for Scene Rendering and Manipulation. (arXiv:2204.10850v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10850">
<div class="article-summary-box-inner">
<span><p>We present a novel method for performing flexible, 3D-aware image content
manipulation while enabling high-quality novel view synthesis. While NeRF-based
approaches are effective for novel view synthesis, such models memorize the
radiance for every point in a scene within a neural network. Since these models
are scene-specific and lack a 3D scene representation, classical editing such
as shape manipulation, or combining scenes is not possible. Hence, editing and
combining NeRF-based scenes has not been demonstrated. With the aim of
obtaining interpretable and controllable scene representations, our model
couples learnt scene-specific feature volumes with a scene agnostic neural
rendering network. With this hybrid representation, we decouple neural
rendering from scene-specific geometry and appearance. We can generalize to
novel scenes by optimizing only the scene-specific 3D feature representation,
while keeping the parameters of the rendering network fixed. The rendering
function learnt during the initial training stage can thus be easily applied to
new scenes, making our approach more flexible. More importantly, since the
feature volumes are independent of the rendering model, we can manipulate and
combine scenes by editing their corresponding feature volumes. The edited
volume can then be plugged into the rendering model to synthesize high-quality
novel views. We demonstrate various scene manipulations, including mixing
scenes, deforming objects and inserting objects into scenes, while still
producing photo-realistic results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Dual Formulation of Boosting Algorithms. (arXiv:0901.3590v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/0901.3590">
<div class="article-summary-box-inner">
<span><p>We study boosting algorithms from a new perspective. We show that the
Lagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with
generalized hinge loss are all entropy maximization problems. By looking at the
dual problems of these boosting algorithms, we show that the success of
boosting algorithms can be understood in terms of maintaining a better margin
distribution by maximizing margins and at the same time controlling the margin
variance.We also theoretically prove that, approximately, AdaBoost maximizes
the average margin, instead of the minimum margin. The duality formulation also
enables us to develop column generation based optimization algorithms, which
are totally corrective. We show that they exhibit almost identical
classification results to that of standard stage-wise additive boosting
algorithms but with much faster convergence rates. Therefore fewer weak
classifiers are needed to build the ensemble using our proposed optimization
technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Land-Cover Classification with High-Resolution Remote Sensing Images Using Transferable Deep Models. (arXiv:1807.05713v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1807.05713">
<div class="article-summary-box-inner">
<span><p>In recent years, large amount of high spatial-resolution remote sensing
(HRRS) images are available for land-cover mapping. However, due to the complex
information brought by the increased spatial resolution and the data
disturbances caused by different conditions of image acquisition, it is often
difficult to find an efficient method for achieving accurate land-cover
classification with high-resolution and heterogeneous remote sensing images. In
this paper, we propose a scheme to apply deep model obtained from labeled
land-cover dataset to classify unlabeled HRRS images. The main idea is to rely
on deep neural networks for presenting the contextual information contained in
different types of land-covers and propose a pseudo-labeling and sample
selection scheme for improving the transferability of deep models. More
precisely, a deep Convolutional Neural Networks is first pre-trained with a
well-annotated land-cover dataset, referred to as the source data. Then, given
a target image with no labels, the pre-trained CNN model is utilized to
classify the image in a patch-wise manner. The patches with high confidence are
assigned with pseudo-labels and employed as the queries to retrieve related
samples from the source data. The pseudo-labels confirmed with the retrieved
results are regarded as supervised information for fine-tuning the pre-trained
deep model. To obtain a pixel-wise land-cover classification with the target
image, we rely on the fine-tuned CNN and develop a hybrid classification by
combining patch-wise classification and hierarchical segmentation. In addition,
we create a large-scale land-cover dataset containing 150 Gaofen-2 satellite
images for CNN pre-training. Experiments on multi-source HRRS images show
encouraging results and demonstrate the applicability of the proposed scheme to
land-cover classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Detectors for Digital and Physical Adversarial Inputs to Perception Systems. (arXiv:2002.09792v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.09792">
<div class="article-summary-box-inner">
<span><p>Deep neural network (DNN) models have proven to be vulnerable to adversarial
digital and physical attacks. In this paper, we propose a novel attack- and
dataset-agnostic and real-time detector for both types of adversarial inputs to
DNN-based perception systems. In particular, the proposed detector relies on
the observation that adversarial images are sensitive to certain
label-invariant transformations. Specifically, to determine if an image has
been adversarially manipulated, the proposed detector checks if the output of
the target classifier on a given input image changes significantly after
feeding it a transformed version of the image under investigation. Moreover, we
show that the proposed detector is computationally-light both at runtime and
design-time which makes it suitable for real-time applications that may also
involve large-scale image domains. To highlight this, we demonstrate the
efficiency of the proposed detector on ImageNet, a task that is computationally
challenging for the majority of relevant defenses, and on physically attacked
traffic signs that may be encountered in real-time autonomy applications.
Finally, we propose the first adversarial dataset, called AdvNet that includes
both clean and physical traffic sign images. Our extensive comparative
experiments on the MNIST, CIFAR10, ImageNet, and AdvNet datasets show that
VisionGuard outperforms existing defenses in terms of scalability and detection
performance. We have also evaluated the proposed detector on field test data
obtained on a moving vehicle equipped with a perception-based DNN being under
attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery. (arXiv:2005.02264v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02264">
<div class="article-summary-box-inner">
<span><p>Monitoring of land cover and land use is crucial in natural resources
management. Automatic visual mapping can carry enormous economic value for
agriculture, forestry, or public administration. Satellite or aerial images
combined with computer vision and deep learning enable precise assessment and
can significantly speed up change detection. Aerial imagery usually provides
images with much higher pixel resolution than satellite data allowing more
detailed mapping. However, there is still a lack of aerial datasets made for
the segmentation, covering rural areas with a resolution of tens centimeters
per pixel, manual fine labels, and highly publicly important environmental
instances like buildings, woods, water, or roads.
</p>
<p>Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for
semantic segmentation. We collected images of 216.27 sq. km rural areas across
Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per
pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine
annotated four following classes of objects: buildings, woodlands, water, and
roads. Additionally, we report simple benchmark results, achieving 85.56% of
mean intersection over union on the test set. It proves that the automatic
mapping of land cover is possible with a relatively small, cost-efficient,
RGB-only dataset. The dataset is publicly available at
https://landcover.ai.linuxpolska.com/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind Non-Uniform Motion Deblurring using Atrous Spatial Pyramid Deformable Convolution and Deblurring-Reblurring Consistency. (arXiv:2106.14336v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14336">
<div class="article-summary-box-inner">
<span><p>Many deep learning based methods are designed to remove non-uniform
(spatially variant) motion blur caused by object motion and camera shake
without knowing the blur kernel. Some methods directly output the latent sharp
image in one stage, while others utilize a multi-stage strategy (\eg
multi-scale, multi-patch, or multi-temporal) to gradually restore the sharp
image. However, these methods have the following two main issues: 1) The
computational cost of multi-stage is high; 2) The same convolution kernel is
applied in different regions, which is not an ideal choice for non-uniform
blur. Hence, non-uniform motion deblurring is still a challenging and open
problem. In this paper, we propose a new architecture which consists of
multiple Atrous Spatial Pyramid Deformable Convolution (ASPDC) modules to
deblur an image end-to-end with more flexibility. Multiple ASPDC modules
implicitly learn the pixel-specific motion with different dilation rates in the
same layer to handle movements of different magnitude. To improve the training,
we also propose a reblurring network to map the deblurred output back to the
blurred input, which constrains the solution space. Our experimental results
show that the proposed method outperforms state-of-the-art methods on the
benchmark datasets. The code is available at https://github.com/Dong-Huo/ASPDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation. (arXiv:2107.08982v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08982">
<div class="article-summary-box-inner">
<span><p>Multi-person pose estimation is an attractive and challenging task. Existing
methods are mostly based on two-stage frameworks, which include top-down and
bottom-up methods. Two-stage methods either suffer from high computational
redundancy for additional person detectors or they need to group keypoints
heuristically after predicting all the instance-agnostic keypoints. The
single-stage paradigm aims to simplify the multi-person pose estimation
pipeline and receives a lot of attention. However, recent single-stage methods
have the limitation of low performance due to the difficulty of regressing
various full-body poses from a single feature vector. Different from previous
solutions that involve complex heuristic designs, we present a simple yet
effective solution by employing instance-aware dynamic networks. Specifically,
we propose an instance-aware module to adaptively adjust (part of) the network
parameters for each instance. Our solution can significantly increase the
capacity and adaptive-ability of the network for recognizing various poses,
while maintaining a compact end-to-end trainable pipeline. Extensive
experiments on the MS-COCO dataset demonstrate that our method achieves
significant improvement over existing single-stage methods, and makes a better
balance of accuracy and efficiency compared to the state-of-the-art two-stage
approaches. The code and models are available at
\url{https://github.com/hikvision-research/opera}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer for Single Image Super-Resolution. (arXiv:2108.11084v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11084">
<div class="article-summary-box-inner">
<span><p>Single image super-resolution (SISR) has witnessed great strides with the
development of deep learning. However, most existing studies focus on building
more complex networks with a massive number of layers. Recently, more and more
researchers start to explore the application of Transformer in computer vision
tasks. However, the heavy computational cost and high GPU memory occupation of
the vision Transformer cannot be ignored. In this paper, we propose a novel
Efficient Super-Resolution Transformer (ESRT) for SISR. ESRT is a hybrid model,
which consists of a Lightweight CNN Backbone (LCB) and a Lightweight
Transformer Backbone (LTB). Among them, LCB can dynamically adjust the size of
the feature map to extract deep features with a low computational cost. LTB is
composed of a series of Efficient Transformers (ET), which occupies a small GPU
memory occupation, thanks to the specially designed Efficient Multi-Head
Attention (EMHA). Extensive experiments show that ESRT achieves competitive
results with low computational costs. Compared with the original Transformer
which occupies 16,057M GPU memory, ESRT only occupies 4,191M GPU memory. All
codes are available at https://github.com/luissen/ESRT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Robustness of Adversarial Attacks Using an Affine-Invariant Gradient Estimator. (arXiv:2109.05820v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05820">
<div class="article-summary-box-inner">
<span><p>As designers of artificial intelligence try to outwit hackers, both sides
continue to hone in on AI's inherent vulnerabilities. Designed and trained from
certain statistical distributions of data, AI's deep neural networks (DNNs)
remain vulnerable to deceptive inputs that violate a DNN's statistical,
predictive assumptions. Before being fed into a neural network, however, most
existing adversarial examples cannot maintain malicious functionality when
applied to an affine transformation. For practical purposes, maintaining that
malicious functionality serves as an important measure of the robustness of
adversarial attacks. To help DNNs learn to defend themselves more thoroughly
against attacks, we propose an affine-invariant adversarial attack, which can
consistently produce more robust adversarial examples over affine
transformations. For efficiency, we propose to disentangle current
affine-transformation strategies from the Euclidean geometry coordinate plane
with its geometric translations, rotations and dilations; we reformulate the
latter two in polar coordinates. Afterwards, we construct an affine-invariant
gradient estimator by convolving the gradient at the original image with
derived kernels, which can be integrated with any gradient-based attack
methods. Extensive experiments on ImageNet, including some experiments under
physical condition, demonstrate that our method can significantly improve the
affine invariance of adversarial examples and, as a byproduct, improve the
transferability of adversarial examples, compared with alternative
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Attribute and Structure Subspace Clustering Network. (arXiv:2109.13742v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13742">
<div class="article-summary-box-inner">
<span><p>Deep self-expressiveness-based subspace clustering methods have demonstrated
effectiveness. However, existing works only consider the attribute information
to conduct the self-expressiveness, which may limit the clustering performance.
In this paper, we propose a novel adaptive attribute and structure subspace
clustering network (AASSC-Net) to simultaneously consider the attribute and
structure information in an adaptive graph fusion manner. Specifically, we
first exploit an auto-encoder to represent input data samples with latent
features for the construction of an attribute matrix. We also construct a mixed
signed and symmetric structure matrix to capture the local geometric structure
underlying data samples. Then, we perform self-expressiveness on the
constructed attribute and structure matrices to learn their affinity graphs
separately. Finally, we design a novel attention-based fusion module to
adaptively leverage these two affinity graphs to construct a more
discriminative affinity graph. Extensive experimental results on commonly used
benchmark datasets demonstrate that our AASSC-Net significantly outperforms
state-of-the-art methods. In addition, we conduct comprehensive ablation
studies to discuss the effectiveness of the designed modules. The code will be
publicly available at https://github.com/ZhihaoPENG-CityU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi Proxy Anchor Loss and Effectiveness of Deep Metric Learning Performance Metrics. (arXiv:2110.03997v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03997">
<div class="article-summary-box-inner">
<span><p>Deep metric learning (DML) learns the mapping, which maps into embedding
space in which similar data is near and dissimilar data is far. However,
conventional proxy-based losses for DML have two problems: gradient problems
and applying the real-world dataset with multiple local centers. Besides, DML
performance metrics also have some issues have stability and flexibility. This
paper proposes multi-proxies anchor (MPA) loss and normalized discounted
cumulative gain (nDCG@k) metric. This study contributes three following: (1)
MPA loss is able to learn the real-world dataset with multi-local centers. (2)
MPA loss improves the training capacity of a neural network owing to solving
the gradient issues. (3) nDCG@k metric encourages complete evaluation for
various datasets. Finally, we demonstrate MPA loss's effectiveness, and MPA
loss achieves higher accuracy on two datasets for fine-grained images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset Condensation with Distribution Matching. (arXiv:2110.04181v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04181">
<div class="article-summary-box-inner">
<span><p>Computational cost of training state-of-the-art deep models in many learning
problems is rapidly increasing due to more sophisticated models and larger
datasets. A recent promising direction for reducing training cost is dataset
condensation that aims to replace the original large training set with a
significantly smaller learned synthetic set while preserving the original
information. While training deep models on the small set of condensed images
can be extremely fast, their synthesis remains computationally expensive due to
the complex bi-level optimization and second-order derivative computation. In
this work, we propose a simple yet effective method that synthesizes condensed
images by matching feature distributions of the synthetic and original training
images in many sampled embedding spaces. Our method significantly reduces the
synthesis cost while achieving comparable or better performance. Thanks to its
efficiency, we apply our method to more realistic and larger datasets with
sophisticated neural architectures and obtain a significant performance boost.
We also show promising practical benefits of our method in continual learning
and neural architecture search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auxiliary Loss Reweighting for Image Inpainting. (arXiv:2111.07279v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07279">
<div class="article-summary-box-inner">
<span><p>Image Inpainting is a task that aims to fill in missing regions of corrupted
images with plausible contents. Recent inpainting methods have introduced
perceptual and style losses as auxiliary losses to guide the learning of
inpainting generators. Perceptual and style losses help improve the perceptual
quality of inpainted results by supervising deep features of generated regions.
However, two challenges have emerged with the usage of auxiliary losses: (i)
the time-consuming grid search is required to decide weights for perceptual and
style losses to properly perform, and (ii) loss terms with different auxiliary
abilities are equally weighted by perceptual and style losses. To meet these
two challenges, we propose a novel framework that independently weights
auxiliary loss terms and adaptively adjusts their weights within a single
training process, without a time-consuming grid search. Specifically, to
release the auxiliary potential of perceptual and style losses, we propose two
auxiliary losses, Tunable Perceptual Loss (TPL) and Tunable Style Loss (TSL) by
using different tunable weights to consider the contributions of different loss
terms. TPL and TSL are supersets of perceptual and style losses and release the
auxiliary potential of standard perceptual and style losses. We further propose
the Auxiliary Weights Adaptation (AWA) algorithm, which efficiently reweights
TPL and TSL in a single training process. AWA is based on the principle that
the best auxiliary weights would lead to the most improvement in inpainting
performance. We conduct experiments on publically available datasets and find
that our framework helps current SOTA methods achieve better results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViCE: Visual Concept Embedding Discovery and Superpixelization. (arXiv:2111.12460v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12460">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised computer vision methods have demonstrated equal or
better performance to supervised methods, opening for AI systems to learn
visual representations from practically unlimited data. However, these methods
are classification-based and thus ineffective for learning dense feature maps
required for unsupervised semantic segmentation. This work presents a method to
effectively learn dense semantically rich visual concept embeddings applicable
to high-resolution images. We introduce superpixelization as a means to
decompose images into a small set of visually coherent regions, allowing
efficient learning of dense semantics by swapped prediction. The expressiveness
of our dense embeddings is demonstrated by significantly improving the SOTA
representation quality benchmarks on COCO (+16.27 mIoU) and Cityscapes (+19.24
mIoU) for both low- and high-resolution images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Camera LiDAR Inertial Extension to the Newer College Dataset. (arXiv:2112.08854v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08854">
<div class="article-summary-box-inner">
<span><p>We present a multi-camera LiDAR inertial dataset of 4.5 km walking distance
as an expansion of the Newer College Dataset. The global shutter multi-camera
device is hardware synchronized with both the IMU and LiDAR. This dataset also
provides six Degrees of Freedom (DoF) ground truth poses at LiDAR frequency (10
Hz). Three data collections are described and an example use case of
multi-camera visual-inertial odometry is demonstrated. This expansion dataset
contains small and narrow passages, large scale open spaces, as well as
vegetated areas, to test localization and mapping systems. Furthermore, some
sequences present challenging situations such as abrupt lighting change,
textureless surfaces, and aggressive motion. The dataset is available at:
https: //ori-drs.github.io/newer-college-dataset/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobilePhys: Personalized Mobile Camera-Based Contactless Physiological Sensing. (arXiv:2201.04039v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04039">
<div class="article-summary-box-inner">
<span><p>Camera-based contactless photoplethysmography refers to a set of popular
techniques for contactless physiological measurement. The current
state-of-the-art neural models are typically trained in a supervised manner
using videos accompanied by gold standard physiological measurements. However,
they often generalize poorly out-of-domain examples (i.e., videos that are
unlike those in the training set). Personalizing models can help improve model
generalizability, but many personalization techniques still require some gold
standard data. To help alleviate this dependency, in this paper, we present a
novel mobile sensing system called MobilePhys, the first mobile personalized
remote physiological sensing system, that leverages both front and rear cameras
on a smartphone to generate high-quality self-supervised labels for training
personalized contactless camera-based PPG models. To evaluate the robustness of
MobilePhys, we conducted a user study with 39 participants who completed a set
of tasks under different mobile devices, lighting conditions/intensities,
motion tasks, and skin types. Our results show that MobilePhys significantly
outperforms the state-of-the-art on-device supervised training and few-shot
adaptation methods. Through extensive user studies, we further examine how does
MobilePhys perform in complex real-world settings. We envision that calibrated
or personalized camera-based contactless PPG models generated from our proposed
dual-camera mobile sensing system will open the door for numerous future
applications such as smart mirrors, fitness and mobile health applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards 3D Scene Reconstruction from Locally Scale-Aligned Monocular Video Depth. (arXiv:2202.01470v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01470">
<div class="article-summary-box-inner">
<span><p>Existing monocular depth estimation methods have achieved excellent
robustness in diverse scenes, but they can only retrieve affine-invariant
depth, up to an unknown scale and shift. However, in some video-based scenarios
such as video depth estimation and 3D scene reconstruction from a video, the
unknown scale and shift residing in per-frame prediction may cause the depth
inconsistency. To solve this problem, we propose a locally weighted linear
regression method to recover the scale and shift with very sparse anchor
points, which ensures the scale consistency along consecutive frames. Extensive
experiments show that our method can boost the performance of existing
state-of-the-art approaches by 50% at most over several zero-shot benchmarks.
Besides, we merge over 6.3 million RGBD images to train strong and robust depth
models. Our produced ResNet50-backbone model even outperforms the
state-of-the-art DPT ViT-Large model. Combining with geometry-based
reconstruction methods, we formulate a new dense 3D scene reconstruction
pipeline, which benefits from both the scale consistency of sparse points and
the robustness of monocular methods. By performing the simple per-frame
prediction over a video, the accurate 3D scene shape can be recovered.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSSNet: Planarity-sensible Semantic Segmentation of Large-scale Urban Meshes. (arXiv:2202.03209v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03209">
<div class="article-summary-box-inner">
<span><p>We introduce a novel deep learning-based framework to interpret 3D urban
scenes represented as textured meshes. Based on the observation that object
boundaries typically align with the boundaries of planar regions, our framework
achieves semantic segmentation in two steps: planarity-sensible
over-segmentation followed by semantic classification. The over-segmentation
step generates an initial set of mesh segments that capture the planar and
non-planar regions of urban scenes. In the subsequent classification step, we
construct a graph that encodes the geometric and photometric features of the
segments in its nodes and the multi-scale contextual features in its edges. The
final semantic segmentation is obtained by classifying the segments using a
graph convolutional network. Experiments and comparisons on two semantic urban
mesh benchmarks demonstrate that our approach outperforms the state-of-the-art
methods in terms of boundary quality, mean IoU (intersection over union), and
generalization ability. We also introduce several new metrics for evaluating
mesh over-segmentation methods dedicated to semantic segmentation, and our
proposed over-segmentation approach outperforms state-of-the-art methods on all
metrics. Our source code is available at
\url{https://github.com/WeixiaoGao/PSSNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Modeling for Out-of-Distribution Generalization. (arXiv:2202.03958v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03958">
<div class="article-summary-box-inner">
<span><p>Though remarkable progress has been achieved in various vision tasks, deep
neural networks still suffer obvious performance degradation when tested in
out-of-distribution scenarios. We argue that the feature statistics (mean and
standard deviation), which carry the domain characteristics of the training
data, can be properly manipulated to improve the generalization ability of deep
learning models. Common methods often consider the feature statistics as
deterministic values measured from the learned features and do not explicitly
consider the uncertain statistics discrepancy caused by potential domain shifts
during testing. In this paper, we improve the network generalization ability by
modeling the uncertainty of domain shifts with synthesized feature statistics
during training. Specifically, we hypothesize that the feature statistic, after
considering the potential uncertainties, follows a multivariate Gaussian
distribution. Hence, each feature statistic is no longer a deterministic value,
but a probabilistic point with diverse distribution possibilities. With the
uncertain feature statistics, the models can be trained to alleviate the domain
perturbations and achieve better robustness against potential domain shifts.
Our method can be readily integrated into networks without additional
parameters. Extensive experiments demonstrate that our proposed method
consistently improves the network generalization ability on multiple vision
tasks, including image classification, semantic segmentation, and instance
retrieval. The code can be available at https://github.com/lixiaotong97/DSU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-Means for Noise-Insensitive Multi-Dimensional Feature Learning. (arXiv:2202.07754v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07754">
<div class="article-summary-box-inner">
<span><p>Many measurement modalities which perform imaging by probing an object
pixel-by-pixel, such as via Photoacoustic Microscopy, produce a
multi-dimensional feature (typically a time-domain signal) at each pixel. In
principle, the many degrees of freedom in the time-domain signal would admit
the possibility of significant multi-modal information being implicitly
present, much more than a single scalar "brightness", regarding the underlying
targets being observed. However, the measured signal is neither a weighted-sum
of basis functions (such as principal components) nor one of a set of
prototypes (K-means), which has motivated the novel clustering method proposed
here. Signals are clustered based on their shape, but not amplitude, via
angular distance and centroids are calculated as the direction of maximal
intra-cluster variance, resulting in a clustering algorithm capable of learning
centroids (signal shapes) that are related to the underlying, albeit unknown,
target characteristics in a scalable and noise-robust manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning. (arXiv:2203.03137v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03137">
<div class="article-summary-box-inner">
<span><p>The key challenge of zero-shot learning (ZSL) is how to infer the latent
semantic knowledge between visual and attribute features on seen classes, and
thus achieving a desirable knowledge transfer to unseen classes. Prior works
either simply align the global features of an image with its associated class
semantic vector or utilize unidirectional attention to learn the limited latent
semantic representations, which could not effectively discover the intrinsic
semantic knowledge e.g., attribute semantics) between visual and attribute
features. To solve the above dilemma, we propose a Mutually Semantic
Distillation Network (MSDN), which progressively distills the intrinsic
semantic representations between visual and attribute features for ZSL. MSDN
incorporates an attribute$\rightarrow$visual attention sub-net that learns
attribute-based visual features, and a visual$\rightarrow$attribute attention
sub-net that learns visual-based attribute features. By further introducing a
semantic distillation loss, the two mutual attention sub-nets are capable of
learning collaboratively and teaching each other throughout the training
process. The proposed MSDN yields significant improvements over the strong
baselines, leading to new state-of-the-art performances on three popular
challenging benchmarks, i.e., CUB, SUN, and AWA2. Our codes have been available
at: \url{https://github.com/shiming-chen/MSDN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Polyp Segmentation Network. (arXiv:2203.04118v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04118">
<div class="article-summary-box-inner">
<span><p>Cancer is a disease that occurs as a result of the uncontrolled division and
proliferation of cells. Colon cancer is one of the most common types of cancer
in the world. Polyps that can be seen in the large intestine can cause cancer
if not removed with early intervention. Deep learning and image segmentation
techniques are used to minimize the number of polyps that goes unnoticed by the
experts during these interventions. Although these techniques perform well in
terms of accuracy, they require too many parameters. We propose a new model to
address this problem. Our proposed model requires fewer parameters as well as
outperforms the state-of-the-art models. We use EfficientNetB0 for the encoder
part, as it performs well in various tasks while requiring fewer parameters. We
use partial decoder, which is used to reduce the number of parameters while
achieving high accuracy in segmentation. Since polyps have variable appearances
and sizes, we use an asymmetric convolution block instead of a classic
convolution block. Then, we weight each feature map using a squeeze and
excitation block to improve our segmentation results. We used different splits
of Kvasir and CVC-ClinicDB datasets for training, validation, and testing,
while we use CVC- ColonDB, ETIS, and Endoscene datasets for testing. Our model
outperforms state-of-art models with a Dice metric of %71.8 on the ColonDB test
dataset, %89.3 on the EndoScene test dataset, and %74.8 on the ETIS test
dataset while requiring fewer parameters. Our model requires 2.626.337
parameters in total while the closest model in the state-of-the-art is U-Net++
with 9.042.177 parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Black-box Skeleton-based Human Activity Classifiers. (arXiv:2203.04713v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04713">
<div class="article-summary-box-inner">
<span><p>Deep learning has been regarded as the `go to' solution for many tasks today,
but its intrinsic vulnerability to malicious attacks has become a major
concern. The vulnerability is affected by a variety of factors including
models, tasks, data, and attackers. Consequently, methods such as Adversarial
Training and Randomized Smoothing have been proposed to tackle the problem in a
wide range of applications. In this paper, we investigate skeleton-based Human
Activity Recognition, which is an important type of time-series data but
under-explored in defense against attacks. Our method is featured by (1) a new
Bayesian Energy-based formulation of robust discriminative classifiers, (2) a
new parameterization of the adversarial sample manifold of actions, and (3) a
new post-train Bayesian treatment on both the adversarial samples and the
classifier. We name our framework Bayesian Energy-based Adversarial Training or
BEAT. BEAT is straightforward but elegant, which turns vulnerable black-box
classifiers into robust ones without sacrificing accuracy. It demonstrates
surprising and universal effectiveness across a wide range of action
classifiers and datasets, under various attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HunYuan_tvr for Text-Video Retrieval. (arXiv:2204.03382v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03382">
<div class="article-summary-box-inner">
<span><p>Text-Video Retrieval plays an important role in multi-modal understanding and
has attracted increasing attention in recent years. Most existing methods focus
on constructing contrastive pairs between whole videos and complete caption
sentences, while ignoring fine-grained cross-modal relationships, e.g., short
clips and phrases or single frame and word. In this paper, we propose a novel
method, named HunYuan\_tvr, to explore hierarchical cross-modal interactions by
simultaneously exploring video-sentence, clip-phrase, and frame-word
relationships. Considering intrinsic semantic relations between frames,
HunYuan\_tvr first performs self-attention to explore frame-wise correlations
and adaptively clusters correlated frames into clip-level representations.
Then, the clip-wise correlation is explored to aggregate clip representations
into a compact one to describe the video globally. In this way, we can
construct hierarchical video representations for frame-clip-video
granularities, and also explore word-wise correlations to form
word-phrase-sentence embeddings for the text modality. Finally, hierarchical
contrastive learning is designed to explore cross-modal
relationships,~\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which
enables HunYuan\_tvr to achieve a comprehensive multi-modal understanding.
Further boosted by adaptive label denosing and marginal sample enhancement,
HunYuan\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,
Rank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,
DiDemo, and ActivityNet respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. As the core of CNNs, the image
convolution operation helps CNNs to get good performance on image-related
tasks. However, the image convolution is hard to be implemented and
parallelized. This paper proposes a novel neural network model, namely CEMNet,
which can be trained in the frequency domain. The most important motivation of
this research is that we can use the straightforward element-wise
multiplication operation to replace the image convolution in the frequency
domain based on the Cross-Correlation Theorem. We further introduce a Weight
Fixation mechanism to alleviate the problem of over-fitting, and analyze the
working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases. To the best of our knowledge,
CEMNet is the first model trained in Fourier Domain that achieves more than
70\% validation accuracy on CIFAR-10 database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-Inertial Odometry with Online Calibration of Velocity-Control Based Kinematic Motion Models. (arXiv:2204.06776v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06776">
<div class="article-summary-box-inner">
<span><p>Visual-inertial odometry (VIO) is an important technology for autonomous
robots with power and payload constraints. In this paper, we propose a novel
approach for VIO with stereo cameras which integrates and calibrates the
velocity-control based kinematic motion model of wheeled mobile robots online.
Including such a motion model can help to improve the accuracy of VIO. Compared
to several previous approaches proposed to integrate wheel odometer
measurements for this purpose, our method does not require wheel encoders and
can be applied when the robot motion can be modeled with velocity-control based
kinematic motion model. We use radial basis function (RBF) kernels to
compensate for the time delay and deviations between control commands and
actual robot motion. The motion model is calibrated online by the VIO system
and can be used as a forward model for motion control and planning. We evaluate
our approach with data obtained in variously sized indoor environments,
demonstrate improvements over a pure VIO method, and evaluate the prediction
accuracy of the online calibrated model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep CardioSound-An Ensembled Deep Learning Model for Heart Sound MultiLabelling. (arXiv:2204.07420v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07420">
<div class="article-summary-box-inner">
<span><p>Heart sound diagnosis and classification play an essential role in detecting
cardiovascular disorders, especially when the remote diagnosis becomes standard
clinical practice. Most of the current work is designed for single category
based heard sound classification tasks. To further extend the landscape of the
automatic heart sound diagnosis landscape, this work proposes a deep multilabel
learning model that can automatically annotate heart sound recordings with
labels from different label groups, including murmur's timing, pitch, grading,
quality, and shape. Our experiment results show that the proposed method has
achieved outstanding performance on the holdout data for the multi-labelling
task with sensitivity=0.990, specificity=0.999, F1=0.990 at the segments level,
and an overall accuracy=0.969 at the patient's recording level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Linear Attention for Fast and Accurate Keypoint Matching. (arXiv:2204.07731v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07731">
<div class="article-summary-box-inner">
<span><p>Recently Transformers have provided state-of-the-art performance in sparse
matching, crucial to realize high-performance 3D vision applications. Yet,
these Transformers lack efficiency due to the quadratic computational
complexity of their attention mechanism. To solve this problem, we employ an
efficient linear attention for the linear computational complexity. Then, we
propose a new attentional aggregation that achieves high accuracy by
aggregating both the global and local information from sparse keypoints. To
further improve the efficiency, we propose the joint learning of feature
matching and description. Our learning enables simpler and faster matching than
Sinkhorn, often used in matching the learned descriptors from Transformers. Our
method achieves competitive performance with only 0.84M learnable parameters
against the bigger SOTAs, SuperGlue (12M parameters) and SGMNet (30M
parameters), on three benchmarks, HPatch, ETH, and Aachen Day-Night.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment. (arXiv:2204.08332v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08332">
<div class="article-summary-box-inner">
<span><p>This work addresses the Burst Super-Resolution (BurstSR) task using a new
architecture, which requires restoring a high-quality image from a sequence of
noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in
BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can
significantly improve the capability of extracting inter-frame information and
reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided
Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin
Transformer Blocks and Groups as our main backbone. More specifically, we
combine optical flows and deformable convolutions, hence our BSRT can handle
misalignment and aggregate the potential texture information in multi-frames
more efficiently. In addition, our Transformer-based structure can capture
long-range dependency to further improve the performance. The evaluation on
both synthetic and real-world tracks demonstrates that our approach achieves a
new state-of-the-art in BurstSR task. Further, our BSRT wins the championship
in the NTIRE2022 Burst Super-Resolution Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction. (arXiv:2204.09204v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09204">
<div class="article-summary-box-inner">
<span><p>When applying multi-instance learning (MIL) to make predictions for bags of
instances, the prediction accuracy of an instance often depends on not only the
instance itself but also its context in the corresponding bag. From the
viewpoint of causal inference, such bag contextual prior works as a confounder
and may result in model robustness and interpretability issues. Focusing on
this problem, we propose a novel interventional multi-instance learning (IMIL)
framework to achieve deconfounded instance-level prediction. Unlike traditional
likelihood-based strategies, we design an Expectation-Maximization (EM)
algorithm based on causal intervention, providing a robust instance selection
in the training phase and suppressing the bias caused by the bag contextual
prior. Experiments on pathological image analysis demonstrate that our IMIL
method substantially reduces false positives and outperforms state-of-the-art
MIL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving The Long-Tailed Problem via Intra- and Inter-Category Balance. (arXiv:2204.09234v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09234">
<div class="article-summary-box-inner">
<span><p>Benchmark datasets for visual recognition assume that data is uniformly
distributed, while real-world datasets obey long-tailed distribution. Current
approaches handle the long-tailed problem to transform the long-tailed dataset
to uniform distribution by re-sampling or re-weighting strategies. These
approaches emphasize the tail classes but ignore the hard examples in head
classes, which result in performance degradation. In this paper, we propose a
novel gradient harmonized mechanism with category-wise adaptive precision to
decouple the difficulty and sample size imbalance in the long-tailed problem,
which are correspondingly solved via intra- and inter-category balance
strategies. Specifically, intra-category balance focuses on the hard examples
in each category to optimize the decision boundary, while inter-category
balance aims to correct the shift of decision boundary by taking each category
as a unit. Extensive experiments demonstrate that the proposed method
consistently outperforms other approaches on all the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Situational Perception Guided Image Matting. (arXiv:2204.09276v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09276">
<div class="article-summary-box-inner">
<span><p>Most automatic matting methods try to separate the salient foreground from
the background. However, the insufficient quantity and subjective bias of the
current existing matting datasets make it difficult to fully explore the
semantic association between object-to-object and object-to-environment in a
given image. In this paper, we propose a Situational Perception Guided Image
Matting (SPG-IM) method that mitigates subjective bias of matting annotations
and captures sufficient situational perception information for better global
saliency distilled from the visual-to-textual task. SPG-IM can better associate
inter-objects and object-to-environment saliency, and compensate the subjective
nature of image matting and its expensive annotation. We also introduce a
textual Semantic Transformation (TST) module that can effectively transform and
integrate the semantic feature stream to guide the visual representations. In
addition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed
to adaptively switch multi-scale receptive fields and focal points to enhance
both global and local details. Extensive experiments demonstrate the
effectiveness of situational perception guidance from the visual-to-textual
tasks on image matting, and our model outperforms the state-of-the-art methods.
We also analyze the significance of different components in our model. The code
will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complete identification of complex salt-geometries from inaccurate migrated images using Deep Learning. (arXiv:2204.09710v2 [physics.geo-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09710">
<div class="article-summary-box-inner">
<span><p>Delimiting salt inclusions from migrated images is a time-consuming activity
that relies on highly human-curated analysis and is subject to interpretation
errors or limitations of the methods available. We propose to use migrated
images produced from an inaccurate velocity model (with a reasonable
approximation of sediment velocity, but without salt inclusions) to predict the
correct salt inclusions shape using a Convolutional Neural Network (CNN). Our
approach relies on subsurface Common Image Gathers to focus the sediments'
reflections around the zero offset and to spread the energy of salt reflections
over large offsets. Using synthetic data, we trained a U-Net to use
common-offset subsurface images as input channels for the CNN and the correct
salt-masks as network output. The network learned to predict the salt
inclusions masks with high accuracy; moreover, it also performed well when
applied to synthetic benchmark data sets that were not previously introduced.
Our training process tuned the U-Net to successfully learn the shape of complex
salt bodies from partially focused subsurface offset images.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-25 23:08:31.973409899 UTC">2022-04-25 23:08:31 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>