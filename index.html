<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-24T01:30:00Z">06-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommendations for Systematic Research on Emergent Language. (arXiv:2206.11302v1 [cs.MA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11302">
<div class="article-summary-box-inner">
<span><p>Emergent language is unique among fields within the discipline of machine
learning for its open-endedness, not obviously presenting well-defined problems
to be solved. As a result, the current research in the field has largely been
exploratory: focusing on establishing new problems, techniques, and phenomena.
Yet after these problems have been established, subsequent progress requires
research which can measurably demonstrate how it improves on prior approaches.
This type of research is what we call systematic research; in this paper, we
illustrate this mode of research specifically for emergent language. We first
identify the overarching goals of emergent language research, categorizing them
as either science or engineering. Using this distinction, we present core
methodological elements of science and engineering, analyze their role in
current emergent language research, and recommend how to apply these elements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GODEL: Large-Scale Pre-Training for Goal-Directed Dialog. (arXiv:2206.11309v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11309">
<div class="article-summary-box-inner">
<span><p>We introduce GODEL (Grounded Open Dialogue Language Model), a large
pre-trained language model for dialog. In contrast with earlier models such as
DialoGPT, GODEL leverages a new phase of grounded pre-training designed to
better support adapting GODEL to a wide range of downstream dialog tasks that
require information external to the current conversation (e.g., a database or
document) to produce good responses. Experiments against an array of benchmarks
that encompass task-oriented dialog, conversational QA, and grounded
open-domain dialog show that GODEL outperforms state-of-the-art pre-trained
dialog models in few-shot fine-tuning setups, in terms of both human and
automatic evaluation. A novel feature of our evaluation methodology is the
introduction of a notion of utility that assesses the usefulness of responses
(extrinsic evaluation) in addition to their communicative features (intrinsic
evaluation). We show that extrinsic evaluation offers improved inter-annotator
agreement and correlation with automated metrics. Code and data processing
scripts are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon. (arXiv:2206.11332v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11332">
<div class="article-summary-box-inner">
<span><p>Finding word boundaries in continuous speech is challenging as there is
little or no equivalent of a 'space' delimiter between words. Popular Bayesian
non-parametric models for text segmentation use a Dirichlet process to jointly
segment sentences and build a lexicon of word types. We introduce DP-Parse,
which uses similar principles but only relies on an instance lexicon of word
tokens, avoiding the clustering errors that arise with a lexicon of word types.
On the Zero Resource Speech Benchmark 2017, our model sets a new speech
segmentation state-of-the-art in 5 languages. The algorithm monotonically
improves with better input representations, achieving yet higher scores when
fed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can
be pipelined to a language model and learn semantic and syntactic
representations as assessed by a new spoken word embedding benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Injection: Parameterization of Fixed Inputs. (arXiv:2206.11349v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11349">
<div class="article-summary-box-inner">
<span><p>Recent works have shown that attaching prompts to the input is effective at
conditioning Language Models (LM) to perform specific tasks. However, prompts
are always included in the input text during inference, thus incurring
substantial computational and memory overhead. Also, there is currently no
straightforward method of utilizing prompts that are longer than the maximum
input length of the LMs without incurring additional costs during inference. We
propose Prompt Injection (PI), a novel formulation of injecting the prompt into
the parameters of an LM to be an efficient alternative to attaching fixed
prompts to the input. We show that in scenarios with long fixed prompts, PI can
be up to 280 times more efficient in terms of total FLOPs than previous
approaches. We further explore methodologies for PI and show promising results
in persona-dependent conversation, semantic parsing, and zero-shot learning
with task instructions. Through these explorations, we show that PI can be a
promising direction for conditioning language models, especially in scenarios
with long and fixed prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models. (arXiv:2206.11484v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11484">
<div class="article-summary-box-inner">
<span><p>This paper presents exploratory work on whether and to what extent biases
against queer and trans people are encoded in large language models (LLMs) such
as BERT. We also propose a method for reducing these biases in downstream
tasks: finetuning the models on data written by and/or about queer people. To
measure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,
modeled after other bias-detection benchmarks but addressing homophobic and
transphobic biases. We found that BERT shows significant homophobic bias, but
this bias can be mostly mitigated by finetuning BERT on a natural language
corpus written by members of the LGBTQ+ community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Error Templates for Grammatical Error Correction. (arXiv:2206.11569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11569">
<div class="article-summary-box-inner">
<span><p>Some grammatical error correction (GEC) systems incorporate hand-crafted
rules and achieve positive results. However, manually defining rules is
time-consuming and laborious. In view of this, we propose a method to mine
error templates for GEC automatically. An error template is a regular
expression aiming at identifying text errors. We use the web crawler to acquire
such error templates from the Internet. For each template, we further select
the corresponding corrective action by using the language model perplexity as a
criterion. We have accumulated 1,119 error templates for Chinese GEC based on
this method. Experimental results on the newly proposed CTC-2021 Chinese GEC
benchmark show that combing our error templates can effectively improve the
performance of a strong GEC system, especially on two error types with very
little training data. Our error templates are available at
\url{https://github.com/HillZhang1999/gec_error_template}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content. (arXiv:2206.11612v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11612">
<div class="article-summary-box-inner">
<span><p>The online health community (OHC) is the primary channel for laypeople to
share health information. To analyze the health consumer-generated content
(HCGC) from the OHCs, identifying the colloquial medical expressions used by
laypeople is a critical challenge. The open-access and collaborative consumer
health vocabulary (OAC CHV) is the controlled vocabulary for addressing such a
challenge. Nevertheless, OAC CHV is only available in English, limiting the
applicability to other languages. This research aims to propose a cross-lingual
automatic term recognition framework for extending the English OAC CHV into a
cross-lingual one. Our framework requires an English HCGC corpus and a
non-English (i.e., Chinese in this study) HCGC corpus as inputs. Two
monolingual word vector spaces are determined using skip-gram algorithm so that
each space encodes common word associations from laypeople within a language.
Based on isometry assumption, the framework align two monolingual spaces into a
bilingual word vector space, where we employ cosine similarity as a metric for
identifying semantically similar words across languages. In the experiments,
our framework demonstrates that it can effectively retrieve similar medical
terms, including colloquial expressions, across languages and further
facilitate compilation of cross-lingual CHV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models. (arXiv:2206.11684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11684">
<div class="article-summary-box-inner">
<span><p>NLP models trained on text have been shown to reproduce human stereotypes,
which can magnify harms to marginalized groups when systems are deployed at
scale. We adapt the Agency-Belief-Communion (ABC) stereotype model of Koch et
al. (2016) from social psychology as a framework for the systematic study and
discovery of stereotypic group-trait associations in language models (LMs). We
introduce the sensitivity test (SeT) for measuring stereotypical associations
from language models. To evaluate SeT and other measures using the ABC model,
we collect group-trait judgments from U.S.-based subjects to compare with
English LM stereotypes. Finally, we extend this framework to measure LM
stereotyping of intersectional identities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery. (arXiv:2206.11706v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11706">
<div class="article-summary-box-inner">
<span><p>Latent Dirichlet allocation (LDA) is widely used for unsupervised topic
modelling on sets of documents. No temporal information is used in the model.
However, there is often a relationship between the corresponding topics of
consecutive tokens. In this paper, we present an extension to LDA that uses a
Markov chain to model temporal information. We use this new model for acoustic
unit discovery from speech. As input tokens, the model takes a discretised
encoding of speech from a vector quantised (VQ) neural network with 512 codes.
The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in
order to more closely resemble true phones. In contrast to the base LDA, which
only considers how VQ codes co-occur within utterances (documents), the Markov
chain LDA additionally captures how consecutive codes follow one another. This
extension leads to an increase in cluster quality and phone segmentation
results compared to the base LDA. Compared to a recent vector quantised neural
network approach that also learns 50 units, the extended LDA model performs
better in phone segmentation but worse in mutual information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models. (arXiv:2206.11719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11719">
<div class="article-summary-box-inner">
<span><p>The objective of pre-trained language models is to learn contextual
representations of textual data. Pre-trained language models have become
mainstream in natural language processing and code modeling. Using probes, a
technique to study the linguistic properties of hidden vector spaces, previous
works have shown that these pre-trained language models encode simple
linguistic properties in their hidden representations. However, none of the
previous work assessed whether these models encode the whole grammatical
structure of a programming language. In this paper, we prove the existence of a
\textit{syntactic subspace}, lying in the hidden representations of pre-trained
language models, which contain the syntactic information of the programming
language. We show that this subspace can be extracted from the models'
representations and define a novel probing method, the AST-Probe, that enables
recovering the whole abstract syntax tree (AST) of an input code snippet. In
our experimentations, we show that this syntactic subspace exists in five
state-of-the-art pre-trained language models. In addition, we highlight that
the middle layers of the models are the ones that encode most of the AST
information. Finally, we estimate the optimal size of this syntactic subspace
and show that its dimension is substantially lower than those of the models'
representation spaces. This suggests that pre-trained language models use a
small part of their representation spaces to encode syntactic information of
the programming languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chat, Shift and Perform: Bridging the Gap between Task-oriented and Non-task-oriented Dialog Systems. (arXiv:2206.11813v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11813">
<div class="article-summary-box-inner">
<span><p>We propose CASPER (ChAt, Shift and PERform), a novel dialog system consisting
of three types of dialog models: chatter, shifter, and performer. Shifter,
which is designed for topic switching, enables a seamless flow of dialog from
open-domain chat- to task-oriented dialog. In a user study, CASPER gave a
better impression in terms of naturalness of response, lack of forced topic
switching, and satisfaction compared with a baseline dialog system trained in
an end-to-end manner. In an ablation study, we found that naturalness of
response, dialog satisfaction, and task-elicitation rate improved compared with
when shifter was removed from CASPER, indicating that topic shift with shifter
supports the introduction of natural task-oriented dialog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Always Keep your Target in Mind: Studying Semantics and Improving Performance of Neural Lexical Substitution. (arXiv:2206.11815v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11815">
<div class="article-summary-box-inner">
<span><p>Lexical substitution, i.e. generation of plausible words that can replace a
particular target word in a given context, is an extremely powerful technology
that can be used as a backbone of various NLP applications, including word
sense induction and disambiguation, lexical relation extraction, data
augmentation, etc. In this paper, we present a large-scale comparative study of
lexical substitution methods employing both rather old and most recent language
and masked language models (LMs and MLMs), such as context2vec, ELMo, BERT,
RoBERTa, XLNet. We show that already competitive results achieved by SOTA
LMs/MLMs can be further substantially improved if information about the target
word is injected properly. Several existing and new target word injection
methods are compared for each LM/MLM using both intrinsic evaluation on lexical
substitution datasets and extrinsic evaluation on word sense induction (WSI)
datasets. On two WSI datasets we obtain new SOTA results. Besides, we analyze
the types of semantic relations between target words and their substitutes
generated by different models or given by annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Obj2Sub: Unsupervised Conversion of Objective to Subjective Questions. (arXiv:2206.11848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11848">
<div class="article-summary-box-inner">
<span><p>Exams are conducted to test the learner's understanding of the subject. To
prevent the learners from guessing or exchanging solutions, the mode of tests
administered must have sufficient subjective questions that can gauge whether
the learner has understood the concept by mandating a detailed answer. Hence,
in this paper, we propose a novel hybrid unsupervised approach leveraging
rule-based methods and pre-trained dense retrievers for the novel task of
automatically converting the objective questions to subjective questions. We
observe that our approach outperforms the existing data-driven approaches by
36.45% as measured by Recall@k and Precision@k.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-based Virtual Adversarial Training for Text Classification with Noisy Labels. (arXiv:2206.11851v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11851">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have a high capacity to completely memorize noisy
labels given sufficient training time, and its memorization, unfortunately,
leads to performance degradation. Recently, virtual adversarial training (VAT)
attracts attention as it could further improve the generalization of DNNs in
semi-supervised learning. The driving force behind VAT is to prevent the models
from overfitting data points by enforcing consistency between the inputs and
the perturbed inputs. This strategy could be helpful in learning from noisy
labels if it prevents neural models from learning noisy samples while
encouraging the models to generalize clean samples. In this paper, we propose
context-based virtual adversarial training (ConVAT) to prevent a text
classifier from overfitting to noisy labels. Unlike the previous works, the
proposed method performs the adversarial training at the context level rather
than the inputs. It makes the classifier not only learn its label but also its
contextual neighbors, which alleviates the learning from noisy labels by
preserving contextual semantics on each data point. We conduct extensive
experiments on four text classification datasets with two types of label
noises. Comprehensive experimental results clearly show that the proposed
method works quite well even with extremely noisy settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HYU at SemEval-2022 Task 2: Effective Idiomaticity Detection with Consideration at Different Levels of Contextualization. (arXiv:2206.11854v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11854">
<div class="article-summary-box-inner">
<span><p>We propose a unified framework that enables us to consider various aspects of
contextualization at different levels to better identify the idiomaticity of
multi-word expressions. Through extensive experiments, we demonstrate that our
approach based on the inter- and inner-sentence context of a target MWE is
effective in improving the performance of related models. We also share our
experience in detail on the task of SemEval-2022 Tasks 2 such that future work
on the same task can be benefited from this.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Transliterated Words for Finding Similarity in Inter-Language News Articles using Machine Learning. (arXiv:2206.11860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11860">
<div class="article-summary-box-inner">
<span><p>Finding similarities between two inter-language news articles is a
challenging problem of Natural Language Processing (NLP). It is difficult to
find similar news articles in a different language other than the native
language of user, there is a need for a Machine Learning based automatic system
to find the similarity between two inter-language news articles. In this
article, we propose a Machine Learning model with the combination of English
Urdu word transliteration which will show whether the English news article is
similar to the Urdu news article or not. The existing approaches to find
similarities has a major drawback when the archives contain articles of
low-resourced languages like Urdu along with English news article. The existing
approaches to find similarities has drawback when the archives contain
low-resourced languages like Urdu along with English news articles. We used
lexicon to link Urdu and English news articles. As Urdu language processing
applications like machine translation, text to speech, etc are unable to handle
English text at the same time so this research proposed technique to find
similarities in English and Urdu news articles based on transliteration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of Programming Exercises and Code Explanations with Large Language Models. (arXiv:2206.11861v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11861">
<div class="article-summary-box-inner">
<span><p>OpenAI Codex is a recent large language model from the GPT-3 family for
translating code into natural language and vice versa. Recent explorations of
Codex have highlighted that given typical introductory programming exercise
problem statements as input, the model can generate code solutions well above
the level of an average student. In this article, we explore the natural
language generation capabilities of Codex in two different phases of the life
of a programming exercise; automatically creating programming exercises
(including sample solutions and test cases) and explanations of written code,
assessing these qualitatively and quantitatively. We find the majority of this
automatically generated content both novel and sensible, and in many cases
ready to use as is. We further find that influencing the content of the created
programming exercises is remarkably easy with minor modifications to the input.
Our analysis suggests that there is significant value in massive generative
machine learning models as a tool for instructors, although some oversight
might be needed to ensure the quality of the generated content before it is
delivered to students. We further discuss the implications of OpenAI Codex and
similar tools for introductory programming education and highlight future
research streams that have the potential to improve the quality of the
educational experience for both teachers and students alike.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Urdu News Article Recommendation Model using Natural Language Processing Techniques. (arXiv:2206.11862v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11862">
<div class="article-summary-box-inner">
<span><p>There are several online newspapers in urdu but for the users it is difficult
to find the content they are looking for because these most of them contain
irrelevant data and most users did not get what they want to retrieve. Our
proposed framework will help to predict Urdu news in the interests of users and
reduce the users searching time for news. For this purpose, NLP techniques are
used for pre-processing, and then TF-IDF with cosine similarity is used for
gaining the highest similarity and recommended news on user preferences.
Moreover, the BERT language model is also used for similarity, and by using the
BERT model similarity increases as compared to TF-IDF so the approach works
better with the BERT language model and recommends news to the user on their
interest. The news is recommended when the similarity of the articles is above
60 percent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking. (arXiv:2206.11863v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11863">
<div class="article-summary-box-inner">
<span><p>The explosion of misinformation spreading in the media ecosystem urges for
automated fact-checking. While misinformation spans both geographic and
linguistic boundaries, most work in the field has focused on English. Datasets
and tools available in other languages, such as Chinese, are limited. In order
to bridge this gap, we construct CHEF, the first CHinese Evidence-based
Fact-checking dataset of 10K real-world claims. The dataset covers multiple
domains, ranging from politics to public health, and provides annotated
evidence retrieved from the Internet. Further, we develop established baselines
and a novel approach that is able to model the evidence retrieval as a latent
variable, allowing jointly training with the veracity prediction model in an
end-to-end fashion. Extensive experiments show that CHEF will provide a
challenging testbed for the development of fact-checking systems designed to
retrieve and reason over non-English claims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Romantic-Computing. (arXiv:2206.11864v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11864">
<div class="article-summary-box-inner">
<span><p>In this paper we compare various text generation models' ability to write
poetry in the style of early English Romanticism. These models include:
Character-Level Recurrent Neural Networks with Long Short-Term Memory, Hugging
Face's GPT-2, OpenAI's GPT-3, and EleutherAI's GPT-NEO. Quality was measured
based syllable count and coherence with the automatic evaluation metric GRUEN.
Character-Level Recurrent Neural Networks performed far worse compared to
transformer models. And, as parameter-size increased, the quality of
transformer models' poems improved. These models are typically not compared in
a creative context, and we are happy to contribute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BOS at LSCDiscovery: Lexical Substitution for Interpretable Lexical Semantic Change Detection. (arXiv:2206.11865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11865">
<div class="article-summary-box-inner">
<span><p>We propose a solution for the LSCDiscovery shared task on Lexical Semantic
Change Detection in Spanish. Our approach is based on generating lexical
substitutes that describe old and new senses of a given word. This approach
achieves the second best result in sense loss and sense gain detection
subtasks. By observing those substitutes that are specific for only one time
period, one can understand which senses were obtained or lost. This allows
providing more detailed information about semantic change to the user and makes
our method interpretable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Policy Framework for Deep Learning-Based Fake News Detection. (arXiv:2206.11866v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11866">
<div class="article-summary-box-inner">
<span><p>Connectivity plays an ever-increasing role in modern society, with people all
around the world having easy access to rapidly disseminated information.
However, a more interconnected society enables the spread of intentionally
false information. To mitigate the negative impacts of fake news, it is
essential to improve detection methodologies. This work introduces Multi-Policy
Statement Checker (MPSC), a framework that automates fake news detection by
using deep learning techniques to analyze a statement itself and its related
news articles, predicting whether it is seemingly credible or suspicious. The
proposed framework was evaluated using four merged datasets containing real and
fake news. Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and
Bidirectional Encoder Representations from Transformers (BERT) models were
trained to utilize both lexical and syntactic features, and their performance
was evaluated. The obtained results demonstrate that a multi-policy analysis
reliably identifies suspicious statements, which can be advantageous for fake
news detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Learning Natural Language Processing Approach for Multilingual Data Classification. (arXiv:2206.11867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11867">
<div class="article-summary-box-inner">
<span><p>The abundance of information in digital media, which in today's world is the
main source of knowledge about current events for the masses, makes it possible
to spread disinformation on a larger scale than ever before. Consequently,
there is a need to develop novel fake news detection approaches capable of
adapting to changing factual contexts and generalizing previously or
concurrently acquired knowledge. To deal with this problem, we propose a
lifelong learning-inspired approach, which allows for fake news detection in
multiple languages and the mutual transfer of knowledge acquired in each of
them. Both classical feature extractors, such as Term frequency-inverse
document frequency or Latent Dirichlet Allocation, and integrated deep NLP
(Natural Language Processing) BERT (Bidirectional Encoder Representations from
Transformers) models paired with MLP (Multilayer Perceptron) classifier, were
employed. The results of experiments conducted on two datasets dedicated to the
fake news classification task (in English and Spanish, respectively), supported
by statistical analysis, confirmed that utilization of additional languages
could improve performance for traditional methods. Also, in some cases
supplementing the deep learning method with classical ones can positively
impact obtained results. The ability of models to generalize the knowledge
acquired between the analyzed languages was also observed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offline RL for Natural Language Generation with Implicit Language Q Learning. (arXiv:2206.11871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11871">
<div class="article-summary-box-inner">
<span><p>Large language models distill broad knowledge from text corpora. However,
they can be inconsistent when it comes to completing user specified tasks. This
issue can be addressed by finetuning such models via supervised learning on
curated datasets, or via reinforcement learning. In this work, we propose a
novel offline RL motivated method, implicit language Q-learning (ILQL),
designed for use on language models, that combines both the flexible utility
optimization framework of traditional RL algorithms with supervised learning's
ability to leverage existing data and its simplicity and stability. Our method,
based on dynamic programming, employs a blend of value conservatism alongside
an implicit dataset support constraint in learning value functions, which are
then used to guide language model generations towards maximizing utility. In
addition to empirically validating ILQL, we present a detailed empirical
analysis of situations where offline RL can be useful in natural language
generation settings, demonstrating how it can be a more effective utility
optimizer than prior approaches for end-to-end dialogue, and how it can
effectively optimize high variance reward functions based on subjective
judgement, such as whether to label a comment as an example of toxic speech or
not.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters. (arXiv:2003.12739v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.12739">
<div class="article-summary-box-inner">
<span><p>How to best integrate linguistic and perceptual processing in multi-modal
tasks that involve language and vision is an important open problem. In this
work, we argue that the common practice of using language in a top-down manner,
to direct visual attention over high-level visual features, may not be optimal.
We hypothesize that the use of language to also condition the bottom-up
processing from pixels to high-level features can provide benefits to the
overall performance. To support our claim, we propose a U-Net-based model and
perform experiments on two language-vision dense-prediction tasks: referring
expression segmentation and language-guided image colorization. We compare
results where either one or both of the top-down and bottom-up visual branches
are conditioned on language. Our experiments reveal that using language to
control the filters for bottom-up visual processing in addition to top-down
attention leads to better results on both tasks and achieves competitive
performance. Our linguistic analysis suggests that bottom-up conditioning
improves segmentation of objects especially when input text refers to low-level
visual concepts. Code is available at https://github.com/ilkerkesen/bvpr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base. (arXiv:2007.03875v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.03875">
<div class="article-summary-box-inner">
<span><p>Complex question answering over knowledge base (Complex KBQA) is challenging
because it requires various compositional reasoning capabilities, such as
multi-hop inference, attribute comparison, set operation. Existing benchmarks
have some shortcomings that limit the development of Complex KBQA: 1) they only
provide QA pairs without explicit reasoning processes; 2) questions are poor in
diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex
KBQA including ~120K diverse natural language questions. We introduce a
compositional and interpretable programming language KoPL to represent the
reasoning process of complex questions. For each question, we provide the
corresponding KoPL program and SPARQL query, so that KQA Pro serves for both
KBQA and semantic parsing tasks. Experimental results show that SOTA KBQA
methods cannot achieve promising results on KQA Pro as on current datasets,
which suggests that KQA Pro is challenging and Complex KBQA requires further
research efforts. We also treat KQA Pro as a diagnostic dataset for testing
multiple reasoning skills, conduct a thorough evaluation of existing models and
discuss further directions for Complex KBQA. Our codes and datasets can be
obtained from https://github.com/shijx12/KQAPro_Baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08315">
<div class="article-summary-box-inner">
<span><p>Large language models have shown promising results in zero-shot settings
(Brown et al.,2020; Radford et al., 2019). For example, they can perform
multiple choice tasks simply by conditioning on a question and selecting the
answer with the highest probability.
</p>
<p>However, ranking by string probability can be problematic due to surface form
competition-wherein different surface forms compete for probability mass, even
if they represent the same underlying concept, e.g. "computer" and "PC." Since
probability mass is finite, this lowers the probability of the correct answer,
due to competition from other strings that are valid answers (but not one of
the multiple choice options).
</p>
<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative
scoring function that directly compensates for surface form competition by
simply reweighing each option according to a term that is proportional to its a
priori likelihood within the context of the specific zero-shot task. It
achieves consistent gains in zero-shot performance over both calibrated (Zhao
et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models
over a variety of multiple choice datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining. (arXiv:2109.01411v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01411">
<div class="article-summary-box-inner">
<span><p>The Linked Open Data practice has led to a significant growth of structured
data on the Web in the last decade. Such structured data describe real-world
entities in a machine-readable way, and have created an unprecedented
opportunity for research in the field of Natural Language Processing. However,
there is a lack of studies on how such data can be used, for what kind of
tasks, and to what extent they can be useful for these tasks. This work focuses
on the e-commerce domain to explore methods of utilising such structured data
to create language resources that may be used for product classification and
linking. We process billions of structured data points in the form of RDF
n-quads, to create multi-million words of product-related corpora that are
later used in three different ways for creating of language resources: training
word embedding models, continued pre-training of BERT-like language models, and
training Machine Translation models that are used as a proxy to generate
product-related keywords. Our evaluation on an extensive set of benchmarks
shows word embeddings to be the most reliable and consistent method to improve
the accuracy on both tasks (with up to 6.9 percentage points in macro-average
F1 on some datasets). The other two methods however, are not as useful. Our
analysis shows that this could be due to a number of reasons, including the
biased domain representation in the structured data and lack of vocabulary
coverage. We share our datasets and discuss how our lessons learned could be
taken forward to inform future research in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-based automatic personality prediction: A bibliographic review. (arXiv:2110.01186v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01186">
<div class="article-summary-box-inner">
<span><p>Personality detection is an old topic in psychology and Automatic Personality
Prediction (or Perception) (APP) is the automated (computationally) forecasting
of the personality on different types of human generated/exchanged contents
(such as text, speech, image, video). The principal objective of this study is
to offer a shallow (overall) review of natural language processing approaches
on APP since 2010. With the advent of deep learning and following it
transfer-learning and pre-trained model in NLP, APP research area has been a
hot topic, so in this review, methods are categorized into three; pre-trained
independent, pre-trained model based, multimodal approaches. Also, to achieve a
comprehensive comparison, reported results are informed by datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-horizon Robot Manipulation Tasks. (arXiv:2112.03227v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03227">
<div class="article-summary-box-inner">
<span><p>General-purpose robots coexisting with humans in their environment must learn
to relate human language to their perceptions and actions to be useful in a
range of daily tasks. Moreover, they need to acquire a diverse repertoire of
general-purpose skills that allow composing long-horizon tasks by following
unconstrained language instructions. In this paper, we present CALVIN
(Composing Actions from Language and Vision), an open-source simulated
benchmark to learn long-horizon language-conditioned tasks. Our aim is to make
it possible to develop agents that can solve many robotic manipulation tasks
over a long horizon, from onboard sensors, and specified only via human
language. CALVIN tasks are more complex in terms of sequence length, action
space, and language than existing vision-and-language task datasets and
supports flexible specification of sensor suites. We evaluate the agents in
zero-shot to novel language instructions and to novel environments and objects.
We show that a baseline model based on multi-context imitation learning
performs poorly on CALVIN, suggesting that there is significant room for
developing innovative agents that learn to relate human language to their world
models with this benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refined Commonsense Knowledge from Large-Scale Web Contents. (arXiv:2112.04596v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04596">
<div class="article-summary-box-inner">
<span><p>Commonsense knowledge (CSK) about concepts and their properties is helpful
for AI applications. Prior works, such as ConceptNet, have compiled large CSK
collections. However, they are restricted in their expressiveness to
subject-predicate-object (SPO) triples with simple concepts for S and strings
for P and O. This paper presents a method called ASCENT++ to automatically
build a large-scale knowledge base (KB) of CSK assertions, with refined
expressiveness and both better precision and recall than prior works. ASCENT++
goes beyond SPO triples by capturing composite concepts with subgroups and
aspects, and by refining assertions with semantic facets. The latter is
essential to express the temporal and spatial validity of assertions and
further qualifiers. Furthermore, ASCENT++ combines open information extraction
(OpenIE) with judicious cleaning and ranking by typicality and saliency scores.
For high coverage, our method taps into the large-scale crawl C4 with broad web
contents. The evaluation with human judgments shows the superior quality of the
ASCENT++ KB, and an extrinsic evaluation for QA-support tasks underlines the
benefits of ASCENT++. A web interface, data, and code can be accessed at
https://ascentpp.mpi-inf.mpg.de/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing language context confusion for end-to-end code-switching automatic speech recognition. (arXiv:2201.12155v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12155">
<div class="article-summary-box-inner">
<span><p>Code-switching deals with alternative languages in communication process.
Training end-to-end (E2E) automatic speech recognition (ASR) systems for
code-switching is especially challenging as code-switching training data are
always insufficient to combat the increased multilingual context confusion due
to the presence of more than one language. We propose a language-related
attention mechanism to reduce multilingual context confusion for the E2E
code-switching ASR model based on the Equivalence Constraint (EC) Theory. The
linguistics theory requires that any monolingual fragment that occurs in the
code-switching sentence must occur in one of the monolingual sentences. The
theory establishes a bridge between monolingual data and code-switching data.
We leverage this linguistics theory to design the code-switching E2E ASR model.
The proposed model efficiently transfers language knowledge from rich
monolingual data to improve the performance of the code-switching ASR model. We
evaluate our model on ASRU 2019 Mandarin-English code-switching challenge
dataset. Compared to the baseline model, our proposed model achieves a 17.12%
relative error reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One-Shot Learning. (arXiv:2202.02394v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02394">
<div class="article-summary-box-inner">
<span><p>Large Language Models have been successful in a wide variety of Natural
Language Processing tasks by capturing the compositionality of the text
representations. In spite of their great success, these vector representations
fail to capture meaning of idiomatic multi-word expressions (MWEs). In this
paper, we focus on the detection of idiomatic expressions by using binary
classification. We use a dataset consisting of the literal and idiomatic usage
of MWEs in English and Portuguese. Thereafter, we perform the classification in
two different settings: zero shot and one shot, to determine if a given
sentence contains an idiom or not. N shot classification for this task is
defined by N number of common idioms between the training and testing sets. In
this paper, we train multiple Large Language Models in both the settings and
achieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score
(macro) of 0.85 for the one shot setting. An implementation of our work can be
found at
https://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Short Math Answer Grading via In-context Meta-learning. (arXiv:2205.15219v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15219">
<div class="article-summary-box-inner">
<span><p>Automatic short answer grading is an important research direction in the
exploration of how to use artificial intelligence (AI)-based tools to improve
education. Current state-of-the-art approaches use neural language models to
create vectorized representations of students responses, followed by
classifiers to predict the score. However, these approaches have several key
limitations, including i) they use pre-trained language models that are not
well-adapted to educational subject domains and/or student-generated text and
ii) they almost always train one model per question, ignoring the linkage
across a question and result in a significant model storage problem due to the
size of advanced language models. In this paper, we study the problem of
automatic short answer grading for students' responses to math questions and
propose a novel framework for this task. First, we use MathBERT, a variant of
the popular language model BERT adapted to mathematical content, as our base
model and fine-tune it for the downstream task of student response grading.
Second, we use an in-context learning approach that provides scoring examples
as input to the language model to provide additional context information and
promote generalization to previously unseen questions. We evaluate our
framework on a real-world dataset of student responses to open-ended math
questions and show that our framework (often significantly) outperforms
existing approaches, especially for new questions that are not seen during
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval. (arXiv:2206.02873v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02873">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that small distilled language models are strong
competitors to models that are orders of magnitude larger and slower in a wide
range of information retrieval tasks. This has made distilled and dense models,
due to latency constraints, the go-to choice for deployment in real-world
retrieval applications. In this work, we question this practice by showing that
the number of parameters and early query-document interaction play a
significant role in the generalization ability of retrieval models. Our
experiments show that increasing model size results in marginal gains on
in-domain test sets, but much larger gains in new domains never seen during
fine-tuning. Furthermore, we show that rerankers largely outperform dense ones
of similar size in several tasks. Our largest reranker reaches the state of the
art in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the
previous state of the art by 3 average points. Finally, we confirm that
in-domain effectiveness is not a good indicator of zero-shot effectiveness.
Code is available at
https://github.com/guilhermemr04/scaling-zero-shot-retrieval.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEMv2: Multilingual NLG Benchmarking in a Single Line of Code. (arXiv:2206.11249v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11249">
<div class="article-summary-box-inner">
<span><p>Evaluation in machine learning is usually informed by past choices, for
example which datasets or metrics to use. This standardization enables the
comparison on equal footing using leaderboards, but the evaluation choices
become sub-optimal as better alternatives arise. This problem is especially
pertinent in natural language generation which requires ever-improving suites
of datasets, metrics, and human evaluation to make definitive claims. To make
following best model evaluation practices easier, we introduce GEMv2. The new
version of the Generation, Evaluation, and Metrics Benchmark introduces a
modular infrastructure for dataset, model, and metric developers to benefit
from each others work. GEMv2 supports 40 documented datasets in 51 languages.
Models for all datasets can be evaluated online and our interactive data card
creation and rendering tools make it easier to add new datasets to the living
benchmark.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Long-Tailed Bird Audio Recognition. (arXiv:2206.11260v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11260">
<div class="article-summary-box-inner">
<span><p>It is easier to hear birds than see them. However, they still play an
essential role in nature and are excellent indicators of deteriorating
environmental quality and pollution. Recent advances in Machine Learning and
Convolutional Neural Networks allow us to process continuous audio data to
detect and classify bird sounds. This technology can assist researchers in
monitoring bird populations' status and trends and ecosystems' biodiversity.
</p>
<p>We propose a sound detection and classification pipeline to analyze complex
soundscape recordings and identify birdcalls in the background. Our method
learns from weak labels and few data and acoustically recognizes the bird
species. Our solution achieved 18th place of 807 teams at the BirdCLEF 2022
Challenge hosted on Kaggle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doubly Reparameterized Importance Weighted Structure Learning for Scene Graph Generation. (arXiv:2206.11352v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11352">
<div class="article-summary-box-inner">
<span><p>As a structured prediction task, scene graph generation, given an input
image, aims to explicitly model objects and their relationships by constructing
a visually-grounded scene graph. In the current literature, such task is
universally solved via a message passing neural network based mean field
variational Bayesian methodology. The classical loose evidence lower bound is
generally chosen as the variational inference objective, which could induce
oversimplified variational approximation and thus underestimate the underlying
complex posterior. In this paper, we propose a novel doubly reparameterized
importance weighted structure learning method, which employs a tighter
importance weighted lower bound as the variational inference objective. It is
computed from multiple samples drawn from a reparameterizable Gumbel-Softmax
sampler and the resulting constrained variational inference task is solved by a
generic entropic mirror descent algorithm. The resulting doubly reparameterized
gradient estimator reduces the variance of the corresponding derivatives with a
beneficial impact on learning. The proposed method achieves the
state-of-the-art performance on various popular scene graph generation
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Spherical Depth Estimation with Explicitly Connected Weak Layout Cues. (arXiv:2206.11358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11358">
<div class="article-summary-box-inner">
<span><p>Spherical cameras capture scenes in a holistic manner and have been used for
room layout estimation. Recently, with the availability of appropriate
datasets, there has also been progress in depth estimation from a single
omnidirectional image. While these two tasks are complementary, few works have
been able to explore them in parallel to advance indoor geometric perception,
and those that have done so either relied on synthetic data, or used small
scale datasets, as few options are available that include both layout
annotations and dense depth maps in real scenes. This is partly due to the
necessity of manual annotations for room layouts. In this work, we move beyond
this limitation and generate a 360 geometric vision (360V) dataset that
includes multiple modalities, multi-view stereo data and automatically
generated weak layout cues. We also explore an explicit coupling between the
two tasks to integrate them into a singleshot trained model. We rely on
depth-based layout reconstruction and layout-based depth attention,
demonstrating increased performance across both tasks. By using single 360
cameras to scan rooms, the opportunity for facile and quick building-scale 3D
scanning arises.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Online Skeleton Extraction and Gesture Recognition on Pepper. (arXiv:2206.11376v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11376">
<div class="article-summary-box-inner">
<span><p>We present a multi-stage pipeline for simple gesture recognition. The novelty
of our approach is the association of different technologies, resulting in the
first real-time system as of now to conjointly extract skeletons and recognise
gesture on a Pepper robot. For this task, Pepper has been augmented with an
embedded GPU for running deep CNNs and a fish-eye camera to capture whole scene
interaction. We show in this article that real-case scenarios are challenging,
and the state-of-the-art approaches hardly deal with unknown human gestures. We
present here a way to handle such cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The ArtBench Dataset: Benchmarking Generative Models with Artworks. (arXiv:2206.11404v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11404">
<div class="article-summary-box-inner">
<span><p>We introduce ArtBench-10, the first class-balanced, high-quality, cleanly
annotated, and standardized dataset for benchmarking artwork generation. It
comprises 60,000 images of artwork from 10 distinctive artistic styles, with
5,000 training images and 1,000 testing images per style. ArtBench-10 has
several advantages over previous artwork datasets. Firstly, it is
class-balanced while most previous artwork datasets suffer from the long tail
class distributions. Secondly, the images are of high quality with clean
annotations. Thirdly, ArtBench-10 is created with standardized data collection,
annotation, filtering, and preprocessing procedures. We provide three versions
of the dataset with different resolutions ($32\times32$, $256\times256$, and
original image size), formatted in a way that is easy to be incorporated by
popular machine learning frameworks. We also conduct extensive benchmarking
experiments using representative image synthesis models with ArtBench-10 and
present in-depth analysis. The dataset is available at
https://github.com/liaopeiyuan/artbench under a Fair Use license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LidarMutliNet: Unifying LiDAR Semantic Segmentation, 3D Object Detection, and Panoptic Segmentation in a Single Multi-task Network. (arXiv:2206.11428v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11428">
<div class="article-summary-box-inner">
<span><p>This technical report presents the 1st place winning solution for the Waymo
Open Dataset 3D semantic segmentation challenge 2022. Our network, termed
LidarMultiNet, unifies the major LiDAR perception tasks such as 3D semantic
segmentation, object detection, and panoptic segmentation in a single
framework. At the core of LidarMultiNet is a strong 3D voxel-based
encoder-decoder network with a novel Global Context Pooling (GCP) module
extracting global contextual features from a LiDAR frame to complement its
local features. An optional second stage is proposed to refine the first-stage
segmentation or generate accurate panoptic segmentation results. Our solution
achieves a mIoU of 71.13 and is the best for most of the 22 classes on the
Waymo 3D semantic segmentation test set, outperforming all the other 3D
semantic segmentation methods on the official leaderboard. We demonstrate for
the first time that major LiDAR perception tasks can be unified in a single
strong network that can be trained end-to-end.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-based Stability Quantification. (arXiv:2206.11443v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11443">
<div class="article-summary-box-inner">
<span><p>Quantitative evaluation of human stability using foot pressure/force
measurement hardware and motion capture (mocap) technology is expensive, time
consuming, and restricted to the laboratory (lab-based). We propose a novel
image-based method to estimate three key components for stability computation:
Center of Mass (CoM), Base of Support (BoS), and Center of Pressure (CoP).
Furthermore, we quantitatively validate our image-based methods for computing
two classic stability measures against the ones generated directly from
lab-based sensory output (ground truth) using a publicly available
multi-modality (mocap, foot pressure, 2-view videos), ten-subject human motion
dataset. Using leave-one-subject-out cross validation, our experimental results
show: 1) our CoM estimation method (CoMNet) consistently outperforms
state-of-the-art inertial sensor-based CoM estimation techniques; 2) our
image-based method combined with insole foot-pressure alone produces consistent
and statistically significant correlation with ground truth stability measures
(CoMtoCoP R=0.79 P&lt;0.001, CoMtoBoS R=0.75 P&lt;0.001); 3) our fully image-based
stability metric estimation produces consistent, positive, and statistically
significant correlation on the two stability metrics (CoMtoCoP R=0.31 P&lt;0.001,
CoMtoBoS R=0.22 P&lt;0.001). Our study provides promising quantitative evidence
for stability computations and monitoring in natural environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weighted Concordance Index Loss-based Multimodal Survival Modeling for Radiation Encephalopathy Assessment in Nasopharyngeal Carcinoma Radiotherapy. (arXiv:2206.11458v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11458">
<div class="article-summary-box-inner">
<span><p>Radiation encephalopathy (REP) is the most common complication for
nasopharyngeal carcinoma (NPC) radiotherapy. It is highly desirable to assist
clinicians in optimizing the NPC radiotherapy regimen to reduce
radiotherapy-induced temporal lobe injury (RTLI) according to the probability
of REP onset. To the best of our knowledge, it is the first exploration of
predicting radiotherapy-induced REP by jointly exploiting image and non-image
data in NPC radiotherapy regimen. We cast REP prediction as a survival analysis
task and evaluate the predictive accuracy in terms of the concordance index
(CI). We design a deep multimodal survival network (MSN) with two feature
extractors to learn discriminative features from multimodal data. One feature
extractor imposes feature selection on non-image data, and the other learns
visual features from images. Because the priorly balanced CI (BCI) loss
function directly maximizing the CI is sensitive to uneven sampling per batch.
Hence, we propose a novel weighted CI (WCI) loss function to leverage all REP
samples effectively by assigning their different weights with a dual average
operation. We further introduce a temperature hyper-parameter for our WCI to
sharpen the risk difference of sample pairs to help model convergence. We
extensively evaluate our WCI on a private dataset to demonstrate its
favourability against its counterparts. The experimental results also show
multimodal data of NPC radiotherapy can bring more gains for REP risk
prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore Spatio-temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline. (arXiv:2206.11459v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11459">
<div class="article-summary-box-inner">
<span><p>We endeavor on a rarely explored task named Insubstantial Object Detection
(IOD), which aims to localize the object with following characteristics: (1)
amorphous shape with indistinct boundary; (2) similarity to surroundings; (3)
absence in color. Accordingly, it is far more challenging to distinguish
insubstantial objects in a single static frame and the collaborative
representation of spatial and temporal information is crucial. Thus, we
construct an IOD-Video dataset comprised of 600 videos (141,017 frames)
covering various distances, sizes, visibility, and scenes captured by different
spectral ranges. In addition, we develop a spatio-temporal aggregation
framework for IOD, in which different backbones are deployed and a
spatio-temporal aggregation loss (STAloss) is elaborately designed to leverage
the consistency along the time axis. Experiments conducted on IOD-Video dataset
demonstrate that spatio-temporal aggregation can significantly improve the
performance of IOD. We hope our work will attract further researches into this
valuable yet challenging task. The code will be available at:
\url{https://github.com/CalayZhou/IOD-Video}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better User Studies in Computer Graphics and Vision. (arXiv:2206.11461v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11461">
<div class="article-summary-box-inner">
<span><p>Online crowdsourcing platforms make it easy to perform evaluations of
algorithm outputs with surveys that ask questions like "which image is better,
A or B?") The proliferation of these "user studies" in vision and graphics
research papers has led to an increase of hastily conducted studies that are
sloppy and uninformative at best, and potentially harmful and misleading. We
argue that more attention needs to be paid to both the design and reporting of
user studies in computer vision and graphics papers. In an attempt to improve
practitioners' knowledge and increase the trustworthiness and replicability of
user studies, we provide an overview of methodologies from user experience
research (UXR), human-computer interaction (HCI), and related fields. We
discuss foundational user research methods (e.g., needfinding) that are
presently underutilized in computer vision and graphics research, but can
provide valuable guidance for research projects. We provide further pointers to
the literature for readers interested in exploring other UXR methodologies.
Finally, we describe broader open issues and recommendations for the research
community. We encourage authors and reviewers alike to recognize that not every
research contribution requires a user study, and that having no study at all is
better than having a carelessly conducted one.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICME 2022 Few-shot LOGO detection top 9 solution. (arXiv:2206.11462v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11462">
<div class="article-summary-box-inner">
<span><p>ICME-2022 few-shot logo detection competition is held in May, 2022.
Participants are required to develop a single model to detect logos by handling
tiny logo instances, similar brands, and adversarial images at the same time,
with limited annotations. Our team achieved rank 16 and 11 in the first and
second round of the competition respectively, with a final rank of 9th. This
technical report summarized our major techniques used in this competitions, and
potential improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complementary datasets to COCO for object detection. (arXiv:2206.11473v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11473">
<div class="article-summary-box-inner">
<span><p>For nearly a decade, the COCO dataset has been the central test bed of
research in object detection. According to the recent benchmarks, however, it
seems that performance on this dataset has started to saturate. One possible
reason can be that perhaps it is not large enough for training deep models. To
address this limitation, here we introduce two complementary datasets to COCO:
i) COCO_OI, composed of images from COCO and OpenImages (from their 80 classes
in common) with 1,418,978 training bounding boxes over 380,111 images, and
41,893 validation bounding boxes over 18,299 images, and ii) ObjectNet_D
containing objects in daily life situations (originally created for object
recognition known as ObjectNet; 29 categories in common with COCO). The latter
can be used to test the generalization ability of object detectors. We evaluate
some models on these datasets and pinpoint the source of errors. We encourage
the community to utilize these datasets for training and testing object
detection models. Code and data is available at
https://github.com/aliborji/COCO_OI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entropy-driven Sampling and Training Scheme for Conditional Diffusion Generation. (arXiv:2206.11474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11474">
<div class="article-summary-box-inner">
<span><p>Denoising Diffusion Probabilistic Model (DDPM) is able to make flexible
conditional image generation from prior noise to real data, by introducing an
independent noise-aware classifier to provide conditional gradient guidance at
each time step of denoising process. However, due to the ability of classifier
to easily discriminate an incompletely generated image only with high-level
structure, the gradient, which is a kind of class information guidance, tends
to vanish early, leading to the collapse from conditional generation process
into the unconditional process. To address this problem, we propose two simple
but effective approaches from two perspectives. For sampling procedure, we
introduce the entropy of predicted distribution as the measure of guidance
vanishing level and propose an entropy-aware scaling method to adaptively
recover the conditional semantic guidance. % for each generated sample. For
training stage, we propose the entropy-aware optimization objectives to
alleviate the overconfident prediction for noisy data.On ImageNet1000 256x256,
with our proposed sampling scheme and trained classifier, the pretrained
conditional and unconditional DDPM model can achieve 10.89% (4.59 to 4.09) and
43.5% (12 to 6.78) FID improvement respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Scene Deblurring Base on Continuous Cross-Layer Attention Transmission. (arXiv:2206.11476v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11476">
<div class="article-summary-box-inner">
<span><p>The deep convolutional neural networks (CNNs) using attention mechanism have
achieved great success for dynamic scene deblurring. In most of these networks,
only the features refined by the attention maps can be passed to the next layer
and the attention maps of different layers are separated from each other, which
does not make full use of the attention information from different layers in
the CNN. To address this problem, we introduce a new continuous cross-layer
attention transmission (CCLAT) mechanism that can exploit hierarchical
attention information from all the convolutional layers. Based on the CCLAT
mechanism, we use a very simple attention module to construct a novel residual
dense attention fusion block (RDAFB). In RDAFB, the attention maps inferred
from the outputs of the preceding RDAFB and each layer are directly connected
to the subsequent ones, leading to a CRLAT mechanism. Taking RDAFB as the
building block, we design an effective architecture for dynamic scene
deblurring named RDAFNet. The experiments on benchmark datasets show that the
proposed model outperforms the state-of-the-art deblurring approaches, and
demonstrate the effectiveness of CCLAT mechanism. The source code is available
on: https://github.com/xjmz6/RDAFNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Algorithm for Exact Concave Hull Extraction. (arXiv:2206.11481v1 [cs.CG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11481">
<div class="article-summary-box-inner">
<span><p>Region extraction is necessary in a wide range of applications, from object
detection in autonomous driving to analysis of subcellular morphology in cell
biology. There exist two main approaches: convex hull extraction, for which
exact and efficient algorithms exist and concave hulls, which are better at
capturing real-world shapes but do not have a single solution. Especially in
the context of a uniform grid, concave hull algorithms are largely approximate,
sacrificing region integrity for spatial and temporal efficiency. In this
study, we present a novel algorithm that can provide vertex-minimized concave
hulls with maximal (i.e. pixel-perfect) resolution and is tunable for
speed-efficiency tradeoffs. Our method provides advantages in multiple
downstream applications including data compression, retrieval, visualization,
and analysis. To demonstrate the practical utility of our approach, we focus on
image compression. We demonstrate significant improvements through
context-dependent compression on disparate regions within a single image
(entropy encoding for noisy and predictive encoding for the structured
regions). We show that these improvements range from biomedical images to
natural images. Beyond image compression, our algorithm can be applied more
broadly to aid in a wide range of practical applications for data retrieval,
visualization, and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Pre-Training for Federated Learning. (arXiv:2206.11488v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11488">
<div class="article-summary-box-inner">
<span><p>In most of the literature on federated learning (FL), neural networks are
initialized with random weights. In this paper, we present an empirical study
on the effect of pre-training on FL. Specifically, we aim to investigate if
pre-training can alleviate the drastic accuracy drop when clients'
decentralized data are non-IID. We focus on FedAvg, the fundamental and most
widely used FL algorithm. We found that pre-training does largely close the gap
between FedAvg and centralized learning under non-IID data, but this does not
come from alleviating the well-known model drifting problem in FedAvg's local
training. Instead, how pre-training helps FedAvg is by making FedAvg's global
aggregation more stable. When pre-training using real data is not feasible for
FL, we propose a novel approach to pre-train with synthetic data. On various
image datasets (including one for segmentation), our approach with synthetic
pre-training leads to a notable gain, essentially a critical step toward
scaling up federated learning for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Refactor Action and Co-occurrence Features for Temporal Action Localization. (arXiv:2206.11493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11493">
<div class="article-summary-box-inner">
<span><p>The main challenge of Temporal Action Localization is to retrieve subtle
human actions from various co-occurring ingredients, e.g., context and
background, in an untrimmed video. While prior approaches have achieved
substantial progress through devising advanced action detectors, they still
suffer from these co-occurring ingredients which often dominate the actual
action content in videos. In this paper, we explore two orthogonal but
complementary aspects of a video snippet, i.e., the action features and the
co-occurrence features. Especially, we develop a novel auxiliary task by
decoupling these two types of features within a video snippet and recombining
them to generate a new feature representation with more salient action
information for accurate action localization. We term our method RefactorNet,
which first explicitly factorizes the action content and regularizes its
co-occurrence features, and then synthesizes a new action-dominated video
representation. Extensive experimental results and ablation studies on THUMOS14
and ActivityNet v1.3 demonstrate that our new representation, combined with a
simple action detector, can significantly improve the action localization
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Structure from Motion for UAV Images via Weighted Connected Dominating Set. (arXiv:2206.11499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11499">
<div class="article-summary-box-inner">
<span><p>Incremental Structure from Motion (ISfM) has been widely used for UAV image
orientation. Its efficiency, however, decreases dramatically due to the
sequential constraint. Although the divide-and-conquer strategy has been
utilized for efficiency improvement, cluster merging becomes difficult or
depends on seriously designed overlap structures. This paper proposes an
algorithm to extract the global model for cluster merging and designs a
parallel SfM solution to achieve efficient and accurate UAV image orientation.
First, based on vocabulary tree retrieval, match pairs are selected to
construct an undirected weighted match graph, whose edge weights are calculated
by considering both the number and distribution of feature matches. Second, an
algorithm, termed weighted connected dominating set (WCDS), is designed to
achieve the simplification of the match graph and build the global model, which
incorporates the edge weight in the graph node selection and enables the
successful reconstruction of the global model. Third, the match graph is
simultaneously divided into compact and non-overlapped clusters. After the
parallel reconstruction, cluster merging is conducted with the aid of common 3D
points between the global and cluster models. Finally, by using three UAV
datasets that are captured by classical oblique and recent optimized views
photogrammetry, the validation of the proposed solution is verified through
comprehensive analysis and comparison. The experimental results demonstrate
that the proposed parallel SfM can achieve 17.4 times efficiency improvement
and comparative orientation accuracy. In absolute BA, the geo-referencing
accuracy is approximately 2.0 and 3.0 times the GSD (Ground Sampling Distance)
value in the horizontal and vertical directions, respectively. For parallel
SfM, the proposed solution is a more reliable alternative.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel adversarial learning strategy for medical image classification. (arXiv:2206.11501v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11501">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) techniques have been extensively utilized for medical
image classification. Most DL-based classification networks are generally
structured hierarchically and optimized through the minimization of a single
loss function measured at the end of the networks. However, such a single loss
design could potentially lead to optimization of one specific value of interest
but fail to leverage informative features from intermediate layers that might
benefit classification performance and reduce the risk of overfitting.
Recently, auxiliary convolutional neural networks (AuxCNNs) have been employed
on top of traditional classification networks to facilitate the training of
intermediate layers to improve classification performance and robustness. In
this study, we proposed an adversarial learning-based AuxCNN to support the
training of deep neural networks for medical image classification. Two main
innovations were adopted in our AuxCNN classification framework. First, the
proposed AuxCNN architecture includes an image generator and an image
discriminator for extracting more informative image features for medical image
classification, motivated by the concept of generative adversarial network
(GAN) and its impressive ability in approximating target data distribution.
Second, a hybrid loss function is designed to guide the model training by
incorporating different objectives of the classification network and AuxCNN to
reduce overfitting. Comprehensive experimental studies demonstrated the
superior classification performance of the proposed model. The effect of the
network-related factors on classification performance was investigated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Published Machine Learning Natural Language Processing Applications for Protocolling Radiology Imaging. (arXiv:2206.11502v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11502">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) is a subfield of Artificial intelligence (AI), and its
applications in radiology are growing at an ever-accelerating rate. The most
studied ML application is the automated interpretation of images. However,
natural language processing (NLP), which can be combined with ML for text
interpretation tasks, also has many potential applications in radiology. One
such application is automation of radiology protocolling, which involves
interpreting a clinical radiology referral and selecting the appropriate
imaging technique. It is an essential task which ensures that the correct
imaging is performed. However, the time that a radiologist must dedicate to
protocolling could otherwise be spent reporting, communicating with referrers,
or teaching. To date, there have been few publications in which ML models were
developed that use clinical text to automate protocol selection. This article
reviews the existing literature in this field. A systematic assessment of the
published models is performed with reference to best practices suggested by
machine learning convention. Progress towards implementing automated
protocolling in a clinical setting is discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICOS Protein Expression Segmentation: Can Transformer Networks Give Better Results?. (arXiv:2206.11520v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11520">
<div class="article-summary-box-inner">
<span><p>Biomarkers identify a patients response to treatment. With the recent
advances in artificial intelligence based on the Transformer networks, there is
only limited research has been done to measure the performance on challenging
histopathology images. In this paper, we investigate the efficacy of the
numerous state-of-the-art Transformer networks for immune-checkpoint biomarker,
Inducible Tcell COStimulator (ICOS) protein cell segmentation in colon cancer
from immunohistochemistry (IHC) slides. Extensive and comprehensive
experimental results confirm that MiSSFormer achieved the highest Dice score of
74.85% than the rest evaluated Transformer and Efficient U-Net methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neuromorphic Vision-Based Measurement for Robust Relative Localization in Future Space Exploration Missions. (arXiv:2206.11541v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11541">
<div class="article-summary-box-inner">
<span><p>Space exploration has witnessed revolutionary changes upon landing of the
Perseverance Rover on the Martian surface and demonstrating the first flight
beyond Earth by the Mars helicopter, Ingenuity. During their mission on Mars,
Perseverance Rover and Ingenuity collaboratively explore the Martian surface,
where Ingenuity scouts terrain information for rover's safe traversability.
Hence, determining the relative poses between both the platforms is of
paramount importance for the success of this mission. Driven by this necessity,
this work proposes a robust relative localization system based on a fusion of
neuromorphic vision-based measurements (NVBMs) and inertial measurements. The
emergence of neuromorphic vision triggered a paradigm shift in the computer
vision community, due to its unique working principle delineated with
asynchronous events triggered by variations of light intensities occurring in
the scene. This implies that observations cannot be acquired in static scenes
due to illumination invariance. To circumvent this limitation, high frequency
active landmarks are inserted in the scene to guarantee consistent event
firing. These landmarks are adopted as salient features to facilitate relative
localization. A novel event-based landmark identification algorithm using
Gaussian Mixture Models (GMM) is developed for matching the landmarks
correspondences formulating our NVBMs. The NVBMs are fused with inertial
measurements in proposed state estimators, landmark tracking Kalman filter
(LTKF) and translation decoupled Kalman filter (TDKF) for landmark tracking and
relative localization, respectively. The proposed system was tested in a
variety of experiments and has outperformed state-of-the-art approaches in
accuracy and range.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Towards the Largest Margins. (arXiv:2206.11589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11589">
<div class="article-summary-box-inner">
<span><p>One of the main challenges for feature representation in deep learning-based
classification is the design of appropriate loss functions that exhibit strong
discriminative power. The classical softmax loss does not explicitly encourage
discriminative learning of features. A popular direction of research is to
incorporate margins in well-established losses in order to enforce extra
intra-class compactness and inter-class separability, which, however, were
developed through heuristic means, as opposed to rigorous mathematical
principles. In this work, we attempt to address this limitation by formulating
the principled optimization objective as learning towards the largest margins.
Specifically, we firstly define the class margin as the measure of inter-class
separability, and the sample margin as the measure of intra-class compactness.
Accordingly, to encourage discriminative representation of features, the loss
function should promote the largest possible margins for both classes and
samples. Furthermore, we derive a generalized margin softmax loss to draw
general conclusions for the existing margin-based losses. Not only does this
principled framework offer new perspectives to understand and interpret
existing margin-based losses, but it also provides new insights that can guide
the design of new tools, including sample margin regularization and largest
margin softmax loss for the class-balanced case, and zero-centroid
regularization for the class-imbalanced case. Experimental results demonstrate
the effectiveness of our strategy on a variety of tasks, including visual
classification, imbalanced classification, person re-identification, and face
verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Learned Image Compression With Low Computational Cost. (arXiv:2206.11599v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11599">
<div class="article-summary-box-inner">
<span><p>Recently, learned image compression methods have developed rapidly and
exhibited excellent rate-distortion performance when compared to traditional
standards, such as JPEG, JPEG2000 and BPG. However, the learning-based methods
suffer from high computational costs, which is not beneficial for deployment on
devices with limited resources. To this end, we propose shift-addition parallel
modules (SAPMs), including SAPM-E for the encoder and SAPM-D for the decoder,
to largely reduce the energy consumption. To be specific, they can be taken as
plug-and-play components to upgrade existing CNN-based architectures, where the
shift branch is used to extract large-grained features as compared to
small-grained features learned by the addition branch. Furthermore, we
thoroughly analyze the probability distribution of latent representations and
propose to use Laplace Mixture Likelihoods for more accurate entropy
estimation. Experimental results demonstrate that the proposed methods can
achieve comparable or even better performance on both PSNR and MS-SSIM metrics
to that of the convolutional counterpart with an about 2x energy reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototype-Anchored Learning for Learning with Imperfect Annotations. (arXiv:2206.11602v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11602">
<div class="article-summary-box-inner">
<span><p>The success of deep neural networks greatly relies on the availability of
large amounts of high-quality annotated data, which however are difficult or
expensive to obtain. The resulting labels may be class imbalanced, noisy or
human biased. It is challenging to learn unbiased classification models from
imperfectly annotated datasets, on which we usually suffer from overfitting or
underfitting. In this work, we thoroughly investigate the popular softmax loss
and margin-based loss, and offer a feasible approach to tighten the
generalization error bound by maximizing the minimal sample margin. We further
derive the optimality condition for this purpose, which indicates how the class
prototypes should be anchored. Motivated by theoretical analysis, we propose a
simple yet effective method, namely prototype-anchored learning (PAL), which
can be easily incorporated into various learning-based classification schemes
to handle imperfect annotation. We verify the effectiveness of PAL on
class-imbalanced learning and noise-tolerant learning by extensive experiments
on synthetic and real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022). (arXiv:2206.11610v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11610">
<div class="article-summary-box-inner">
<span><p>This report presents the methods of the winning entry of the RxR-Habitat
Competition in CVPR 2022. The competition addresses the problem of
Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
requires an agent to follow step-by-step natural language instructions to reach
a target. We present a modular plan-and-control approach for the task. Our
model consists of three modules: the candidate waypoints predictor (CWP), the
history enhanced planner and the tryout controller. In each decision loop, CWP
first predicts a set of candidate waypoints based on depth observations from
multiple views. It can reduce the complexity of the action space and facilitate
planning. Then, a history-enhanced planner is adopted to select one of the
candidate waypoints as the subgoal. The planner additionally encodes historical
memory to track the navigation progress, which is especially effective for
long-horizon navigation. Finally, we propose a non-parametric heuristic
controller named tryout to execute low-level actions to reach the planned
subgoal. It is based on the trial-and-error mechanism which can help the agent
to avoid obstacles and escape from getting stuck. All three modules work
hierarchically until the agent stops. We further take several recent advances
of Vision-and-Language Navigation (VLN) to improve the performance such as
pretraining based on large-scale synthetic in-domain dataset, environment-level
data augmentation and snapshot model ensemble. Our model won the RxR-Habitat
Competition 2022, with 48% and 90% relative improvements over existing methods
on NDTW and SR metrics respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Waypoint Generation in Row-based Crops with Deep Learning and Contrastive Clustering. (arXiv:2206.11623v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11623">
<div class="article-summary-box-inner">
<span><p>The development of precision agriculture has gradually introduced automation
in the agricultural process to support and rationalize all the activities
related to field management. In particular, service robotics plays a
predominant role in this evolution by deploying autonomous agents able to
navigate in fields while executing different tasks without the need for human
intervention, such as monitoring, spraying and harvesting. In this context,
global path planning is the first necessary step for every robotic mission and
ensures that the navigation is performed efficiently and with complete field
coverage. In this paper, we propose a learning-based approach to tackle
waypoint generation for planning a navigation path for row-based crops,
starting from a top-view map of the region-of-interest. We present a novel
methodology for waypoint clustering based on a contrastive loss, able to
project the points to a separable latent space. The proposed deep neural
network can simultaneously predict the waypoint position and cluster assignment
with two specialized heads in a single forward pass. The extensive
experimentation on simulated and real-world images demonstrates that the
proposed approach effectively solves the waypoint generation problem for both
straight and curved row-based crops, overcoming the limitations of previous
state-of-the-art methodologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Sensing and Measurements Reuse for Image Compressed Sensing. (arXiv:2206.11629v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11629">
<div class="article-summary-box-inner">
<span><p>Recently, deep network-based image compressed sensing methods achieved high
reconstruction quality and reduced computational overhead compared with
traditional methods. However, existing methods obtain measurements only from
partial features in the network and use them only once for image
reconstruction. They ignore there are low, mid, and high-level features in the
network\cite{zeiler2014visualizing} and all of them are essential for
high-quality reconstruction. Moreover, using measurements only once may not be
enough for extracting richer information from measurements. To address these
issues, we propose a novel Measurements Reuse Convolutional Compressed Sensing
Network (MR-CCSNet) which employs Global Sensing Module (GSM) to collect all
level features for achieving an efficient sensing and Measurements Reuse Block
(MRB) to reuse measurements multiple times on multi-scale. Finally,
experimental results on three benchmark datasets show that our model can
significantly outperform state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning To Generate Scene Graph from Head to Tail. (arXiv:2206.11653v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11653">
<div class="article-summary-box-inner">
<span><p>Scene Graph Generation (SGG) represents objects and their interactions with a
graph structure. Recently, many works are devoted to solving the imbalanced
problem in SGG. However, underestimating the head predicates in the whole
training process, they wreck the features of head predicates that provide
general features for tail ones. Besides, assigning excessive attention to the
tail predicates leads to semantic deviation. Based on this, we propose a novel
SGG framework, learning to generate scene graphs from Head to Tail (SGG-HT),
containing Curriculum Re-weight Mechanism (CRM) and Semantic Context Module
(SCM). CRM learns head/easy samples firstly for robust features of head
predicates and then gradually focuses on tail/hard ones. SCM is proposed to
relieve semantic deviation by ensuring the semantic consistency between the
generated scene graph and the ground truth in global and local representations.
Experiments show that SGG-HT significantly alleviates the biased problem and
chieves state-of-the-art performances on Visual Genome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Warped Convolution Networks for Homography Estimation. (arXiv:2206.11657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11657">
<div class="article-summary-box-inner">
<span><p>Homography transformation has an essential relationship with special linear
group and the embedding Lie algebra structure. Although the Lie algebra
representation is elegant, few researchers have established the connection
between homography estimation and algebra expression. In this paper, we propose
Warped Convolution Networks (WCN) to effectively estimate the homography
transformation by SL(3) group and sl(3) algebra with group convolution. To this
end, six commutative subgroups within SL(3) group are composed to form a
homography transformation. For each subgroup, a warping function is proposed to
bridge the Lie algebra structure to its corresponding parameters in tomography.
By taking advantage of the warped convolution, homography estimation is
formulated into several simple pseudo-translation regressions. By walking along
the Lie topology, our proposed WCN is able to learn the features that are
invariant to homography transformation. It can be easily plugged into other
popular CNN-based methods. Extensive experiments on POT benchmark and
MNIST-Proj dataset show that our proposed method is effective for both
homography estimation and classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Short-range forecasts of global precipitation using using deep learning-augmented numerical weather prediction. (arXiv:2206.11669v1 [physics.ao-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11669">
<div class="article-summary-box-inner">
<span><p>Precipitation governs Earth's hydroclimate, and its daily spatiotemporal
fluctuations have major socioeconomic effects. Advances in Numerical weather
prediction (NWP) have been measured by the improvement of forecasts for various
physical fields such as temperature and pressure; however, large biases exist
in precipitation prediction. We augment the output of the well-known NWP model
CFSv2 with deep learning to create a hybrid model that improves short-range
global precipitation at 1-, 2-, and 3-day lead times. To hybridise, we address
the sphericity of the global data by using modified DLWP-CS architecture which
transforms all the fields to cubed-sphere projection. Dynamical model
precipitation and surface temperature outputs are fed into a modified DLWP-CS
(UNET) to forecast ground truth precipitation. While CFSv2's average bias is +5
to +7 mm/day over land, the multivariate deep learning model decreases it to
within -1 to +1 mm/day. Hurricane Katrina in 2005, Hurricane Ivan in 2004,
China floods in 2010, India floods in 2005, and Myanmar storm Nargis in 2008
are used to confirm the substantial enhancement in the skill for the hybrid
dynamical-deep learning model. CFSv2 typically shows a moderate to large bias
in the spatial pattern and overestimates the precipitation at short-range time
scales. The proposed deep learning augmented NWP model can address these biases
and vastly improve the spatial pattern and magnitude of predicted
precipitation. Deep learning enhanced CFSv2 reduces mean bias by 8x over
important land regions for 1 day lead compared to CFSv2. The spatio-temporal
deep learning system opens pathways to further the precision and accuracy in
global short-range precipitation forecasts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation. (arXiv:2206.11678v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11678">
<div class="article-summary-box-inner">
<span><p>We present BlazePose GHUM Holistic, a lightweight neural network pipeline for
3D human body landmarks and pose estimation, specifically tailored to real-time
on-device inference. BlazePose GHUM Holistic enables motion capture from a
single RGB image including avatar control, fitness tracking and AR/VR effects.
Our main contributions include i) a novel method for 3D ground truth data
acquisition, ii) updated 3D body tracking with additional hand landmarks and
iii) full body pose estimation from a monocular image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NTIRE 2022 Challenge on Perceptual Image Quality Assessment. (arXiv:2206.11695v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11695">
<div class="article-summary-box-inner">
<span><p>This paper reports on the NTIRE 2022 challenge on perceptual image quality
assessment (IQA), held in conjunction with the New Trends in Image Restoration
and Enhancement workshop (NTIRE) workshop at CVPR 2022. This challenge is held
to address the emerging challenge of IQA by perceptual image processing
algorithms. The output images of these algorithms have completely different
characteristics from traditional distortions and are included in the PIPAL
dataset used in this challenge. This challenge is divided into two tracks, a
full-reference IQA track similar to the previous NTIRE IQA challenge and a new
track that focuses on the no-reference IQA methods. The challenge has 192 and
179 registered participants for two tracks. In the final testing stage, 7 and 8
participating teams submitted their models and fact sheets. Almost all of them
have achieved better results than existing IQA methods, and the winning method
can demonstrate state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11723">
<div class="article-summary-box-inner">
<span><p>Deep convolutional autoencoders provide an effective tool for learning
non-linear dimensionality reduction in an unsupervised way. Recently, they have
been used for the task of anomaly detection in the visual domain. By optimising
for the reconstruction error using anomaly-free examples, the common belief is
that a trained network will have difficulties to reconstruct anomalous parts
during the test phase. This is usually done by controlling the capacity of the
network by either reducing the size of the bottleneck layer or enforcing
sparsity constraints on its activations. However, neither of these techniques
does explicitly penalise reconstruction of anomalous signals often resulting in
a poor detection. We tackle this problem by adapting a self-supervised learning
regime which allows to use discriminative information during training while
regularising the model to focus on the data manifold by means of a modified
reconstruction error resulting in an accurate detection. Unlike related
approaches, the inference of the proposed method during training and prediction
is very efficient processing the whole input image in one single step. Our
experiments on the MVTec Anomaly Detection dataset demonstrate high recognition
and localisation performance of the proposed method. On the texture-subset, in
particular, our approach consistently outperforms a bunch of recent anomaly
detection methods by a big margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds. (arXiv:2206.11736v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11736">
<div class="article-summary-box-inner">
<span><p>In order for artificial agents to perform useful tasks in changing
environments, they must be able to both detect and adapt to novelty. However,
visual novelty detection research often only evaluates on repurposed datasets
such as CIFAR-10 originally intended for object classification. This practice
restricts novelties to well-framed images of distinct object types. We suggest
that new benchmarks are needed to represent the challenges of navigating an
open world. Our new NovelCraft dataset contains multi-modal episodic data of
the images and symbolic world-states seen by an agent completing a pogo-stick
assembly task within a video game world. In some episodes, we insert novel
objects that can impact gameplay. Novelty can vary in size, position, and
occlusion within complex scenes. We benchmark state-of-the-art novelty
detection and generalized category discovery models with a focus on
comprehensive evaluation. Results suggest an opportunity for future research:
models aware of task-specific costs of different types of mistakes could more
effectively detect and adapt to novelty in open worlds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evidence fusion with contextual discounting for multi-modality medical image segmentation. (arXiv:2206.11739v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11739">
<div class="article-summary-box-inner">
<span><p>As information sources are usually imperfect, it is necessary to take into
account their reliability in multi-source information fusion tasks. In this
paper, we propose a new deep framework allowing us to merge multi-MR image
segmentation results using the formalism of Dempster-Shafer theory while taking
into account the reliability of different modalities relative to different
classes. The framework is composed of an encoder-decoder feature extraction
module, an evidential segmentation module that computes a belief function at
each voxel for each modality, and a multi-modality evidence fusion module,
which assigns a vector of discount rates to each modality evidence and combines
the discounted evidence using Dempster's rule. The whole framework is trained
by minimizing a new loss function based on a discounted Dice index to increase
segmentation accuracy and reliability. The method was evaluated on the BraTs
2021 database of 1251 patients with brain tumors. Quantitative and qualitative
results show that our method outperforms the state of the art, and implements
an effective new idea for merging multi-information within deep neural
networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptPose: Language Prompt Helps Animal Pose Estimation. (arXiv:2206.11752v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11752">
<div class="article-summary-box-inner">
<span><p>Recently, animal pose estimation is attracting increasing interest from the
academia (e.g., wildlife and conservation biology) focusing on animal behavior
understanding. However, currently animal pose estimation suffers from small
datasets and large data variances, making it difficult to obtain robust
performance. To tackle this problem, we propose that the rich knowledge about
relations between pose-related semantics learned by language models can be
utilized to improve the animal pose estimation. Therefore, in this study, we
introduce a novel PromptPose framework to effectively apply language models for
better understanding the animal poses based on prompt training. In PromptPose,
we propose that adapting the language knowledge to the visual animal poses is
key to achieve effective animal pose estimation. To this end, we first
introduce textual prompts to build connections between textual semantic
descriptions and supporting animal keypoint features. Moreover, we further
devise a pixel-level contrastive loss to build dense connections between
textual descriptions and local image features, as well as a semantic-level
contrastive loss to bridge the gap between global contrasts in language-image
cross-modal pre-training and local contrasts in dense prediction. In practice,
the PromptPose has shown great benefits for improving animal pose estimation.
By conducting extensive experiments, we show that our PromptPose achieves
superior performance under both supervised and few-shot settings, outperforming
representative methods by a large margin. The source code and models will be
made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What makes you, you? Analyzing Recognition by Swapping Face Parts. (arXiv:2206.11759v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11759">
<div class="article-summary-box-inner">
<span><p>Deep learning advanced face recognition to an unprecedented accuracy.
However, understanding how local parts of the face affect the overall
recognition performance is still mostly unclear. Among others, face swap has
been experimented to this end, but just for the entire face. In this paper, we
propose to swap facial parts as a way to disentangle the recognition relevance
of different face parts, like eyes, nose and mouth. In our method, swapping
parts from a source face to a target one is performed by fitting a 3D prior,
which establishes dense pixels correspondence between parts, while also
handling pose differences. Seamless cloning is then used to obtain smooth
transitions between the mapped source regions and the shape and skin tone of
the target face. We devised an experimental protocol that allowed us to draw
some preliminary conclusions when the swapped images are classified by deep
networks, indicating a prominence of the eyes and eyebrows region. Code
available at https://github.com/clferrari/FacePartsSwap
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FitGAN: Fit- and Shape-Realistic Generative Adversarial Networks for Fashion. (arXiv:2206.11768v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11768">
<div class="article-summary-box-inner">
<span><p>Amidst the rapid growth of fashion e-commerce, remote fitting of fashion
articles remains a complex and challenging problem and a main driver of
customers' frustration. Despite the recent advances in 3D virtual try-on
solutions, such approaches still remain limited to a very narrow - if not only
a handful - selection of articles, and often for only one size of those fashion
items. Other state-of-the-art approaches that aim to support customers find
what fits them online mostly require a high level of customer engagement and
privacy-sensitive data (such as height, weight, age, gender, belly shape,
etc.), or alternatively need images of customers' bodies in tight clothing.
They also often lack the ability to produce fit and shape aware visual guidance
at scale, coming up short by simply advising which size to order that would
best match a customer's physical body attributes, without providing any
information on how the garment may fit and look. Contributing towards taking a
leap forward and surpassing the limitations of current approaches, we present
FitGAN, a generative adversarial model that explicitly accounts for garments'
entangled size and fit characteristics of online fashion at scale. Conditioned
on the fit and shape of the articles, our model learns disentangled item
representations and generates realistic images reflecting the true fit and
shape properties of fashion articles. Through experiments on real world data at
scale, we demonstrate how our approach is capable of synthesizing visually
realistic and diverse fits of fashion items and explore its ability to control
fit and shape of images for thousands of online garments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need. (arXiv:2206.11804v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11804">
<div class="article-summary-box-inner">
<span><p>Data diversity and volume are crucial to the success of training deep
learning models, while in the medical imaging field, the difficulty and cost of
data collection and annotation are especially huge. Specifically in robotic
surgery, data scarcity and imbalance have heavily affected the model accuracy
and limited the design and deployment of deep learning-based surgical
applications such as surgical instrument segmentation. Considering this, in
this paper, we rethink the surgical instrument segmentation task and propose a
one-to-many data generation solution that gets rid of the complicated and
expensive process of data collection and annotation from robotic surgery. In
our method, we only utilize a single surgical background tissue image and a few
open-source instrument images as the seed images and apply multiple
augmentations and blending techniques to synthesize amounts of image
variations. In addition, we also introduce the chained augmentation mixing
during training to further enhance the data diversities. The proposed approach
is evaluated on the real datasets of the EndoVis-2018 and EndoVis-2017 surgical
scene segmentation. Our empirical analysis suggests that without the high cost
of data collection and annotation, we can achieve decent surgical instrument
segmentation performance. Moreover, we also observe that our method can deal
with novel instrument prediction in the deployment domain. We hope our
inspiring results would encourage researchers to emphasize data-centric methods
to overcome demanding deep learning limitations besides data shortage, such as
class imbalance, domain adaptation, and incremental learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unseen Object 6D Pose Estimation: A Benchmark and Baselines. (arXiv:2206.11808v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11808">
<div class="article-summary-box-inner">
<span><p>Estimating the 6D pose for unseen objects is in great demand for many
real-world applications. However, current state-of-the-art pose estimation
methods can only handle objects that are previously trained. In this paper, we
propose a new task that enables and facilitates algorithms to estimate the 6D
pose estimation of novel objects during testing. We collect a dataset with both
real and synthetic images and up to 48 unseen objects in the test set. In the
mean while, we propose a new metric named Infimum ADD (IADD) which is an
invariant measurement for objects with different types of pose ambiguity. A
two-stage baseline solution for this task is also provided. By training an
end-to-end 3D correspondences network, our method finds corresponding points
between an unseen object and a partial view RGBD image accurately and
efficiently. It then calculates the 6D pose from the correspondences using an
algorithm robust to object symmetry. Extensive experiments show that our method
outperforms several intuitive baselines and thus verify its effectiveness. All
the data, code and models will be made publicly available. Project page:
www.graspnet.net/unseen6d
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLOSA: Object detection based on 2D local feature superimposed self-attention. (arXiv:2206.11825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11825">
<div class="article-summary-box-inner">
<span><p>We analyzed the network structure of real-time object detection models and
found that the features in the feature concatenation stage are very rich.
Applying an attention module here can effectively improve the detection
accuracy of the model. However, the commonly used attention module or
self-attention module shows poor performance in detection accuracy and
inference efficiency. Therefore, we propose a novel self-attention module,
called 2D local feature superimposed self-attention, for the feature
concatenation stage of the neck network. This self-attention module reflects
global features through local features and local receptive fields. We also
propose and optimize an efficient decoupled head and AB-OTA, and achieve SOTA
results. Average precisions of 49.0\% (66.2 FPS), 46.1\% (80.6 FPS), and 39.1\%
(100 FPS) were obtained for large, medium, and small-scale models built using
our proposed improvements. Our models exceeded YOLOv5 by 0.8\% -- 3.1\% in
average precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Clinically Assisted Colorectal Polyp Recognition via Structured Cross-modal Representation Consistency. (arXiv:2206.11826v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11826">
<div class="article-summary-box-inner">
<span><p>The colorectal polyps classification is a critical clinical examination. To
improve the classification accuracy, most computer-aided diagnosis algorithms
recognize colorectal polyps by adopting Narrow-Band Imaging (NBI). However, the
NBI usually suffers from missing utilization in real clinic scenarios since the
acquisition of this specific image requires manual switching of the light mode
when polyps have been detected by using White-Light (WL) images. To avoid the
above situation, we propose a novel method to directly achieve accurate
white-light colonoscopy image classification by conducting structured
cross-modal representation consistency. In practice, a pair of multi-modal
images, i.e. NBI and WL, are fed into a shared Transformer to extract
hierarchical feature representations. Then a novel designed Spatial Attention
Module (SAM) is adopted to calculate the similarities between the class token
and patch tokens %from multi-levels for a specific modality image. By aligning
the class tokens and spatial attention maps of paired NBI and WL images at
different levels, the Transformer achieves the ability to keep both global and
local representation consistency for the above two modalities. Extensive
experimental results illustrate the proposed method outperforms the recent
studies with a margin, realizing multi-modal prediction with a single
Transformer while greatly improving the classification accuracy when only with
WL images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sample Condensation in Online Continual Learning. (arXiv:2206.11849v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11849">
<div class="article-summary-box-inner">
<span><p>Online Continual learning is a challenging learning scenario where the model
must learn from a non-stationary stream of data where each sample is seen only
once. The main challenge is to incrementally learn while avoiding catastrophic
forgetting, namely the problem of forgetting previously acquired knowledge
while learning from new data. A popular solution in these scenario is to use a
small memory to retain old data and rehearse them over time. Unfortunately, due
to the limited memory size, the quality of the memory will deteriorate over
time. In this paper we propose OLCGM, a novel replay-based continual learning
strategy that uses knowledge condensation techniques to continuously compress
the memory and achieve a better use of its limited size. The sample
condensation step compresses old samples, instead of removing them like other
replay strategies. As a result, the experiments show that, whenever the memory
budget is limited compared to the complexity of the data, OLCGM improves the
final accuracy compared to state-of-the-art replay strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Remote Sensing Change Detection (Segmentation) using Denoising Diffusion Probabilistic Models. (arXiv:2206.11892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11892">
<div class="article-summary-box-inner">
<span><p>Human civilization has an increasingly powerful influence on the earth
system, and earth observations are an invaluable tool for assessing and
mitigating the negative impacts. To this end, observing precisely defined
changes on Earth's surface is essential, and we propose an effective way to
achieve this goal. Notably, our change detection (CD)/ segmentation method
proposes a novel way to incorporate the millions of off-the-shelf, unlabeled,
remote sensing images available through different earth observation programs
into the training process through denoising diffusion probabilistic models. We
first leverage the information from these off-the-shelf, uncurated, and
unlabeled remote sensing images by using a pre-trained denoising diffusion
probabilistic model and then employ the multi-scale feature representations
from the diffusion model decoder to train a lightweight CD classifier to detect
precise changes. The experiments performed on four publically available CD
datasets show that the proposed approach achieves remarkably better results
than the state-of-the-art methods in F1, IoU, and overall accuracy. Code and
pre-trained models are available at: https://github.com/wgcban/ddpm-cd
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaskViT: Masked Visual Pre-Training for Video Prediction. (arXiv:2206.11894v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11894">
<div class="article-summary-box-inner">
<span><p>The ability to predict future visual observations conditioned on past
observations and motor commands can enable embodied agents to plan solutions to
a variety of tasks in complex environments. This work shows that we can create
good video prediction models by pre-training transformers via masked visual
modeling. Our approach, named MaskViT, is based on two simple design decisions.
First, for memory and training efficiency, we use two types of window
attention: spatial and spatiotemporal. Second, during training, we mask a
variable percentage of tokens instead of a fixed mask ratio. For inference,
MaskViT generates all tokens via iterative refinement where we incrementally
decrease the masking ratio following a mask scheduling function. On several
datasets we demonstrate that MaskViT outperforms prior works in video
prediction, is parameter efficient, and can generate high-resolution videos
(256x256). Further, we demonstrate the benefits of inference speedup (up to
512x) due to iterative decoding by using MaskViT for planning on a real robot.
Our work suggests that we can endow embodied agents with powerful predictive
models by leveraging the general framework of masked visual modeling with
minimal domain knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space. (arXiv:2206.11895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11895">
<div class="article-summary-box-inner">
<span><p>Humans are remarkably flexible in understanding viewpoint changes due to
visual cortex supporting the perception of 3D structure. In contrast, most of
the computer vision models that learn visual representation from a pool of 2D
images often fail to generalize over novel camera viewpoints. Recently, the
vision architectures have shifted towards convolution-free architectures,
visual Transformers, which operate on tokens derived from image patches.
However, neither these Transformers nor 2D convolutional networks perform
explicit operations to learn viewpoint-agnostic representation for visual
understanding. To this end, we propose a 3D Token Representation Layer (3DTRL)
that estimates the 3D positional information of the visual tokens and leverages
it for learning viewpoint-agnostic representations. The key elements of 3DTRL
include a pseudo-depth estimator and a learned camera matrix to impose
geometric transformations on the tokens. These enable 3DTRL to recover the 3D
positional information of the tokens from 2D patches. In practice, 3DTRL is
easily plugged-in into a Transformer. Our experiments demonstrate the
effectiveness of 3DTRL in many vision tasks including image classification,
multi-view video alignment, and action recognition. The models with 3DTRL
outperform their backbone Transformers in all the tasks with minimal added
computation. Our project page is at
https://www3.cs.stonybrook.edu/~jishang/3dtrl/3dtrl.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventNeRF: Neural Radiance Fields from a Single Colour Event Camera. (arXiv:2206.11896v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11896">
<div class="article-summary-box-inner">
<span><p>Learning coordinate-based volumetric 3D scene representations such as neural
radiance fields (NeRF) has been so far studied assuming RGB or RGB-D images as
inputs. At the same time, it is known from the neuroscience literature that
human visual system (HVS) is tailored to process asynchronous brightness
changes rather than synchronous RGB images, in order to build and continuously
update mental 3D representations of the surroundings for navigation and
survival. Visual sensors that were inspired by HVS principles are event
cameras. Thus, events are sparse and asynchronous per-pixel brightness (or
colour channel) change signals. In contrast to existing works on neural 3D
scene representation learning, this paper approaches the problem from a new
perspective. We demonstrate that it is possible to learn NeRF suitable for
novel-view synthesis in the RGB space from asynchronous event streams. Our
models achieve high visual accuracy of the rendered novel views of challenging
scenes in the RGB space, even though they are trained with substantially fewer
data (i.e., event streams from a single event camera moving around the object)
and more efficiently (due to the inherent sparsity of event streams) than the
existing NeRF models trained with RGB images. We will release our datasets and
the source code, see https://4dqv.mpi-inf.mpg.de/EventNeRF/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters. (arXiv:2003.12739v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.12739">
<div class="article-summary-box-inner">
<span><p>How to best integrate linguistic and perceptual processing in multi-modal
tasks that involve language and vision is an important open problem. In this
work, we argue that the common practice of using language in a top-down manner,
to direct visual attention over high-level visual features, may not be optimal.
We hypothesize that the use of language to also condition the bottom-up
processing from pixels to high-level features can provide benefits to the
overall performance. To support our claim, we propose a U-Net-based model and
perform experiments on two language-vision dense-prediction tasks: referring
expression segmentation and language-guided image colorization. We compare
results where either one or both of the top-down and bottom-up visual branches
are conditioned on language. Our experiments reveal that using language to
control the filters for bottom-up visual processing in addition to top-down
attention leads to better results on both tasks and achieves competitive
performance. Our linguistic analysis suggests that bottom-up conditioning
improves segmentation of objects especially when input text refers to low-level
visual concepts. Code is available at https://github.com/ilkerkesen/bvpr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEAN: graph-based pruning for convolutional neural networks by extracting longest chains. (arXiv:2011.06923v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06923">
<div class="article-summary-box-inner">
<span><p>Neural network pruning techniques can substantially reduce the computational
cost of applying convolutional neural networks (CNNs). Common pruning methods
determine which convolutional filters to remove by ranking the filters
individually, i.e., without taking into account their interdependence. In this
paper, we advocate the viewpoint that pruning should consider the
interdependence between series of consecutive operators. We propose the
LongEst-chAiN (LEAN) method that prunes CNNs by using graph-based algorithms to
select relevant chains of convolutions. A CNN is interpreted as a graph, with
the operator norm of each operator as distance metric for the edges. LEAN
pruning iteratively extracts the highest value path from the graph to keep. In
our experiments, we test LEAN pruning on several image-to-image tasks,
including the well-known CamVid dataset, and a real-world X-ray CT dataset.
Results indicate that LEAN pruning can result in networks with similar
accuracy, while using 1.7-12x fewer convolutional filters than existing
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end One-shot Human Parsing. (arXiv:2105.01241v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01241">
<div class="article-summary-box-inner">
<span><p>Previous human parsing models are limited to parsing humans into pre-defined
classes, which is inflexible for practical fashion applications that often have
new fashion item classes. In this paper, we define a novel one-shot human
parsing (OSHP) task that requires parsing humans into an open set of classes
defined by any test example. During training, only base classes are exposed,
which only overlap with part of the test-time classes. To address three main
challenges in OSHP, i.e., small sizes, testing bias, and similar parts, we
devise an End-to-end One-shot human Parsing Network (EOP-Net). Firstly, an
end-to-end human parsing framework is proposed to parse the query image into
both coarse-grained and fine-grained human classes, which builds a strong
embedding network with rich semantic information shared across different
granularities, facilitating identifying small-sized human classes. Then, we
propose learning momentum-updated prototypes by gradually smoothing the
training time static prototypes, which helps stabilize the training and learn
robust features. Moreover, we devise a dual metric learning scheme which
encourages the network to enhance features' both representational capability
and transferability. Therefore, our EOP-Net can learn representative features
that can quickly adapt to the novel classes and mitigate the testing bias
issue. In addition, we employ a contrastive loss at the prototype level,
thereby enforcing the distances among the classes in the fine-grained metric
space to discriminate similar parts. We tailor three existing popular human
parsing benchmarks to the OSHP task. Experiments on the new benchmarks
demonstrate that EOP-Net outperforms representative one-shot segmentation
models by large margins, which serves as a strong baseline for further research
on this new task. The source code is available at
https://github.com/Charleshhy/One-shot-Human-Parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10270">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have been shown to attain highly competitive
performance for a wide range of vision applications, such as image
classification, object detection and semantic image segmentation. In comparison
to convolutional neural networks, the Vision Transformer's weaker inductive
bias is generally found to cause an increased reliance on model regularization
or data augmentation ("AugReg" for short) when training on smaller training
datasets. We conduct a systematic empirical study in order to better understand
the interplay between the amount of training data, AugReg, model size and
compute budget. As one result of this study we find that the combination of
increased compute and AugReg can yield models with the same performance as
models trained on an order of magnitude more training data: we train ViT models
of various sizes on the public ImageNet-21k dataset which either match or
outperform their counterparts trained on the larger, but not publicly available
JFT-300M dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-Based Generative Cooperative Saliency Prediction. (arXiv:2106.13389v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13389">
<div class="article-summary-box-inner">
<span><p>Conventional saliency prediction models typically learn a deterministic
mapping from an image to its saliency map, and thus fail to explain the
subjective nature of human attention. In this paper, to model the uncertainty
of visual saliency, we study the saliency prediction problem from the
perspective of generative models by learning a conditional probability
distribution over the saliency map given an input image, and treating the
saliency prediction as a sampling process from the learned distribution.
Specifically, we propose a generative cooperative saliency prediction
framework, where a conditional latent variable model (LVM) and a conditional
energy-based model (EBM) are jointly trained to predict salient objects in a
cooperative manner. The LVM serves as a fast but coarse predictor to
efficiently produce an initial saliency map, which is then refined by the
iterative Langevin revision of the EBM that serves as a slow but fine
predictor. Such a coarse-to-fine cooperative saliency prediction strategy
offers the best of both worlds. Moreover, we propose a "cooperative learning
while recovering" strategy and apply it to weakly supervised saliency
prediction, where saliency annotations of training images are partially
observed. Lastly, we find that the learned energy function in the EBM can serve
as a refinement module that can refine the results of other pre-trained
saliency prediction models. Experimental results show that our model can
produce a set of diverse and plausible saliency maps of an image, and obtain
state-of-the-art performance in both fully supervised and weakly supervised
saliency prediction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Bias in Visual Datasets. (arXiv:2107.07919v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07919">
<div class="article-summary-box-inner">
<span><p>Computer Vision (CV) has achieved remarkable results, outperforming humans in
several tasks. Nonetheless, it may result in significant discrimination if not
handled properly as CV systems highly depend on the data they are fed with and
can learn and amplify biases within such data. Thus, the problems of
understanding and discovering biases are of utmost importance. Yet, there is no
comprehensive survey on bias in visual datasets. Hence, this work aims to: i)
describe the biases that might manifest in visual datasets; ii) review the
literature on methods for bias discovery and quantification in visual datasets;
iii) discuss existing attempts to collect bias-aware visual datasets. A key
conclusion of our study is that the problem of bias discovery and
quantification in visual datasets is still open, and there is room for
improvement in terms of both methods and the range of biases that can be
addressed. Moreover, there is no such thing as a bias-free dataset, so
scientists and practitioners must become aware of the biases in their datasets
and make them explicit. To this end, we propose a checklist to spot different
types of bias during visual dataset collection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Stacking Ensemble Approach for Supervised Video Summarization. (arXiv:2109.12581v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12581">
<div class="article-summary-box-inner">
<span><p>Video summarization methods are usually classified into shot-level or
frame-level methods, which are individually used in a general way. This paper
investigates the underlying complementarity between the frame-level and
shot-level methods, and a stacking ensemble approach is proposed for supervised
video summarization. Firstly, we build up a stacking model to predict both the
key frame probabilities and the temporal interest segments simultaneously. The
two components are then combined via soft decision fusion to obtain the final
scores of each frame in the video. A joint loss function is proposed here to
train the model. The ablation experimental results show that the proposed
method outperforms both the two corresponding individual method. Furthermore,
extensive experiments and analysis on two benchmark datasets demonstrate the
effectiveness of our method and its superior performance in comparison with the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks. (arXiv:2111.05955v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05955">
<div class="article-summary-box-inner">
<span><p>Spiking neural networks (SNNs) have become an interesting alternative to
conventional artificial neural networks (ANN) thanks to their temporal
processing capabilities and energy efficient implementations in neuromorphic
hardware. However the challenges involved in training SNNs have limited their
performance in terms of accuracy and thus their applications. Improving
learning algorithms and neural architectures for a more accurate feature
extraction is therefore one of the current priorities in SNN research. In this
paper we present a study on the key components of modern spiking architectures.
We design a spiking version of the successful residual network architecture and
provide an in-depth study on the possible implementations of spiking residual
connections. This study shows how, depending on the use case, the optimal
residual connection implementation may vary. Additionally, we empirically
compare different techniques in image classification datasets taken from the
best performing networks. Our results provide a state of the art guide to SNN
design, which allows to make informed choices when trying to build the optimal
visual feature extractor. Finally, our network outperforms previous SNN
architectures in CIFAR-10 (94.14%) and CIFAR-100 (74.65%) datasets and matches
the state of the art in DVS-CIFAR10 (72.98%), with less parameters than the
previous state of the art and without the need for ANN-SNN conversion. Code
available at https://github.com/VicenteAlex/Spiking_ResNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-Based Single Image Deep Dehazing. (arXiv:2111.10943v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10943">
<div class="article-summary-box-inner">
<span><p>Model-based single image dehazing algorithms restore images with sharp edges
and rich details at the expense of low PSNR values. Data-driven ones restore
images with high PSNR values but with low contrast, and even some remaining
haze. In this paper, a novel single image dehazing algorithm is introduced by
fusing model-based and data-driven approaches. Both transmission map and
atmospheric light are initialized by the model-based methods, and refined by
deep learning approaches which form a neural augmentation. Haze-free images are
restored by using the transmission map and atmospheric light. Experimental
results indicate that the proposed algorithm can remove haze well from
real-world and synthetic hazy images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-horizon Robot Manipulation Tasks. (arXiv:2112.03227v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03227">
<div class="article-summary-box-inner">
<span><p>General-purpose robots coexisting with humans in their environment must learn
to relate human language to their perceptions and actions to be useful in a
range of daily tasks. Moreover, they need to acquire a diverse repertoire of
general-purpose skills that allow composing long-horizon tasks by following
unconstrained language instructions. In this paper, we present CALVIN
(Composing Actions from Language and Vision), an open-source simulated
benchmark to learn long-horizon language-conditioned tasks. Our aim is to make
it possible to develop agents that can solve many robotic manipulation tasks
over a long horizon, from onboard sensors, and specified only via human
language. CALVIN tasks are more complex in terms of sequence length, action
space, and language than existing vision-and-language task datasets and
supports flexible specification of sensor suites. We evaluate the agents in
zero-shot to novel language instructions and to novel environments and objects.
We show that a baseline model based on multi-context imitation learning
performs poorly on CALVIN, suggesting that there is significant room for
developing innovative agents that learn to relate human language to their world
models with this benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PriFit: Learning to Fit Primitives Improves Few Shot Point Cloud Segmentation. (arXiv:2112.13942v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13942">
<div class="article-summary-box-inner">
<span><p>We present PriFit, a semi-supervised approach for label-efficient learning of
3D point cloud segmentation networks. PriFit combines geometric primitive
fitting with point-based representation learning. Its key idea is to learn
point representations whose clustering reveals shape regions that can be
approximated well by basic geometric primitives, such as cuboids and
ellipsoids. The learned point representations can then be re-used in existing
network architectures for 3D point cloud segmentation, and improves their
performance in the few-shot setting. According to our experiments on the widely
used ShapeNet and PartNet benchmarks, PriFit outperforms several
state-of-the-art methods in this setting, suggesting that decomposability into
primitives is a useful prior for learning representations predictive of
semantic parts. We present a number of ablative experiments varying the choice
of geometric primitives and downstream tasks to demonstrate the effectiveness
of the method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Concatenation Volume for Accurate and Efficient Stereo Matching. (arXiv:2203.02146v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02146">
<div class="article-summary-box-inner">
<span><p>Stereo matching is a fundamental building block for many vision and robotics
applications. An informative and concise cost volume representation is vital
for stereo matching of high accuracy and efficiency. In this paper, we present
a novel cost volume construction method which generates attention weights from
correlation clues to suppress redundant information and enhance
matching-related information in the concatenation volume. To generate reliable
attention weights, we propose multi-level adaptive patch matching to improve
the distinctiveness of the matching cost at different disparities even for
textureless regions. The proposed cost volume is named attention concatenation
volume (ACV) which can be seamlessly embedded into most stereo matching
networks, the resulting networks can use a more lightweight aggregation network
and meanwhile achieve higher accuracy, e.g. using only 1/25 parameters of the
aggregation network can achieve higher accuracy for GwcNet. Furthermore, we
design a highly accurate network (ACVNet) based on our ACV, which achieves
state-of-the-art performance on several benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Remote blood pressure measurement via spatiotemporal mapping of a short-time facial video. (arXiv:2203.03634v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03634">
<div class="article-summary-box-inner">
<span><p>Blood pressure (BP) monitoring is vital in daily healthcare, especially for
cardiovascular diseases. However, BP values are mainly acquired through the
contact sensing method, which is inconvenient and unfriendly to continuous BP
measurement. Hence, we propose an efficient end-to-end network to estimate the
BP values from a facial video to achieve remote BP measurement in daily life.
In this study, we first derived a Spatial-temporal map of a short-time (~15s)
facial video. According to the Spatial-temporal map, we then regressed the BP
ranges by a designed blood pressure classifier and simultaneously calculated
the specific value by a blood pressure calculator in each BP range. In
addition, we also developed an innovative oversampling training strategy to
handle the unbalanced data distribution problem. Finally, we trained the
proposed network on a private dataset ASPD and tested it on the popular dataset
MMSE-HR. As a result, the proposed network achieved a state-of-the-art MAE of
12.35 mmHg and 9.5 mmHg on systolic and diastolic BP measurements, which is
better than the recent works. It concludes that the proposed method has
excellent potential for camera-based BP monitoring in real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VGQ-CNN: Moving Beyond Fixed Cameras and Top-Grasps for Grasp Quality Prediction. (arXiv:2203.04874v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04874">
<div class="article-summary-box-inner">
<span><p>We present the Versatile Grasp Quality Convolutional Neural Network
(VGQ-CNN), a grasp quality prediction network for 6-DOF grasps. VGQ-CNN can be
used when evaluating grasps for objects seen from a wide range of camera poses
or mobile robots without the need to retrain the network. By defining the grasp
orientation explicitly as an input to the network, VGQ-CNN can evaluate 6-DOF
grasp poses, moving beyond the 4-DOF grasps used in most image-based grasp
evaluation methods like GQ-CNN. To train VGQ-CNN, we generate the new Versatile
Grasp dataset (VG-dset) containing 6-DOF grasps observed from a wide range of
camera poses. VGQ-CNN achieves a balanced accuracy of 82.1% on our test-split
while generalising to a variety of camera poses. Meanwhile, it achieves
competitive performance for overhead cameras and top-grasps with a balanced
accuracy of 74.2% compared to GQ-CNN's 76.6%. We also propose a modified
network architecture, FAST-VGQ-CNN, that speeds up inference using a shared
encoder architecture and can make 128 grasp quality predictions in 12ms on a
CPU. Code and data are available at https://aucoroboticsmu.github.io/vgq-cnn/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09550">
<div class="article-summary-box-inner">
<span><p>Few-shot semantic segmentation aims at recognizing the object regions of
unseen categories with only a few annotated examples as supervision. The key to
few-shot segmentation is to establish a robust semantic relationship between
the support and query images and to prevent overfitting. In this paper, we
propose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle
the few-shot semantic segmentation problem. In MSHNet, we propose a new
Generative Prototype Similarity (GPS), which together with cosine similarity
can establish a strong semantic relation between the support and query images.
The locally generated prototype similarity based on global feature is logically
complementary to the global cosine similarity based on local feature, and the
relationship between the query image and the supported image can be expressed
more comprehensively by using the two similarities simultaneously. In addition,
we propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge
multi-layer, multi-shot and multi-similarity hyperrelational features. MSHNet
is built on the basis of similarity rather than specific category features,
which can achieve more general unity and effectively reduce overfitting. On two
benchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet
achieves new state-of-the-art performances on 1-shot and 5-shot semantic
segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Real Time Satellite Pose Estimation on Low Power Edge TPU. (arXiv:2204.03296v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03296">
<div class="article-summary-box-inner">
<span><p>Pose estimation of an uncooperative space resident object is a key asset
towards autonomy in close proximity operations. In this context monocular
cameras are a valuable solution because of their low system requirements.
However, the associated image processing algorithms are either too
computationally expensive for real time on-board implementation, or not enough
accurate. In this paper we propose a pose estimation software exploiting neural
network architectures which can be scaled to different accuracy-latency
trade-offs. We designed our pipeline to be compatible with Edge Tensor
Processing Units to show how low power machine learning accelerators could
enable Artificial Intelligence exploitation in space. The neural networks were
tested both on the benchmark Spacecraft Pose Estimation Dataset, and on the
purposely developed Cosmo Photorealistic Dataset, which depicts a COSMO-SkyMed
satellite in a variety of random poses and steerable solar panels orientations.
The lightest version of our architecture achieves state-of-the-art accuracy on
both datasets but at a fraction of networks complexity, running at 7.7 frames
per second on a Coral Dev Board Mini consuming just 2.2W.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Diffusion Models. (arXiv:2204.03458v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03458">
<div class="article-summary-box-inner">
<span><p>Generating temporally coherent high fidelity video is an important milestone
in generative modeling research. We make progress towards this milestone by
proposing a diffusion model for video generation that shows very promising
initial results. Our model is a natural extension of the standard image
diffusion architecture, and it enables jointly training from image and video
data, which we find to reduce the variance of minibatch gradients and speed up
optimization. To generate long and higher resolution videos we introduce a new
conditional sampling technique for spatial and temporal video extension that
performs better than previously proposed methods. We present the first results
on a large text-conditioned video generation task, as well as state-of-the-art
results on established benchmarks for video prediction and unconditional video
generation. Supplementary material is available at
https://video-diffusion.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Ensemble Framework for Off-Nadir Geocentric Pose Prediction. (arXiv:2205.11230v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11230">
<div class="article-summary-box-inner">
<span><p>Roughly 6,800 natural disasters occur worldwide annually, and this alarming
number continues to grow due to the effects of climate change. Effective
methods to improve natural disaster response include performing change
detection, map alignment, and vision-aided navigation to allow for the
time-efficient delivery of life-saving aid. Current software functions
optimally only on nadir images taken ninety degrees above ground level. The
inability to generalize to oblique images increases the need to compute an
image's geocentric pose, which is its spatial orientation with respect to
gravity. This Deep Learning investigation presents three convolutional models
to predict geocentric pose using 5,923 nadir and oblique red, green, and blue
(RGB) satellite images of cities worldwide. The first model is an autoencoder
that condenses the 256 x 256 x 3 images to 32 x 32 x 16 latent space
representations, demonstrating the ability to learn useful features from the
data. The second model is a U-Net Fully Convolutional Network with skip
connections used to predict each image's corresponding pixel-level elevation
mask. This model achieves a median absolute deviation of 0.335 meters and an R2
of 0.865 on test data. Afterward, the elevation masks are concatenated with the
RGB images to form four-channel inputs fed into the third model, which predicts
each image's rotation angle and scale, the components of its geocentric pose.
This Deep Convolutional Neural Network achieves an R2 of 0.943 on test data,
significantly outperforming previous models designed by researchers. The
high-accuracy software built in this study contributes to mapping and
navigation procedures to accelerate disaster relief and save human lives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mesh-based Dynamics with Occlusion Reasoning for Cloth Manipulation. (arXiv:2206.02881v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02881">
<div class="article-summary-box-inner">
<span><p>Self-occlusion is challenging for cloth manipulation, as it makes it
difficult to estimate the full state of the cloth. Ideally, a robot trying to
unfold a crumpled or folded cloth should be able to reason about the cloth's
occluded regions. We leverage recent advances in pose estimation for cloth to
build a system that uses explicit occlusion reasoning to unfold a crumpled
cloth. Specifically, we first learn a model to reconstruct the mesh of the
cloth. However, the model will likely have errors due to the complexities of
the cloth configurations and due to ambiguities from occlusions. Our main
insight is that we can further refine the predicted reconstruction by
performing test-time finetuning with self-supervised losses. The obtained
reconstructed mesh allows us to use a mesh-based dynamics model for planning
while reasoning about occlusions. We evaluate our system both on cloth
flattening as well as on cloth canonicalization, in which the objective is to
manipulate the cloth into a canonical pose. Our experiments show that our
method significantly outperforms prior methods that do not explicitly account
for occlusions or perform test-time optimization. Videos and visualizations can
be found on our
$\href{https://sites.google.com/view/occlusion-reason/home}{\text{project
website}}.$
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps. (arXiv:2206.03799v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03799">
<div class="article-summary-box-inner">
<span><p>Self-supervised monocular depth estimation has been a subject of intense
study in recent years, because of its applications in robotics and autonomous
driving. Much of the recent work focuses on improving depth estimation by
increasing architecture complexity. This paper shows that state-of-the-art
performance can also be achieved by improving the learning process rather than
increasing model complexity. More specifically, we propose (i) only using
invariant pose loss for the first few epochs during training, (ii) disregarding
small potentially dynamic objects when training, and (iii) employing an
appearance-based approach to separately estimate object pose for truly dynamic
objects. We demonstrate that these simplifications reduce GPU memory usage by
29% and result in qualitatively and quantitatively improved depth maps. The
code is available at https://github.com/kieran514/Dyna-DM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CASS: Cross Architectural Self-Supervision for Medical Image Analysis. (arXiv:2206.04170v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04170">
<div class="article-summary-box-inner">
<span><p>Recent advances in Deep Learning and Computer Vision have alleviated many of
the bottlenecks, allowing algorithms to be label-free with better performance.
Specifically, Transformers provide a global perspective of the image, which
Convolutional Neural Networks (CNN) lack by design. Here we present Cross
Architectural Self-Supervision, a novel self-supervised learning approach which
leverages transformers and CNN simultaneously, while also being computationally
accessible to general practitioners via easily available cloud services.
Compared to existing state-of-the-art self-supervised learning approaches, we
empirically show CASS trained CNNs, and Transformers gained an average of 8.5%
with 100% labelled data, 7.3% with 10% labelled data, and 11.5% with 1%
labelled data, across three diverse datasets. Notably, one of the employed
datasets included histopathology slides of an autoimmune disease, a topic
underrepresented in Medical Imaging and has minimal data. In addition, our
findings reveal that CASS is twice as efficient as other state-of-the-art
methods in terms of training time. The code is open source and is available on
GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?. (arXiv:2206.05266v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05266">
<div class="article-summary-box-inner">
<span><p>We investigate whether self-supervised learning (SSL) can improve online
reinforcement learning (RL) from pixels. We extend the contrastive
reinforcement learning framework (e.g., CURL) that jointly optimizes SSL and RL
losses and conduct an extensive amount of experiments with various
self-supervised losses. Our observations suggest that the existing SSL
framework for RL fails to bring meaningful improvement over the baselines only
taking advantage of image augmentation when the same amount of data and
augmentation is used. We further perform an evolutionary search to find the
optimal combination of multiple self-supervised losses for RL, but find that
even such a loss combination fails to meaningfully outperform the methods that
only utilize carefully designed image augmentations. Often, the use of
self-supervised losses under the existing framework lowered RL performances. We
evaluate the approach in multiple different environments including a real-world
robot environment and confirm that no single self-supervised loss or image
augmentation method can dominate all environments and that the current
framework for joint optimization of SSL and RL is limited. Finally, we
empirically investigate the pretraining framework for SSL + RL and the
properties of representations learned with different approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Polyp Segmentation with Multiple Kernel Dilated Convolution Network. (arXiv:2206.06264v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06264">
<div class="article-summary-box-inner">
<span><p>The detection and removal of precancerous polyps through colonoscopy is the
primary technique for the prevention of colorectal cancer worldwide. However,
the miss rate of colorectal polyp varies significantly among the endoscopists.
It is well known that a computer-aided diagnosis (CAD) system can assist
endoscopists in detecting colon polyps and minimize the variation among
endoscopists. In this study, we introduce a novel deep learning architecture,
named MKDCNet, for automatic polyp segmentation robust to significant changes
in polyp data distribution. MKDCNet is simply an encoder-decoder neural network
that uses the pre-trained ResNet50 as the encoder and novel multiple kernel
dilated convolution (MKDC) block that expands the field of view to learn more
robust and heterogeneous representation. Extensive experiments on four publicly
available polyp datasets and cell nuclei dataset show that the proposed MKDCNet
outperforms the state-of-the-art methods when trained and tested on the same
dataset as well when tested on unseen polyp datasets from different
distributions. With rich results, we demonstrated the robustness of the
proposed architecture. From an efficiency perspective, our algorithm can
process at (approx 45) frames per second on RTX 3090 GPU. MKDCNet can be a
strong benchmark for building real-time systems for clinical colonoscopies. The
code of the proposed MKDCNet is available at
https://github.com/nikhilroxtomar/MKDCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO. (arXiv:2206.06761v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06761">
<div class="article-summary-box-inner">
<span><p>This work conducts the first analysis on the robustness against adversarial
attacks on self-supervised Vision Transformers trained using DINO. First, we
evaluate whether features learned through self-supervision are more robust to
adversarial attacks than those emerging from supervised learning. Then, we
present properties arising for attacks in the latent space. Finally, we
evaluate whether three well-known defense strategies can increase adversarial
robustness in downstream tasks by only fine-tuning the classification head to
provide robustness even in view of limited compute resources. These defense
strategies are: Adversarial Training, Ensemble Adversarial Training and
Ensemble of Specialized Networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Broken News: Making Newspapers Accessible to Print-Impaired. (arXiv:2206.10225v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10225">
<div class="article-summary-box-inner">
<span><p>Accessing daily news content still remains a big challenge for people with
print-impairment including blind and low-vision due to opacity of printed
content and hindrance from online sources. In this paper, we present our
approach for digitization of print newspaper into an accessible file format
such as HTML. We use an ensemble of instance segmentation and detection
framework for newspaper layout analysis and then OCR to recognize text elements
such as headline and article text. Additionally, we propose EdgeMask loss
function for Mask-RCNN framework to improve segmentation mask boundary and
hence accuracy of downstream OCR task. Empirically, we show that our proposed
loss function reduces the Word Error Rate (WER) of news article text by 32.5 %.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HealNet -- Self-Supervised Acute Wound Heal-Stage Classification. (arXiv:2206.10536v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10536">
<div class="article-summary-box-inner">
<span><p>Identifying, tracking, and predicting wound heal-stage progression is a
fundamental task towards proper diagnosis, effective treatment, facilitating
healing, and reducing pain. Traditionally, a medical expert might observe a
wound to determine the current healing state and recommend treatment. However,
sourcing experts who can produce such a diagnosis solely from visual indicators
can be difficult, time-consuming and expensive. In addition, lesions may take
several weeks to undergo the healing process, demanding resources to monitor
and diagnose continually. Automating this task can be challenging; datasets
that follow wound progression from onset to maturation are small, rare, and
often collected without computer vision in mind. To tackle these challenges, we
introduce a self-supervised learning scheme composed of (a) learning embeddings
of wound's temporal dynamics, (b) clustering for automatic stage discovery, and
(c) fine-tuned classification. The proposed self-supervised and flexible
learning framework is biologically inspired and trained on a small dataset with
zero human labeling. The HealNet framework achieved high pre-text and
downstream classification accuracy; when evaluated on held-out test data,
HealNet achieved 97.7% pre-text accuracy and 90.62% heal-stage classification
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning. (arXiv:2206.10698v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10698">
<div class="article-summary-box-inner">
<span><p>We present Transformation Invariance and Covariance Contrast (TiCo) for
self-supervised visual representation learning. Similar to other recent
self-supervised learning methods, our method is based on maximizing the
agreement among embeddings of different distorted versions of the same image,
which pushes the encoder to produce transformation invariant representations.
To avoid the trivial solution where the encoder generates constant vectors, we
regularize the covariance matrix of the embeddings from different images by
penalizing low rank solutions. By jointly minimizing the transformation
invariance loss and covariance contrast loss, we get an encoder that is able to
produce useful representations for downstream tasks. We analyze our method and
show that it can be viewed as a variant of MoCo with an implicit memory bank of
unlimited size at no extra memory cost. This makes our method perform better
than alternative methods when using small batch sizes. TiCo can also be seen as
a modification of Barlow Twins. By connecting the contrastive and
redundancy-reduction methods together, TiCo gives us new insights into how
joint embedding methods work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correct and Certify: A New Approach to Self-Supervised 3D-Object Perception. (arXiv:2206.11215v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11215">
<div class="article-summary-box-inner">
<span><p>We consider an object pose estimation and model fitting problem, where -
given a partial point cloud of an object - the goal is to estimate the object
pose by fitting a CAD model to the sensor data. We solve this problem by
combining (i) a semantic keypoint-based pose estimation model, (ii) a novel
self-supervised training approach, and (iii) a certification procedure, that
not only verifies whether the output produced by the model is correct or not,
but also flags uniqueness of the produced solution. The semantic keypoint
detector model is initially trained in simulation and does not perform well on
real-data due to the domain gap. Our self-supervised training procedure uses a
corrector and a certification module to improve the detector. The corrector
module corrects the detected keypoints to compensate for the domain gap, and is
implemented as a declarative layer, for which we develop a simple
differentiation rule. The certification module declares whether the corrected
output produced by the model is certifiable (i.e. correct) or not. At each
iteration, the approach optimizes over the loss induced only by the certifiable
input-output pairs. As training progresses, we see that the fraction of outputs
that are certifiable increases, eventually reaching near $100\%$ in many cases.
We also introduce the notion of strong certifiability wherein the model can
determine if the predicted object model fit is unique or not. The detected
semantic keypoints help us implement this in the forward pass. We conduct
extensive experiments to evaluate the performance of the corrector, the
certification, and the proposed self-supervised training using the ShapeNet and
YCB datasets, and show the proposed approach achieves performance comparable to
fully supervised baselines while not requiring pose or keypoint supervision on
real data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-26 23:07:35.214994273 UTC">2022-06-26 23:07:35 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>