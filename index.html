<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-08T01:30:00Z">07-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Model Sizes and the Parameter Gap. (arXiv:2207.02852v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02852">
<div class="article-summary-box-inner">
<span><p>We study trends in model size of notable machine learning systems over time
using a curated dataset. From 1950 to 2018, model size in language models
increased steadily by seven orders of magnitude. The trend then accelerated,
with model size increasing by another five orders of magnitude in just 4 years
from 2018 to 2022. Vision models grew at a more constant pace, totaling 7
orders of magnitude of growth between 1950 and 2022.
</p>
<p>We also identify that, since 2020, there have been many language models below
20B parameters, many models above 70B parameters, but a scarcity of models in
the 20-70B parameter range. We refer to that scarcity as the parameter gap.
</p>
<p>We provide some stylized facts about the parameter gap and propose a few
hypotheses to explain it. The explanations we favor are: (a) increasing model
size beyond 20B parameters requires adopting different parallelism techniques,
which makes mid-sized models less cost-effective, (b) GPT-3 was one order of
magnitude larger than previous language models, and researchers afterwards
primarily experimented with bigger models to outperform it. While these
dynamics likely exist, and we believe they play some role in generating the
gap, we don't have high confidence that there are no other, more important
dynamics at play.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding. (arXiv:2207.02971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02971">
<div class="article-summary-box-inner">
<span><p>Conformer has proven to be effective in many speech processing tasks. It
combines the benefits of extracting local dependencies using convolutions and
global dependencies using self-attention. Inspired by this, we propose a more
flexible, interpretable and customizable encoder alternative, Branchformer,
with parallel branches for modeling various ranged dependencies in end-to-end
speech processing. In each encoder layer, one branch employs self-attention or
its variant to capture long-range dependencies, while the other branch utilizes
an MLP module with convolutional gating (cgMLP) to extract local relationships.
We conduct experiments on several speech recognition and spoken language
understanding benchmarks. Results show that our model outperforms both
Transformer and cgMLP. It also matches with or outperforms state-of-the-art
results achieved by Conformer. Furthermore, we show various strategies to
reduce computation thanks to the two-branch architecture, including the ability
to have variable inference complexity in a single trained model. The weights
learned for merging branches indicate how local and global dependencies are
utilized in different layers, which benefits model designing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling. (arXiv:2207.03030v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03030">
<div class="article-summary-box-inner">
<span><p>This paper studies multi-task training of retrieval-augmented generation
models for knowledge-intensive tasks. We propose to clean the training set by
utilizing a distinct property of knowledge-intensive generation: The connection
of query-answer pairs to items in the knowledge base. We filter training
examples via a threshold of confidence on the relevance labels, whether a pair
is answerable by the knowledge base or not. We train a single Fusion-in-Decoder
(FiD) generator on seven combined tasks of the KILT benchmark. The experimental
results suggest that our simple yet effective approach substantially improves
competitive baselines on two strongly imbalanced tasks; and shows either
smaller improvements or no significant regression on the remaining tasks.
Furthermore, we demonstrate our multi-task training with relevance label
sampling scales well with increased model capacity and achieves
state-of-the-art results in five out of seven KILT tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensitivity Analysis on Transferred Neural Architectures of BERT and GPT-2 for Financial Sentiment Analysis. (arXiv:2207.03037v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03037">
<div class="article-summary-box-inner">
<span><p>The explosion in novel NLP word embedding and deep learning techniques has
induced significant endeavors into potential applications. One of these
directions is in the financial sector. Although there is a lot of work done in
state-of-the-art models like GPT and BERT, there are relatively few works on
how well these methods perform through fine-tuning after being pre-trained, as
well as info on how sensitive their parameters are. We investigate the
performance and sensitivity of transferred neural architectures from
pre-trained GPT-2 and BERT models. We test the fine-tuning performance based on
freezing transformer layers, batch size, and learning rate. We find the
parameters of BERT are hypersensitive to stochasticity in fine-tuning and that
GPT-2 is more stable in such practice. It is also clear that the earlier layers
of GPT-2 and BERT contain essential word pattern information that should be
maintained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03038">
<div class="article-summary-box-inner">
<span><p>This paper describes our champion solution for the CVPR2022 Generic Event
Boundary Captioning (GEBC) competition. GEBC requires the captioning model to
have a comprehension of instantaneous status changes around the given video
boundary, which makes it much more challenging than conventional video
captioning task. In this paper, a Dual-Stream Transformer with improvements on
both video content encoding and captions generation is proposed: (1) We utilize
three pre-trained models to extract the video features from different
granularities. Moreover, we exploit the types of boundary as hints to help the
model generate captions. (2) We particularly design an model, termed as
Dual-Stream Transformer, to learn discriminative representations for boundary
captioning. (3) Towards generating content-relevant and human-like captions, we
improve the description quality by designing a word-level ensemble strategy.
The promising results on the GEBC test split demonstrate the efficacy of our
proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Few-Shot Image Classification Using Machine- and User-Generated Natural Language Descriptions. (arXiv:2207.03133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03133">
<div class="article-summary-box-inner">
<span><p>Humans can obtain the knowledge of novel visual concepts from language
descriptions, and we thus use the few-shot image classification task to
investigate whether a machine learning model can have this capability. Our
proposed model, LIDE (Learning from Image and DEscription), has a text decoder
to generate the descriptions and a text encoder to obtain the text
representations of machine- or user-generated descriptions. We confirmed that
LIDE with machine-generated descriptions outperformed baseline models.
Moreover, the performance was improved further with high-quality user-generated
descriptions. The generated descriptions can be viewed as the explanations of
the model's predictions, and we observed that such explanations were consistent
with prediction results. We also investigated why the language description
improved the few-shot image classification performance by comparing the image
representations and the text representations in the feature spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning and Multi-label Classification for Ellipsis and Coreference Detection in Conversational Question-Answering. (arXiv:2207.03145v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03145">
<div class="article-summary-box-inner">
<span><p>In human conversations, ellipsis and coreference are commonly occurring
linguistic phenomena. Although these phenomena are a mean of making
human-machine conversations more fluent and natural, only few dialogue corpora
contain explicit indications on which turns contain ellipses and/or
coreferences. In this paper we address the task of automatically detecting
ellipsis and coreferences in conversational question answering. We propose to
use a multi-label classifier based on DistilBERT. Multi-label classification
and active learning are employed to compensate the limited amount of labeled
data. We show that these methods greatly enhance the performance of the
classifier for detecting these phenomena on a manually labeled dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Speech-to-Punctuated-Text Recognition. (arXiv:2207.03169v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03169">
<div class="article-summary-box-inner">
<span><p>Conventional automatic speech recognition systems do not produce punctuation
marks which are important for the readability of the speech recognition
results. They are also needed for subsequent natural language processing tasks
such as machine translation. There have been a lot of works on punctuation
prediction models that insert punctuation marks into speech recognition results
as post-processing. However, these studies do not utilize acoustic information
for punctuation prediction and are directly affected by speech recognition
errors. In this study, we propose an end-to-end model that takes speech as
input and outputs punctuated texts. This model is expected to predict
punctuation robustly against speech recognition errors while using acoustic
information. We also propose to incorporate an auxiliary loss to train the
model using the output of the intermediate layer and unpunctuated texts.
Through experiments, we compare the performance of the proposed model to that
of a cascaded system. The proposed model achieves higher punctuation prediction
accuracy than the cascaded system without sacrificing the speech recognition
error rate. It is also demonstrated that the multi-task learning using the
intermediate output against the unpunctuated text is effective. Moreover, the
proposed model has only about 1/7th of the parameters compared to the cascaded
system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoQAR: Question Rewriting on CoQA. (arXiv:2207.03240v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03240">
<div class="article-summary-box-inner">
<span><p>Questions asked by humans during a conversation often contain contextual
dependencies, i.e., explicit or implicit references to previous dialogue turns.
These dependencies take the form of coreferences (e.g., via pronoun use) or
ellipses, and can make the understanding difficult for automated systems. One
way to facilitate the understanding and subsequent treatments of a question is
to rewrite it into an out-of-context form, i.e., a form that can be understood
without the conversational context. We propose CoQAR, a corpus containing
$4.5$K conversations from the Conversational Question-Answering dataset CoQA,
for a total of $53$K follow-up question-answer pairs. Each original question
was manually annotated with at least 2 at most 3 out-of-context rewritings.
CoQAR can be used in the supervised learning of three tasks: question
paraphrasing, question rewriting and conversational question answering. In
order to assess the quality of CoQAR's rewritings, we conduct several
experiments consisting in training and evaluating models for these three tasks.
Our results support the idea that question rewriting can be used as a
preprocessing step for question answering models, thereby increasing their
performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Part-of-Speech Tagging of Odia Language Using statistical and Deep Learning-Based Approaches. (arXiv:2207.03256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03256">
<div class="article-summary-box-inner">
<span><p>Automatic Part-of-speech (POS) tagging is a preprocessing step of many
natural language processing (NLP) tasks such as name entity recognition (NER),
speech processing, information extraction, word sense disambiguation, and
machine translation. It has already gained a promising result in English and
European languages, but in Indian languages, particularly in Odia language, it
is not yet well explored because of the lack of supporting tools, resources,
and morphological richness of language. Unfortunately, we were unable to locate
an open source POS tagger for Odia, and only a handful of attempts have been
made to develop POS taggers for Odia language. The main contribution of this
research work is to present a conditional random field (CRF) and deep
learning-based approaches (CNN and Bidirectional Long Short-Term Memory) to
develop Odia part-of-speech tagger. We used a publicly accessible corpus and
the dataset is annotated with the Bureau of Indian Standards (BIS) tagset.
However, most of the languages around the globe have used the dataset annotated
with Universal Dependencies (UD) tagset. Hence, to maintain uniformity Odia
dataset should use the same tagset. So we have constructed a simple mapping
from BIS tagset to UD tagset. We experimented with various feature set inputs
to the CRF model, observed the impact of constructed feature set. The deep
learning-based model includes Bi-LSTM network, CNN network, CRF layer,
character sequence information, and pre-trained word vector. Character sequence
information was extracted by using convolutional neural network (CNN) and
Bi-LSTM network. Six different combinations of neural sequence labelling models
are implemented, and their performance measures are investigated. It has been
observed that Bi-LSTM model with character sequence feature and pre-trained
word vector achieved a significant state-of-the-art result.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity Recognition. (arXiv:2207.03300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03300">
<div class="article-summary-box-inner">
<span><p>For Named Entity Recognition (NER), sequence labeling-based and span-based
paradigms are quite different. Previous research has demonstrated that the two
paradigms have clear complementary advantages, but few models have attempted to
leverage these advantages in a single NER model as far as we know. In our
previous work, we proposed a paradigm known as Bundling Learning (BL) to
address the above problem. The BL paradigm bundles the two NER paradigms,
enabling NER models to jointly tune their parameters by weighted summing each
paradigm's training loss. However, three critical issues remain unresolved:
When does BL work? Why does BL work? Can BL enhance the existing
state-of-the-art (SOTA) NER models? To address the first two issues, we
implement three NER models, involving a sequence labeling-based model--SeqNER,
a span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER
together. We draw two conclusions regarding the two issues based on the
experimental results on eleven NER datasets from five domains. We then apply BL
to five existing SOTA NER models to investigate the third issue, consisting of
three sequence labeling-based models and two span-based models. Experimental
results indicate that BL consistently enhances their performance, suggesting
that it is possible to construct a new SOTA NER system by incorporating BL into
the current SOTA system. Moreover, we find that BL reduces both entity boundary
and type prediction errors. In addition, we compare two commonly used labeling
tagging methods as well as three types of span semantic representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Emotion: Investigating Model Representations, Multi-Task Learning and Knowledge Distillation. (arXiv:2207.03334v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03334">
<div class="article-summary-box-inner">
<span><p>Estimating dimensional emotions, such as activation, valence and dominance,
from acoustic speech signals has been widely explored over the past few years.
While accurate estimation of activation and dominance from speech seem to be
possible, the same for valence remains challenging. Previous research has shown
that the use of lexical information can improve valence estimation performance.
Lexical information can be obtained from pre-trained acoustic models, where the
learned representations can improve valence estimation from speech. We
investigate the use of pre-trained model representations to improve valence
estimation from acoustic speech signal. We also explore fusion of
representations to improve emotion estimation across all three emotion
dimensions: activation, valence and dominance. Additionally, we investigate if
representations from pre-trained models can be distilled into models trained
with low-level features, resulting in models with a less number of parameters.
We show that fusion of pre-trained model embeddings result in a 79% relative
improvement in concordance correlation coefficient CCC on valence estimation
compared to standard acoustic feature baseline (mel-filterbank energies), while
distillation from pre-trained model embeddings to lower-dimensional
representations yielded a relative 12% improvement. Such performance gains were
observed over two evaluation sets, indicating that our proposed architecture
generalizes across those evaluation sets. We report new state-of-the-art
"text-free" acoustic-only dimensional emotion estimation $CCC$ values on two
MSP-Podcast evaluation sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Language Models are not Born Equal to Fit Brain Data, but Training Helps. (arXiv:2207.03380v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03380">
<div class="article-summary-box-inner">
<span><p>Neural Language Models (NLMs) have made tremendous advances during the last
years, achieving impressive performance on various linguistic tasks.
Capitalizing on this, studies in neuroscience have started to use NLMs to study
neural activity in the human brain during language processing. However, many
questions remain unanswered regarding which factors determine the ability of a
neural language model to capture brain activity (aka its 'brain score'). Here,
we make first steps in this direction and examine the impact of test loss,
training corpus and model architecture (comparing GloVe, LSTM, GPT-2 and BERT),
on the prediction of functional Magnetic Resonance Imaging timecourses of
participants listening to an audiobook. We find that (1) untrained versions of
each model already explain significant amount of signal in the brain by
capturing similarity in brain responses across identical words, with the
untrained LSTM outperforming the transformerbased models, being less impacted
by the effect of context; (2) that training NLP models improves brain scores in
the same brain regions irrespective of the model's architecture; (3) that
Perplexity (test loss) is not a good predictor of brain score; (4) that
training data have a strong influence on the outcome and, notably, that
off-the-shelf models may lack statistical power to detect brain activations.
Overall, we outline the impact of modeltraining choices, and suggest good
practices for future studies aiming at explaining the human language system
using neural language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Impact of Cross-lingual Acoustic-Phonetic Similarities on Multilingual Speech Recognition. (arXiv:2207.03390v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03390">
<div class="article-summary-box-inner">
<span><p>Multilingual automatic speech recognition (ASR) systems mostly benefit low
resource languages but suffer degradation in performance across several
languages relative to their monolingual counterparts. Limited studies have
focused on understanding the languages behaviour in the multilingual speech
recognition setups. In this paper, a novel data-driven approach is proposed to
investigate the cross-lingual acoustic-phonetic similarities. This technique
measures the similarities between posterior distributions from various
monolingual acoustic models against a target speech signal. Deep neural
networks are trained as mapping networks to transform the distributions from
different acoustic models into a directly comparable form. The analysis
observes that the languages closeness can not be truly estimated by the volume
of overlapping phonemes set. Entropy analysis of the proposed mapping networks
exhibits that a language with lesser overlap can be more amenable to
cross-lingual transfer, and hence more beneficial in the multilingual setup.
Finally, the proposed posterior transformation approach is leveraged to fuse
monolingual models for a target language. A relative improvement of ~8% over
monolingual counterpart is achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion. (arXiv:2207.03391v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03391">
<div class="article-summary-box-inner">
<span><p>Multilingual speech recognition has drawn significant attention as an
effective way to compensate data scarcity for low-resource languages.
End-to-end (e2e) modelling is preferred over conventional hybrid systems,
mainly because of no lexicon requirement. However, hybrid DNN-HMMs still
outperform e2e models in limited data scenarios. Furthermore, the problem of
manual lexicon creation has been alleviated by publicly available trained
models of grapheme-to-phoneme (G2P) and text to IPA transliteration for a lot
of languages. In this paper, a novel approach of hybrid DNN-HMM acoustic models
fusion is proposed in a multilingual setup for the low-resource languages.
Posterior distributions from different monolingual acoustic models, against a
target language speech signal, are fused together. A separate regression neural
network is trained for each source-target language pair to transform posteriors
from source acoustic model to the target language. These networks require very
limited data as compared to the ASR training. Posterior fusion yields a
relative gain of 14.65% and 6.5% when compared with multilingual and
monolingual baselines respectively. Cross-lingual model fusion shows that the
comparable results can be achieved without using posteriors from the language
dependent ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Modeling of Language-Evoked Event-Related Potentials. (arXiv:2207.03392v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03392">
<div class="article-summary-box-inner">
<span><p>Bayesian hierarchical models are well-suited to analyzing the often noisy
data from electroencephalography experiments in cognitive neuroscience: these
models provide an intuitive framework to account for structures and
correlations in the data, and they allow a straightforward handling of
uncertainty. In a typical neurolinguistic experiment, event-related potentials
show only very small effect sizes and frequentist approaches to data analysis
fail to establish the significance of some of these effects. Here, we present a
Bayesian approach to analyzing event-related potentials using as an example
data from an experiment which relates word surprisal and neural response. Our
model is able to estimate the effect of word surprisal on most components of
the event-related potential and provides a richer description of the data. The
Bayesian framework also allows easier comparison between estimates based on
surprisal values calculated using different language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AsNER -- Annotated Dataset and Baseline for Assamese Named Entity recognition. (arXiv:2207.03422v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03422">
<div class="article-summary-box-inner">
<span><p>We present the AsNER, a named entity annotation dataset for low resource
Assamese language with a baseline Assamese NER model. The dataset contains
about 99k tokens comprised of text from the speech of the Prime Minister of
India and Assamese play. It also contains person names, location names and
addresses. The proposed NER dataset is likely to be a significant resource for
deep neural based Assamese language processing. We benchmark the dataset by
training NER models and evaluating using state-of-the-art architectures for
supervised named entity recognition (NER) such as Fasttext, BERT, XLM-R, FLAIR,
MuRIL etc. We implement several baseline approaches with state-of-the-art
sequence tagging Bi-LSTM-CRF architecture. The highest F1-score among all
baselines achieves an accuracy of 80.69% when using MuRIL as a word embedding
method. The annotated dataset and the top performing model are made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VeriDark: A Large-Scale Benchmark for Authorship Verification on the Dark Web. (arXiv:2207.03477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03477">
<div class="article-summary-box-inner">
<span><p>The DarkWeb represents a hotbed for illicit activity, where users communicate
on different market forums in order to exchange goods and services. Law
enforcement agencies benefit from forensic tools that perform authorship
analysis, in order to identify and profile users based on their textual
content. However, authorship analysis has been traditionally studied using
corpora featuring literary texts such as fragments from novels or fan fiction,
which may not be suitable in a cybercrime context. Moreover, the few works that
employ authorship analysis tools for cybercrime prevention usually employ
ad-hoc experimental setups and datasets. To address these issues, we release
VeriDark: a benchmark comprised of three large scale authorship verification
datasets and one authorship identification dataset obtained from user activity
from either Dark Web related Reddit communities or popular illicit Dark Web
market forums. We evaluate competitive NLP baselines on the three datasets and
perform an analysis of the predictions to better understand the limitations of
such approaches. We make the datasets and baselines publicly available at
https://github.com/bit-ml/VeriDark
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adposition and Case Supersenses v2.6: Guidelines for English. (arXiv:1704.02134v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1704.02134">
<div class="article-summary-box-inner">
<span><p>This document offers a detailed linguistic description of SNACS (Semantic
Network of Adposition and Case Supersenses; Schneider et al., 2018), an
inventory of 52 semantic labels ("supersenses") that characterize the use of
adpositions and case markers at a somewhat coarse level of granularity, as
demonstrated in the STREUSLE corpus (https://github.com/nert-nlp/streusle/ ;
version 4.5 tracks guidelines version 2.6). Though the SNACS inventory aspires
to be universal, this document is specific to English; documentation for other
languages will be published separately.
</p>
<p>Version 2 is a revision of the supersense inventory proposed for English by
Schneider et al. (2015, 2016) (henceforth "v1"), which in turn was based on
previous schemes. The present inventory was developed after extensive review of
the v1 corpus annotations for English, plus previously unanalyzed genitive case
possessives (Blodgett and Schneider, 2018), as well as consideration of
adposition and case phenomena in Hebrew, Hindi, Korean, and German. Hwang et
al. (2017) present the theoretical underpinnings of the v2 scheme. Schneider et
al. (2018) summarize the scheme, its application to English corpus data, and an
automatic disambiguation task. Liu et al. (2021) offer an English Lexical
Semantic Recognition tagger that includes SNACS labels in its output.
</p>
<p>This documentation can also be browsed alongside corpus data on the Xposition
website (Gessler et al., 2022): <a href="http://www.xposition.org/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for Enabling Hardware-agnostic Programming with True Performance Portability for Heterogeneous HPC. (arXiv:2011.10896v5 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10896">
<div class="article-summary-box-inner">
<span><p>This paper presents HALO 1.0, an open-ended extensible multi-agent software
framework that implements a set of proposed hardware-agnostic accelerator
orchestration (HALO) principles. HALO implements a novel compute-centric
message passing interface (C^2MPI) specification for enabling the performance
portable execution of a hardware-agnostic host application across heterogeneous
accelerators. The experiment results of evaluating eight widely used HPC
subroutines based on Intel Xeon E5-2620 CPUs, Intel Arria 10 GX FPGAs, and
NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control
flow for host programs to run across all the computing devices with a
consistently top performance portability score, which is up to five orders of
magnitude higher than the OpenCL-based solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence tagging for biomedical extractive question answering. (arXiv:2104.07535v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07535">
<div class="article-summary-box-inner">
<span><p>Current studies in extractive question answering (EQA) have modeled the
single-span extraction setting, where a single answer span is a label to
predict for a given question-passage pair. This setting is natural for general
domain EQA as the majority of the questions in the general domain can be
answered with a single span. Following general domain EQA models, current
biomedical EQA (BioEQA) models utilize the single-span extraction setting with
post-processing steps. In this article, we investigate the question
distribution across the general and biomedical domains and discover biomedical
questions are more likely to require list-type answers (multiple answers) than
factoid-type answers (single answer). This necessitates the models capable of
producing multiple answers for a question. Based on this preliminary study, we
propose a sequence tagging approach for BioEQA, which is a multi-span
extraction setting. Our approach directly tackles questions with a variable
number of phrases as their answer and can learn to decide the number of answers
for a question from training data. Our experimental results on the BioASQ 7b
and 8b list-type questions outperformed the best-performing existing models
without requiring post-processing steps. Source codes and resources are freely
available for download at https://github.com/dmis-lab/SeqTagQA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning grammar with a divide-and-concur neural network. (arXiv:2201.07341v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07341">
<div class="article-summary-box-inner">
<span><p>We implement a divide-and-concur iterative projection approach to
context-free grammar inference. Unlike most state-of-the-art models of natural
language processing, our method requires a relatively small number of discrete
parameters, making the inferred grammar directly interpretable -- one can read
off from a solution how to construct grammatically valid sentences. Another
advantage of our approach is the ability to infer meaningful grammatical rules
from just a few sentences, compared to the hundreds of gigabytes of training
data many other models employ. We demonstrate several ways of applying our
approach: classifying words and inferring a grammar from scratch, taking an
existing grammar and refining its categories and rules, and taking an existing
grammar and expanding its lexicon as it encounters new words in new data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resources for Turkish Natural Language Processing: A critical survey. (arXiv:2204.05042v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05042">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive survey of corpora and lexical resources
available for Turkish. We review a broad range of resources, focusing on the
ones that are publicly available. In addition to providing information about
the available linguistic resources, we present a set of recommendations, and
identify gaps in the data available for conducting research and building
applications in Turkish Linguistics and Natural Language Processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness. (arXiv:2205.00001v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00001">
<div class="article-summary-box-inner">
<span><p>Having a rich multimodal inner language is an important component of human
intelligence that enables several necessary core cognitive functions such as
multimodal prediction, translation, and generation. Building upon the Conscious
Turing Machine (CTM), a machine model for consciousness proposed by Blum and
Blum (2021), we describe the desiderata of a multimodal language called
Brainish, comprising words, images, audio, and sensations combined in
representations that the CTM's processors use to communicate with each other.
We define the syntax and semantics of Brainish before operationalizing this
language through the lens of multimodal artificial intelligence, a vibrant
research area studying the computational tools necessary for processing and
relating information from heterogeneous signals. Our general framework for
learning Brainish involves designing (1) unimodal encoders to segment and
represent unimodal data, (2) a coordinated representation space that relates
and composes unimodal features to derive holistic meaning across multimodal
inputs, and (3) decoders to map multimodal representations into predictions
(for fusion) or raw data (for translation or generation). Through discussing
how Brainish is crucial for communication and coordination in order to achieve
consciousness in the CTM, and by implementing a simple version of Brainish and
evaluating its capability of demonstrating intelligence on multimodal
prediction and retrieval tasks on several real-world image, text, and audio
datasets, we argue that such an inner language will be important for advances
in machine models of intelligence and consciousness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paragraph-based Transformer Pre-training for Multi-Sentence Inference. (arXiv:2205.01228v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01228">
<div class="article-summary-box-inner">
<span><p>Inference tasks such as answer sentence selection (AS2) or fact verification
are typically solved by fine-tuning transformer-based models as individual
sentence-pair classifiers. Recent studies show that these tasks benefit from
modeling dependencies across multiple candidate sentences jointly. In this
paper, we first show that popular pre-trained transformers perform poorly when
used for fine-tuning on multi-candidate inference tasks. We then propose a new
pre-training objective that models the paragraph-level semantics across
multiple input sentences. Our evaluation on three AS2 and one fact verification
datasets demonstrates the superiority of our pre-training technique over the
traditional ones for transformers used as joint models for multi-candidate
inference tasks, as well as when used as cross-encoders for sentence-pair
formulations of these tasks. Our code and pre-trained models are released at
https://github.com/amazon-research/wqa-multi-sentence-inference .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Machine Translation Systems for the Next Thousand Languages. (arXiv:2205.03983v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03983">
<div class="article-summary-box-inner">
<span><p>In this paper we share findings from our effort to build practical machine
translation (MT) systems capable of translating across over one thousand
languages. We describe results in three research domains: (i) Building clean,
web-mined datasets for 1500+ languages by leveraging semi-supervised
pre-training for language identification and developing data-driven filtering
techniques; (ii) Developing practical MT models for under-served languages by
leveraging massively multilingual models trained with supervised parallel data
for over 100 high-resource languages and monolingual datasets for an additional
1000+ languages; and (iii) Studying the limitations of evaluation metrics for
these languages and conducting qualitative analysis of the outputs from our MT
models, highlighting several frequent error modes of these types of models. We
hope that our work provides useful insights to practitioners working towards
building MT systems for currently understudied languages, and highlights
research directions that can complement the weaknesses of massively
multilingual models in data-sparse settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehensive Analysis of Negative Sampling in Knowledge Graph Representation Learning. (arXiv:2206.10140v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10140">
<div class="article-summary-box-inner">
<span><p>Negative sampling (NS) loss plays an important role in learning knowledge
graph embedding (KGE) to handle a huge number of entities. However, the
performance of KGE degrades without hyperparameters such as the margin term and
number of negative samples in NS loss being appropriately selected. Currently,
empirical hyperparameter tuning addresses this problem at the cost of
computational time. To solve this problem, we theoretically analyzed NS loss to
assist hyperparameter tuning and understand the better use of the NS loss in
KGE learning. Our theoretical analysis showed that scoring methods with
restricted value ranges, such as TransE and RotatE, require appropriate
adjustment of the margin term or the number of negative samples different from
those without restricted value ranges, such as RESCAL, ComplEx, and DistMult.
We also propose subsampling methods specialized for the NS loss in KGE studied
from a theoretical aspect. Our empirical analysis on the FB15k-237, WN18RR, and
YAGO3-10 datasets showed that the results of actually trained models agree with
our theoretical findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keyword Extraction in Scientific Documents. (arXiv:2207.01888v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01888">
<div class="article-summary-box-inner">
<span><p>The scientific publication output grows exponentially. Therefore, it is
increasingly challenging to keep track of trends and changes. Understanding
scientific documents is an important step in downstream tasks such as knowledge
graph building, text mining, and discipline classification. In this workshop,
we provide a better understanding of keyword and keyphrase extraction from the
abstract of scientific publications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa. (arXiv:2207.02424v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02424">
<div class="article-summary-box-inner">
<span><p>Text sentiment analysis, also known as opinion mining, is research on the
calculation of people's views, evaluations, attitude and emotions expressed by
entities. Text sentiment analysis can be divided into text-level sentiment
analysis, sen-tence-level sentiment analysis and aspect-level sentiment
analysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the
field of sentiment analysis, which aims to predict the polarity of aspects. The
research of pre-training neural model has significantly improved the
performance of many natural language processing tasks. In recent years, pre
training model (PTM) has been applied in ABSA. Therefore, there has been a
question, which is whether PTMs contain sufficient syntactic information for
ABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced
BERT with disentangled attention) to solve Aspect-Based Sentiment Analysis
problem. DeBERTa is a kind of neural language model based on transformer, which
uses self-supervised learning to pre-train on a large number of original text
corpora. Based on the Local Context Focus (LCF) mechanism, by integrating
DeBERTa model, we purpose a multi-task learning model for aspect-based
sentiment analysis. The experiments result on the most commonly used the laptop
and restaurant datasets of SemEval-2014 and the ACL twitter dataset show that
LCF mechanism with DeBERTa has significant improvement.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient fine-grained road segmentation using superpixel-based CNN and CRF models. (arXiv:2207.02844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02844">
<div class="article-summary-box-inner">
<span><p>Towards a safe and comfortable driving, road scene segmentation is a
rudimentary problem in camera-based advance driver assistance systems (ADAS).
Despite of the great achievement of Convolutional Neural Networks (CNN) for
semantic segmentation task, the high computational efforts of CNN based methods
is still a challenging area. In recent work, we proposed a novel approach to
utilise the advantages of CNNs for the task of road segmentation at reasonable
computational effort. The runtime benefits from using irregular super pixels as
basis for the input for the CNN rather than the image grid, which tremendously
reduces the input size. Although, this method achieved remarkable low
computational time in both training and testing phases, the lower resolution of
the super pixel domain yields naturally lower accuracy compared to high cost
state of the art methods. In this work, we focus on a refinement of the road
segmentation utilising a Conditional Random Field (CRF).The refinement
procedure is limited to the super pixels touching the predicted road boundary
to keep the additional computational effort low. Reducing the input to the
super pixel domain allows the CNNs structure to stay small and efficient to
compute while keeping the advantage of convolutional layers and makes them
eligible for ADAS. Applying CRF compensate the trade off between accuracy and
computational efficiency. The proposed system obtained comparable performance
among the top performing algorithms on the KITTI road benchmark and its fast
inference makes it particularly suitable for realtime applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perfusion imaging in deep prostate cancer detection from mp-MRI: can we take advantage of it?. (arXiv:2207.02854v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02854">
<div class="article-summary-box-inner">
<span><p>To our knowledge, all deep computer-aided detection and diagnosis (CAD)
systems for prostate cancer (PCa) detection consider bi-parametric magnetic
resonance imaging (bp-MRI) only, including T2w and ADC sequences while
excluding the 4D perfusion sequence,which is however part of standard clinical
protocols for this diagnostic task. In this paper, we question strategies to
integrate information from perfusion imaging in deep neural architectures. To
do so, we evaluate several ways to encode the perfusion information in a U-Net
like architecture, also considering early versus mid fusion strategies. We
compare performance of multiparametric MRI (mp-MRI) models with the baseline
bp-MRI model based on a private dataset of 219 mp-MRI exams. Perfusion maps
derived from dynamic contrast enhanced MR exams are shown to positively impact
segmentation and grading performance of PCa lesions, especially the 3D MR
volume corresponding to the maximum slope of the wash-in curve as well as Tmax
perfusion maps. The latter mp-MRI models indeed outperform the bp-MRI one
whatever the fusion strategy, with Cohen's kappa score of 0.318$\pm$0.019 for
the bp-MRI model and 0.378 $\pm$ 0.033 for the model including the maximum
slope with a mid fusion strategy, also achieving competitive Cohen's kappa
score compared to state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physical Interaction and Manipulation of the Environment using Aerial Robots. (arXiv:2207.02856v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02856">
<div class="article-summary-box-inner">
<span><p>The physical interaction of aerial robots with their environment has
countless potential applications and is an emerging area with many open
challenges. Fully-actuated multirotors have been introduced to tackle some of
these challenges. They provide complete control over position and orientation
and eliminate the need for attaching a multi-DoF manipulation arm to the robot.
However, there are many open problems before they can be used in real-world
applications. Researchers have introduced some methods for physical interaction
in limited settings. Their experiments primarily use prototype-level software
without an efficient path to integration with real-world applications. We
describe a new cost-effective solution for integrating these robots with the
existing software and hardware flight systems for real-world applications and
expand it to physical interaction applications. On the other hand, the existing
control approaches for fully-actuated robots assume conservative limits for the
thrusts and moments available to the robot. Using conservative assumptions for
these already-inefficient robots makes their interactions even less optimal and
may even result in many feasible physical interaction applications becoming
infeasible. This work proposes a real-time method for estimating the complete
set of instantaneously available forces and moments that robots can use to
optimize their physical interaction performance. Finally, many real-world
applications where aerial robots can improve the existing manual solutions deal
with deformable objects. However, the perception and planning for their
manipulation is still challenging. This research explores how aerial physical
interaction can be extended to deformable objects. It provides a detection
method suitable for manipulating deformable one-dimensional objects and
introduces a new perspective on planning the manipulation of these objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Humans Social Relationship Classification during Accompaniment. (arXiv:2207.02890v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02890">
<div class="article-summary-box-inner">
<span><p>This paper presents the design of deep learning architectures which allow to
classify the social relationship existing between two people who are walking in
a side-by-side formation into four possible categories --colleagues, couple,
family or friendship. The models are developed using Neural Networks or
Recurrent Neural Networks to achieve the classification and are trained and
evaluated using a database of readings obtained from humans performing an
accompaniment process in an urban environment. The best achieved model
accomplishes a relatively good accuracy in the classification problem and its
results enhance partially the outcomes from a previous study [1]. Furthermore,
the model proposed shows its future potential to improve its efficiency and to
be implemented in a real robot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Transparency in Dermatology Image Datasets with Skin Tone Annotations by Experts, Crowds, and an Algorithm. (arXiv:2207.02942v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02942">
<div class="article-summary-box-inner">
<span><p>While artificial intelligence (AI) holds promise for supporting healthcare
providers and improving the accuracy of medical diagnoses, a lack of
transparency in the composition of datasets exposes AI models to the
possibility of unintentional and avoidable mistakes. In particular, public and
private image datasets of dermatological conditions rarely include information
on skin color. As a start towards increasing transparency, AI researchers have
appropriated the use of the Fitzpatrick skin type (FST) from a measure of
patient photosensitivity to a measure for estimating skin tone in algorithmic
audits of computer vision applications including facial recognition and
dermatology diagnosis. In order to understand the variability of estimated FST
annotations on images, we compare several FST annotation methods on a diverse
set of 460 images of skin conditions from both textbooks and online dermatology
atlases. We find the inter-rater reliability between three board-certified
dermatologists is comparable to the inter-rater reliability between the
board-certified dermatologists and two crowdsourcing methods. In contrast, we
find that the Individual Typology Angle converted to FST (ITA-FST) method
produces annotations that are significantly less correlated with the experts'
annotations than the experts' annotations are correlated with each other. These
results demonstrate that algorithms based on ITA-FST are not reliable for
annotating large-scale image datasets, but human-centered, crowd-based
protocols can reliably add skin type transparency to dermatology datasets.
Furthermore, we introduce the concept of dynamic consensus protocols with
tunable parameters including expert review that increase the visibility of
crowdwork and provide guidance for future crowdsourced annotations of large
image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual staining of defocused autofluorescence images of unlabeled tissue using deep neural networks. (arXiv:2207.02946v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02946">
<div class="article-summary-box-inner">
<span><p>Deep learning-based virtual staining was developed to introduce image
contrast to label-free tissue sections, digitally matching the histological
staining, which is time-consuming, labor-intensive, and destructive to tissue.
Standard virtual staining requires high autofocusing precision during the whole
slide imaging of label-free tissue, which consumes a significant portion of the
total imaging time and can lead to tissue photodamage. Here, we introduce a
fast virtual staining framework that can stain defocused autofluorescence
images of unlabeled tissue, achieving equivalent performance to virtual
staining of in-focus label-free images, also saving significant imaging time by
lowering the microscope's autofocusing precision. This framework incorporates a
virtual-autofocusing neural network to digitally refocus the defocused images
and then transforms the refocused images into virtually stained images using a
successive network. These cascaded networks form a collaborative inference
scheme: the virtual staining model regularizes the virtual-autofocusing network
through a style loss during the training. To demonstrate the efficacy of this
framework, we trained and blindly tested these networks using human lung
tissue. Using 4x fewer focus points with 2x lower focusing precision, we
successfully transformed the coarsely-focused autofluorescence images into
high-quality virtually stained H&amp;E images, matching the standard virtual
staining framework that used finely-focused autofluorescence input images.
Without sacrificing the staining quality, this framework decreases the total
image acquisition time needed for virtual staining of a label-free whole-slide
image (WSI) by ~32%, together with a ~89% decrease in the autofocusing time,
and has the potential to eliminate the laborious and costly histochemical
staining process in pathology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-aware Self-supervised Learning for Medical Images Using Graph Neural Network. (arXiv:2207.02957v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02957">
<div class="article-summary-box-inner">
<span><p>Although self-supervised learning enables us to bootstrap the training by
exploiting unlabeled data, the generic self-supervised methods for natural
images do not sufficiently incorporate the context. For medical images, a
desirable method should be sensitive enough to detect deviation from
normal-appearing tissue of each anatomical region; here, anatomy is the
context. We introduce a novel approach with two levels of self-supervised
representation learning objectives: one on the regional anatomical level and
another on the patient-level. We use graph neural networks to incorporate the
relationship between different anatomical regions. The structure of the graph
is informed by anatomical correspondences between each patient and an
anatomical atlas. In addition, the graph representation has the advantage of
handling any arbitrarily sized image in full resolution. Experiments on
large-scale Computer Tomography (CT) datasets of lung images show that our
approach compares favorably to baseline methods that do not account for the
context. We use the learned embedding for staging lung tissue abnormalities
related to COVID-19.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SphereVLAD++: Attention-based and Signal-enhanced Viewpoint Invariant Descriptor. (arXiv:2207.02958v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02958">
<div class="article-summary-box-inner">
<span><p>LiDAR-based localization approach is a fundamental module for large-scale
navigation tasks, such as last-mile delivery and autonomous driving, and
localization robustness highly relies on viewpoints and 3D feature extraction.
Our previous work provides a viewpoint-invariant descriptor to deal with
viewpoint differences; however, the global descriptor suffers from a low
signal-noise ratio in unsupervised clustering, reducing the distinguishable
feature extraction ability. We develop SphereVLAD++, an attention-enhanced
viewpoint invariant place recognition method in this work. SphereVLAD++
projects the point cloud on the spherical perspective for each unique area and
captures the contextual connections between local features and their
dependencies with global 3D geometry distribution. In return, clustered
elements within the global descriptor are conditioned on local and global
geometries and support the original viewpoint-invariant property of SphereVLAD.
In the experiments, we evaluated the localization performance of SphereVLAD++
on both public KITTI360 datasets and self-generated datasets from the city of
Pittsburgh. The experiment results show that SphereVLAD++ outperforms all
relative state-of-the-art 3D place recognition methods under small or even
totally reversed viewpoint differences and shows 0.69% and 15.81% successful
retrieval rates with better than the second best. Low computation requirements
and high time efficiency also help its application for low-cost robots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Weaknesses of Adversarial Camouflage in Overhead Imagery. (arXiv:2207.02963v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02963">
<div class="article-summary-box-inner">
<span><p>Machine learning is increasingly critical for analysis of the ever-growing
corpora of overhead imagery. Advanced computer vision object detection
techniques have demonstrated great success in identifying objects of interest
such as ships, automobiles, and aircraft from satellite and drone imagery. Yet
relying on computer vision opens up significant vulnerabilities, namely, the
susceptibility of object detection algorithms to adversarial attacks. In this
paper we explore the efficacy and drawbacks of adversarial camouflage in an
overhead imagery context. While a number of recent papers have demonstrated the
ability to reliably fool deep learning classifiers and object detectors with
adversarial patches, most of this work has been performed on relatively uniform
datasets and only a single class of objects. In this work we utilize the
VisDrone dataset, which has a large range of perspectives and object sizes. We
explore four different object classes: bus, car, truck, van. We build a library
of 24 adversarial patches to disguise these objects, and introduce a patch
translucency variable to our patches. The translucency (or alpha value) of the
patches is highly correlated to their efficacy. Further, we show that while
adversarial patches may fool object detectors, the presence of such patches is
often easily uncovered, with patches on average 24% more detectable than the
objects the patches were meant to hide. This raises the question of whether
such patches truly constitute camouflage. Source code is available at
https://github.com/IQTLabs/camolo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Network Binarization via Contrastive Learning. (arXiv:2207.02970v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02970">
<div class="article-summary-box-inner">
<span><p>Neural network binarization accelerates deep models by quantizing their
weights and activations into 1-bit. However, there is still a huge performance
gap between Binary Neural Networks (BNNs) and their full-precision (FP)
counterparts. As the quantization error caused by weights binarization has been
reduced in earlier works, the activations binarization becomes the major
obstacle for further improvement of the accuracy. BNN characterises a unique
and interesting structure, where the binary and latent FP activations exist in
the same forward pass (\textit{i.e.} $\text{Binarize}(\mathbf{a}_F) =
\mathbf{a}_B$). To mitigate the information degradation caused by the
binarization operation from FP to binary activations, we establish a novel
contrastive learning framework while training BNNs through the lens of Mutual
Information (MI) maximization. MI is introduced as the metric to measure the
information shared between binary and FP activations, which assists
binarization with contrastive learning. Specifically, the representation
ability of the BNNs is greatly strengthened via pulling the positive pairs with
binary and FP activations from the same input samples, as well as pushing
negative pairs from different samples (the number of negative pairs can be
exponentially large). This benefits the downstream tasks, not only
classification but also segmentation and depth estimation,~\textit{etc}. The
experimental results show that our method can be implemented as a pile-up
module on existing state-of-the-art binarization methods and can remarkably
improve the performance over them on CIFAR-10/100 and ImageNet, in addition to
the great generalization ability on NYUD-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Human Pose Estimation in Art-historical Images. (arXiv:2207.02976v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02976">
<div class="article-summary-box-inner">
<span><p>Gesture as \enquote*{language} of non-verbal communication has been
theoretically established since the 17th century. However, its relevance for
the visual arts has been expressed only sporadically. This may be primarily due
to the sheer overwhelming amount of data that traditionally had to be processed
by hand. With the steady progress of digitization, though, a growing number of
historical artifacts have been indexed and made available to the public,
creating a need for automatic retrieval of art-historical motifs with similar
body constellations or poses. Since the domain of art differs significantly
from existing real-world data sets for human pose estimation due to its style
variance, this presents new challenges. In this paper, we propose a novel
approach to estimate human poses in art-historical images. In contrast to
previous work that attempts to bridge the domain gap with pre-trained models or
through style transfer, we suggest semi-supervised learning for both object and
keypoint detection. Furthermore, we introduce a novel domain-specific art data
set that includes both bounding box and keypoint annotations of human figures.
Our approach achieves significantly better results than methods that use
pre-trained models or style transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orthogonal Matrix Retrieval with Spatial Consensus for 3D Unknown-View Tomography. (arXiv:2207.02985v1 [math.OC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02985">
<div class="article-summary-box-inner">
<span><p>Unknown-view tomography (UVT) reconstructs a 3D density map from its 2D
projections at unknown, random orientations. A line of work starting with Kam
(1980) employs the method of moments (MoM) with rotation-invariant Fourier
features to solve UVT in the frequency domain, assuming that the orientations
are uniformly distributed. This line of work includes the recent orthogonal
matrix retrieval (OMR) approaches based on matrix factorization, which, while
elegant, either require side information about the density that is not
available, or fail to be sufficiently robust. In order for OMR to break free
from those restrictions, we propose to jointly recover the density map and the
orthogonal matrices by requiring that they be mutually consistent. We
regularize the resulting non-convex optimization problem by a denoised
reference projection and a nonnegativity constraint. This is enabled by the new
closed-form expressions for spatial autocorrelation features. Further, we
design an easy-to-compute initial density map which effectively mitigates the
non-convexity of the reconstruction problem. Experimental results show that the
proposed OMR with spatial consensus is more robust and performs significantly
better than the previous state-of-the-art OMR approach in the typical low-SNR
scenario of 3D UVT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaiT: Leverage Attention Masks for More Efficient Image Transformers. (arXiv:2207.03006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03006">
<div class="article-summary-box-inner">
<span><p>Though image transformers have shown competitive results with convolutional
neural networks in computer vision tasks, lacking inductive biases such as
locality still poses problems in terms of model efficiency especially for
embedded applications. In this work, we address this issue by introducing
attention masks to incorporate spatial locality into self-attention heads.
Local dependencies are captured efficiently with masked attention heads along
with global dependencies captured by unmasked attention heads. With Masked
attention image Transformer - MaiT, top-1 accuracy increases by up to 1.7%
compared to CaiT with fewer parameters and FLOPs, and the throughput improves
by up to 1.5X compared to Swin. Encoding locality with attention masks is model
agnostic, and thus it applies to monolithic, hierarchical, or other novel
transformer architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial Shape Similarity via Alignment of Multi-Metric Hamiltonian Spectra. (arXiv:2207.03018v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03018">
<div class="article-summary-box-inner">
<span><p>Evaluating the similarity of non-rigid shapes with significant partiality is
a fundamental task in numerous computer vision applications. Here, we propose a
novel axiomatic method to match similar regions across shapes. Matching similar
regions is formulated as the alignment of the spectra of operators closely
related to the Laplace-Beltrami operator (LBO). The main novelty of the
proposed approach is the consideration of differential operators defined on a
manifold with multiple metrics. The choice of a metric relates to fundamental
shape properties while considering the same manifold under different metrics
can thus be viewed as analyzing the underlying manifold from different
perspectives. Specifically, we examine the scale-invariant metric and the
corresponding scale-invariant Laplace-Beltrami operator (SI-LBO) along with the
regular metric and the regular LBO. We demonstrate that the scale-invariant
metric emphasizes the locations of important semantic features in articulated
shapes. A truncated spectrum of the SI-LBO consequently better captures locally
curved regions and complements the global information encapsulated in the
truncated spectrum of the regular LBO. We show that matching these dual spectra
outperforms competing axiomatic frameworks when tested on standard benchmarks.
We introduced a new dataset and compare the proposed method with the
state-of-the-art learning based approach in a cross-database configuration.
Specifically, we show that, when trained on one data set and tested on another,
the proposed axiomatic approach which does not involve training, outperforms
the deep learning alternative.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03038">
<div class="article-summary-box-inner">
<span><p>This paper describes our champion solution for the CVPR2022 Generic Event
Boundary Captioning (GEBC) competition. GEBC requires the captioning model to
have a comprehension of instantaneous status changes around the given video
boundary, which makes it much more challenging than conventional video
captioning task. In this paper, a Dual-Stream Transformer with improvements on
both video content encoding and captions generation is proposed: (1) We utilize
three pre-trained models to extract the video features from different
granularities. Moreover, we exploit the types of boundary as hints to help the
model generate captions. (2) We particularly design an model, termed as
Dual-Stream Transformer, to learn discriminative representations for boundary
captioning. (3) Towards generating content-relevant and human-like captions, we
improve the description quality by designing a word-level ensemble strategy.
The promising results on the GEBC test split demonstrate the efficacy of our
proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformers: State of the Art and Research Challenges. (arXiv:2207.03041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03041">
<div class="article-summary-box-inner">
<span><p>Transformers have achieved great success in natural language processing. Due
to the powerful capability of self-attention mechanism in transformers,
researchers develop the vision transformers for a variety of computer vision
tasks, such as image recognition, object detection, image segmentation, pose
estimation, and 3D reconstruction. This paper presents a comprehensive overview
of the literature on different architecture designs and training tricks
(including self-supervised learning) for vision transformers. Our goal is to
provide a systematic review with the open research opportunities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised RF Signal Representation Learning for NextG Signal Classification with Deep Learning. (arXiv:2207.03046v1 [cs.NI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03046">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) finds rich applications in the wireless domain to improve
spectrum awareness. Typically, the DL models are either randomly initialized
following a statistical distribution or pretrained on tasks from other data
domains such as computer vision (in the form of transfer learning) without
accounting for the unique characteristics of wireless signals. Self-supervised
learning enables the learning of useful representations from Radio Frequency
(RF) signals themselves even when only limited training data samples with
labels are available. We present the first self-supervised RF signal
representation learning model and apply it to the automatic modulation
recognition (AMR) task by specifically formulating a set of transformations to
capture the wireless signal characteristics. We show that the sample efficiency
(the number of labeled samples required to achieve a certain accuracy
performance) of AMR can be significantly increased (almost an order of
magnitude) by learning signal representations with self-supervised learning.
This translates to substantial time and cost savings. Furthermore,
self-supervised learning increases the model accuracy compared to the
state-of-the-art DL methods and maintains high accuracy even when a small set
of training data samples is used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-image Defocus Deblurring by Integration of Defocus Map Prediction Tracing the Inverse Problem Computation. (arXiv:2207.03047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03047">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problem in defocus image deblurring. Previous
classical methods follow two-steps approaches, i.e., first defocus map
estimation and then the non-blind deblurring. In the era of deep learning, some
researchers have tried to address these two problems by CNN. However, the
simple concatenation of defocus map, which represents the blur level, leads to
suboptimal performance. Considering the spatial variant property of the defocus
blur and the blur level indicated in the defocus map, we employ the defocus map
as conditional guidance to adjust the features from the input blurring images
instead of simple concatenation. Then we propose a simple but effective network
with spatial modulation based on the defocus map. To achieve this, we design a
network consisting of three sub-networks, including the defocus map estimation
network, a condition network that encodes the defocus map into condition
features, and the defocus deblurring network that performs spatially dynamic
modulation based on the condition features. Moreover, the spatially dynamic
modulation is based on an affine transform function to adjust the features from
the input blurry images. Experimental results show that our method can achieve
better quantitative and qualitative evaluation performance than the existing
state-of-the-art methods on the commonly used public test datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AV-Gaze: A Study on the Effectiveness of Audio Guided Visual Attention Estimation for Non-Profilic Faces. (arXiv:2207.03048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03048">
<div class="article-summary-box-inner">
<span><p>In challenging real-life conditions such as extreme head-pose, occlusions,
and low-resolution images where the visual information fails to estimate visual
attention/gaze direction, audio signals could provide important and
complementary information. In this paper, we explore if audio-guided coarse
head-pose can further enhance visual attention estimation performance for
non-prolific faces. Since it is difficult to annotate audio signals for
estimating the head-pose of the speaker, we use off-the-shelf state-of-the-art
models to facilitate cross-modal weak-supervision. During the training phase,
the framework learns complementary information from synchronized audio-visual
modality. Our model can utilize any of the available modalities i.e. audio,
visual or audio-visual for task-specific inference. It is interesting to note
that, when AV-Gaze is tested on benchmark datasets with these specific
modalities, it achieves competitive results on multiple datasets, while being
highly adaptive towards challenging scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Lung Nodule Detection in Chest Radiographs with a Dual Head Network. (arXiv:2207.03050v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03050">
<div class="article-summary-box-inner">
<span><p>Lung nodules can be an alarming precursor to potential lung cancer. Missed
nodule detections during chest radiograph analysis remains a common challenge
among thoracic radiologists. In this work, we present a multi-task lung nodule
detection algorithm for chest radiograph analysis. Unlike past approaches, our
algorithm predicts a global-level label indicating nodule presence along with
local-level labels predicting nodule locations using a Dual Head Network (DHN).
We demonstrate the favorable nodule detection performance that our multi-task
formulation yields in comparison to conventional methods. In addition, we
introduce a novel Dual Head Augmentation (DHA) strategy tailored for DHN, and
we demonstrate its significance in further enhancing global and local nodule
predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Rotation Correction without Angle Prior. (arXiv:2207.03054v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03054">
<div class="article-summary-box-inner">
<span><p>Not everybody can be equipped with professional photography skills and
sufficient shooting time, and there can be some tilts in the captured images
occasionally. In this paper, we propose a new and practical task, named
Rotation Correction, to automatically correct the tilt with high content
fidelity in the condition that the rotated angle is unknown. This task can be
easily integrated into image editing applications, allowing users to correct
the rotated images without any manual operations. To this end, we leverage a
neural network to predict the optical flows that can warp the tilted images to
be perceptually horizontal. Nevertheless, the pixel-wise optical flow
estimation from a single image is severely unstable, especially in large-angle
tilted images. To enhance its robustness, we propose a simple but effective
prediction strategy to form a robust elastic warp. Particularly, we first
regress the mesh deformation that can be transformed into robust initial
optical flows. Then we estimate residual optical flows to facilitate our
network the flexibility of pixel-wise deformation, further correcting the
details of the tilted images. To establish an evaluation benchmark and train
the learning framework, a comprehensive rotation correction dataset is
presented with a large diversity in scenes and rotated angles. Extensive
experiments demonstrate that even in the absence of the angle prior, our
algorithm can outperform other state-of-the-art solutions requiring this prior.
The codes and dataset will be available at
https://github.com/nie-lang/RotationCorrection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to the Basics: Revisiting Out-of-Distribution Detection Baselines. (arXiv:2207.03061v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03061">
<div class="article-summary-box-inner">
<span><p>We study simple methods for out-of-distribution (OOD) image detection that
are compatible with any already trained classifier, relying on only its
predictions or learned representations. Evaluating the OOD detection
performance of various methods when utilized with ResNet-50 and Swin
Transformer models, we find methods that solely consider the model's
predictions can be easily outperformed by also considering the learned
representations. Based on our analysis, we advocate for a dead-simple approach
that has been neglected in other studies: simply flag as OOD images whose
average distance to their K nearest neighbors is large (in the representation
space of an image classifier trained on the in-distribution data).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shadow-Background-Noise 3D Spatial Decomposition Using Sparse Low-Rank Gaussian Properties for Video-SAR Moving Target Shadow Enhancement. (arXiv:2207.03064v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03064">
<div class="article-summary-box-inner">
<span><p>Moving target shadows among video synthetic aperture radar (Video-SAR) images
are always interfered by low scattering backgrounds and cluttered noises,
causing poor moving target shadow detection-tracking performance. To solve this
problem, this letter proposes a shadow-background-noise 3D spatial
de-composition method named SBN-3D-SD to boost shadow saliency for better
Video-SAR moving target shadow detection-tracking performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning from Spatio-Temporal Mixed Skeleton Sequences for Self-Supervised Skeleton-Based Action Recognition. (arXiv:2207.03065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03065">
<div class="article-summary-box-inner">
<span><p>Self-supervised skeleton-based action recognition with contrastive learning
has attracted much attention. Recent literature shows that data augmentation
and large sets of contrastive pairs are crucial in learning such
representations. In this paper, we found that directly extending contrastive
pairs based on normal augmentations brings limited returns in terms of
performance, because the contribution of contrastive pairs from the normal data
augmentation to the loss get smaller as training progresses. Therefore, we
delve into hard contrastive pairs for contrastive learning. Motivated by the
success of mixing augmentation strategy which improves the performance of many
tasks by synthesizing novel samples, we propose SkeleMixCLR: a contrastive
learning framework with a spatio-temporal skeleton mixing augmentation
(SkeleMix) to complement current contrastive learning approaches by providing
hard contrastive samples. First, SkeleMix utilizes the topological information
of skeleton data to mix two skeleton sequences by randomly combing the cropped
skeleton fragments (the trimmed view) with the remaining skeleton sequences
(the truncated view). Second, a spatio-temporal mask pooling is applied to
separate these two views at the feature level. Third, we extend contrastive
pairs with these two views. SkeleMixCLR leverages the trimmed and truncated
views to provide abundant hard contrastive pairs since they involve some
context information from each other due to the graph convolution operations,
which allows the model to learn better motion representations for action
recognition. Extensive experiments on NTU-RGB+D, NTU120-RGB+D, and PKU-MMD
datasets show that SkeleMixCLR achieves state-of-the-art performance. Codes are
available at https://github.com/czhaneva/SkeleMixCLR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes for Automatic Reconstruction of Pulmonary Segments. (arXiv:2207.03078v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03078">
<div class="article-summary-box-inner">
<span><p>3D reconstruction of pulmonary segments plays an important role in surgical
treatment planning of lung cancer, which facilitates preservation of pulmonary
function and helps ensure low recurrence rates. However, automatic
reconstruction of pulmonary segments remains unexplored in the era of deep
learning. In this paper, we investigate what makes for automatic reconstruction
of pulmonary segments. First and foremost, we formulate, clinically and
geometrically, the anatomical definitions of pulmonary segments, and propose
evaluation metrics adhering to these definitions. Second, we propose ImPulSe
(Implicit Pulmonary Segment), a deep implicit surface model designed for
pulmonary segment reconstruction. The automatic reconstruction of pulmonary
segments by ImPulSe is accurate in metrics and visually appealing. Compared
with canonical segmentation methods, ImPulSe outputs continuous predictions of
arbitrary resolutions with higher training efficiency and fewer parameters.
Lastly, we experiment with different network inputs to analyze what matters in
the task of pulmonary segment reconstruction. Our code is available at
https://github.com/M3DV/ImPulSe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning. (arXiv:2207.03081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03081">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a multi-objective camera ISP framework that
utilizes Deep Reinforcement Learning (DRL) and camera ISP toolbox that consist
of network-based and conventional ISP tools. The proposed DRL-based camera ISP
framework iteratively selects a proper tool from the toolbox and applies it to
the image to maximize a given vision task-specific reward function. For this
purpose, we implement total 51 ISP tools that include exposure correction,
color-and-tone correction, white balance, sharpening, denoising, and the
others. We also propose an efficient DRL network architecture that can extract
the various aspects of an image and make a rigid mapping relationship between
images and a large number of actions. Our proposed DRL-based ISP framework
effectively improves the image quality according to each vision task such as
RAW-to-RGB image restoration, 2D object detection, and monocular depth
estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptation of Surgical Activity Recognition Models Across Operating Rooms. (arXiv:2207.03083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03083">
<div class="article-summary-box-inner">
<span><p>Automatic surgical activity recognition enables more intelligent surgical
devices and a more efficient workflow. Integration of such technology in new
operating rooms has the potential to improve care delivery to patients and
decrease costs. Recent works have achieved a promising performance on surgical
activity recognition; however, the lack of generalizability of these models is
one of the critical barriers to the wide-scale adoption of this technology. In
this work, we study the generalizability of surgical activity recognition
models across operating rooms. We propose a new domain adaptation method to
improve the performance of the surgical activity recognition model in a new
operating room for which we only have unlabeled videos. Our approach generates
pseudo labels for unlabeled video clips that it is confident about and trains
the model on the augmented version of the clips. We extend our method to a
semi-supervised domain adaptation setting where a small portion of the target
domain is also labeled. In our experiments, our proposed method consistently
outperforms the baselines on a dataset of more than 480 long surgical videos
collected from two operating rooms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022: Team HNU-FPV Technical Report. (arXiv:2207.03095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03095">
<div class="article-summary-box-inner">
<span><p>In this report, we present the technical details of our submission to the
2022 EPIC-Kitchens Unsupervised Domain Adaptation (UDA) Challenge. Existing UDA
methods align the global features extracted from the whole video clips across
the source and target domains but suffer from the spatial redundancy of feature
matching in video recognition. Motivated by the observation that in most cases
a small image region in each video frame can be informative enough for the
action recognition task, we propose to exploit informative image regions to
perform efficient domain alignment. Specifically, we first use lightweight CNNs
to extract the global information of the input two-stream video frames and
select the informative image patches by a differentiable interpolation-based
selection strategy. Then the global information from videos frames and local
information from image patches are processed by an existing video adaptation
method, i.e., TA3N, in order to perform feature alignment for the source domain
and the target domain. Our method (without model ensemble) ranks 4th among this
year's teams on the test set of EPIC-KITCHENS-100.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Self-supervised Neural Network for Liver $T_{1\rho}$ Mapping with Relaxation Constraint. (arXiv:2207.03105v1 [q-bio.TO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03105">
<div class="article-summary-box-inner">
<span><p>$T_{1\rho}$ mapping is a promising quantitative MRI technique for the
non-invasive assessment of tissue properties. Learning-based approaches can map
$T_{1\rho}$ from a reduced number of $T_{1\rho}$ weighted images, but requires
significant amounts of high quality training data. Moreover, existing methods
do not provide the confidence level of the $T_{1\rho}$ estimation. To address
these problems, we proposed a self-supervised learning neural network that
learns a $T_{1\rho}$ mapping using the relaxation constraint in the learning
process. Epistemic uncertainty and aleatoric uncertainty are modelled for the
$T_{1\rho}$ quantification network to provide a Bayesian confidence estimation
of the $T_{1\rho}$ mapping. The uncertainty estimation can also regularize the
model to prevent it from learning imperfect data. We conducted experiments on
$T_{1\rho}$ data collected from 52 patients with non-alcoholic fatty liver
disease. The results showed that our method outperformed the existing methods
for $T_{1\rho}$ quantification of the liver using as few as two
$T_{1\rho}$-weighted images. Our uncertainty estimation provided a feasible way
of modelling the confidence of the self-supervised learning based $T_{1\rho}$
estimation, which is consistent with the reality in liver $T_{1\rho}$ imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Surfel Prediction for Self-Supervised Point Cloud Learning. (arXiv:2207.03111v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03111">
<div class="article-summary-box-inner">
<span><p>Masked auto-encoding is a popular and effective self-supervised learning
approach to point cloud learning. However, most of the existing methods
reconstruct only the masked points and overlook the local geometry information,
which is also important to understand the point cloud data. In this work, we
make the first attempt, to the best of our knowledge, to consider the local
geometry information explicitly into the masked auto-encoding, and propose a
novel Masked Surfel Prediction (MaskSurf) method. Specifically, given the input
point cloud masked at a high ratio, we learn a transformer-based
encoder-decoder network to estimate the underlying masked surfels by
simultaneously predicting the surfel positions (i.e., points) and per-surfel
orientations (i.e., normals). The predictions of points and normals are
supervised by the Chamfer Distance and a newly introduced Position-Indexed
Normal Distance in a set-to-set manner. Our MaskSurf is validated on six
downstream tasks under three fine-tuning strategies. In particular, MaskSurf
outperforms its closest competitor, Point-MAE, by 1.2\% on the real-world
dataset of ScanObjectNN under the OBJ-BG setting, justifying the advantages of
masked surfel prediction over masked point cloud reconstruction. Codes will be
available at https://github.com/YBZh/MaskSurf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design of Human Machine Interface through vision-based low-cost Hand Gesture Recognition system based on deep CNN with transfer-learning approach. (arXiv:2207.03112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03112">
<div class="article-summary-box-inner">
<span><p>In this work, a real-time hand gesture recognition system-based
human-computer interface (HCI) is presented. The system consists of six stages:
(1) hand detection, (2) gesture segmentation, (3) use of six pre-trained CNN
models by using the transfer-learning method, (4) building an interactive
human-machine interface, (5) development of a gesture-controlled virtual mouse,
(6) use of Kalman filter to estimate the hand position, based on that the
smoothness of the motion of pointer is improved. Six pre-trained convolutional
neural network (CNN) models (VGG16, VGG19, ResNet50, ResNet101, Inception-V1,
and MobileNet-V1) have been used to classify hand gesture images. Three
multi-class datasets (two publicly and one custom) have been used to evaluate
the model performances. Considering the models' performances, it has been
observed that Inception-V1 has significantly shown a better classification
performance compared to the other five pre-trained models in terms of accuracy,
precision, recall, and F-score values. The gesture recognition system is
expanded and used to control multimedia applications (like VLC player, audio
player, file management, playing 2D Super-Mario-Bros game, etc.) with different
customized gesture commands in real-time scenarios. The average speed of this
system has reached 35 fps (frame per seconds), which meets the requirements for
the real-time scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Vision-to-Geometry Knowledge Transfer for 3D Point Cloud Shape Analysis. (arXiv:2207.03128v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03128">
<div class="article-summary-box-inner">
<span><p>As two fundamental representation modalities of 3D objects, 2D multi-view
images and 3D point clouds reflect shape information from different aspects of
visual appearances and geometric structures. Unlike deep learning-based 2D
multi-view image modeling, which demonstrates leading performances in various
3D shape analysis tasks, 3D point cloud-based geometric modeling still suffers
from insufficient learning capacity. In this paper, we innovatively construct a
unified cross-modal knowledge transfer framework, which distills discriminative
visual descriptors of 2D images into geometric descriptors of 3D point clouds.
Technically, under a classic teacher-student learning paradigm, we propose
multi-view vision-to-geometry distillation, consisting of a deep 2D image
encoder as teacher and a deep 3D point cloud encoder as student. To achieve
heterogeneous feature alignment, we further propose visibility-aware feature
projection, through which per-point embeddings can be aggregated into
multi-view geometric descriptors. Extensive experiments on 3D shape
classification, part segmentation, and unsupervised learning validate the
superiority of our method. We will make the code and data publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style Interleaved Learning for Generalizable Person Re-identification. (arXiv:2207.03132v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03132">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) for person re-identification (ReID) is a
challenging problem, as there is no access to target domain data permitted
during the training process. Most existing DG ReID methods employ the same
features for the updating of the feature extractor and classifier parameters.
This common practice causes the model to overfit to existing feature styles in
the source domain, resulting in sub-optimal generalization ability on target
domains even if meta-learning is used. To solve this problem, we propose a
novel style interleaved learning framework. Unlike conventional learning
strategies, interleaved learning incorporates two forward propagations and one
backward propagation for each iteration. We employ the features of interleaved
styles to update the feature extractor and classifiers using different forward
propagations, which helps the model avoid overfitting to certain domain styles.
In order to fully explore the advantages of style interleaved learning, we
further propose a novel feature stylization approach to diversify feature
styles. This approach not only mixes the feature styles of multiple training
samples, but also samples new and meaningful feature styles from batch-level
style distribution. Extensive experimental results show that our model
consistently outperforms state-of-the-art methods on large-scale benchmarks for
DG ReID, yielding clear advantages in computational efficiency. Code is
available at https://github.com/WentaoTan/Interleaved-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Few-Shot Image Classification Using Machine- and User-Generated Natural Language Descriptions. (arXiv:2207.03133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03133">
<div class="article-summary-box-inner">
<span><p>Humans can obtain the knowledge of novel visual concepts from language
descriptions, and we thus use the few-shot image classification task to
investigate whether a machine learning model can have this capability. Our
proposed model, LIDE (Learning from Image and DEscription), has a text decoder
to generate the descriptions and a text encoder to obtain the text
representations of machine- or user-generated descriptions. We confirmed that
LIDE with machine-generated descriptions outperformed baseline models.
Moreover, the performance was improved further with high-quality user-generated
descriptions. The generated descriptions can be viewed as the explanations of
the model's predictions, and we observed that such explanations were consistent
with prediction results. We also investigated why the language description
improved the few-shot image classification performance by comparing the image
representations and the text representations in the feature spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Velocity Estimation for Automotive Radar Object Detection Networks. (arXiv:2207.03146v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03146">
<div class="article-summary-box-inner">
<span><p>This paper presents a method to learn the Cartesian velocity of objects using
an object detection network on automotive radar data. The proposed method is
self-supervised in terms of generating its own training signal for the
velocities. Labels are only required for single-frame, oriented bounding boxes
(OBBs). Labels for the Cartesian velocities or contiguous sequences, which are
expensive to obtain, are not required. The general idea is to pre-train an
object detection network without velocities using single-frame OBB labels, and
then exploit the network's OBB predictions on unlabelled data for velocity
training. In detail, the network's OBB predictions of the unlabelled frames are
updated to the timestamp of a labelled frame using the predicted velocities and
the distances between the updated OBBs of the unlabelled frame and the OBB
predictions of the labelled frame are used to generate a self-supervised
training signal for the velocities. The detection network architecture is
extended by a module to account for the temporal relation of multiple scans and
a module to represent the radars' radial velocity measurements explicitly. A
two-step approach of first training only OBB detection, followed by training
OBB detection and velocities is used. Further, a pre-training with
pseudo-labels generated from radar radial velocity measurements bootstraps the
self-supervised method of this paper. Experiments on the publicly available
nuScenes dataset show that the proposed method almost reaches the velocity
estimation performance of a fully supervised training, but does not require
expensive velocity labels. Furthermore, we outperform a baseline method which
uses only radial velocity measurements as labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastHebb: Scaling Hebbian Training of Deep Neural Networks to ImageNet Level. (arXiv:2207.03172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03172">
<div class="article-summary-box-inner">
<span><p>Learning algorithms for Deep Neural Networks are typically based on
supervised end-to-end Stochastic Gradient Descent (SGD) training with error
backpropagation (backprop). Backprop algorithms require a large number of
labelled training samples to achieve high performance. However, in many
realistic applications, even if there is plenty of image samples, very few of
them are labelled, and semi-supervised sample-efficient training strategies
have to be used. Hebbian learning represents a possible approach towards sample
efficient training; however, in current solutions, it does not scale well to
large datasets. In this paper, we present FastHebb, an efficient and scalable
solution for Hebbian learning which achieves higher efficiency by 1) merging
together update computation and aggregation over a batch of inputs, and 2)
leveraging efficient matrix multiplication algorithms on GPU. We validate our
approach on different computer vision benchmarks, in a semi-supervised learning
scenario. FastHebb outperforms previous solutions by up to 50 times in terms of
training speed, and notably, for the first time, we are able to bring Hebbian
algorithms to ImageNet scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformer: Towards Displacement Field Learning for Unsupervised Medical Image Registration. (arXiv:2207.03180v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03180">
<div class="article-summary-box-inner">
<span><p>Recently, deep-learning-based approaches have been widely studied for
deformable image registration task. However, most efforts directly map the
composite image representation to spatial transformation through the
convolutional neural network, ignoring its limited ability to capture spatial
correspondence. On the other hand, Transformer can better characterize the
spatial relationship with attention mechanism, its long-range dependency may be
harmful to the registration task, where voxels with too large distances are
unlikely to be corresponding pairs. In this study, we propose a novel Deformer
module along with a multi-scale framework for the deformable image registration
task. The Deformer module is designed to facilitate the mapping from image
representation to spatial transformation by formulating the displacement vector
prediction as the weighted summation of several bases. With the multi-scale
framework to predict the displacement fields in a coarse-to-fine manner,
superior performance can be achieved compared with traditional and
learning-based approaches. Comprehensive experiments on two public datasets are
conducted to demonstrate the effectiveness of the proposed Deformer module as
well as the multi-scale framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty of Atmospheric Motion Vectors by Sampling Tempered Posterior Distributions. (arXiv:2207.03182v1 [stat.ME])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03182">
<div class="article-summary-box-inner">
<span><p>Atmospheric motion vectors (AMVs) extracted from satellite imagery are the
only wind observations with good global coverage. They are important features
for feeding numerical weather prediction (NWP) models. Several Bayesian models
have been proposed to estimate AMVs. Although critical for correct assimilation
into NWP models, very few methods provide a thorough characterization of the
estimation errors. The difficulty of estimating errors stems from the
specificity of the posterior distribution, which is both very high dimensional,
and highly ill-conditioned due to a singular likelihood, which becomes critical
in particular in the case of missing data (unobserved pixels). This work
studies the evaluation of the expected error of AMVs using gradient-based
Markov Chain Monte Carlo (MCMC) algorithms. Our main contribution is to propose
a tempering strategy, which amounts to sampling a local approximation of the
joint posterior distribution of AMVs and image variables in the neighborhood of
a point estimate. In addition, we provide efficient preconditioning with the
covariance related to the prior family itself (fractional Brownian motion),
with possibly different hyper-parameters. From a theoretical point of view, we
show that under regularity assumptions, the family of tempered posterior
distributions converges in distribution as temperature decreases to an
{optimal} Gaussian approximation at a point estimate given by the Maximum A
Posteriori (MAP) log-density. From an empirical perspective, we evaluate the
proposed approach based on some quantitative Bayesian evaluation criteria. Our
numerical simulations performed on synthetic and real meteorological data
reveal a significant gain in terms of accuracy of the AMV point estimates and
of their associated expected error estimates, but also a substantial
acceleration in the convergence speed of the MCMC algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning of Music-Dance Representation through Explicit-Implicit Rhythm Synchronization. (arXiv:2207.03190v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03190">
<div class="article-summary-box-inner">
<span><p>Although audio-visual representation has been proved to be applicable in many
downstream tasks, the representation of dancing videos, which is more specific
and always accompanied by music with complex auditory contents, remains
challenging and uninvestigated. Considering the intrinsic alignment between the
cadent movement of dancer and music rhythm, we introduce MuDaR, a novel
Music-Dance Representation learning framework to perform the synchronization of
music and dance rhythms both in explicit and implicit ways. Specifically, we
derive the dance rhythms based on visual appearance and motion cues inspired by
the music rhythm analysis. Then the visual rhythms are temporally aligned with
the music counterparts, which are extracted by the amplitude of sound
intensity. Meanwhile, we exploit the implicit coherence of rhythms implied in
audio and visual streams by contrastive learning. The model learns the joint
embedding by predicting the temporal consistency between audio-visual pairs.
The music-dance representation, together with the capability of detecting audio
and visual rhythms, can further be applied to three downstream tasks: (a) dance
classification, (b) music-dance retrieval, and (c) music-dance retargeting.
Extensive experiments demonstrate that our proposed framework outperforms other
self-supervised methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCTS with Refinement for Proposals Selection Games in Scene Understanding. (arXiv:2207.03204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03204">
<div class="article-summary-box-inner">
<span><p>We propose a novel method applicable in many scene understanding problems
that adapts the Monte Carlo Tree Search (MCTS) algorithm, originally designed
to learn to play games of high-state complexity. From a generated pool of
proposals, our method jointly selects and optimizes proposals that minimize the
objective term. In our first application for floor plan reconstruction from
point clouds, our method selects and refines the room proposals, modelled as 2D
polygons, by optimizing on an objective function combining the fitness as
predicted by a deep network and regularizing terms on the room shapes. We also
introduce a novel differentiable method for rendering the polygonal shapes of
these proposals. Our evaluations on the recent and challenging Structured3D and
Floor-SP datasets show significant improvements over the state-of-the-art,
without imposing hard constraints nor assumptions on the floor plan
configurations. In our second application, we extend our approach to
reconstruct general 3D room layouts from a color image and obtain accurate room
layouts. We also show that our differentiable renderer can easily be extended
for rendering 3D planar polygons and polygon embeddings. Our method shows high
performance on the Matterport3D-Layout dataset, without introducing hard
constraints on room layout configurations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Stream Computer-Generated Image Detection Network Based On Channel Joint And Softpool. (arXiv:2207.03205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03205">
<div class="article-summary-box-inner">
<span><p>With the development of computer graphics technology, the images synthesized
by computer software become more and more closer to the photographs. While
computer graphics technology brings us a grand visual feast in the field of
games and movies, it may also be utilized by someone with bad intentions to
guide public opinions and cause political crisis or social unrest. Therefore,
how to distinguish the computer-generated graphics (CG) from the photographs
(PG) has become an important topic in the field of digital image forensics.
This paper proposes a dual stream convolutional neural network based on channel
joint and softpool. The proposed network architecture includes a residual
module for extracting image noise information and a joint channel information
extraction module for capturing the shallow semantic information of image. In
addition, we also design a residual structure to enhance feature extraction and
reduce the loss of information in residual flow. The joint channel information
extraction module can obtain the shallow semantic information of the input
image which can be used as the information supplement block of the residual
module. The whole network uses SoftPool to reduce the information loss of
down-sampling for image. Finally, we fuse the two flows to get the
classification results. Experiments on SPL2018 and DsTok show that the proposed
method outperforms existing methods, especially on the DsTok dataset. For
example, the performance of our model surpasses the state-of-the-art by a large
margin of 3%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BMD-GAN: Bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning. (arXiv:2207.03210v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03210">
<div class="article-summary-box-inner">
<span><p>We propose a method for estimating the bone mineral density (BMD) from a
plain x-ray image. Dual-energy X-ray absorptiometry (DXA) and quantitative
computed tomography (QCT) provide high accuracy in diagnosing osteoporosis;
however, these modalities require special equipment and scan protocols.
Measuring BMD from an x-ray image provides an opportunistic screening, which is
potentially useful for early diagnosis. The previous methods that directly
learn the relationship between x-ray images and BMD require a large training
dataset to achieve high accuracy because of large intensity variations in the
x-ray images. Therefore, we propose an approach using the QCT for training a
generative adversarial network (GAN) and decomposing an x-ray image into a
projection of bone-segmented QCT. The proposed hierarchical learning improved
the robustness and accuracy of quantitatively decomposing a small-area target.
The evaluation of 200 patients with osteoarthritis using the proposed method,
which we named BMD-GAN, demonstrated a Pearson correlation coefficient of 0.888
between the predicted and ground truth DXA-measured BMD. Besides not requiring
a large-scale training database, another advantage of our method is its
extensibility to other anatomical areas, such as the vertebrae and rib bones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entropy-Based Feature Extraction For Real-Time Semantic Segmentation. (arXiv:2207.03233v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03233">
<div class="article-summary-box-inner">
<span><p>This paper introduces an efficient patch-based computational module, coined
Entropy-based Patch Encoder (EPE) module, for resource-constrained semantic
segmentation. The EPE module consists of three lightweight fully-convolutional
encoders, each extracting features from image patches with a different amount
of entropy. Patches with high entropy are being processed by the encoder with
the largest number of parameters, patches with moderate entropy are processed
by the encoder with a moderate number of parameters, and patches with low
entropy are processed by the smallest encoder. The intuition behind the module
is the following: as patches with high entropy contain more information, they
need an encoder with more parameters, unlike low entropy patches, which can be
processed using a small encoder. Consequently, processing part of the patches
via the smaller encoder can significantly reduce the computational cost of the
module. Experiments show that EPE can boost the performance of existing
real-time semantic segmentation models with a slight increase in the
computational cost. Specifically, EPE increases the mIOU performance of DFANet
A by 0.9% with only 1.2% increase in the number of parameters and the mIOU
performance of EDANet by 1% with 10% increase of the model parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D2HNet: Joint Denoising and Deblurring with Hierarchical Network for Robust Night Image Restoration. (arXiv:2207.03294v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03294">
<div class="article-summary-box-inner">
<span><p>Night imaging with modern smartphone cameras is troublesome due to low photon
count and unavoidable noise in the imaging system. Directly adjusting exposure
time and ISO ratings cannot obtain sharp and noise-free images at the same time
in low-light conditions. Though many methods have been proposed to enhance
noisy or blurry night images, their performances on real-world night photos are
still unsatisfactory due to two main reasons: 1) Limited information in a
single image and 2) Domain gap between synthetic training images and real-world
photos (e.g., differences in blur area and resolution). To exploit the
information from successive long- and short-exposure images, we propose a
learning-based pipeline to fuse them. A D2HNet framework is developed to
recover a high-quality image by deblurring and enhancing a long-exposure image
under the guidance of a short-exposure image. To shrink the domain gap, we
leverage a two-phase DeblurNet-EnhanceNet architecture, which performs accurate
blur removal on a fixed low resolution so that it is able to handle large
ranges of blur in different resolution inputs. In addition, we synthesize a
D2-Dataset from HD videos and experiment on it. The results on the validation
set and real photos demonstrate our methods achieve better visual quality and
state-of-the-art quantitative scores. The D2HNet codes, models, and D2-Dataset
can be found at https://github.com/zhaoyuzhi/D2HNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExpansionNet: exploring the sequence length bottleneck in the Transformer for Image Captioning. (arXiv:2207.03327v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03327">
<div class="article-summary-box-inner">
<span><p>Most recent state of art architectures rely on combinations and variations of
three approaches: convolutional, recurrent and self-attentive methods. Our work
attempts in laying the basis for a new research direction for sequence modeling
based upon the idea of modifying the sequence length. In order to do that, we
propose a new method called ``Expansion Mechanism'' which transforms either
dynamically or statically the input sequence into a new one featuring a
different sequence length. Furthermore, we introduce a novel architecture that
exploits such method and achieves competitive performances on the MS-COCO 2014
data set, yielding 134.6 and 131.4 CIDEr-D on the Karpathy test split in the
ensemble and single model configuration respectively and 130 CIDEr-D in the
official online testing server, despite being neither recurrent nor fully
attentive. At the same time we address the efficiency aspect in our design and
introduce a convenient training strategy suitable for most computational
resources in contrast to the standard one. Source code is available at
https://github.com/jchenghu/ExpansionNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text to Image Synthesis using Stacked Conditional Variational Autoencoders and Conditional Generative Adversarial Networks. (arXiv:2207.03332v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03332">
<div class="article-summary-box-inner">
<span><p>Synthesizing a realistic image from textual description is a major challenge
in computer vision. Current text to image synthesis approaches falls short of
producing a highresolution image that represent a text descriptor. Most
existing studies rely either on Generative Adversarial Networks (GANs) or
Variational Auto Encoders (VAEs). GANs has the capability to produce sharper
images but lacks the diversity of outputs, whereas VAEs are good at producing a
diverse range of outputs, but the images generated are often blurred. Taking
into account the relative advantages of both GANs and VAEs, we proposed a new
stacked Conditional VAE (CVAE) and Conditional GAN (CGAN) network architecture
for synthesizing images conditioned on a text description. This study uses
Conditional VAEs as an initial generator to produce a high-level sketch of the
text descriptor. This high-level sketch output from first stage and a text
descriptor is used as an input to the conditional GAN network. The second stage
GAN produces a 256x256 high resolution image. The proposed architecture
benefits from a conditioning augmentation and a residual block on the
Conditional GAN network to achieve the results. Multiple experiments were
conducted using CUB and Oxford-102 dataset and the result of the proposed
approach is compared against state-ofthe-art techniques such as StackGAN. The
experiments illustrate that the proposed method generates a high-resolution
image conditioned on text descriptions and yield competitive results based on
Inception and Frechet Inception Score using both datasets
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewSOL: A Dataset for Few-Shot Object Learning in Robotic Environments. (arXiv:2207.03333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03333">
<div class="article-summary-box-inner">
<span><p>We introduce the Few-Shot Object Learning (FewSOL) dataset for object
recognition with a few images per object. We captured 336 real-world objects
with 9 RGB-D images per object from different views. Object segmentation masks,
object poses and object attributes are provided. In addition, synthetic images
generated using 330 3D object models are used to augment the dataset. We
investigated (i) few-shot object classification and (ii) joint object
segmentation and few-shot classification with the state-of-the-art methods for
few-shot learning and meta-learning using our dataset. The evaluation results
show that there is still a large margin to be improved for few-shot object
classification in robotic environments. Our dataset can be used to study a set
of few-shot object recognition problems such as classification, detection and
segmentation, shape reconstruction, pose estimation, keypoint correspondences
and attribute recognition. The dataset and code are available at
https://irvlutd.github.io/FewSOL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Ensemble of Explanations for Weakly-Supervised Pre-Training of Image Segmentation Models. (arXiv:2207.03335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03335">
<div class="article-summary-box-inner">
<span><p>While fine-tuning pre-trained networks has become a popular way to train
image segmentation models, such backbone networks for image segmentation are
frequently pre-trained using image classification source datasets, e.g.,
ImageNet. Though image classification datasets could provide the backbone
networks with rich visual features and discriminative ability, they are
incapable of fully pre-training the target model (i.e., backbone+segmentation
modules) in an end-to-end manner. The segmentation modules are left to random
initialization in the fine-tuning process due to the lack of segmentation
labels in classification datasets. In our work, we propose a method that
leverages Pseudo Semantic Segmentation Labels (PSSL), to enable the end-to-end
pre-training for image segmentation models based on classification datasets.
PSSL was inspired by the observation that the explanation results of
classification models, obtained through explanation algorithms such as CAM,
SmoothGrad and LIME, would be close to the pixel clusters of visual objects.
Specifically, PSSL is obtained for each image by interpreting the
classification results and aggregating an ensemble of explanations queried from
multiple classifiers to lower the bias caused by single models. With PSSL for
every image of ImageNet, the proposed method leverages a weighted segmentation
learning procedure to pre-train the segmentation network en masse. Experiment
results show that, with ImageNet accompanied by PSSL as the source dataset, the
proposed end-to-end pre-training strategy successfully boosts the performance
of various segmentation models, i.e., PSPNet-ResNet50, DeepLabV3-ResNet50, and
OCRNet-HRNetW18, on a number of segmentation tasks, such as CamVid, VOC-A,
VOC-C, ADE20K, and CityScapes, with significant improvements. The source code
is availabel at https://github.com/PaddlePaddle/PaddleSeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factorizing Knowledge in Neural Networks. (arXiv:2207.03337v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03337">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore a novel and ambitious knowledge-transfer task,
termed Knowledge Factorization~(KF). The core idea of KF lies in the
modularization and assemblability of knowledge: given a pretrained network
model as input, KF aims to decompose it into several factor networks, each of
which handles only a dedicated task and maintains task-specific knowledge
factorized from the source network. Such factor networks are task-wise
disentangled and can be directly assembled, without any fine-tuning, to produce
the more competent combined-task networks. In other words, the factor networks
serve as Lego-brick-like building blocks, allowing us to construct customized
networks in a plug-and-play manner. Specifically, each factor network comprises
two modules, a common-knowledge module that is task-agnostic and shared by all
factor networks, alongside with a task-specific module dedicated to the factor
network itself. We introduce an information-theoretic objective,
InfoMax-Bottleneck~(IMB), to carry out KF by optimizing the mutual information
between the learned representations and input. Experiments across various
benchmarks demonstrate that, the derived factor networks yield gratifying
performances on not only the dedicated tasks but also disentanglement, while
enjoying much better interpretability and modularity. Moreover, the learned
common-knowledge representations give rise to impressive results on transfer
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Softmax-free Linear Transformers. (arXiv:2207.03341v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03341">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have pushed the state-of-the-art for various
visual recognition tasks by patch-wise image tokenization followed by stacked
self-attention operations. Employing self-attention modules results in a
quadratic complexity in both computation and memory usage. Various attempts on
approximating the self-attention computation with linear complexity have thus
been made in Natural Language Processing. However, an in-depth analysis in this
work reveals that they are either theoretically flawed or empirically
ineffective for visual recognition. We identify that their limitations are
rooted in retaining the softmax self-attention during approximations.
Specifically, conventional self-attention is computed by normalizing the scaled
dot-product between token feature vectors. Preserving the softmax operation
challenges any subsequent linearization efforts. Under this insight, a
SOftmax-Free Transformer (abbreviated as SOFT) is proposed for the first time.
To eliminate the softmax operator in self-attention, a Gaussian kernel function
is adopted to replace the dot-product similarity. This enables a full
self-attention matrix to be approximated via a low-rank matrix decomposition.
The robustness of our approximation is achieved by calculating its
Moore-Penrose inverse using a Newton-Raphson method. Further, an efficient
symmetric normalization is introduced on the low-rank self-attention for
enhancing model generalizability and transferability. Extensive experiments on
ImageNet, COCO and ADE20K show that our SOFT significantly improves the
computational efficiency of existing ViT variants. Crucially, with a linear
complexity, much longer token sequences are permitted in SOFT, resulting in
superior trade-off between accuracy and complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monkeypox Skin Lesion Detection Using Deep Learning Models: A Feasibility Study. (arXiv:2207.03342v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03342">
<div class="article-summary-box-inner">
<span><p>The recent monkeypox outbreak has become a public health concern due to its
rapid spread in more than 40 countries outside Africa. Clinical diagnosis of
monkeypox in an early stage is challenging due to its similarity with
chickenpox and measles. In cases where the confirmatory Polymerase Chain
Reaction (PCR) tests are not readily available, computer-assisted detection of
monkeypox lesions could be beneficial for surveillance and rapid identification
of suspected cases. Deep learning methods have been found effective in the
automated detection of skin lesions, provided that sufficient training examples
are available. However, as of now, such datasets are not available for the
monkeypox disease. In the current study, we first develop the ``Monkeypox Skin
Lesion Dataset (MSLD)" consisting skin lesion images of monkeypox, chickenpox,
and measles. The images are mainly collected from websites, news portals, and
publicly accessible case reports. Data augmentation is used to increase the
sample size, and a 3-fold cross-validation experiment is set up. In the next
step, several pre-trained deep learning models, namely, VGG-16, ResNet50, and
InceptionV3 are employed to classify monkeypox and other diseases. An ensemble
of the three models is also developed. ResNet50 achieves the best overall
accuracy of $82.96(\pm4.57\%)$, while VGG16 and the ensemble system achieved
accuracies of $81.48(\pm6.87\%)$ and $79.26(\pm1.05\%)$, respectively. A
prototype web-application is also developed as an online monkeypox screening
tool. While the initial results on this limited dataset are promising, a larger
demographically diverse dataset is required to further enhance the
generalizability of these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Classification of General Movements in Infants Using a Two-stream Spatiotemporal Fusion Network. (arXiv:2207.03344v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03344">
<div class="article-summary-box-inner">
<span><p>The assessment of general movements (GMs) in infants is a useful tool in the
early diagnosis of neurodevelopmental disorders. However, its evaluation in
clinical practice relies on visual inspection by experts, and an automated
solution is eagerly awaited. Recently, video-based GMs classification has
attracted attention, but this approach would be strongly affected by irrelevant
information, such as background clutter in the video. Furthermore, for
reliability, it is necessary to properly extract the spatiotemporal features of
infants during GMs. In this study, we propose an automated GMs classification
method, which consists of preprocessing networks that remove unnecessary
background information from GMs videos and adjust the infant's body position,
and a subsequent motion classification network based on a two-stream structure.
The proposed method can efficiently extract the essential spatiotemporal
features for GMs classification while preventing overfitting to irrelevant
information for different recording environments. We validated the proposed
method using videos obtained from 100 infants. The experimental results
demonstrate that the proposed method outperforms several baseline models and
the existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A simple normalization technique using window statistics to improve the out-of-distribution generalization in medical images. (arXiv:2207.03366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03366">
<div class="article-summary-box-inner">
<span><p>Since data scarcity and data heterogeneity are prevailing for medical images,
well-trained Convolutional Neural Networks (CNNs) using previous normalization
methods may perform poorly when deployed to a new site. However, a reliable
model for real-world applications should be able to generalize well both on
in-distribution (IND) and out-of-distribution (OOD) data (e.g., the new site
data). In this study, we present a novel normalization technique called window
normalization (WIN), which is a simple yet effective alternative to existing
normalization methods. Specifically, WIN perturbs the normalizing statistics
with the local statistics computed on a window of features. This feature-level
augmentation technique regularizes the models well and improves their OOD
generalization significantly. Taking its advantage, we propose a novel
self-distillation method called WIN-WIN to further improve the OOD
generalization in classification. WIN-WIN is easily implemented with twice
forward passes and a consistency constraint, which can be a simple extension
for existing methods. Extensive experimental results on various tasks (such as
glaucoma detection, breast cancer detection, chromosome classification, optic
disc and cup segmentation, etc.) and datasets (26 datasets) demonstrate the
generality and effectiveness of our methods. The code is available at
https://github.com/joe1chief/windowNormalizaion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Super-Resolution and Inverse Tone-Mapping: A Feature Decomposition Aggregation Network and A New Benchmark. (arXiv:2207.03367v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03367">
<div class="article-summary-box-inner">
<span><p>Joint Super-Resolution and Inverse Tone-Mapping (joint SR-ITM) aims to
increase the resolution and dynamic range of low-resolution and standard
dynamic range images.Recent methods mainly resort to image decomposition
techniques with the multi-branch network architecture.However, the rigid
decomposition employed by these methods largely restricts their power on
diverse images.To exploit its potential power, in this paper, we generalize the
decomposition mechanism from the image domain to the broader feature domain. To
this end, we propose a lightweight Feature Decomposition Aggregation Network
(FDAN). In particular, we design a Feature Decomposition Block (FDB), which can
achieve learnable separation of feature details and contrasts.By cascading
FDBs, we can build up a Hierarchical Feature Decomposition Group for powerful
multi-level feature decomposition.Moreover, we collect a new benchmark dataset
for joint SR-ITM, \ie, SRITM-4K, which is large-scale and provides versatile
scenarios for sufficient model training and evaluation.Experimental results on
two benchmark datasets demonstrate that our FDAN is efficient and outperforms
previous methods on joint SR-ITM.Our code and dataset will be publicly
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diagnosing and Remedying Shot Sensitivity with Cosine Few-Shot Learners. (arXiv:2207.03398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03398">
<div class="article-summary-box-inner">
<span><p>Few-shot recognition involves training an image classifier to distinguish
novel concepts at test time using few examples (shot). Existing approaches
generally assume that the shot number at test time is known in advance. This is
not realistic, and the performance of a popular and foundational method has
been shown to suffer when train and test shots do not match. We conduct a
systematic empirical study of this phenomenon. In line with prior work, we find
that shot sensitivity is broadly present across metric-based few-shot learners,
but in contrast to prior work, larger neural architectures provide a degree of
built-in robustness to varying test shot. More importantly, a simple,
previously known but greatly overlooked class of approaches based on cosine
distance consistently and greatly improves robustness to shot variation, by
removing sensitivity to sample noise. We derive cosine alternatives to popular
and recent few-shot classifiers, broadening their applicability to realistic
settings. These cosine models consistently improve shot-robustness, outperform
prior shot-robust state of the art, and provide competitive accuracy on a range
of benchmarks and architectures, including notable gains in the very-low-shot
regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Watermarking for Video Forgery Detection with Improved Imperceptibility and Robustness. (arXiv:2207.03409v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03409">
<div class="article-summary-box-inner">
<span><p>Videos are prone to tampering attacks that alter the meaning and deceive the
audience. Previous video forgery detection schemes find tiny clues to locate
the tampered areas. However, attackers can successfully evade supervision by
destroying such clues using video compression or blurring. This paper proposes
a video watermarking network for tampering localization. We jointly train a
3D-UNet-based watermark embedding network and a decoder that predicts the
tampering mask. The perturbation made by watermark embedding is close to
imperceptible. Considering that there is no off-the-shelf differentiable video
codec simulator, we propose to mimic video compression by ensembling simulation
results of other typical attacks, e.g., JPEG compression and blurring, as an
approximation. Experimental results demonstrate that our method generates
watermarked videos with good imperceptibility and robustly and accurately
locates tampered areas within the attacked version.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VecGAN: Image-to-Image Translation with Interpretable Latent Directions. (arXiv:2207.03411v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03411">
<div class="article-summary-box-inner">
<span><p>We propose VecGAN, an image-to-image translation framework for facial
attribute editing with interpretable latent directions. Facial attribute
editing task faces the challenges of precise attribute editing with
controllable strength and preservation of the other attributes of an image. For
this goal, we design the attribute editing by latent space factorization and
for each attribute, we learn a linear direction that is orthogonal to the
others. The other component is the controllable strength of the change, a
scalar value. In our framework, this scalar can be either sampled or encoded
from a reference image by projection. Our work is inspired by the latent space
factorization works of fixed pretrained GANs. However, while those models
cannot be trained end-to-end and struggle to edit encoded images precisely,
VecGAN is end-to-end trained for image translation task and successful at
editing an attribute while preserving the others. Our extensive experiments
show that VecGAN achieves significant improvements over state-of-the-arts for
both local and global edits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Knowledge Driven 3D Dose Prediction Using Moment-Based Loss Function. (arXiv:2207.03414v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03414">
<div class="article-summary-box-inner">
<span><p>Dose volume histogram (DVH) metrics are widely accepted evaluation criteria
in the clinic. However, incorporating these metrics into deep learning dose
prediction models is challenging due to their non-convexity and
non-differentiability. We propose a novel moment-based loss function for
predicting 3D dose distribution for the challenging conventional lung intensity
modulated radiation therapy (IMRT) plans. The moment-based loss function is
convex and differentiable and can easily incorporate DVH metrics in any deep
learning framework without computational overhead. The moments can also be
customized to reflect the clinical priorities in 3D dose prediction. For
instance, using high-order moments allows better prediction in high-dose areas
for serial structures. We used a large dataset of 360 (240 for training, 50 for
validation and 70 for testing) conventional lung patients with 2Gy $\times$ 30
fractions to train the deep learning (DL) model using clinically treated plans
at our institution. We trained a UNet like CNN architecture using computed
tomography (CT), planning target volume (PTV) and organ-at-risk contours (OAR)
as input to infer corresponding voxel-wise 3D dose distribution. We evaluated
three different loss functions: (1) The popular Mean Absolute Error (MAE) Loss,
(2) the recently developed MAE + DVH Loss, and (3) the proposed MAE + Moments
Loss. The quality of the predictions was compared using different DVH metrics
as well as dose-score and DVH-score, recently introduced by the AAPM
knowledge-based planning grand challenge. Model with (MAE + Moment) loss
function outperformed the model with MAE loss by significantly improving the
DVH-score (11%, p$&lt;$0.01) while having similar computational cost. It also
outperformed the model trained with (MAE+DVH) by significantly improving the
computational cost (48%) and the DVH-score (8%, p$&lt;$0.01).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion. (arXiv:2207.03430v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03430">
<div class="article-summary-box-inner">
<span><p>Multi-modal medical image completion has been extensively applied to
alleviate the missing modality issue in a wealth of multi-modal diagnostic
tasks. However, for most existing synthesis methods, their inferences of
missing modalities can collapse into a deterministic mapping from the available
ones, ignoring the uncertainties inherent in the cross-modal relationships.
Here, we propose the Unified Multi-Modal Conditional Score-based Generative
Model (UMM-CSGM) to take advantage of Score-based Generative Model (SGM) in
modeling and stochastically sampling a target probability distribution, and
further extend SGM to cross-modal conditional synthesis for various
missing-modality configurations in a unified framework. Specifically, UMM-CSGM
employs a novel multi-in multi-out Conditional Score Network (mm-CSN) to learn
a comprehensive set of cross-modal conditional distributions via conditional
diffusion and reverse generation in the complete modality space. In this way,
the generation process can be accurately conditioned by all available
information, and can fit all possible configurations of missing modalities in a
single network. Experiments on BraTS19 dataset show that the UMM-CSGM can more
reliably synthesize the heterogeneous enhancement and irregular area in
tumor-induced lesions for any missing modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Object Detection via Virtual Category Learning. (arXiv:2207.03433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03433">
<div class="article-summary-box-inner">
<span><p>Due to the costliness of labelled data in real-world applications,
semi-supervised object detectors, underpinned by pseudo labelling, are
appealing. However, handling confusing samples is nontrivial: discarding
valuable confusing samples would compromise the model generalisation while
using them for training would exacerbate the confirmation bias issue caused by
inevitable mislabelling. To solve this problem, this paper proposes to use
confusing samples proactively without label correction. Specifically, a virtual
category (VC) is assigned to each confusing sample such that they can safely
contribute to the model optimisation even without a concrete label. It is
attributed to specifying the embedding distance between the training sample and
the virtual category as the lower bound of the inter-class distance. Moreover,
we also modify the localisation loss to allow high-quality boundaries for
location regression. Extensive experiments demonstrate that the proposed VC
learning significantly surpasses the state-of-the-art, especially with small
amounts of available labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LASSIE: Learning Articulated Shapes from Sparse Image Ensemble via 3D Part Discovery. (arXiv:2207.03434v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03434">
<div class="article-summary-box-inner">
<span><p>Creating high-quality articulated 3D models of animals is challenging either
via manual creation or using 3D scanning tools. Therefore, techniques to
reconstruct articulated 3D objects from 2D images are crucial and highly
useful. In this work, we propose a practical problem setting to estimate 3D
pose and shape of animals given only a few (10-30) in-the-wild images of a
particular animal species (say, horse). Contrary to existing works that rely on
pre-defined template shapes, we do not assume any form of 2D or 3D ground-truth
annotations, nor do we leverage any multi-view or temporal information.
Moreover, each input image ensemble can contain animal instances with varying
poses, backgrounds, illuminations, and textures. Our key insight is that 3D
parts have much simpler shape compared to the overall animal and that they are
robust w.r.t. animal pose articulations. Following these insights, we propose
LASSIE, a novel optimization framework which discovers 3D parts in a
self-supervised manner with minimal user intervention. A key driving force
behind LASSIE is the enforcing of 2D-3D part consistency using self-supervisory
deep features. Experiments on Pascal-Part and self-collected in-the-wild animal
datasets demonstrate considerably better 3D reconstructions as well as both 2D
and 3D part discovery compared to prior arts. Project page:
chhankyao.github.io/lassie/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to the Source: Diffusion-Driven Test-Time Adaptation. (arXiv:2207.03442v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03442">
<div class="article-summary-box-inner">
<span><p>Test-time adaptation harnesses test inputs to improve the accuracy of a model
trained on source data when tested on shifted target data. Existing methods
update the source model by (re-)training on each target domain. While
effective, re-training is sensitive to the amount and order of the data and the
hyperparameters for optimization. We instead update the target data, by
projecting all test inputs toward the source domain with a generative diffusion
model. Our diffusion-driven adaptation method, DDA, shares its models for
classification and generation across all domains. Both models are trained on
the source domain, then fixed during testing. We augment diffusion with image
guidance and self-ensembling to automatically decide how much to adapt. Input
adaptation by DDA is more robust than prior model adaptation approaches across
a variety of corruptions, architectures, and data regimes on the ImageNet-C
benchmark. With its input-wise updates, DDA succeeds where model adaptation
degrades on too little data in small batches, dependent data in non-uniform
order, or mixed data with multiple corruptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness and Bias in Robot Learning. (arXiv:2207.03444v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03444">
<div class="article-summary-box-inner">
<span><p>Machine learning has significantly enhanced the abilities of robots, enabling
them to perform a wide range of tasks in human environments and adapt to our
uncertain real world. Recent works in various domains of machine learning have
highlighted the importance of accounting for fairness to ensure that these
algorithms do not reproduce human biases and consequently lead to
discriminatory outcomes. With robot learning systems increasingly performing
more and more tasks in our everyday lives, it is crucial to understand the
influence of such biases to prevent unintended behavior toward certain groups
of people. In this work, we present the first survey on fairness in robot
learning from an interdisciplinary perspective spanning technical, ethical, and
legal challenges. We propose a taxonomy for sources of bias and the resulting
types of discrimination due to them. Using examples from different robot
learning domains, we examine scenarios of unfair outcomes and strategies to
mitigate them. We present early advances in the field by covering different
fairness definitions, ethical and legal considerations, and methods for fair
robot learning. With this work, we aim at paving the road for groundbreaking
developments in fair robot learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to restore images degraded by atmospheric turbulence using uncertainty. (arXiv:2207.03447v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03447">
<div class="article-summary-box-inner">
<span><p>Atmospheric turbulence can significantly degrade the quality of images
acquired by long-range imaging systems by causing spatially and temporally
random fluctuations in the index of refraction of the atmosphere. Variations in
the refractive index causes the captured images to be geometrically distorted
and blurry. Hence, it is important to compensate for the visual degradation in
images caused by atmospheric turbulence. In this paper, we propose a deep
learning-based approach for restring a single image degraded by atmospheric
turbulence. We make use of the epistemic uncertainty based on Monte Carlo
dropouts to capture regions in the image where the network is having hard time
restoring. The estimated uncertainty maps are then used to guide the network to
obtain the restored image. Extensive experiments are conducted on synthetic and
real images to show the significance of the proposed work. Code is available at
: https://github.com/rajeevyasarla/AT-Net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TFCNs: A CNN-Transformer Hybrid Network for Medical Image Segmentation. (arXiv:2207.03450v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03450">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation is one of the most fundamental tasks concerning
medical information analysis. Various solutions have been proposed so far,
including many deep learning-based techniques, such as U-Net, FC-DenseNet, etc.
However, high-precision medical image segmentation remains a highly challenging
task due to the existence of inherent magnification and distortion in medical
images as well as the presence of lesions with similar density to normal
tissues. In this paper, we propose TFCNs (Transformers for Fully Convolutional
denseNets) to tackle the problem by introducing ResLinear-Transformer
(RL-Transformer) and Convolutional Linear Attention Block (CLAB) to
FC-DenseNet. TFCNs is not only able to utilize more latent information from the
CT images for feature extraction, but also can capture and disseminate semantic
features and filter non-semantic features more effectively through the CLAB
module. Our experimental results show that TFCNs can achieve state-of-the-art
performance with dice scores of 83.72\% on the Synapse dataset. In addition, we
evaluate the robustness of TFCNs for lesion area effects on the COVID-19 public
datasets. The Python code will be made publicly available on
https://github.com/HUANGLIZI/TFCNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Red PANDA: Disambiguating Anomaly Detection by Removing Nuisance Factors. (arXiv:2207.03478v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03478">
<div class="article-summary-box-inner">
<span><p>Anomaly detection methods strive to discover patterns that differ from the
norm in a semantic way. This goal is ambiguous as a data point differing from
the norm by an attribute e.g., age, race or gender, may be considered anomalous
by some operators while others may consider this attribute irrelevant. Breaking
from previous research, we present a new anomaly detection method that allows
operators to exclude an attribute from being considered as relevant for anomaly
detection. Our approach then learns representations which do not contain
information over the nuisance attributes. Anomaly scoring is performed using a
density-based approach. Importantly, our approach does not require specifying
the attributes that are relevant for detecting anomalies, which is typically
impossible in anomaly detection, but only attributes to ignore. An empirical
investigation is presented verifying the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection. (arXiv:2207.03482v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03482">
<div class="article-summary-box-inner">
<span><p>Existing open-vocabulary object detectors typically enlarge their vocabulary
sizes by leveraging different forms of weak supervision. This helps generalize
to novel objects at inference. Two popular forms of weak-supervision used in
open-vocabulary detection (OVD) include pretrained CLIP model and image-level
supervision. We note that both these modes of supervision are not optimally
aligned for the detection task: CLIP is trained with image-text pairs and lacks
precise localization of objects while the image-level supervision has been used
with heuristics that do not accurately specify local object regions. In this
work, we propose to address this problem by performing object-centric alignment
of the language embeddings from the CLIP model. Furthermore, we visually ground
the objects with only image-level supervision using a pseudo-labeling process
that provides high-quality object proposals and helps expand the vocabulary
during training. We establish a bridge between the above two object-alignment
strategies via a novel weight transfer function that aggregates their
complimentary strengths. In essence, the proposed model seeks to minimize the
gap between object and image-centric representations in the OVD setting. On the
COCO benchmark, our proposed approach achieves 40.3 AP50 on novel classes, an
absolute 11.9 gain over the previous best performance.For LVIS, we surpass the
state-of-the-art ViLD model by 5.0 mask AP for rare categories and 3.4 overall.
Code: https://bit.ly/3byZoQp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Fallen Objects Via Asynchronous Audio-Visual Integration. (arXiv:2207.03483v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03483">
<div class="article-summary-box-inner">
<span><p>The way an object looks and sounds provide complementary reflections of its
physical properties. In many settings cues from vision and audition arrive
asynchronously but must be integrated, as when we hear an object dropped on the
floor and then must find it. In this paper, we introduce a setting in which to
study multi-modal object localization in 3D virtual environments. An object is
dropped somewhere in a room. An embodied robot agent, equipped with a camera
and microphone, must determine what object has been dropped -- and where -- by
combining audio and visual signals with knowledge of the underlying physics. To
study this problem, we have generated a large-scale dataset -- the Fallen
Objects dataset -- that includes 8000 instances of 30 physical object
categories in 64 rooms. The dataset uses the ThreeDWorld platform which can
simulate physics-based impact sounds and complex physical interactions between
objects in a photorealistic setting. As a first step toward addressing this
challenge, we develop a set of embodied agent baselines, based on imitation
learning, reinforcement learning, and modular planning, and perform an in-depth
analysis of the challenge of this new task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAiSEE: Towards User Engagement Recognition in the Wild. (arXiv:1609.01885v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1609.01885">
<div class="article-summary-box-inner">
<span><p>We introduce DAiSEE, the first multi-label video classification dataset
comprising of 9068 video snippets captured from 112 users for recognizing the
user affective states of boredom, confusion, engagement, and frustration in the
wild. The dataset has four levels of labels namely - very low, low, high, and
very high for each of the affective states, which are crowd annotated and
correlated with a gold standard annotation created using a team of expert
psychologists. We have also established benchmark results on this dataset using
state-of-the-art video classification methods that are available today. We
believe that DAiSEE will provide the research community with challenges in
feature extraction, context-based inference, and development of suitable
machine learning methods for related tasks, thus providing a springboard for
further research. The dataset is available for download at
https://people.iith.ac.in/vineethnb/resources/daisee/index.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks. (arXiv:2012.11230v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11230">
<div class="article-summary-box-inner">
<span><p>Quantizing deep convolutional neural networks for image super-resolution
substantially reduces their computational costs. However, existing works either
suffer from a severe performance drop in ultra-low precision of 4 or lower
bit-widths, or require a heavy fine-tuning process to recover the performance.
To our knowledge, this vulnerability to low precisions relies on two
statistical observations of feature map values. First, distribution of feature
map values varies significantly per channel and per input image. Second,
feature maps have outliers that can dominate the quantization error. Based on
these observations, we propose a novel distribution-aware quantization scheme
(DAQ) which facilitates accurate training-free quantization in ultra-low
precision. A simple function of DAQ determines dynamic range of feature maps
and weights with low computational burden. Furthermore, our method enables
mixed-precision quantization by calculating the relative sensitivity of each
channel, without any training process involved. Nonetheless, quantization-aware
training is also applicable for auxiliary performance gain. Our new method
outperforms recent training-free and even training-based quantization methods
to the state-of-the-art image super-resolution networks in ultra-low precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Depth and Ego-Motion Estimation for Monocular Thermal Video Using Multi-Spectral Consistency Loss. (arXiv:2103.00760v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00760">
<div class="article-summary-box-inner">
<span><p>A thermal camera can robustly capture thermal radiation images under harsh
light conditions such as night scenes, tunnels, and disaster scenarios.
However, despite this advantage, neither depth nor ego-motion estimation
research for the thermal camera have not been actively explored so far. In this
paper, we propose a self-supervised learning method for depth and ego-motion
estimation from thermal images. The proposed method exploits multi-spectral
consistency that consists of temperature and photometric consistency loss. The
temperature consistency loss provides a fundamental self-supervisory signal by
reconstructing clipped and colorized thermal images. Additionally, we design a
differentiable forward warping module that can transform the coordinate system
of the estimated depth map and relative pose from thermal camera to visible
camera. Based on the proposed module, the photometric consistency loss can
provide complementary self-supervision to networks. Networks trained with the
proposed method robustly estimate the depth and pose from monocular thermal
video under low-light and even zero-light conditions. To the best of our
knowledge, this is the first work to simultaneously estimate both depth and
ego-motion from monocular thermal video in a self-supervised manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in Prosthetic Hand Control. (arXiv:2104.03893v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03893">
<div class="article-summary-box-inner">
<span><p>Objective: For lower arm amputees, robotic prosthetic hands promise to regain
the capability to perform daily living activities. Current control methods
based on physiological signals such as electromyography (EMG) are prone to
yielding poor inference outcomes due to motion artifacts, muscle fatigue, and
many more. Vision sensors are a major source of information about the
environment state and can play a vital role in inferring feasible and intended
gestures. However, visual evidence is also susceptible to its own artifacts,
most often due to object occlusion, lighting changes, etc. Multimodal evidence
fusion using physiological and vision sensor measurements is a natural approach
due to the complementary strengths of these modalities. Methods: In this paper,
we present a Bayesian evidence fusion framework for grasp intent inference
using eye-view video, eye-gaze, and EMG from the forearm processed by neural
network models. We analyze individual and fused performance as a function of
time as the hand approaches the object to grasp it. For this purpose, we have
also developed novel data processing and augmentation techniques to train
neural network components. Results: Our results indicate that, on average,
fusion improves the instantaneous upcoming grasp type classification accuracy
while in the reaching phase by 13.66% and 14.8%, relative to EMG and visual
evidence individually, resulting in an overall fusion accuracy of 95.3%.
Conclusion: Our experimental data analyses demonstrate that EMG and visual
evidence show complementary strengths, and as a consequence, fusion of
multimodal evidence can outperform each individual evidence modality at any
given time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v6 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05642">
<div class="article-summary-box-inner">
<span><p>While the importance of automatic image analysis is continuously increasing,
recent meta-research revealed major flaws with respect to algorithm validation.
Performance metrics are particularly key for meaningful, objective, and
transparent performance assessment and validation of the used automatic
algorithms, but relatively little attention has been given to the practical
pitfalls when using specific metrics for a given image analysis task. These are
typically related to (1) the disregard of inherent metric properties, such as
the behaviour in the presence of class imbalance or small target structures,
(2) the disregard of inherent data set properties, such as the non-independence
of the test cases, and (3) the disregard of the actual biomedical domain
interest that the metrics should reflect. This living dynamically document has
the purpose to illustrate important limitations of performance metrics commonly
applied in the field of image analysis. In this context, it focuses on
biomedical image analysis problems that can be phrased as image-level
classification, semantic segmentation, instance segmentation, or object
detection task. The current version is based on a Delphi process on metrics
conducted by an international consortium of image analysis experts from more
than 60 institutions worldwide.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. (arXiv:2105.07112v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07112">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an efficient and robust deep learning solution for
novel view synthesis of complex scenes. In our approach, a 3D scene is
represented as a light field, i.e., a set of rays, each of which has a
corresponding color when reaching the image plane. For efficient novel view
rendering, we adopt a two-plane parameterization of the light field, where each
ray is characterized by a 4D parameter. We then formulate the light field as a
4D function that maps 4D coordinates to corresponding color values. We train a
deep fully connected network to optimize this implicit function and memorize
the 3D scene. Then, the scene-specific model is used to synthesize novel views.
Different from previous light field approaches which require dense view
sampling to reliably render novel views, our method can render novel views by
sampling rays and querying the color for each ray from the network directly,
thus enabling high-quality light field rendering with a sparser set of training
images. Per-ray depth can be optionally predicted by the network, thus enabling
applications such as auto refocus. Our novel view synthesis results are
comparable to the state-of-the-arts, and even superior in some challenging
scenes with refraction and reflection. We achieve this while maintaining an
interactive frame rate and a small memory footprint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09785">
<div class="article-summary-box-inner">
<span><p>This paper investigates two techniques for developing efficient
self-supervised vision transformers (EsViT) for visual representation learning.
First, we show through a comprehensive empirical study that multi-stage
architectures with sparse self-attentions can significantly reduce modeling
complexity but with a cost of losing the ability to capture fine-grained
correspondences between image regions. Second, we propose a new pre-training
task of region matching which allows the model to capture fine-grained region
dependencies and as a result significantly improves the quality of the learned
vision representations. Our results show that combining the two techniques,
EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,
outperforming prior arts with around an order magnitude of higher throughput.
When transferring to downstream linear classification tasks, EsViT outperforms
its supervised counterpart on 17 out of 18 datasets. The code and models are
publicly available: https://github.com/microsoft/esvit
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TA2N: Two-Stage Action Alignment Network for Few-shot Action Recognition. (arXiv:2107.04782v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04782">
<div class="article-summary-box-inner">
<span><p>Few-shot action recognition aims to recognize novel action classes (query)
using just a few samples (support). The majority of current approaches follow
the metric learning paradigm, which learns to compare the similarity between
videos. Recently, it has been observed that directly measuring this similarity
is not ideal since different action instances may show distinctive temporal
distribution, resulting in severe misalignment issues across query and support
videos. In this paper, we arrest this problem from two distinct aspects --
action duration misalignment and action evolution misalignment. We address them
sequentially through a Two-stage Action Alignment Network (TA2N). The first
stage locates the action by learning a temporal affine transform, which warps
each video feature to its action duration while dismissing the
action-irrelevant feature (e.g. background). Next, the second stage coordinates
query feature to match the spatial-temporal action evolution of support by
performing temporally rearrange and spatially offset prediction. Extensive
experiments on benchmark datasets show the potential of the proposed method in
achieving state-of-the-art performance for few-shot action recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13802">
<div class="article-summary-box-inner">
<span><p>Depth completion deals with the problem of recovering dense depth maps from
sparse ones, where color images are often used to facilitate this task. Recent
approaches mainly focus on image guided learning frameworks to predict dense
depth. However, blurry guidance in the image and unclear structure in the depth
still impede the performance of the image guided frameworks. To tackle these
problems, we explore a repetitive design in our image guided network to
gradually and sufficiently recover depth values. Specifically, the repetition
is embodied in both the image guidance branch and depth generation branch. In
the former branch, we design a repetitive hourglass network to extract
discriminative image features of complex environments, which can provide
powerful contextual instruction for depth prediction. In the latter branch, we
introduce a repetitive guidance module based on dynamic convolution, in which
an efficient convolution factorization is proposed to simultaneously reduce its
complexity and progressively model high-frequency structures. Extensive
experiments show that our method achieves superior or competitive results on
KITTI benchmark and NYUv2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Person Re-identification with Stochastic Training Strategy. (arXiv:2108.06938v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06938">
<div class="article-summary-box-inner">
<span><p>Unsupervised person re-identification (re-ID) has attracted increasing
research interests because of its scalability and possibility for real-world
applications. State-of-the-art unsupervised re-ID methods usually follow a
clustering-based strategy, which generates pseudo labels by clustering and
maintains a memory to store instance features and represent the centroid of the
clusters for contrastive learning. This approach suffers two problems. First,
the centroid generated by unsupervised learning may not be a perfect prototype.
Forcing images to get closer to the centroid emphasizes the result of
clustering, which could accumulate clustering errors during iterations. Second,
previous methods utilize features obtained at different training iterations to
represent one centroid, which is not consistent with the current training
sample, since the features are not directly comparable. To this end, we propose
an unsupervised re-ID approach with a stochastic learning strategy.
Specifically, we adopt a stochastic updated memory, where a random instance
from a cluster is used to update the cluster-level memory for contrastive
learning. In this way, the relationship between randomly selected pair of
images are learned to avoid the training bias caused by unreliable pseudo
labels. The stochastic memory is also always up-to-date for classifying to keep
the consistency. Besides, to relieve the issue of camera variance, a unified
distance matrix is proposed during clustering, where the distance bias from
different camera domain is reduced and the variances of identities is
emphasized.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v5 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05539">
<div class="article-summary-box-inner">
<span><p>Brain-inspired computation and information processing alongside compatibility
with neuromorphic hardware have made spiking neural networks (SNN) a promising
method for solving learning tasks in machine learning (ML). Spiking neurons are
only one of the requirements for building a bio-plausible learning model.
Network architecture and learning rules are other important factors to consider
when developing such artificial agents. In this work, inspired by the human
visual pathway and the role of dopamine in learning, we propose a
reward-modulated locally connected spiking neural network, BioLCNet, for visual
learning tasks. To extract visual features from Poisson-distributed spike
trains, we used local filters that are more analogous to the biological visual
system compared to convolutional filters with weight sharing. In the decoding
layer, we applied a spike population-based voting scheme to determine the
decision of the network. We employed Spike-timing-dependent plasticity (STDP)
for learning the visual features, and its reward-modulated variant (R-STDP) for
training the decoder based on the reward or punishment feedback signal. For
evaluation, we first assessed the robustness of our rewarding mechanism to
varying target responses in a classical conditioning experiment. Afterwards, we
evaluated the performance of our network on image classification tasks of MNIST
and XOR MNIST datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Space Time Recurrent Memory Network. (arXiv:2109.06474v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06474">
<div class="article-summary-box-inner">
<span><p>Transformers have recently been popular for learning and inference in the
spatial-temporal domain. However, their performance relies on storing and
applying attention to the feature tensor of each frame in video. Hence, their
space and time complexity increase linearly as the length of video grows, which
could be very costly for long videos. We propose a novel visual memory network
architecture for the learning and inference problem in the spatial-temporal
domain. We maintain a fixed set of memory slots in our memory network and
propose an algorithm based on Gumbel-Softmax to learn an adaptive strategy to
update this memory. Finally, this architecture is benchmarked on the video
object segmentation (VOS) and video prediction problems. We demonstrate that
our memory architecture achieves state-of-the-art results, outperforming
transformer-based methods on VOS and other recent methods on video prediction
while maintaining constant memory capacity independent of the sequence length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14051">
<div class="article-summary-box-inner">
<span><p>Machine learning models often encounter samples that are diverged from the
training distribution. Failure to recognize an out-of-distribution (OOD)
sample, and consequently assign that sample to an in-class label significantly
compromises the reliability of a model. The problem has gained significant
attention due to its importance for safety deploying models in open-world
settings. Detecting OOD samples is challenging due to the intractability of
modeling all possible unknown distributions. To date, several research domains
tackle the problem of detecting unfamiliar samples, including anomaly
detection, novelty detection, one-class learning, open set recognition, and
out-of-distribution detection. Despite having similar and shared concepts,
out-of-distribution, open-set, and anomaly detection have been investigated
independently. Accordingly, these research avenues have not cross-pollinated,
creating research barriers. While some surveys intend to provide an overview of
these approaches, they seem to only focus on a specific domain without
examining the relationship between different domains. This survey aims to
provide a cross-domain and comprehensive review of numerous eminent works in
respective areas while identifying their commonalities. Researchers can benefit
from the overview of research advances in different fields and develop future
methodology synergistically. Furthermore, to the best of our knowledge, while
there are surveys in anomaly detection or one-class learning, there is no
comprehensive or up-to-date survey on out-of-distribution detection, which our
survey covers extensively. Finally, having a unified cross-domain perspective,
we discuss and shed light on future lines of research, intending to bring these
fields closer together.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric Human Trajectory Forecasting with a Wearable Camera and Multi-Modal Fusion. (arXiv:2111.00993v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00993">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of forecasting the trajectory of an
egocentric camera wearer (ego-person) in crowded spaces. The trajectory
forecasting ability learned from the data of different camera wearers walking
around in the real world can be transferred to assist visually impaired people
in navigation, as well as to instill human navigation behaviours in mobile
robots, enabling better human-robot interactions. To this end, a novel
egocentric human trajectory forecasting dataset was constructed, containing
real trajectories of people navigating in crowded spaces wearing a camera, as
well as extracted rich contextual data. We extract and utilize three different
modalities to forecast the trajectory of the camera wearer, i.e., his/her past
trajectory, the past trajectories of nearby people, and the environment such as
the scene semantics or the depth of the scene. A Transformer-based
encoder-decoder neural network model, integrated with a novel cascaded
cross-attention mechanism that fuses multiple modalities, has been designed to
predict the future trajectory of the camera wearer. Extensive experiments have
been conducted, with results showing that our model outperforms the
state-of-the-art methods in egocentric human trajectory forecasting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composition and Style Attributes Guided Image Aesthetic Assessment. (arXiv:2111.04647v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04647">
<div class="article-summary-box-inner">
<span><p>The aesthetic quality of an image is defined as the measure or appreciation
of the beauty of an image. Aesthetics is inherently a subjective property but
there are certain factors that influence it such as, the semantic content of
the image, the attributes describing the artistic aspect, the photographic
setup used for the shot, etc. In this paper we propose a method for the
automatic prediction of the aesthetics of an image that is based on the
analysis of the semantic content, the artistic style and the composition of the
image. The proposed network includes: a pre-trained network for semantic
features extraction (the Backbone); a Multi Layer Perceptron (MLP) network that
relies on the Backbone features for the prediction of image attributes (the
AttributeNet); a self-adaptive Hypernetwork that exploits the attributes prior
encoded into the embedding generated by the AttributeNet to predict the
parameters of the target network dedicated to aesthetic estimation (the
AestheticNet). Given an image, the proposed multi-network is able to predict:
style and composition attributes, and aesthetic score distribution. Results on
three benchmark datasets demonstrate the effectiveness of the proposed method,
while the ablation study gives a better understanding of the proposed network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Equivalence between Neural Network and Support Vector Machine. (arXiv:2111.06063v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06063">
<div class="article-summary-box-inner">
<span><p>Recent research shows that the dynamics of an infinitely wide neural network
(NN) trained by gradient descent can be characterized by Neural Tangent Kernel
(NTK) \citep{jacot2018neural}. Under the squared loss, the infinite-width NN
trained by gradient descent with an infinitely small learning rate is
equivalent to kernel regression with NTK \citep{arora2019exact}. However, the
equivalence is only known for ridge regression currently
\citep{arora2019harnessing}, while the equivalence between NN and other kernel
machines (KMs), e.g. support vector machine (SVM), remains unknown. Therefore,
in this work, we propose to establish the equivalence between NN and SVM, and
specifically, the infinitely wide NN trained by soft margin loss and the
standard soft margin SVM with NTK trained by subgradient descent. Our main
theoretical results include establishing the equivalences between NNs and a
broad family of $\ell_2$ regularized KMs with finite-width bounds, which cannot
be handled by prior work, and showing that every finite-width NN trained by
such regularized loss functions is approximately a KM. Furthermore, we
demonstrate our theory can enable three practical applications, including (i)
\textit{non-vacuous} generalization bound of NN via the corresponding KM; (ii)
\textit{non-trivial} robustness certificate for the infinite-width NN (while
existing robustness verification methods would provide vacuous bounds); (iii)
intrinsically more robust infinite-width NNs than those from previous kernel
regression. Our code for the experiments is available at
\url{https://github.com/leslie-CH/equiv-nn-svm}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Image Generation with Mixup-based Distance Learning. (arXiv:2111.11672v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11672">
<div class="article-summary-box-inner">
<span><p>Producing diverse and realistic images with generative models such as GANs
typically requires large scale training with vast amount of images. GANs
trained with limited data can easily memorize few training samples and display
undesirable properties like "stairlike" latent space where interpolation in the
latent space yields discontinuous transitions in the output space. In this
work, we consider a challenging task of pretraining-free few-shot image
synthesis, and seek to train existing generative models with minimal
overfitting and mode collapse. We propose mixup-based distance regularization
on the feature space of both a generator and the counterpart discriminator that
encourages the two players to reason not only about the scarce observed data
points but the relative distances in the feature space they reside. Qualitative
and quantitative evaluation on diverse datasets demonstrates that our method is
generally applicable to existing models to enhance both fidelity and diversity
under few-shot setting. Code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepAdversaries: Examining the Robustness of Deep Learning Models for Galaxy Morphology Classification. (arXiv:2112.14299v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14299">
<div class="article-summary-box-inner">
<span><p>With increased adoption of supervised deep learning methods for processing
and analysis of cosmological survey data, the assessment of data perturbation
effects (that can naturally occur in the data processing and analysis
pipelines) and the development of methods that increase model robustness are
increasingly important. In the context of morphological classification of
galaxies, we study the effects of perturbations in imaging data. In particular,
we examine the consequences of using neural networks when training on baseline
data and testing on perturbed data. We consider perturbations associated with
two primary sources: 1) increased observational noise as represented by higher
levels of Poisson noise and 2) data processing noise incurred by steps such as
image compression or telescope errors as represented by one-pixel adversarial
attacks. We also test the efficacy of domain adaptation techniques in
mitigating the perturbation-driven errors. We use classification accuracy,
latent space visualizations, and latent space distance to assess model
robustness. Without domain adaptation, we find that processing pixel-level
errors easily flip the classification into an incorrect class and that higher
observational noise makes the model trained on low-noise data unable to
classify galaxy morphologies. On the other hand, we show that training with
domain adaptation improves model robustness and mitigates the effects of these
perturbations, improving the classification accuracy by 23% on data with higher
observational noise. Domain adaptation also increases by a factor of ~2.3 the
latent space distance between the baseline and the incorrectly classified
one-pixel perturbed image, making the model more robust to inadvertent
perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore and Match: A New Paradigm for Temporal Video Grounding with Natural Language. (arXiv:2201.10168v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10168">
<div class="article-summary-box-inner">
<span><p>Temporal Video Grounding (TVG) aims to localize time segments in an untrimmed
video according to natural language queries. In this work, we present a new
paradigm named Explore-and-Match for TVG that seamlessly unifies two streams of
TVG methods: proposal-free and proposal-based; the former explores the search
space to find segments directly, and the latter matches the predefined
proposals with ground truths. To achieve this goal, we view TVG as a set
prediction problem and design an end-to-end trainable Language Video
Transformer (LVTR) that utilizes the architectural strengths of rich
contextualization and parallel decoding for set prediction. The overall
training schedule is balanced by two key losses that play different roles,
namely temporal localization loss and set guidance loss. These two losses allow
each proposal to regress the target segment and identify the target query. More
specifically, LVTR first explores the search space to diversify the initial
proposals, and then matches the proposals to the corresponding targets to align
them in a fine-grained manner. The Explore-and-Match scheme successfully
combines the strengths of two complementary methods without encoding prior
knowledge (e.g., non-maximum suppression) into the TVG pipeline. As a result,
LVTR sets new state-of-the-art results on two TVG benchmarks (ActivityCaptions
and Charades-STA) with double the inference speed. Codes are available at
https://github.com/sangminwoo/Explore-and-Match.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better Understanding of Self-Supervised Representations. (arXiv:2203.01881v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01881">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning methods have shown impressive results in downstream
classification tasks. However, there is limited work in understanding and
interpreting their learned representations. In this paper, we study the
representation space of several state-of-the-art self-supervised models
including SimCLR, SwaV, MoCo V2 and BYOL. Without the use of class label
information, we first discover discriminative features that are highly active
for various subsets of samples and correspond to unique physical attributes in
images. We show that, using such discriminative features, one can compress the
representation space of self-supervised models up to 50% without affecting
downstream linear classification significantly. Next, we propose a sample-wise
Self-Supervised Representation Quality Score (or, Q-Score) that can be computed
without access to any label information. Q-Score, utilizes discriminative
features to reliably predict if a given sample is likely to be mis-classified
in the downstream classification task achieving AUPRC of 0.91 on SimCLR and
BYOL trained on ImageNet-100. Q-Score can also be used as a regularization term
to remedy low-quality representations leading up to 8% relative improvement in
accuracy on all 4 self-supervised baselines on ImageNet-100, CIFAR-10,
CIFAR-100 and STL-10. Moreover, through heatmap analysis, we show that Q-Score
regularization enhances discriminative features and reduces feature noise, thus
improving model interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Automotive mmWave Radar Detection Points Based Instance Segmentation. (arXiv:2203.06553v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06553">
<div class="article-summary-box-inner">
<span><p>The automotive mmWave radar plays a key role in advanced driver assistance
systems (ADAS) and autonomous driving. Deep learning-based instance
segmentation enables real-time object identification from the radar detection
points. In the conventional training process, accurate annotation is the key.
However, high-quality annotations of radar detection points are challenging to
achieve due to their ambiguity and sparsity. To address this issue, we propose
a contrastive learning approach for implementing radar detection points-based
instance segmentation. We define the positive and negative samples according to
the ground-truth label, apply the contrastive loss to train the model first,
and then perform fine-tuning for the following downstream task. In addition,
these two steps can be merged into one, and pseudo labels can be generated for
the unlabeled data to improve the performance further. Thus, there are four
different training settings for our method. Experiments show that when the
ground-truth information is only available for a small proportion of the
training data, our method still achieves a comparable performance to the
approach trained in a supervised manner with 100% ground-truth information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training. (arXiv:2203.12602v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12602">
<div class="article-summary-box-inner">
<span><p>Pre-training video transformers on extra large-scale datasets is generally
required to achieve premier performance on relatively small datasets. In this
paper, we show that video masked autoencoders (VideoMAE) are data-efficient
learners for self-supervised video pre-training (SSVP). We are inspired by the
recent ImageMAE and propose customized video tube masking with an extremely
high ratio. This simple design makes video reconstruction a more challenging
self-supervision task, thus encouraging extracting more effective video
representations during this pre-training process. We obtain three important
findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90%
to 95%) still yields favorable performance of VideoMAE. The temporally
redundant video content enables a higher masking ratio than that of images. (2)
VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k
videos) without using any extra data. (3) VideoMAE shows that data quality is
more important than data quantity for SSVP. Domain shift between pre-training
and target datasets is an important issue. Notably, our VideoMAE with the
vanilla ViT can achieve 85.8% on Kinetics-400, 75.3% on Something-Something V2,
90.8% on UCF101, and 61.1% on HMDB51, without using any extra data. Code is
available at https://github.com/MCG-NJU/VideoMAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation. (arXiv:2204.07548v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07548">
<div class="article-summary-box-inner">
<span><p>Recent works on 3D semantic segmentation propose to exploit the synergy
between images and point clouds by processing each modality with a dedicated
network and projecting learned 2D features onto 3D points. Merging large-scale
point clouds and images raises several challenges, such as constructing a
mapping between points and pixels, and aggregating features between multiple
views. Current methods require mesh reconstruction or specialized sensors to
recover occlusions, and use heuristics to select and aggregate available
images. In contrast, we propose an end-to-end trainable multi-view aggregation
model leveraging the viewing conditions of 3D points to merge features from
images taken at arbitrary positions. Our method can combine standard 2D and 3D
networks and outperforms both 3D models operating on colorized point clouds and
hybrid 2D/3D networks without requiring colorization, meshing, or true depth
maps. We set a new state-of-the-art for large-scale indoor/outdoor semantic
segmentation on S3DIS (74.7 mIoU 6-Fold) and on KITTI-360 (58.3 mIoU). Our full
pipeline is accessible at https://github.com/drprojects/DeepViewAgg, and only
requires raw 3D scans and a set of images and poses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Y-Net: A Spatiospectral Dual-Encoder Networkfor Medical Image Segmentation. (arXiv:2204.07613v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07613">
<div class="article-summary-box-inner">
<span><p>Automated segmentation of retinal optical coherence tomography (OCT) images
has become an important recent direction in machine learning for medical
applications. We hypothesize that the anatomic structure of layers and their
high-frequency variation in OCT images make retinal OCT a fitting choice for
extracting spectral-domain features and combining them with spatial domain
features. In this work, we present $\Upsilon$-Net, an architecture that
combines the frequency domain features with the image domain to improve the
segmentation performance of OCT images. The results of this work demonstrate
that the introduction of two branches, one for spectral and one for spatial
domain features, brings a very significant improvement in fluid segmentation
performance and allows outperformance as compared to the well-known U-Net
model. Our improvement was 13% on the fluid segmentation dice score and 1.9% on
the average dice score. Finally, removing selected frequency ranges in the
spectral domain demonstrates the impact of these features on the fluid
segmentation outperformance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10965">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose CLIP-Dissect, a new technique to automatically
describe the function of individual hidden neurons inside vision networks.
CLIP-Dissect leverages recent advances in multimodal vision/language models to
label internal neurons with open-ended concepts without the need for any
labeled data or human examples, which are required for existing tools to
succeed. We show that CLIP-Dissect provides more accurate descriptions than
existing methods for last layer neurons where the ground-truth is available as
well as qualitatively good descriptions for hidden layer neurons. In addition,
our method is very flexible: it is model agnostic, can easily handle new
concepts and can be extended to take advantage of better multimodal models in
the future. Finally CLIP-Dissect is computationally efficient and can label all
neurons from five layers of ResNet-50 in just four minutes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An asynchronous event-based algorithm for periodic signals. (arXiv:2205.04691v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04691">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a simple event-oriented algorithm for detection of
pixel-size signals with a known frequency, by the novel technology of an event
camera. In addition, we analyze the ability of the algorithm to filter out the
desired periodic signals from random fluctuations. We demonstrate this ability
and show how the algorithm can distinguish, during twilight, between the
signals of a streetlight that flicker with frequency of 100 Hz, and sun glitter
originating from windows in far-away buildings in the field of view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cardiomegaly Detection using Deep Convolutional Neural Network with U-Net. (arXiv:2205.11515v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11515">
<div class="article-summary-box-inner">
<span><p>Cardiomegaly is indeed a medical disease in which the heart is enlarged.
Cardiomegaly is better to handle if caught early, so early detection is
critical. The chest X-ray, being one of the most often used radiography
examinations, has been used to detect and visualize abnormalities of human
organs for decades. X-ray is also a significant medical diagnosis tool for
cardiomegaly. Even for domain experts, distinguishing the many types of
diseases from the X-ray is a difficult and time-consuming task. Deep learning
models are also most effective when used on huge data sets, yet due to privacy
concerns, large datasets are rarely available inside the medical industry. A
Deep learning-based customized retrained U-Net model for detecting Cardiomegaly
disease is presented in this research. In the training phase, chest X-ray
images from the "ChestX-ray8" open source real dataset are used. To reduce
computing time, this model performs data preprocessing, picture improvement,
image compression, and classification before moving on to the training step.
The work used a chest x-ray image dataset to simulate and produced a diagnostic
accuracy of 94%, a sensitivity of 96.2 percent, and a specificity of 92.5
percent, which beats prior pre-trained model findings for identifying
Cardiomegaly disease.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12693">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has achieved a great success in the representation
learning of visual and textual data. However, the current methods are mainly
validated on the well-curated datasets, which do not exhibit the real-world
long-tailed distribution. Recent attempts to consider self-supervised
long-tailed learning are made by rebalancing in the loss perspective or the
model perspective, resembling the paradigms in the supervised long-tailed
learning. Nevertheless, without the aid of labels, these explorations have not
shown the expected significant promise due to the limitation in tail sample
discovery or the heuristic structure design. Different from previous works, we
explore this direction from an alternative perspective, i.e., the data
perspective, and propose a novel Boosted Contrastive Learning (BCL) method.
Specifically, BCL leverages the memorization effect of deep neural networks to
automatically drive the information discrepancy of the sample views in
contrastive learning, which is more efficient to enhance the long-tailed
learning in the label-unaware context. Extensive experiments on a range of
benchmark datasets demonstrate the effectiveness of BCL over several
state-of-the-art methods. Our code is available at
https://github.com/MediaBrain-SJTU/BCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13326">
<div class="article-summary-box-inner">
<span><p>This paper describes the methods submitted for evaluation to the SHREC 2022
track on pothole and crack detection in the road pavement. A total of 7
different runs for the semantic segmentation of the road surface are compared,
6 from the participants plus a baseline method. All methods exploit Deep
Learning techniques and their performance is tested using the same environment
(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic
segmentation image/mask pairs and 797 RGB-D video clips collected with the
latest depth cameras was made available to the participants. The methods are
then evaluated on the 496 image/mask pairs in the validation set, on the 504
pairs in the test set and finally on 8 video clips. The analysis of the results
is based on quantitative metrics for image segmentation and qualitative
analysis of the video clips. The participation and the results show that the
scenario is of great interest and that the use of RGB-D data is still
challenging in this context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fitting and recognition of geometric primitives in segmented 3D point clouds using a localized voting procedure. (arXiv:2205.15426v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15426">
<div class="article-summary-box-inner">
<span><p>The automatic creation of geometric models from point clouds has numerous
applications in CAD (e.g., reverse engineering, manufacturing, assembling) and,
more in general, in shape modelling and processing. Given a segmented point
cloud representing a man-made object, we propose a method for recognizing
simple geometric primitives and their interrelationships. Our approach is based
on the Hough transform (HT) for its ability to deal with noise, missing parts
and outliers. In our method we introduce a novel technique for processing
segmented point clouds that, through a voting procedure, is able to provide an
initial estimate of the geometric parameters characterizing each primitive
type. By using these estimates, we localize the search of the optimal solution
in a dimensionally-reduced parameter space thus making it efficient to extend
the HT to more primitives than those that are generally found in the
literature, i.e. planes and spheres. Then, we extract a number of geometric
descriptors that uniquely characterize a segment, and, on the basis of these
descriptors, we show how to aggregate parts of primitives (segments).
Experiments on both synthetic and industrial scans reveal the robustness of the
primitive fitting method and its effectiveness for inferring relations among
segments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metrics reloaded: Pitfalls and recommendations for image analysis validation. (arXiv:2206.01653v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01653">
<div class="article-summary-box-inner">
<span><p>The field of automatic biomedical image analysis crucially depends on robust
and meaningful performance metrics for algorithm validation. Current metric
usage, however, is often ill-informed and does not reflect the underlying
domain interest. Here, we present a comprehensive framework that guides
researchers towards choosing performance metrics in a problem-aware manner.
Specifically, we focus on biomedical image analysis problems that can be
interpreted as a classification task at image, object or pixel level. The
framework first compiles domain interest-, target structure-, data set- and
algorithm output-related properties of a given problem into a problem
fingerprint, while also mapping it to the appropriate problem category, namely
image-level classification, semantic segmentation, instance segmentation, or
object detection. It then guides users through the process of selecting and
applying a set of appropriate validation metrics while making them aware of
potential pitfalls related to individual choices. In this paper, we describe
the current status of the Metrics Reloaded recommendation framework, with the
goal of obtaining constructive feedback from the image analysis community. The
current version has been developed within an international consortium of more
than 60 image analysis experts and will be made openly available as a
user-friendly toolkit after community-driven optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixGen: A New Multi-Modal Data Augmentation. (arXiv:2206.08358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08358">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a necessity to enhance data efficiency in deep learning.
For vision-language pre-training, data is only augmented either for images or
for text in previous works. In this paper, we present MixGen: a joint data
augmentation for vision-language representation learning to further improve
data efficiency. It generates new image-text pairs with semantic relationships
preserved by interpolating images and concatenating text. It's simple, and can
be plug-and-played into existing pipelines. We evaluate MixGen on four
architectures, including CLIP, ViLT, ALBEF and TCL, across five downstream
vision-language tasks to show its versatility and effectiveness. For example,
adding MixGen in ALBEF pre-training leads to absolute performance improvements
on downstream tasks: image-text retrieval (+6.2% on COCO fine-tuned and +5.3%
on Flicker30K zero-shot), visual grounding (+0.9% on RefCOCO+), visual
reasoning (+0.9% on NLVR$^{2}$), visual question answering (+0.3% on VQA2.0),
and visual entailment (+0.4% on SNLI-VE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel adversarial learning strategy for medical image classification. (arXiv:2206.11501v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11501">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) techniques have been extensively utilized for medical
image classification. Most DL-based classification networks are generally
structured hierarchically and optimized through the minimization of a single
loss function measured at the end of the networks. However, such a single loss
design could potentially lead to optimization of one specific value of interest
but fail to leverage informative features from intermediate layers that might
benefit classification performance and reduce the risk of overfitting.
Recently, auxiliary convolutional neural networks (AuxCNNs) have been employed
on top of traditional classification networks to facilitate the training of
intermediate layers to improve classification performance and robustness. In
this study, we proposed an adversarial learning-based AuxCNN to support the
training of deep neural networks for medical image classification. Two main
innovations were adopted in our AuxCNN classification framework. First, the
proposed AuxCNN architecture includes an image generator and an image
discriminator for extracting more informative image features for medical image
classification, motivated by the concept of generative adversarial network
(GAN) and its impressive ability in approximating target data distribution.
Second, a hybrid loss function is designed to guide the model training by
incorporating different objectives of the classification network and AuxCNN to
reduce overfitting. Comprehensive experimental studies demonstrated the
superior classification performance of the proposed model. The effect of the
network-related factors on classification performance was investigated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need. (arXiv:2206.11804v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11804">
<div class="article-summary-box-inner">
<span><p>Data diversity and volume are crucial to the success of training deep
learning models, while in the medical imaging field, the difficulty and cost of
data collection and annotation are especially huge. Specifically in robotic
surgery, data scarcity and imbalance have heavily affected the model accuracy
and limited the design and deployment of deep learning-based surgical
applications such as surgical instrument segmentation. Considering this, we
rethink the surgical instrument segmentation task and propose a one-to-many
data generation solution that gets rid of the complicated and expensive process
of data collection and annotation from robotic surgery. In our method, we only
utilize a single surgical background tissue image and a few open-source
instrument images as the seed images and apply multiple augmentations and
blending techniques to synthesize amounts of image variations. In addition, we
also introduce the chained augmentation mixing during training to further
enhance the data diversities. The proposed approach is evaluated on the real
datasets of the EndoVis-2018 and EndoVis-2017 surgical scene segmentation. Our
empirical analysis suggests that without the high cost of data collection and
annotation, we can achieve decent surgical instrument segmentation performance.
Moreover, we also observe that our method can deal with novel instrument
prediction in the deployment domain. We hope our inspiring results will
encourage researchers to emphasize data-centric methods to overcome demanding
deep learning limitations besides data shortage, such as class imbalance,
domain adaptation, and incremental learning. Our code is available at
https://github.com/lofrienger/Single_SurgicalScene_For_Segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Schizophrenia with 3D Structural Brain MRI Using Deep Learning. (arXiv:2206.12980v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12980">
<div class="article-summary-box-inner">
<span><p>Schizophrenia is a chronic neuropsychiatric disorder that causes distinct
structural alterations within the brain. We hypothesize that deep learning
applied to a structural neuroimaging dataset could detect disease-related
alteration and improve classification and diagnostic accuracy. We tested this
hypothesis using a single, widely available, and conventional T1-weighted MRI
scan, from which we extracted the 3D whole-brain structure using standard
post-processing methods. A deep learning model was then developed, optimized,
and evaluated on three open datasets with T1-weighted MRI scans of patients
with schizophrenia. Our proposed model outperformed the benchmark model, which
was also trained with structural MR images using a 3D CNN architecture. Our
model is capable of almost perfectly (area under the ROC curve = 0.987)
distinguishing schizophrenia patients from healthy controls on unseen
structural MRI scans. Regional analysis localized subcortical regions and
ventricles as the most predictive brain regions. Subcortical structures serve a
pivotal role in cognitive, affective, and social functions in humans, and
structural abnormalities of these regions have been associated with
schizophrenia. Our finding corroborates that schizophrenia is associated with
widespread alterations in subcortical brain structure and the subcortical
structural information provides prominent features in diagnostic
classification. Together, these results further demonstrate the potential of
deep learning to improve schizophrenia diagnosis and identify its structural
neuroimaging signatures from a single, standard T1-weighted brain MRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BoT-SORT: Robust Associations Multi-Pedestrian Tracking. (arXiv:2206.14651v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14651">
<div class="article-summary-box-inner">
<span><p>The goal of multi-object tracking (MOT) is detecting and tracking all the
objects in a scene, while keeping a unique identifier for each object. In this
paper, we present a new robust state-of-the-art tracker, which can combine the
advantages of motion and appearance information, along with camera-motion
compensation, and a more accurate Kalman filter state vector. Our new trackers
BoT-SORT, and BoT-SORT-ReID rank first in the datasets of MOTChallenge [29, 11]
on both MOT17 and MOT20 test sets, in terms of all the main MOT metrics: MOTA,
IDF1, and HOTA. For MOT17: 80.5 MOTA, 80.2 IDF1, and 65.0 HOTA are achieved.
The source code and the pre-trained models are available at
https://github.com/NirAharon/BOT-SORT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patient-specific modelling, simulation and real time processing for constrictive respiratory diseases. (arXiv:2207.01082v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01082">
<div class="article-summary-box-inner">
<span><p>Asthma is a common chronic disease of the respiratory system causing
significant disability and societal burden. It affects over 500 million people
worldwide and generates costs exceeding $USD 56 billion in 2011 in the United
States. Managing asthma involves controlling symptoms, preventing
exacerbations, and maintaining lung function. Improving asthma control affects
the daily life of patients and is associated with a reduced risk of
exacerbations and lung function impairment, reduces the cost of asthma care and
indirect costs associated with reduced productivity. Understanding the complex
dynamics of the pulmonary system and the lung's response to disease, injury,
and treatment is fundamental to the advancement of Asthma treatment.
Computational models of the respiratory system seek to provide a theoretical
framework to understand the interaction between structure and function. Their
application can improve pulmonary medicine by a patient-specific approach to
medicinal methodologies optimizing the delivery given the personalized geometry
and personalized ventilation patterns while introducing a patient-specific
technique that maximizes drug delivery. A three-fold objective addressed within
this dissertation becomes prominent at this point. The first part refers to the
comprehension of pulmonary pathophysiology and the mechanics of Asthma and
subsequently of constrictive pulmonary conditions in general. The second part
refers to the design and implementation of tools that facilitate personalized
medicine to improve delivery and effectiveness. Finally, the third part refers
to the self-management of the condition, meaning that medical personnel and
patients have access to tools and methods that allow the first party to easily
track the course of the condition and the second party, i.e. the patient to
easily self-manage it alleviating the significant burden from the health
system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DecisioNet -- A Binary-Tree Structured Neural Network. (arXiv:2207.01127v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01127">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) and decision trees (DTs) are both
state-of-the-art classifiers. DNNs perform well due to their representational
learning capabilities, while DTs are computationally efficient as they perform
inference along one route (root-to-leaf) that is dependent on the input data.
In this paper, we present DecisioNet (DN), a binary-tree structured neural
network. We propose a systematic way to convert an existing DNN into a DN to
create a lightweight version of the original model. DecisioNet takes the best
of both worlds - it uses neural modules to perform representational learning
and utilizes its tree structure to perform only a portion of the computations.
We evaluate various DN architectures, along with their corresponding baseline
models on the FashionMNIST, CIFAR10, and CIFAR100 datasets. We show that the DN
variants achieve similar accuracy while significantly reducing the
computational cost of the original network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selectively increasing the diversity of GAN-generated samples. (arXiv:2207.01561v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01561">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) are powerful models able to synthesize
data samples closely resembling the distribution of real data, yet the
diversity of those generated samples is limited due to the so-called mode
collapse phenomenon observed in GANs. Especially prone to mode collapse are
conditional GANs, which tend to ignore the input noise vector and focus on the
conditional information. Recent methods proposed to mitigate this limitation
increase the diversity of generated samples, yet they reduce the performance of
the models when similarity of samples is required. To address this shortcoming,
we propose a novel method to selectively increase the diversity of
GAN-generated samples. By adding a simple, yet effective regularization to the
training loss function we encourage the generator to discover new data modes
for inputs related to diverse outputs while generating consistent samples for
the remaining ones. More precisely, we maximise the ratio of distances between
generated images and input latent vectors scaling the effect according to the
diversity of samples for a given conditional input. We show the superiority of
our method in a synthetic benchmark as well as a real-life scenario of
simulating data from the Zero Degree Calorimeter of ALICE experiment in LHC,
CERN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Coding for Machines with Omnipotent Feature Learning. (arXiv:2207.01932v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01932">
<div class="article-summary-box-inner">
<span><p>Image Coding for Machines (ICM) aims to compress images for AI tasks analysis
rather than meeting human perception. Learning a kind of feature that is both
general (for AI tasks) and compact (for compression) is pivotal for its
success. In this paper, we attempt to develop an ICM framework by learning
universal features while also considering compression. We name such features as
omnipotent features and the corresponding framework as Omni-ICM. Considering
self-supervised learning (SSL) improves feature generalization, we integrate it
with the compression task into the Omni-ICM framework to learn omnipotent
features. However, it is non-trivial to coordinate semantics modeling in SSL
and redundancy removing in compression, so we design a novel information
filtering (IF) module between them by co-optimization of instance
distinguishment and entropy minimization to adaptively drop information that is
weakly related to AI tasks (e.g., some texture redundancy). Different from
previous task-specific solutions, Omni-ICM could directly support AI tasks
analysis based on the learned omnipotent features without joint training or
extra transformation. Albeit simple and intuitive, Omni-ICM significantly
outperforms existing traditional and learning-based codecs on multiple
fundamental vision tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Label Retinal Disease Classification using Transformers. (arXiv:2207.02335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02335">
<div class="article-summary-box-inner">
<span><p>Early detection of retinal diseases is one of the most important means of
preventing partial or permanent blindness in patients. In this research, a
novel multi-label classification system is proposed for the detection of
multiple retinal diseases, using fundus images collected from a variety of
sources. First, a new multi-label retinal disease dataset, the MuReD dataset,
is constructed, using a number of publicly available datasets for fundus
disease classification. Next, a sequence of post-processing steps is applied to
ensure the quality of the image data and the range of diseases, present in the
dataset. For the first time in fundus multi-label disease classification, a
transformer-based model optimized through extensive experimentation is used for
image analysis and decision making. Numerous experiments are performed to
optimize the configuration of the proposed system. It is shown that the
approach performs better than state-of-the-art works on the same task by 7.9%
and 8.1% in terms of AUC score for disease detection and disease
classification, respectively. The obtained results further support the
potential applications of transformer-based architectures in the medical
imaging field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Counterfactual Image Manipulation via CLIP. (arXiv:2207.02812v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02812">
<div class="article-summary-box-inner">
<span><p>Leveraging StyleGAN's expressivity and its disentangled latent codes,
existing methods can achieve realistic editing of different visual attributes
such as age and gender of facial images. An intriguing yet challenging problem
arises: Can generative models achieve counterfactual editing against their
learnt priors? Due to the lack of counterfactual samples in natural datasets,
we investigate this problem in a text-driven manner with
Contrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic
knowledge even for various counterfactual concepts. Different from in-domain
manipulation, counterfactual manipulation requires more comprehensive
exploitation of semantic knowledge encapsulated in CLIP as well as more
delicate handling of editing directions for avoiding being stuck in local
minimum or undesired editing. To this end, we design a novel contrastive loss
that exploits predefined CLIP-space directions to guide the editing toward
desired directions from different perspectives. In addition, we design a simple
yet effective scheme that explicitly maps CLIP embeddings (of target text) to
the latent space and fuses them with latent codes for effective latent code
optimization and accurate editing. Extensive experiments show that our design
achieves accurate and realistic editing while driving by target texts with
various counterfactual concepts.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-10 23:07:52.010814201 UTC">2022-07-10 23:07:52 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>