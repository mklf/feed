<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-16T01:30:00Z">02-16</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Memory as a Differentiable Search Index. (arXiv:2202.06991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06991">
<div class="article-summary-box-inner">
<span><p>In this paper, we demonstrate that information retrieval can be accomplished
with a single Transformer, in which all information about the corpus is encoded
in the parameters of the model. To this end, we introduce the Differentiable
Search Index (DSI), a new paradigm that learns a text-to-text model that maps
string queries directly to relevant docids; in other words, a DSI model answers
queries directly using only its parameters, dramatically simplifying the whole
retrieval process. We study variations in how documents and their identifiers
are represented, variations in training procedures, and the interplay between
models and corpus sizes. Experiments demonstrate that given appropriate design
choices, DSI significantly outperforms strong baselines such as dual encoder
models. Moreover, DSI demonstrates strong generalization capabilities,
outperforming a BM25 baseline in a zero-shot setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exhaustivity and anti-exhaustivity in the RSA framework: Testing the effect of prior beliefs. (arXiv:2202.07023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07023">
<div class="article-summary-box-inner">
<span><p>During communication, the interpretation of utterances is sensitive to a
listener's probabilistic prior beliefs, something which is captured by one
currently influential model of pragmatics, the Rational Speech Act (RSA)
framework. In this paper we focus on cases when this sensitivity to priors
leads to counterintuitive predictions of the framework. Our domain of interest
is exhaustivity effects, whereby a sentence such as "Mary came" is understood
to mean that only Mary came. We show that in the baseline RSA model, under
certain conditions, anti-exhaustive readings are predicted (e.g., "Mary came"
would be used to convey that both Mary and Peter came). The specific question
we ask is the following: should exhaustive interpretations be derived as purely
pragmatic inferences (as in the classical Gricean view, endorsed in the
baseline RSA model), or should they rather be generated by an encapsulated
semantic mechanism (as argued in some of the recent formal literature)? To
answer this question, we provide a detailed theoretical analysis of different
RSA models and evaluate them against data obtained in a new study which tested
the effects of prior beliefs on both production and comprehension, improving on
previous empirical work. We found no anti-exhaustivity effects, but observed
that message choice is sensitive to priors, as predicted by the RSA framework
overall. The best models turn out to be those which include an encapsulated
exhaustivity mechanism (as other studies concluded on the basis of very
different data). We conclude that, on the one hand, in the division of labor
between semantics and pragmatics, semantics plays a larger role than is often
thought, but, on the other hand, the tradeoff between informativity and cost
which characterizes all RSA models does play a central role for genuine
pragmatic effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07028">
<div class="article-summary-box-inner">
<span><p>We study the problem of developing autonomous agents that can follow human
instructions to infer and perform a sequence of actions to complete the
underlying task. Significant progress has been made in recent years, especially
for tasks with short horizons. However, when it comes to long-horizon tasks
with extended sequences of actions, an agent can easily ignore some
instructions or get stuck in the middle of the long instructions and eventually
fail the task. To address this challenge, we propose a model-agnostic
milestone-based task tracker (M-TRACK) to guide the agent and monitor its
progress. Specifically, we propose a milestone builder that tags the
instructions with navigation and interaction milestones which the agent needs
to complete step by step, and a milestone checker that systemically checks the
agent's progress in its current milestone and determines when to proceed to the
next. On the challenging ALFRED dataset, our M-TRACK leads to a notable 45% and
70% relative improvement in unseen success rate over two competitive base
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regional Differences in Information Privacy Concerns After the Facebook-Cambridge Analytica Data Scandal. (arXiv:2202.07075v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07075">
<div class="article-summary-box-inner">
<span><p>While there is increasing global attention to data privacy, most of their
current theoretical understanding is based on research conducted in a few
countries. Prior work argues that people's cultural backgrounds might shape
their privacy concerns; thus, we could expect people from different world
regions to conceptualize them in diverse ways. We collected and analyzed a
large-scale dataset of tweets about the #CambridgeAnalytica scandal in Spanish
and English to start exploring this hypothesis. We employed word embeddings and
qualitative analysis to identify which information privacy concerns are present
and characterize language and regional differences in emphasis on these
concerns. Our results suggest that related concepts, such as regulations, can
be added to current information privacy frameworks. We also observe a greater
emphasis on data collection in English than in Spanish. Additionally, data from
North America exhibits a narrower focus on awareness compared to other regions
under study. Our results call for more diverse sources of data and nuanced
analysis of data privacy concerns around the globe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing the ICBe Dataset: Very High Recall and Precision Event Extraction from Narratives about International Crises. (arXiv:2202.07081v1 [stat.AP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07081">
<div class="article-summary-box-inner">
<span><p>How do international crises unfold? We conceive of international affairs as a
strategic chess game between adversaries, necessitating a systematic way to
measure pieces, moves, and gambits accurately and consistently over different
contexts and periods. We develop such a measurement strategy with an ontology
of crisis actions and interactions and apply it to a high-quality corpus of
crisis narratives recorded by the International Crisis Behavior (ICB) Project.
We demonstrate that the ontology has high coverage over most of the thoughts,
speech, and actions contained in these narratives and produces high inter-coder
agreement when applied by human coders. We introduce a new crisis event dataset
ICB Events (ICBe). We find that ICBe captures the process of a crisis with
greater accuracy and granularity than other well-regarded events or crisis
datasets. We make the data, replication material, and additional visualizations
available at a companion website www.crisisevents.org.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching Tweets With Applicable Fact-Checks Across Languages. (arXiv:2202.07094v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07094">
<div class="article-summary-box-inner">
<span><p>An important challenge for news fact-checking is the effective dissemination
of existing fact-checks. This in turn brings the need for reliable methods to
detect previously fact-checked claims. In this paper, we focus on automatically
finding existing fact-checks for claims made in social media posts (tweets). We
conduct both classification and retrieval experiments, in monolingual (English
only), multilingual (Spanish, Portuguese), and cross-lingual (Hindi-English)
settings using multilingual transformer models such as XLM-RoBERTa and
multilingual embeddings such as LaBSE and SBERT. We present promising results
for "match" classification (93% average accuracy) in four language pairs. We
also find that a BM25 baseline outperforms state-of-the-art multilingual
embedding models for the retrieval task during our monolingual experiments. We
highlight and discuss NLP challenges while addressing this problem in different
languages, and we introduce a novel curated dataset of fact-checks and
corresponding tweets for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Dynamic Neural Networks for Natural Language Processing. (arXiv:2202.07101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07101">
<div class="article-summary-box-inner">
<span><p>Effectively scaling large Transformer models is a main driver of recent
advances in natural language processing. Dynamic neural networks, as an
emerging research direction, are capable of scaling up neural networks with
sub-linear increases in computation and time by dynamically adjusting their
computational path based on the input. Dynamic neural networks could be a
promising solution to the growing parameter numbers of pretrained language
models, allowing both model pretraining with trillions of parameters and faster
inference on mobile devices. In this survey, we summarize progress of three
types of dynamic neural networks in NLP: skimming, mixture of experts, and
early exit. We also highlight current challenges in dynamic neural networks and
directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Model Compression for Natural Language Processing. (arXiv:2202.07105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07105">
<div class="article-summary-box-inner">
<span><p>With recent developments in new architectures like Transformer and
pretraining techniques, significant progress has been made in applications of
natural language processing (NLP). However, the high energy cost and long
inference delay of Transformer is preventing NLP from entering broader
scenarios including edge and mobile computing. Efficient NLP research aims to
comprehensively consider computation, time and carbon emission for the entire
life-cycle of NLP, including data preparation, model training and inference. In
this survey, we focus on the inference stage and review the current state of
model compression for NLP, including the benchmarks, metrics and methodology.
We outline the current obstacles and future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge. (arXiv:2202.07138v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07138">
<div class="article-summary-box-inner">
<span><p>Automated planning focuses on strategies, building domain models and
synthesizing plans to transit initial states to goals. Natural language
processing concerns with the interactions between agents and human language,
especially processing and analyzing large amounts of natural language data.
These two fields have abilities to generate explicit knowledge, e.g.,
preconditions and effects of action models, and learn from tacit knowledge,
e.g., neural models, respectively. Integrating AI planning and natural language
processing effectively improves the communication between human and intelligent
agents. This paper outlines the commons and relations between AI planning and
natural language processing, argues that each of them can effectively impact on
the other one by four areas: (1) planning-based text understanding, (2)
planning-based text generation, (3) text-based human-robot interaction, and (4)
text-based explainable planning. We also explore some potential future issues
between AI planning and natural language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsPod: Automatic and Interactive News Podcasts. (arXiv:2202.07146v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07146">
<div class="article-summary-box-inner">
<span><p>News podcasts are a popular medium to stay informed and dive deep into news
topics. Today, most podcasts are handcrafted by professionals. In this work, we
advance the state-of-the-art in automatically generated podcasts, making use of
recent advances in natural language processing and text-to-speech technology.
We present NewsPod, an automatically generated, interactive news podcast. The
podcast is divided into segments, each centered on a news event, with each
segment structured as a Question and Answer conversation, whose goal is to
engage the listener. A key aspect of the design is the use of distinct voices
for each role (questioner, responder), to better simulate a conversation.
Another novel aspect of NewsPod allows listeners to interact with the podcast
by asking their own questions and receiving automatically generated answers. We
validate the soundness of this system design through two usability studies,
focused on evaluating the narrative style and interactions with the podcast,
respectively. We find that NewsPod is preferred over a baseline by
participants, with 80% claiming they would use the system in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Tracking Dialogue State by Inheriting Slot Values in Mentioned Slot Pools. (arXiv:2202.07156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07156">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) is a component of the task-oriented dialogue
system. It is responsible for extracting and managing slot values according to
dialogue utterances, where each slot represents an essential part of the
information to accomplish a task, and slot value is updated recurrently in each
dialogue turn. However, many DST models cannot update slot values
appropriately. These models may repeatedly inherit wrong slot values extracted
in previous turns, resulting in the fail of the entire DST task.They cannot
update indirectly mentioned slots well, either. This study designed a model
with a mentioned slot pool (MSP) to tackle the update problem. The MSP is a
slot-specific memory that records all mentioned slot values that may be
inherited, and our model updates slot values according to the MSP and the
dialogue context. Our model rejects inheriting the previous slot value when it
predicates the value is wrong. Then, it re-extracts the slot value from the
current dialogue context. As the contextual information accumulates with the
dialogue progress, the new value is more likely to be correct. It also can
track the indirectly mentioned slot by picking a value from the MSP.
Experimental results showed our model reached state-of-the-art DST performance
on MultiWOZ 2.1 and 2.2 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Pretraining Term Frequencies on Few-Shot Reasoning. (arXiv:2202.07206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07206">
<div class="article-summary-box-inner">
<span><p>Pretrained Language Models (LMs) have demonstrated ability to perform
numerical reasoning by extrapolating from a few examples in few-shot settings.
However, the extent to which this extrapolation relies on robust reasoning is
unclear. In this paper, we investigate how well these models reason with terms
that are less frequent in the pretraining data. In particular, we examine the
correlations between the model performance on test instances and the frequency
of terms from those instances in the pretraining data. We measure the strength
of this correlation for a number of GPT-based language models (pretrained on
the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and
unit conversion). Our results consistently demonstrate that models are more
accurate on instances whose terms are more prevalent, in some cases above
$70\%$ (absolute) more accurate on the top 10\% frequent terms in comparison to
the bottom 10\%. Overall, although LMs exhibit strong performance at few-shot
numerical reasoning tasks, our results raise the question of how much models
actually generalize beyond pretraining data, and we encourage researchers to
take the pretraining data into account when interpreting evaluation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CommerceMM: Large-Scale Commerce MultiModal Representation Learning with Omni Retrieval. (arXiv:2202.07247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07247">
<div class="article-summary-box-inner">
<span><p>We introduce CommerceMM - a multimodal model capable of providing a diverse
and granular understanding of commerce topics associated to the given piece of
content (image, text, image+text), and having the capability to generalize to a
wide range of tasks, including Multimodal Categorization, Image-Text Retrieval,
Query-to-Product Retrieval, Image-to-Product Retrieval, etc. We follow the
pre-training + fine-tuning training regime and present 5 effective pre-training
tasks on image-text pairs. To embrace more common and diverse commerce data
with text-to-multimodal, image-to-multimodal, and multimodal-to-multimodal
mapping, we propose another 9 novel cross-modal and cross-pair retrieval tasks,
called Omni-Retrieval pre-training. The pre-training is conducted in an
efficient manner with only two forward/backward updates for the combined 14
tasks. Extensive experiments and analysis show the effectiveness of each task.
When combining all pre-training tasks, our model achieves state-of-the-art
performance on 7 commerce-related downstream tasks after fine-tuning.
Additionally, we propose a novel approach of modality randomization to
dynamically adjust our model under different efficiency constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Cross-lingual Prompting with Mask Token Augmentation. (arXiv:2202.07255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07255">
<div class="article-summary-box-inner">
<span><p>Prompting shows promising results in few-shot scenarios. However, its
strength for multilingual/cross-lingual problems has not been fully exploited.
Zhao and Sch\"utze (2021) made initial explorations in this direction by
presenting that cross-lingual prompting outperforms cross-lingual finetuning.
In this paper, we conduct empirical analysis on the effect of each component in
cross-lingual prompting and derive Universal Prompting across languages, which
helps alleviate the discrepancies between source-language training and
target-language inference. Based on this, we propose a mask token augmentation
framework to further improve the performance of prompt-based cross-lingual
transfer. Notably, for XNLI, our method achieves 46.54% with only 16 English
training examples per class, significantly better than 34.99% of finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Media Slant is Contagious. (arXiv:2202.07269v1 [econ.GN])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07269">
<div class="article-summary-box-inner">
<span><p>This paper analyzes the influence of partisan content from national cable TV
news on local reporting in U.S. newspapers. We provide a new
machine-learning-based measure of cable news slant, trained on a corpus of 40K
transcribed TV episodes from Fox News Channel (FNC), CNN, and MSNBC
(2005-2008). Applying the method to a corpus of 24M local newspaper articles,
we find that in response to an exogenous increase in local viewership of FNC
relative to CNN/MSNBC, local newspaper articles become more similar to FNC
transcripts (and vice versa). Consistent with newspapers responding to changes
in reader preferences, we see a shift in the framing of local news coverage
rather than just direct borrowing of cable news content. Further, cable news
slant polarizes local news content: right-leaning newspapers tend to adopt
right-wing FNC language, while left-leaning newspapers tend to become more
left-wing. Media slant is contagious.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saving Dense Retriever from Shortcut Dependency in Conversational Search. (arXiv:2202.07280v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07280">
<div class="article-summary-box-inner">
<span><p>In conversational search (CS), it needs holistic understanding over
conversational inputs to retrieve relevant passages. In this paper, we
demonstrate the existence of a retrieval shortcut in CS, which causes models to
retrieve passages solely relying on partial history while disregarding the
latest question. With in-depth analysis, we first show naively trained dense
retrievers heavily exploit the shortcut and hence perform poorly when asked to
answer history-independent questions. To prevent models from solely relying on
the shortcut, we explore iterative hard negatives mined by pre-trained dense
retrievers. Experimental results show that training with the iterative hard
negatives effectively mitigates the dependency on the shortcut and makes
substantial improvement on recent CS benchmarks. Our retrievers achieve new
state-of-the-art results, outperforming the previous best models by 9.7 in
Recall@10 on QReCC and 12.4 in Recall@5 on TopiOCQA. Furthermore, in our
end-to-end QA experiments, FiD readers combined with our retrievers surpass the
previous state-of-the-art models by 3.7 and 1.0 EM scores on QReCC and
TopiOCQA, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Effective Multi-Task Interaction for Entity-Relation Extraction: A Unified Framework with Selection Recurrent Network. (arXiv:2202.07281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07281">
<div class="article-summary-box-inner">
<span><p>Entity-relation extraction aims to jointly solve named entity recognition
(NER) and relation extraction (RE). Recent approaches use either one-way
sequential information propagation in a pipeline manner or two-way implicit
interaction with a shared encoder. However, they still suffer from poor
information interaction due to the gap between the different task forms of NER
and RE, raising a controversial question whether RE is really beneficial to
NER. Motivated by this, we propose a novel and unified cascade framework that
combines the advantages of both sequential information propagation and implicit
interaction. Meanwhile, it eliminates the gap between the two tasks by
reformulating entity-relation extraction as unified span-extraction tasks.
Specifically, we propose a selection recurrent network as a shared encoder to
encode task-specific independent and shared representations and design two
sequential information propagation strategies to realize the sequential
information flow between NER and RE. Extensive experiments demonstrate that our
approaches can achieve state-of-the-art results on two common benchmarks, ACE05
and SciERC, and effectively model the multi-task interaction, which realizes
significant mutual benefits of NER and RE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. (arXiv:2202.07305v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07305">
<div class="article-summary-box-inner">
<span><p>Image narrative generation describes the creation of stories regarding the
content of image data from a subjective viewpoint. Given the importance of the
subjective feelings of writers, characters, and readers in storytelling, image
narrative generation methods must consider human emotion, which is their major
difference from descriptive caption generation tasks. The development of
automated methods to generate story-like text associated with images may be
considered to be of considerable social significance, because stories serve
essential functions both as entertainment and also for many practical purposes
such as education and advertising. In this study, we propose a model called
ViNTER (Visual Narrative Transformer with Emotion arc Representation) to
generate image narratives that focus on time series representing varying
emotions as "emotion arcs," to take advantage of recent advances in multimodal
Transformer-based pre-trained models. We present experimental results of both
manual and automatic evaluations, which demonstrate the effectiveness of the
proposed emotion-aware approach to image narrative generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">textless-lib: a Library for Textless Spoken Language Processing. (arXiv:2202.07359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07359">
<div class="article-summary-box-inner">
<span><p>Textless spoken language processing research aims to extend the applicability
of standard NLP toolset onto spoken language and languages with few or no
textual resources. In this paper, we introduce textless-lib, a PyTorch-based
library aimed to facilitate research in this research area. We describe the
building blocks that the library provides and demonstrate its usability by
discuss three different use-case examples: (i) speaker probing, (ii) speech
resynthesis and compression, and (iii) speech continuation. We believe that
textless-lib substantially simplifies research the textless setting and will be
handful not only for speech researchers but also for the NLP community at
large. The code, documentation, and pre-trained models are available at
https://github.com/facebookresearch/textlesslib/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuLD: The Multitask Long Document Benchmark. (arXiv:2202.07362v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07362">
<div class="article-summary-box-inner">
<span><p>The impressive progress in NLP techniques has been driven by the development
of multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks
focus on tasks for one or two input sentences, there has been exciting work in
designing efficient techniques for processing much longer inputs. In this
paper, we present MuLD: a new long document benchmark consisting of only
documents over 10,000 tokens. By modifying existing NLP tasks, we create a
diverse benchmark which requires models to successfully model long-term
dependencies in the text. We evaluate how existing models perform, and find
that our benchmark is much more challenging than their `short document'
equivalents. Furthermore, by evaluating both regular and efficient
transformers, we show that models with increased context length are better able
to solve the tasks presented, suggesting that future improvements in these
models are vital for solving similar long document problems. We release the
data and code for baselines to encourage further research on efficient NLP
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Prompt Learning for Explainable Recommendation. (arXiv:2202.07371v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07371">
<div class="article-summary-box-inner">
<span><p>Providing user-understandable explanations to justify recommendations could
help users better understand the recommended items, increase the system's ease
of use, and gain users' trust. A typical approach to realize it is natural
language generation. However, previous works mostly adopt recurrent neural
networks to meet the ends, leaving the potentially more effective pre-trained
Transformer models under-explored. In fact, user and item IDs, as important
identifiers in recommender systems, are inherently in different semantic space
as words that pre-trained models were already trained on. Thus, how to
effectively fuse IDs into such models becomes a critical issue. Inspired by
recent advancement in prompt learning, we come up with two solutions: find
alternative words to represent IDs (called discrete prompt learning), and
directly input ID vectors to a pre-trained model (termed continuous prompt
learning). In the latter case, ID vectors are randomly initialized but the
model is trained in advance on large corpora, so they are actually in different
learning stages. To bridge the gap, we further propose two training strategies:
sequential tuning and recommendation as regularization. Extensive experiments
show that our continuous prompt learning approach equipped with the training
strategies consistently outperforms strong baselines on three datasets of
explainable recommendation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07427">
<div class="article-summary-box-inner">
<span><p>Authors of posts in social media communicate their emotions and what causes
them with text and images. While there is work on emotion and stimulus
detection for each modality separately, it is yet unknown if the modalities
contain complementary emotion information in social media. We aim at filling
this research gap and contribute a novel, annotated corpus of English
multimodal Reddit posts. On this resource, we develop models to automatically
detect the relation between image and text, an emotion stimulus category and
the emotion class. We evaluate if these tasks require both modalities and find
for the image-text relations, that text alone is sufficient for most categories
(complementary, illustrative, opposing): the information in the text allows to
predict if an image is required for emotion understanding. The emotions of
anger and sadness are best predicted with a multimodal model, while text alone
is sufficient for disgust, joy, and surprise. Stimuli depicted by objects,
animals, food, or a person are best predicted by image-only models, while
multimodal models are most effective on art, events, memes, places, or
screenshots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer. (arXiv:2202.07543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07543">
<div class="article-summary-box-inner">
<span><p>Memes are prevalent on the internet and continue to grow and evolve alongside
our culture. An automatic understanding of memes propagating on the internet
can shed light on the general sentiment and cultural attitudes of people. In
this work, we present team BLUE's solution for the second edition of the
MEMOTION competition. We showcase two approaches for meme classification (i.e.
sentiment, humour, offensive, sarcasm and motivation levels) using a text-only
method using BERT, and a Multi-Modal-Multi-Task transformer network that
operates on both the meme image and its caption to output the final scores. In
both approaches, we leverage state-of-the-art pretrained models for text (BERT,
Sentence Transformer) and image processing (EfficientNetV4, CLIP). Through our
efforts, we obtain first place in task A, second place in task B and third
place in task C. In addition, our team obtained the highest average score for
all three tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shifting Trends of COVID-19 Tweet Sentiment with Respect to Voting Preferences in the 2020 Election Year of the United States. (arXiv:2202.07587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07587">
<div class="article-summary-box-inner">
<span><p>COVID-19 related policies were extensively politicized during the 2020
election year of the United States, resulting in polarizing viewpoints. Twitter
users were particularly engaged during the 2020 election year. Here we
investigated whether COVID-19 related tweets were associated with the overall
election results at the state level during the period leading up to the
election day. We observed weak correlations between the average sentiment of
COVID-19 related tweets and popular votes in two-week intervals, and the trends
gradually become opposite. We then compared the average sentiments of COVID-19
related tweets between states called in favor of Republican (red states) or
Democratic parties (blue states). We found that at the beginning of lockdowns
sentiments in the blue states were much more positive than those in the red
states. However, sentiments in the red states gradually become more positive
during the summer of 2020 and persisted until the election day.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PILED: An Identify-and-Localize Framework for Few-Shot Event Detection. (arXiv:2202.07615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07615">
<div class="article-summary-box-inner">
<span><p>Practical applications of event extraction systems have long been hindered by
their need for heavy human annotation. In order to scale up to new domains and
event types, models must learn to cope with limited supervision, as in few-shot
learning settings. To this end, the major challenge is to let the model master
the semantics of event types, without requiring abundant event mention
annotations. In our study, we employ cloze prompts to elicit event-related
knowledge from pretrained language models and further use event definitions and
keywords to pinpoint the trigger word. By formulating the event detection task
as an identify-then-localize procedure, we minimize the number of type-specific
parameters, enabling our model to quickly adapt to event detection tasks for
new types. Experiments on three event detection benchmark datasets (ACE,
FewEvent, MAVEN) show that our proposed method performs favorably under fully
supervised settings and surpasses existing few-shot methods by 21% F1 on the
FewEvent dataset and 20% on the MAVEN dataset when only 5 examples are provided
for each event type.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving Deeper into Cross-lingual Visual Question Answering. (arXiv:2202.07630v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07630">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) is one of the crucial vision-and-language
tasks. Yet, the bulk of research until recently has focused only on the English
language due to the lack of appropriate evaluation resources. Previous work on
cross-lingual VQA has reported poor zero-shot transfer performance of current
multilingual multimodal Transformers and large gaps to monolingual performance,
attributed mostly to misalignment of text embeddings between the source and
target languages, without providing any additional deeper analyses. In this
work, we delve deeper and address different aspects of cross-lingual VQA
holistically, aiming to understand the impact of input data, fine-tuning and
evaluation regimes, and interactions between the two modalities in
cross-lingual setups. 1) We tackle low transfer performance via novel methods
that substantially reduce the gap to monolingual English performance, yielding
+10 accuracy points over existing transfer methods. 2) We study and dissect
cross-lingual VQA across different question types of varying complexity, across
different multilingual multi-modal Transformers, and in zero-shot and few-shot
scenarios. 3) We further conduct extensive analyses on modality biases in
training data and models, aimed to further understand why zero-shot performance
gaps remain for some question types and languages. We hope that the novel
methods and detailed analyses will guide further progress in multilingual VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Configuration to Rule Them All? Towards Hyperparameter Transfer in Topic Models using Multi-Objective Bayesian Optimization. (arXiv:2202.07631v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07631">
<div class="article-summary-box-inner">
<span><p>Topic models are statistical methods that extract underlying topics from
document collections. When performing topic modeling, a user usually desires
topics that are coherent, diverse between each other, and that constitute good
document representations for downstream tasks (e.g. document classification).
In this paper, we conduct a multi-objective hyperparameter optimization of
three well-known topic models. The obtained results reveal the conflicting
nature of different objectives and that the training corpus characteristics are
crucial for the hyperparameter selection, suggesting that it is possible to
transfer the optimal hyperparameter configurations between datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Memorization Across Neural Language Models. (arXiv:2202.07646v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07646">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) have been shown to memorize parts of their
training data, and when prompted appropriately, they will emit the memorized
training data verbatim. This is undesirable because memorization violates
privacy (exposing user data), degrades utility (repeated easy-to-memorize text
is often low quality), and hurts fairness (some texts are memorized over
others).
</p>
<p>We describe three log-linear relationships that quantify the degree to which
LMs emit memorized training data. Memorization significantly grows as we
increase (1) the capacity of a model, (2) the number of times an example has
been duplicated, and (3) the number of tokens of context used to prompt the
model. Surprisingly, we find the situation becomes complicated when
generalizing these results across model families. On the whole, we find that
memorization in LMs is more prevalent than previously believed and will likely
get worse as models continues to scale, at least without active mitigations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation. (arXiv:2202.07654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07654">
<div class="article-summary-box-inner">
<span><p>The predictions of question answering (QA) systems are typically evaluated
against manually annotated finite sets of one or more answers. This leads to a
coverage limitation that results in underestimating the true performance of
systems, and is typically addressed by extending over exact match (EM) with
predefined rules or with the token-level F1 measure. In this paper, we present
the first systematic conceptual and data-driven analysis to examine the
shortcomings of token-level equivalence measures.
</p>
<p>To this end, we define the asymmetric notion of answer equivalence (AE),
accepting answers that are equivalent to or improve over the reference, and
collect over 26K human judgements for candidates produced by multiple QA
systems on SQuAD. Through a careful analysis of this data, we reveal and
quantify several concrete limitations of the F1 measure, such as false
impression of graduality, missing dependence on question, and more.
</p>
<p>Since collecting AE annotations for each evaluated model is expensive, we
learn a BERT matching BEM measure to approximate this task. Being a simpler
task than QA, we find BEM to provide significantly better AE approximations
than F1, and more accurately reflect the performance of systems.
</p>
<p>Finally, we also demonstrate the practical utility of AE and BEM on the
concrete application of minimal accurate prediction sets, reducing the number
of required answers by up to 2.6 times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Linearity of Cross-Lingual Word Embedding Mappings. (arXiv:2004.01079v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.01079">
<div class="article-summary-box-inner">
<span><p>The technique of Cross-Lingual Word Embedding (CLWE) plays a fundamental role
in tackling Natural Language Processing challenges for low-resource languages.
Its dominant approaches assumed that the relationship between embeddings could
be represented by a linear mapping, but there has been no exploration of the
conditions under which this assumption holds. Such a research gap becomes very
critical recently, as it has been evidenced that relaxing mappings to be
non-linear can lead to better performance in some cases. We, for the first
time, present a theoretical analysis that identifies the preservation of
analogies encoded in monolingual word embeddings as a necessary and sufficient
condition for the ground-truth CLWE mapping between those embeddings to be
linear. On a novel cross-lingual analogy dataset that covers five
representative analogy categories for twelve distinct languages, we carry out
experiments which provide direct empirical support for our theoretical claim.
These results offer additional insight into the observations of other
researchers and contribute inspiration for the development of more effective
cross-lingual representation learning strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation Inconsistency and Entity Bias in MultiWOZ. (arXiv:2105.14150v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14150">
<div class="article-summary-box-inner">
<span><p>MultiWOZ is one of the most popular multi-domain task-oriented dialog
datasets, containing 10K+ annotated dialogs covering eight domains. It has been
widely accepted as a benchmark for various dialog tasks, e.g., dialog state
tracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog
modeling. In this work, we identify an overlooked issue with dialog state
annotation inconsistencies in the dataset, where a slot type is tagged
inconsistently across similar dialogs leading to confusion for DST modeling. We
propose an automated correction for this issue, which is present in a whopping
70% of the dialogs. Additionally, we notice that there is significant entity
bias in the dataset (e.g., "cambridge" appears in 50% of the destination cities
in the train domain). The entity bias can potentially lead to named entity
memorization in generative models, which may go unnoticed as the test set
suffers from a similar entity bias as well. We release a new test set with all
entities replaced with unseen entities. Finally, we benchmark joint goal
accuracy (JGA) of the state-of-the-art DST baselines on these modified versions
of the data. Our experiments show that the annotation inconsistency corrections
lead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in
JGA when models are evaluated on the new test set with unseen entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced End-to-End Monolingual pre-training for Low-Resourced Indic Languages Code-Switching Speech Recognition. (arXiv:2106.05885v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05885">
<div class="article-summary-box-inner">
<span><p>The success in designing Code-Switching (CS) ASR often depends on the
availability of the transcribed CS resources. Such dependency harms the
development of ASR in low-resourced languages such as Bengali and Hindi. In
this paper, we exploit the transfer learning approach to design End-to-End
(E2E) CS ASR systems for the two low-resourced language pairs using different
monolingual speech data and a small set of noisy CS data. We trained the
CS-ASR, following two steps: (i) building a robust bilingual ASR system using a
convolution-augmented transformer (Conformer) based acoustic model and n-gram
language model, and (ii) fine-tuned the entire E2E ASR with limited noisy CS
data. We tested our method on MUCS 2021 challenge and achieved 3rd place in the
CS track. We then tested the proposed method using noisy CS data released for
Hindi-English and Bengali-English pairs in Multilingual and Code-Switching ASR
Challenges for Low Resource Indian Languages (MUCS 2021) and achieved 3rd place
in the CS track. Unlike, the leading two systems that benefited from crawling
YouTube and learning transliteration pairs, our proposed transfer learning
approach focused on using only the limited CS data with no data-cleaning or
data re-segmentation. Our approach achieved 14.1% relative gain in word error
rate (WER) in Hindi-English and 27.1% in Bengali-English. We provide detailed
guidelines on the steps to finetune the self-attention based model for limited
data for ASR. Moreover, we release the code and recipe used in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Cybersecurity Topic Classification Tool. (arXiv:2109.02473v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02473">
<div class="article-summary-box-inner">
<span><p>In this research, we use user defined labels from three internet text sources
(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models
for the topic classification task of detecting cybersecurity discussions in
natural text. We analyze the false positive and false negative rates of each of
the 21 model's in a cross validation experiment. Then we present a
Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of
the 21 trained machine learning models as the decision mechanism for detecting
cybersecurity related text. We also show that the majority vote mechanism of
the CTC tool provides lower false negative and false positive rates on average
than any of the 21 individual models. We show that the CTC tool is scalable to
the hundreds of thousands of documents with a wall clock time on the order of
hours.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation. (arXiv:2109.05812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05812">
<div class="article-summary-box-inner">
<span><p>With the rapid increase of multimedia data, a large body of literature has
emerged to work on multimodal summarization, the majority of which target at
refining salient information from textual and visual modalities to output a
pictorial summary with the most relevant images. Existing methods mostly focus
on either extractive or abstractive summarization and rely on qualified image
captions to build image references. We are the first to propose a Unified
framework for Multimodal Summarization grounding on BART, UniMS, that
integrates extractive and abstractive objectives, as well as selecting the
image output. Specially, we adopt knowledge distillation from a vision-language
pretrained model to improve image selection, which avoids any requirement on
the existence and quality of image captions. Besides, we introduce a visual
guided decoder to better integrate textual and visual modalities in guiding
abstractive text generation. Results show that our best model achieves a new
state-of-the-art result on a large-scale benchmark dataset. The newly involved
extractive objective as well as the knowledge distillation technique are proven
to bring a noticeable improvement to the multimodal summarization task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. (arXiv:2110.01691v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01691">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) have demonstrated impressive potential
on simple tasks, their breadth of scope, lack of transparency, and insufficient
controllability can make them less effective when assisting humans on more
complex tasks. In response, we introduce the concept of Chaining LLM steps
together, where the output of one step becomes the input for the next, thus
aggregating the gains per step. We first define a set of LLM primitive
operations useful for Chain construction, then present an interactive system
where users can modify these Chains, along with their intermediate results, in
a modular way. In a 20-person user study, we found that Chaining not only
improved the quality of task outcomes, but also significantly enhanced system
transparency, controllability, and sense of collaboration. Additionally, we saw
that users developed new ways of interacting with LLMs through Chains: they
leveraged sub-tasks to calibrate model expectations, compared and contrasted
alternative strategies by observing parallel downstream effects, and debugged
unexpected model outputs by "unit-testing" sub-components of a Chain. In two
case studies, we further explore how LLM Chains may be used in future
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Disentangled Arguments with Prompts: A Simple Event Extraction Framework that Works. (arXiv:2110.04525v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04525">
<div class="article-summary-box-inner">
<span><p>Event Extraction bridges the gap between text and event signals. Based on the
assumption of trigger-argument dependency, existing approaches have achieved
state-of-the-art performance with expert-designed templates or complicated
decoding constraints. In this paper, for the first time we introduce the
prompt-based learning strategy to the domain of Event Extraction, which
empowers the automatic exploitation of label semantics on both input and output
sides. To validate the effectiveness of the proposed generative method, we
conduct extensive experiments with 11 diverse baselines. Empirical results show
that, in terms of F1 score on Argument Extraction, our simple architecture is
stronger than any other generative counterpart and even competitive with
algorithms that require template engineering. Regarding the measure of recall,
it sets new overall records for both Argument and Trigger Extractions. We
hereby recommend this framework to the community, with the code publicly
available at https://git.io/GDAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph informed Fake News Classification via Heterogeneous Representation Ensembles. (arXiv:2110.10457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10457">
<div class="article-summary-box-inner">
<span><p>Increasing amounts of freely available data both in textual and relational
form offers exploration of richer document representations, potentially
improving the model performance and robustness. An emerging problem in the
modern era is fake news detection -- many easily available pieces of
information are not necessarily factually correct, and can lead to wrong
conclusions or are used for manipulation. In this work we explore how different
document representations, ranging from simple symbolic bag-of-words, to
contextual, neural language model-based ones can be used for efficient fake
news identification. One of the key contributions is a set of novel document
representation learning methods based solely on knowledge graphs, i.e.
extensive collections of (grounded) subject-predicate-object triplets. We
demonstrate that knowledge graph-based representations already achieve
competitive performance to conventionally accepted representation learners.
Furthermore, when combined with existing, contextual representations, knowledge
graph-based document representations can achieve state-of-the-art performance.
To our knowledge this is the first larger-scale evaluation of how knowledge
graph-based representations can be systematically incorporated into the process
of fake news classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textless Speech Emotion Conversion using Discrete and Decomposed Representations. (arXiv:2111.07402v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07402">
<div class="article-summary-box-inner">
<span><p>Speech emotion conversion is the task of modifying the perceived emotion of a
speech utterance while preserving the lexical content and speaker identity. In
this study, we cast the problem of emotion conversion as a spoken language
translation task. We use a decomposition of the speech signal into discrete
learned representations, consisting of phonetic-content units, prosodic
features, speaker, and emotion. First, we modify the speech content by
translating the phonetic-content units to a target emotion, and then predict
the prosodic features based on these units. Finally, the speech waveform is
generated by feeding the predicted representations into a neural vocoder. Such
a paradigm allows us to go beyond spectral and parametric changes of the
signal, and model non-verbal vocalizations, such as laughter insertion, yawning
removal, etc. We demonstrate objectively and subjectively that the proposed
method is vastly superior to current approaches and even beats text-based
systems in terms of perceived emotion and audio quality. We rigorously evaluate
all components of such a complex system and conclude with an extensive model
analysis and ablation study to better emphasize the architectural choices,
strengths and weaknesses of the proposed method. Samples are available under
the following link: https://speechbot.github.io/emotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linking-Enhanced Pre-Training for Table Semantic Parsing. (arXiv:2111.09486v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09486">
<div class="article-summary-box-inner">
<span><p>Recently pre-training models have significantly improved the performance of
various NLP tasks by leveraging large-scale text corpora to improve the
contextual representation ability of the neural network. The large pre-training
language model has also been applied in the area of table semantic parsing.
However, existing pre-training approaches have not carefully explored explicit
interaction relationships between a question and the corresponding database
schema, which is a key ingredient for uncovering their semantic and structural
correspondence. Furthermore, the question-aware representation learning in the
schema grounding context has received less attention in pre-training
objective.To alleviate these issues, this paper designs two novel pre-training
objectives to impose the desired inductive bias into the learned
representations for table pre-training. We further propose a schema-aware
curriculum learning approach to mitigate the impact of noise and learn
effectively from the pre-training data in an easy-to-hard manner. We evaluate
our pre-trained framework by fine-tuning it on two benchmarks, Spider and
SQUALL. The results demonstrate the effectiveness of our pre-training objective
and curriculum compared to a variety of baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic and sentiment analysis of selected Bhagavad Gita translations using BERT-based language framework. (arXiv:2201.03115v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03115">
<div class="article-summary-box-inner">
<span><p>It is well known that translations of songs and poems not only break rhythm
and rhyming patterns, but can also result in loss of semantic information. The
Bhagavad Gita is an ancient Hindu philosophical text originally written in
Sanskrit that features a conversation between Lord Krishna and Arjuna prior to
the Mahabharata war. The Bhagavad Gita is also one of the key sacred texts in
Hinduism and is known as the forefront of the Vedic corpus of Hinduism. In the
last two centuries, there has been a lot of interest in Hindu philosophy from
western scholars; hence, the Bhagavad Gita has been translated in a number of
languages. However, there is not much work that validates the quality of the
English translations. Recent progress of language models powered by deep
learning has enabled not only translations but a better understanding of
language and texts with semantic and sentiment analysis. Our work is motivated
by the recent progress of language models powered by deep learning methods. In
this paper, we present a framework that compares selected translations (from
Sanskrit to English) of the Bhagavad Gita using semantic and sentiment
analyses. We use hand-labelled sentiment dataset for tuning state-of-art deep
learning-based language model known as bidirectional encoder representations
from transformers (BERT). We provide sentiment and semantic analysis for
selected chapters and verses across translations. Our results show that
although the style and vocabulary in the respective translations vary widely,
the sentiment analysis and semantic similarity shows that the message conveyed
are mostly similar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v2 [q-bio.BM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11147">
<div class="article-summary-box-inner">
<span><p>Self-supervised protein language models have proved their effectiveness in
learning the proteins representations. With the increasing computational power,
current protein language models pre-trained with millions of diverse sequences
can advance the parameter scale from million-level to billion-level and achieve
remarkable improvement. However, those prevailing approaches rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured
knowledge facts for better protein representations. We argue that informative
biology knowledge in KGs can enhance protein representation with external
knowledge. In this work, we propose OntoProtein, the first general framework
that makes use of structure in GO (Gene Ontology) into protein pre-training
models. We construct a novel large-scale knowledge graph that consists of GO
and its related proteins, and gene annotation texts or protein sequences
describe all nodes in the graph. We propose novel contrastive learning with
knowledge-aware negative sampling to jointly optimize the knowledge graph and
protein embedding during pre-training. Experimental results show that
OntoProtein can surpass state-of-the-art methods with pre-trained protein
language models in TAPE benchmark and yield better performance compared with
baselines in protein-protein interaction and protein function prediction. Code
and datasets are available in https://github.com/zjunlp/OntoProtein.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Knowledge Integration in Language Models with Graph Convolutions. (arXiv:2202.00964v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00964">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (LMs) do not capture factual knowledge very well.
This has led to the development of a number of knowledge integration (KI)
methods which aim to incorporate external knowledge into pretrained LMs. Even
though KI methods show some performance gains over vanilla LMs, the
inner-workings of these methods are not well-understood. For instance, it is
unclear how and what kind of knowledge is effectively integrated into these
models and if such integration may lead to catastrophic forgetting of already
learned knowledge. This paper revisits the KI process in these models with an
information-theoretic view and shows that KI can be interpreted using a graph
convolution operation. We propose a probe model called \textit{Graph
Convolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and
exposing what kind of knowledge is integrated into these models. We conduct
experiments to verify that our GCS can indeed be used to correctly interpret
the KI process, and we use it to analyze two well-known knowledge-enhanced LMs:
ERNIE and K-Adapter, and find that only a small amount of factual knowledge is
integrated in them. We stratify knowledge in terms of various relation types
and find that ERNIE and K-Adapter integrate different kinds of knowledge to
different extent. Our analysis also shows that simply increasing the size of
the KI corpus may not lead to better KI; fundamental advances may be needed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFA: TDNN with Multi-scale Frequency-channel Attention for Text-independent Speaker Verification with Short Utterances. (arXiv:2202.01624v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01624">
<div class="article-summary-box-inner">
<span><p>The time delay neural network (TDNN) represents one of the state-of-the-art
of neural solutions to text-independent speaker verification. However, they
require a large number of filters to capture the speaker characteristics at any
local frequency region. In addition, the performance of such systems may
degrade under short utterance scenarios. To address these issues, we propose a
multi-scale frequency-channel attention (MFA), where we characterize speakers
at different scales through a novel dual-path design which consists of a
convolutional neural network and TDNN. We evaluate the proposed MFA on the
VoxCeleb database and observe that the proposed framework with MFA can achieve
state-of-the-art performance while reducing parameters and computation
complexity. Further, the MFA mechanism is found to be effective for speaker
verification with short test utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Aspect-Based Sentiment Analysis. (arXiv:2202.01924v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01924">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) typically requires in-domain annotated
data for supervised training/fine-tuning. It is a big challenge to scale ABSA
to a large number of new domains. This paper aims to train a unified model that
can perform zero-shot ABSA without using any annotated data for a new domain.
We propose a method called contrastive post-training on review Natural Language
Inference (CORN). Later ABSA tasks can be cast into NLI for zero-shot transfer.
We evaluate CORN on ABSA tasks, ranging from aspect extraction (AE), aspect
sentiment classification (ASC), to end-to-end aspect-based sentiment analysis
(E2E ABSA), which show ABSA can be conducted without any human annotated ABSA
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiFSMN: Binary Neural Network for Keyword Spotting. (arXiv:2202.06483v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06483">
<div class="article-summary-box-inner">
<span><p>The deep neural networks, such as the Deep-FSMN, have been widely studied for
keyword spotting (KWS) applications. However, computational resources for these
networks are significantly constrained since they usually run on-call on edge
devices. In this paper, we present BiFSMN, an accurate and extreme-efficient
binary neural network for KWS. We first construct a High-frequency Enhancement
Distillation scheme for the binarization-aware training, which emphasizes the
high-frequency information from the full-precision network's representation
that is more crucial for the optimization of the binarized network. Then, to
allow the instant and adaptive accuracy-efficiency trade-offs at runtime, we
also propose a Thinnable Binarization Architecture to further liberate the
acceleration potential of the binarized network from the topology perspective.
Moreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8
devices which fully utilizes registers and increases instruction throughput to
push the limit of deployment efficiency. Extensive experiments show that BiFSMN
outperforms existing binarization methods by convincing margins on various
datasets and is even comparable with the full-precision counterpart (e.g., less
than 3% drop on Speech Commands V1-12). We highlight that benefiting from the
thinnable architecture and the optimized 1-bit implementation, BiFSMN can
achieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge
hardware.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DermX: an end-to-end framework for explainable automated dermatological diagnosis. (arXiv:2202.06956v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06956">
<div class="article-summary-box-inner">
<span><p>Dermatological diagnosis automation is essential in addressing the high
prevalence of skin diseases and critical shortage of dermatologists. Despite
approaching expert-level diagnosis performance, convolutional neural network
(ConvNet) adoption in clinical practice is impeded by their limited
explainability, and by subjective, expensive explainability validations. We
introduce DermX and DermX+, an end-to-end framework for explainable automated
dermatological diagnosis. DermX is a clinically-inspired explainable
dermatological diagnosis ConvNet, trained using DermXDB, a 554 images dataset
annotated by eight dermatologists with diagnoses and supporting explanations.
DermX+ extends DermX with guided attention training for explanation attention
maps. Both methods achieve near-expert diagnosis performance, with DermX,
DermX+, and dermatologist F1 scores of 0.79, 0.79, and 0.87, respectively. We
assess the explanation plausibility in terms of identification and
localization, by comparing model-selected with dermatologist-selected
explanations, and gradient-weighted class-activation maps with dermatologist
explanation maps. Both DermX and DermX+ obtain an identification F1 score of
0.78. The localization F1 score is 0.39 for DermX and 0.35 for DermX+.
Explanation faithfulness is assessed through contrasting samples, DermX
obtaining 0.53 faithfulness and DermX+ 0.25. These results show that
explainability does not necessarily come at the expense of predictive power, as
our high-performance models provide both plausible and faithful explanations
for their diagnoses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASC me to Do Anything: Multi-task Training for Embodied AI. (arXiv:2202.06987v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06987">
<div class="article-summary-box-inner">
<span><p>Embodied AI has seen steady progress across a diverse set of independent
tasks. While these varied tasks have different end goals, the basic skills
required to complete them successfully overlap significantly. In this paper,
our goal is to leverage these shared skills to learn to perform multiple tasks
jointly. We propose Atomic Skill Completion (ASC), an approach for multi-task
training for Embodied AI, where a set of atomic skills shared across multiple
tasks are composed together to perform the tasks. The key to the success of
this approach is a pre-training scheme that decouples learning of the skills
from the high-level tasks making joint training effective. We use ASC to train
agents within the AI2-THOR environment to perform four interactive tasks
jointly and find it to be remarkably effective. In a multi-task setting, ASC
improves success rates by a factor of 2x on Seen scenes and 4x on Unseen scenes
compared to no pre-training. Importantly, ASC enables us to train a multi-task
agent that has a 52% higher Success Rate than training 4 independent single
task agents. Finally, our hierarchical agents are more interpretable than
traditional black-box architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Cross-Modality Brain Image Synthesis. (arXiv:2202.06997v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06997">
<div class="article-summary-box-inner">
<span><p>The existence of completely aligned and paired multi-modal neuroimaging data
has proved its effectiveness in diagnosis of brain diseases. However,
collecting the full set of well-aligned and paired data is impractical or even
luxurious, since the practical difficulties may include high cost, long time
acquisition, image corruption, and privacy issues. A realistic solution is to
explore either an unsupervised learning or a semi-supervised learning to
synthesize the absent neuroimaging data. In this paper, we tend to approach
multi-modality brain image synthesis task from different perspectives, which
include the level of supervision, the range of modality synthesis, and the
synthesis-based downstream tasks. Particularly, we provide in-depth analysis on
how cross-modality brain image synthesis can improve the performance of
different downstream tasks. Finally, we evaluate the challenges and provide
several open directions for this community. All resources are available at
https://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handcrafted Histological Transformer (H2T): Unsupervised Representation of Whole Slide Images. (arXiv:2202.07001v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07001">
<div class="article-summary-box-inner">
<span><p>Diagnostic, prognostic and therapeutic decision-making of cancer in pathology
clinics can now be carried out based on analysis of multi-gigapixel tissue
images, also known as whole-slide images (WSIs). Recently, deep convolutional
neural networks (CNNs) have been proposed to derive unsupervised WSI
representations; these are attractive as they rely less on expert annotation
which is cumbersome. However, a major trade-off is that higher predictive power
generally comes at the cost of interpretability, posing a challenge to their
clinical use where transparency in decision-making is generally expected. To
address this challenge, we present a handcrafted framework based on deep CNN
for constructing holistic WSI-level representations. Building on recent
findings about the internal working of the Transformer in the domain of natural
language processing, we break down its processes and handcraft them into a more
transparent framework that we term as the Handcrafted Histological Transformer
or H2T. Based on our experiments involving various datasets consisting of a
total of 5,306 WSIs, the results demonstrate that H2T based holistic WSI-level
representations offer competitive performance compared to recent
state-of-the-art methods and can be readily utilized for various downstream
analysis tasks. Finally, our results demonstrate that the H2T framework can be
up to 14 times faster than the Transformer models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Visual Sensory Anomaly Detection. (arXiv:2202.07006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07006">
<div class="article-summary-box-inner">
<span><p>Visual sensory anomaly detection (AD) is an essential problem in computer
vision, which is gaining momentum recently thanks to the development of AI for
good. Compared with semantic anomaly detection which detects anomaly at the
label level (semantic shift), visual sensory AD detects the abnormal part of
the sample (covariate shift). However, no thorough review has been provided to
summarize this area for the computer vision community. In this survey, we are
the first one to provide a comprehensive review of visual sensory AD and
category into three levels according to the form of anomalies. Furthermore, we
classify each kind of anomaly according to the level of supervision. Finally,
we summarize the challenges and provide open directions for this community. All
resources are available at
https://github.com/M-3LAB/awesome-visual-sensory-anomaly-detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Inspection Toolkit: Unified Evaluation and Strong Baselines for Damage Recognition. (arXiv:2202.07012v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07012">
<div class="article-summary-box-inner">
<span><p>In recent years, several companies and researchers have started to tackle the
problem of damage recognition within the scope of automated inspection of built
structures. While companies are neither willing to publish associated data nor
models, researchers are facing the problem of data shortage on one hand and
inconsistent dataset splitting with the absence of consistent metrics on the
other hand. This leads to incomparable results. Therefore, we introduce the
building inspection toolkit -- bikit -- which acts as a simple to use data hub
containing relevant open-source datasets in the field of damage recognition.
The datasets are enriched with evaluation splits and predefined metrics,
suiting the specific task and their data distribution. For the sake of
compatibility and to motivate researchers in this domain, we also provide a
leaderboard and the possibility to share model weights with the community. As
starting point we provide strong baselines for multi-target classification
tasks utilizing extensive hyperparameter search using three transfer learning
approaches for state-of-the-art algorithms. The toolkit and the leaderboard are
available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Box Supervised Video Segmentation Proposal Network. (arXiv:2202.07025v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07025">
<div class="article-summary-box-inner">
<span><p>Video Object Segmentation (VOS) has been targeted by various fully-supervised
and self-supervised approaches. While fully-supervised methods demonstrate
excellent results, self-supervised ones, which do not use pixel-level ground
truth, attract much attention. However, self-supervised approaches pose a
significant performance gap. Box-level annotations provide a balanced
compromise between labeling effort and result quality for image segmentation
but have not been exploited for the video domain. In this work, we propose a
box-supervised video object segmentation proposal network, which takes
advantage of intrinsic video properties. Our method incorporates object motion
in the following way: first, motion is computed using a bidirectional temporal
difference and a novel bounding box-guided motion compensation. Second, we
introduce a novel motion-aware affinity loss that encourages the network to
predict positive pixel pairs if they share similar motion and color. The
proposed method outperforms the state-of-the-art self-supervised benchmark by
16.4% and 6.9% $\mathcal{J}$ &amp;$\mathcal{F}$ score and the majority of fully
supervised methods on the DAVIS and Youtube-VOS dataset without imposing
network architectural specifications. We provide extensive tests and ablations
on the datasets, demonstrating the robustness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07028">
<div class="article-summary-box-inner">
<span><p>We study the problem of developing autonomous agents that can follow human
instructions to infer and perform a sequence of actions to complete the
underlying task. Significant progress has been made in recent years, especially
for tasks with short horizons. However, when it comes to long-horizon tasks
with extended sequences of actions, an agent can easily ignore some
instructions or get stuck in the middle of the long instructions and eventually
fail the task. To address this challenge, we propose a model-agnostic
milestone-based task tracker (M-TRACK) to guide the agent and monitor its
progress. Specifically, we propose a milestone builder that tags the
instructions with navigation and interaction milestones which the agent needs
to complete step by step, and a milestone checker that systemically checks the
agent's progress in its current milestone and determines when to proceed to the
next. On the challenging ALFRED dataset, our M-TRACK leads to a notable 45% and
70% relative improvement in unseen success rate over two competitive base
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Adversarial Examples in Remote Sensing: Methodology and Benchmark. (arXiv:2202.07054v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07054">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have achieved great success in many important remote
sensing tasks. Nevertheless, their vulnerability to adversarial examples should
not be neglected. In this study, we systematically analyze the universal
adversarial examples in remote sensing data for the first time, without any
knowledge from the victim model. Specifically, we propose a novel black-box
adversarial attack method, namely Mixup-Attack, and its simple variant
Mixcut-Attack, for remote sensing data. The key idea of the proposed methods is
to find common vulnerabilities among different networks by attacking the
features in the shallow layer of a given surrogate model. Despite their
simplicity, the proposed methods can generate transferable adversarial examples
that deceive most of the state-of-the-art deep neural networks in both scene
classification and semantic segmentation tasks with high success rates. We
further provide the generated universal adversarial examples in the dataset
named UAE-RS, which is the first dataset that provides black-box adversarial
samples in the remote sensing field. We hope UAE-RS may serve as a benchmark
that helps researchers to design deep neural networks with strong resistance
toward adversarial attacks in the remote sensing field. Codes and the UAE-RS
dataset will be available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminability-enforcing loss to improve representation learning. (arXiv:2202.07073v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07073">
<div class="article-summary-box-inner">
<span><p>During the training process, deep neural networks implicitly learn to
represent the input data samples through a hierarchy of features, where the
size of the hierarchy is determined by the number of layers. In this paper, we
focus on enforcing the discriminative power of the high-level representations,
that are typically learned by the deeper layers (closer to the output). To this
end, we introduce a new loss term inspired by the Gini impurity, which is aimed
at minimizing the entropy (increasing the discriminative power) of individual
high-level features with respect to the class labels. Although our Gini loss
induces highly-discriminative features, it does not ensure that the
distribution of the high-level features matches the distribution of the
classes. As such, we introduce another loss term to minimize the
Kullback-Leibler divergence between the two distributions. We conduct
experiments on two image classification data sets (CIFAR-100 and Caltech 101),
considering multiple neural architectures ranging from convolutional networks
(ResNet-17, ResNet-18, ResNet-50) to transformers (CvT). Our empirical results
show that integrating our novel loss terms into the training objective
consistently outperforms the models trained with cross-entropy alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaze-Guided Class Activation Mapping: Leveraging Human Attention for Network Attention in Chest X-rays Classification. (arXiv:2202.07107v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07107">
<div class="article-summary-box-inner">
<span><p>The increased availability and accuracy of eye-gaze tracking technology has
sparked attention-related research in psychology, neuroscience, and, more
recently, computer vision and artificial intelligence. The attention mechanism
in artificial neural networks is known to improve learning tasks. However, no
previous research has combined the network attention and human attention. This
paper describes a gaze-guided class activation mapping (GG-CAM) method to
directly regulate the formation of network attention based on expert
radiologists' visual attention for the chest X-ray pathology classification
problem, which remains challenging due to the complex and often nuanced
differences among images. GG-CAM is a lightweight ($3$ additional trainable
parameters for regulating the learning process) and generic extension that can
be easily applied to most classification convolutional neural networks (CNN).
GG-CAM-modified CNNs do not require human attention as an input when fully
trained. Comparative experiments suggest that two standard CNNs with the GG-CAM
extension achieve significantly greater classification performance. The median
area under the curve (AUC) metrics for ResNet50 increases from $0.721$ to
$0.776$. For EfficientNetv2 (s), the median AUC increases from $0.723$ to
$0.801$. The GG-CAM also brings better interpretability of the network that
facilitates the weakly-supervised pathology localization and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task UNet: Jointly Boosting Saliency Prediction and Disease Classification on Chest X-ray Images. (arXiv:2202.07118v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07118">
<div class="article-summary-box-inner">
<span><p>Human visual attention has recently shown its distinct capability in boosting
machine learning models. However, studies that aim to facilitate medical tasks
with human visual attention are still scarce. To support the use of visual
attention, this paper describes a novel deep learning model for visual saliency
prediction on chest X-ray (CXR) images. To cope with data deficiency, we
exploit the multi-task learning method and tackles disease classification on
CXR simultaneously. For a more robust training process, we propose a further
optimized multi-task learning scheme to better handle model overfitting.
Experiments show our proposed deep learning model with our new learning scheme
can outperform existing methods dedicated either for saliency prediction or
image classification. The code used in this paper is available at
https://github.com/hz-zhu/MT-UNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework. (arXiv:2202.07123v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07123">
<div class="article-summary-box-inner">
<span><p>Point cloud analysis is challenging due to irregularity and unordered data
structure. To capture the 3D geometries, prior works mainly rely on exploring
sophisticated local geometric extractors using convolution, graph, or attention
mechanisms. These methods, however, incur unfavorable latency during inference,
and the performance saturates over the past few years. In this paper, we
present a novel perspective on this task. We notice that detailed local
geometrical information probably is not the key to point cloud analysis -- we
introduce a pure residual MLP network, called PointMLP, which integrates no
sophisticated local geometrical extractors but still performs very
competitively. Equipped with a proposed lightweight geometric affine module,
PointMLP delivers the new state-of-the-art on multiple datasets. On the
real-world ScanObjectNN dataset, our method even surpasses the prior best
method by 3.3% accuracy. We emphasize that PointMLP achieves this strong
performance without any sophisticated operations, hence leading to a superior
inference speed. Compared to most recent CurveNet, PointMLP trains 2x faster,
tests 7x faster, and is more accurate on ModelNet40 benchmark. We hope our
PointMLP may help the community towards a better understanding of point cloud
analysis. The code is available at https://github.com/ma-xu/pointMLP-pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-to-Real Domain Adaptation for Lane Detection and Classification in Autonomous Driving. (arXiv:2202.07133v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07133">
<div class="article-summary-box-inner">
<span><p>While supervised detection and classification frameworks in autonomous
driving require large labelled datasets to converge, Unsupervised Domain
Adaptation (UDA) approaches, facilitated by synthetic data generated from
photo-real simulated environments, are considered low-cost and less
time-consuming solutions. In this paper, we propose UDA schemes using
adversarial discriminative and generative methods for lane detection and
classification applications in autonomous driving. We also present Simulanes
dataset generator to create a synthetic dataset that is naturalistic utilizing
CARLA's vast traffic scenarios and weather conditions. The proposed UDA
frameworks take the synthesized dataset with labels as the source domain,
whereas the target domain is the unlabelled real-world data. Using adversarial
generative and feature discriminators, the learnt models are tuned to predict
the lane location and class in the target domain. The proposed techniques are
evaluated using both real-world and our synthetic datasets. The results
manifest that the proposed methods have shown superiority over other baseline
schemes in terms of detection and classification accuracy and consistency. The
ablation study reveals that the size of the simulation dataset plays important
roles in the classification performance of the proposed methods. Our UDA
frameworks are available at https://github.com/anita-hu/sim2real-lane-detection
and our dataset generator is released at https://github.com/anita-hu/simulanes
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Scene Representation Learning via Reconstruction: A Survey. (arXiv:2202.07135v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07135">
<div class="article-summary-box-inner">
<span><p>Visual scene representation learning is an important research problem in the
field of computer vision. The performance on vision tasks could be improved if
more suitable representations are learned for visual scenes. Complex visual
scenes are the composition of relatively simple visual concepts, and have the
property of combinatorial explosion. Compared with directly representing the
entire visual scene, extracting compositional scene representations can better
cope with the diverse combination of background and objects. Because
compositional scene representations abstract the concept of objects, performing
visual scene analysis and understanding based on these representations could be
easier and more interpretable. Moreover, learning compositional scene
representations via reconstruction can greatly reduce the need for training
data annotations. Therefore, compositional scene representation learning via
reconstruction has important research significance. In this survey, we first
discuss representative methods that either learn from a single viewpoint or
multiple viewpoints without object-level supervision, then the applications of
compositional scene representations, and finally the future directions on this
topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiased Pseudo Labeling in Self-Training. (arXiv:2202.07136v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07136">
<div class="article-summary-box-inner">
<span><p>Deep neural networks achieve remarkable performances on a wide range of tasks
with the aid of large-scale labeled datasets. However, large-scale annotations
are time-consuming and labor-exhaustive to obtain on realistic tasks. To
mitigate the requirement for labeled data, self-training is widely used in both
academia and industry by pseudo labeling on readily-available unlabeled data.
Despite its popularity, pseudo labeling is well-believed to be unreliable and
often leads to training instability. Our experimental studies further reveal
that the performance of self-training is biased due to data sampling,
pre-trained models, and training strategies, especially the inappropriate
utilization of pseudo labels. To this end, we propose Debiased, in which the
generation and utilization of pseudo labels are decoupled by two independent
heads. To further improve the quality of pseudo labels, we introduce a
worst-case estimation of pseudo labeling and seamlessly optimize the
representations to avoid the worst-case. Extensive experiments justify that the
proposed Debiased not only yields an average improvement of $14.4$\% against
state-of-the-art algorithms on $11$ tasks (covering generic object recognition,
fine-grained object recognition, texture classification, and scene
classification) but also helps stabilize training and balance performance
across classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAN-generated Faces Detection: A Survey and New Perspectives. (arXiv:2202.07145v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07145">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GAN) have led to the generation of very
realistic face images, which have been used in fake social media accounts and
other disinformation matters that can generate profound impacts. Therefore, the
corresponding GAN-face detection techniques are under active development that
can examine and expose such fake faces. In this work, we aim to provide a
comprehensive review of recent progress in GAN-face detection. We focus on
methods that can detect face images that are generated or synthesized from GAN
models. We classify the existing detection works into four categories: (1) deep
learning-based, (2) physical-based, (3) physiological-based methods, and (4)
evaluation and comparison against human visual performance. For each category,
we summarize the key ideas and connect them with method implementations. We
also discuss open problems and suggest future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To what extent can Plug-and-Play methods outperform neural networks alone in low-dose CT reconstruction. (arXiv:2202.07173v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07173">
<div class="article-summary-box-inner">
<span><p>The Plug-and-Play (PnP) framework was recently introduced for low-dose CT
reconstruction to leverage the interpretability and the flexibility of
model-based methods to incorporate various plugins, such as trained deep
learning (DL) neural networks. However, the benefits of PnP vs.
state-of-the-art DL methods have not been clearly demonstrated. In this work,
we proposed an improved PnP framework to address the previous limitations and
develop clinical-relevant segmentation metrics for quantitative result
assessment. Compared with the DL alone methods, our proposed PnP framework was
slightly inferior in MSE and PSNR. However, the power spectrum of the resulting
images better matched that of full-dose images than that of DL denoised images.
The resulting images supported higher accuracy in airway segmentation than DL
denoised images for all the ten patients in the test set, more substantially on
the airways with a cross-section smaller than 0.61cm$^2$, and outperformed the
DL denoised images for 45 out of 50 lung lobes in lobar segmentation. Our PnP
method proved to be significantly better at preserving the image texture, which
translated to task-specific benefits in automated structure segmentation and
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Neural Trojan Attacks and Defenses in Deep Learning. (arXiv:2202.07183v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07183">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI) relies heavily on deep learning - a technology
that is becoming increasingly popular in real-life applications of AI, even in
the safety-critical and high-risk domains. However, it is recently discovered
that deep learning can be manipulated by embedding Trojans inside it.
Unfortunately, pragmatic solutions to circumvent the computational requirements
of deep learning, e.g. outsourcing model training or data annotation to third
parties, further add to model susceptibility to the Trojan attacks. Due to the
key importance of the topic in deep learning, recent literature has seen many
contributions in this direction. We conduct a comprehensive review of the
techniques that devise Trojan attacks for deep learning and explore their
defenses. Our informative survey systematically organizes the recent literature
and discusses the key concepts of the methods while assuming minimal knowledge
of the domain on the readers part. It provides a comprehensible gateway to the
broader community to understand the recent developments in Neural Trojans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning Networks with Cross-Layer Ranking & k-Reciprocal Nearest Filters. (arXiv:2202.07190v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07190">
<div class="article-summary-box-inner">
<span><p>This paper focuses on filter-level network pruning. A novel pruning method,
termed CLR-RNF, is proposed. We first reveal a "long-tail" long-tail pruning
problem in magnitude-based weight pruning methods, and then propose a
computation-aware measurement for individual weight importance, followed by a
Cross-Layer Ranking (CLR) of weights to identify and remove the bottom-ranked
weights. Consequently, the per-layer sparsity makes up of the pruned network
structure in our filter pruning. Then, we introduce a recommendation-based
filter selection scheme where each filter recommends a group of its closest
filters. To pick the preserved filters from these recommended groups, we
further devise a k-Reciprocal Nearest Filter (RNF) selection scheme where the
selected filters fall into the intersection of these recommended groups. Both
our pruned network structure and the filter selection are non-learning
processes, which thus significantly reduce the pruning complexity, and
differentiate our method from existing works. We conduct image classification
on CIFAR-10 and ImageNet to demonstrate the superiority of our CLR-RNF over the
state-of-the-arts. For example, on CIFAR-10, CLR-RNF removes 74.1% FLOPs and
95.0% parameters from VGGNet-16 with even 0.3\% accuracy improvements. On
ImageNet, it removes 70.2% FLOPs and 64.8% parameters from ResNet-50 with only
1.7% top-5 accuracy drops. Our project is at https://github.com/lmbxmu/CLR-RNF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation. (arXiv:2202.07191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07191">
<div class="article-summary-box-inner">
<span><p>With rising male infertility, sperm head morphology classification becomes
critical for accurate and timely clinical diagnosis. Recent deep learning (DL)
morphology analysis methods achieve promising benchmark results, but leave
performance and robustness on the table by relying on limited and possibly
noisy class labels. To address this, we introduce a new DL training framework
that leverages anatomical and image priors from human sperm microscopy crops to
extract useful features without additional labeling cost. Our core idea is to
distill sperm head information with reliably-generated pseudo-masks and
unsupervised spatial prediction tasks. The predicted foreground masks from this
distillation step are then leveraged to regularize and reduce image and label
noise in the tuning stage. We evaluate our new approach on two public sperm
datasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy
and 96.5% HuSHeM accuracy).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Domain Experts for Long-Tailed Camera-Trap Recognition. (arXiv:2202.07215v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07215">
<div class="article-summary-box-inner">
<span><p>Label distributions in camera-trap images are highly imbalanced and
long-tailed, resulting in neural networks tending to be biased towards
head-classes that appear frequently. Although long-tail learning has been
extremely explored to address data imbalances, few studies have been conducted
to consider camera-trap characteristics, such as multi-domain and multi-frame
setup. Here, we propose a unified framework and introduce two datasets for
long-tailed camera-trap recognition. We first design domain experts, where each
expert learns to balance imperfect decision boundaries caused by data
imbalances and complement each other to generate domain-balanced decision
boundaries. Also, we propose a flow consistency loss to focus on moving
objects, expecting class activation maps of multi-frame matches the flow with
optical flow maps for input images. Moreover, two long-tailed camera-trap
datasets, WCS-LT and DMZ-LT, are introduced to validate our methods.
Experimental results show the effectiveness of our framework, and proposed
methods outperform previous methods on recessive domain samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeshLeTemp: Leveraging the Learnable Vertex-Vertex Relationship to Generalize Human Pose and Mesh Reconstruction for In-the-Wild Scenes. (arXiv:2202.07228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07228">
<div class="article-summary-box-inner">
<span><p>We present MeshLeTemp, a powerful method for 3D human pose and mesh
reconstruction from a single image. In terms of human body priors encoding, we
propose using a learnable template human mesh instead of a constant template
utilized by previous state-of-the-art methods. The proposed learnable template
reflects not only vertex-vertex interactions but also the human pose and body
shape, being able to adapt to diverse images. We also introduce a strategy to
enrich the training data that contains both 2D and 3D annotations. We conduct
extensive experiments to show the generalizability of our method and the
effectiveness of our data strategy. As one of our ablation studies, we adapt
MeshLeTemp to another domain which is 3D hand reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot semantic segmentation via mask aggregation. (arXiv:2202.07231v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07231">
<div class="article-summary-box-inner">
<span><p>Few-shot semantic segmentation aims to recognize novel classes with only very
few labelled data. This challenging task requires mining of the relevant
relationships between the query image and the support images. Previous works
have typically regarded it as a pixel-wise classification problem. Therefore,
various models have been designed to explore the correlation of pixels between
the query image and the support images. However, they focus only on pixel-wise
correspondence and ignore the overall correlation of objects. In this paper, we
introduce a mask-based classification method for addressing this problem. The
mask aggregation network (MANet), which is a simple mask classification model,
is proposed to simultaneously generate a fixed number of masks and their
probabilities of being targets. Then, the final segmentation result is obtained
by aggregating all the masks according to their locations. Experiments on both
the PASCAL-5^i and COCO-20^i datasets show that our method performs comparably
to the state-of-the-art pixel-based methods. This competitive performance
demonstrates the potential of mask classification as an alternative baseline
method in few-shot semantic segmentation. Our source code will be made
available at https://github.com/TinyAway/MANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architecture Search for Dense Prediction Tasks in Computer Vision. (arXiv:2202.07242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07242">
<div class="article-summary-box-inner">
<span><p>The success of deep learning in recent years has lead to a rising demand for
neural network architecture engineering. As a consequence, neural architecture
search (NAS), which aims at automatically designing neural network
architectures in a data-driven manner rather than manually, has evolved as a
popular field of research. With the advent of weight sharing strategies across
architectures, NAS has become applicable to a much wider range of problems. In
particular, there are now many publications for dense prediction tasks in
computer vision that require pixel-level predictions, such as semantic
segmentation or object detection. These tasks come with novel challenges, such
as higher memory footprints due to high-resolution data, learning multi-scale
representations, longer training times, and more complex and larger neural
architectures. In this manuscript, we provide an overview of NAS for dense
prediction tasks by elaborating on these novel challenges and surveying ways to
address them to ease future research and application of existing methods to
novel problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CommerceMM: Large-Scale Commerce MultiModal Representation Learning with Omni Retrieval. (arXiv:2202.07247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07247">
<div class="article-summary-box-inner">
<span><p>We introduce CommerceMM - a multimodal model capable of providing a diverse
and granular understanding of commerce topics associated to the given piece of
content (image, text, image+text), and having the capability to generalize to a
wide range of tasks, including Multimodal Categorization, Image-Text Retrieval,
Query-to-Product Retrieval, Image-to-Product Retrieval, etc. We follow the
pre-training + fine-tuning training regime and present 5 effective pre-training
tasks on image-text pairs. To embrace more common and diverse commerce data
with text-to-multimodal, image-to-multimodal, and multimodal-to-multimodal
mapping, we propose another 9 novel cross-modal and cross-pair retrieval tasks,
called Omni-Retrieval pre-training. The pre-training is conducted in an
efficient manner with only two forward/backward updates for the combined 14
tasks. Extensive experiments and analysis show the effectiveness of each task.
When combining all pre-training tasks, our model achieves state-of-the-art
performance on 7 commerce-related downstream tasks after fine-tuning.
Additionally, we propose a novel approach of modality randomization to
dynamically adjust our model under different efficiency constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Review of the Fingerprint Liveness Detection (LivDet) competition series: from 2009 to 2021. (arXiv:2202.07259v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07259">
<div class="article-summary-box-inner">
<span><p>Fingerprint authentication systems are highly vulnerable to artificial
reproductions of fingerprint, called fingerprint presentation attacks.
Detecting presentation attacks is not trivial because attackers refine their
replication techniques from year to year. The International Fingerprint
liveness Detection Competition (LivDet), an open and well-acknowledged meeting
point of academies and private companies that deal with the problem of
presentation attack detection, has the goal to assess the performance of
fingerprint presentation attack detection (FPAD) algorithms by using standard
experimental protocols and data sets. Each LivDet edition, held biannually
since 2009, is characterized by a different set of challenges against which
competitors must be dealt with. The continuous increase of competitors and the
noticeable decrease in error rates across competitions demonstrate a growing
interest in the topic. This paper reviews the LivDet editions from 2009 to 2021
and points out their evolution over the years.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks. (arXiv:2202.07261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07261">
<div class="article-summary-box-inner">
<span><p>3D dynamic point clouds provide a discrete representation of real-world
objects or scenes in motion, which have been widely applied in immersive
telepresence, autonomous driving, surveillance, \textit{etc}. However, point
clouds acquired from sensors are usually perturbed by noise, which affects
downstream tasks such as surface reconstruction and analysis. Although many
efforts have been made for static point cloud denoising, few works address
dynamic point cloud denoising. In this paper, we propose a novel gradient-based
dynamic point cloud denoising method, exploiting the temporal correspondence
for the estimation of gradient fields -- also a fundamental problem in dynamic
point cloud processing and analysis. The gradient field is the gradient of the
log-probability function of the noisy point cloud, based on which we perform
gradient ascent so as to converge each point to the underlying clean surface.
We estimate the gradient of each surface patch by exploiting the temporal
correspondence, where the temporally corresponding patches are searched
leveraging on rigid motion in classical mechanics. In particular, we treat each
patch as a rigid object, which moves in the gradient field of an adjacent frame
via force until reaching a balanced state, i.e., when the sum of gradients over
the patch reaches 0. Since the gradient would be smaller when the point is
closer to the underlying surface, the balanced patch would fit the underlying
surface well, thus leading to the temporal correspondence. Finally, the
position of each point in the patch is updated along the direction of the
gradient averaged from corresponding patches in adjacent frames. Experimental
results demonstrate that the proposed model outperforms state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyper-relationship Learning Network for Scene Graph Generation. (arXiv:2202.07271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07271">
<div class="article-summary-box-inner">
<span><p>Generating informative scene graphs from images requires integrating and
reasoning from various graph components, i.e., objects and relationships.
However, current scene graph generation (SGG) methods, including the unbiased
SGG methods, still struggle to predict informative relationships due to the
lack of 1) high-level inference such as transitive inference between
relationships and 2) efficient mechanisms that can incorporate all interactions
of graph components. To address the issues mentioned above, we devise a
hyper-relationship learning network, termed HLN, for SGG. Specifically, the
proposed HLN stems from hypergraphs and two graph attention networks (GATs) are
designed to infer relationships: 1) the object-relationship GAT or OR-GAT to
explore interactions between objects and relationships, and 2) the
hyper-relationship GAT or HR-GAT to integrate transitive inference of
hyper-relationships, i.e., the sequential relationships between three objects
for transitive reasoning. As a result, HLN significantly improves the
performance of scene graph generation by integrating and reasoning from object
interactions, relationship interactions, and transitive inference of
hyper-relationships. We evaluate HLN on the most popular SGG dataset, i.e., the
Visual Genome dataset, and the experimental results demonstrate its great
superiority over recent state-of-the-art methods. For example, the proposed HLN
improves the recall per relationship from 11.3\% to 13.1\%, and maintains the
recall per image from 19.8\% to 34.9\%. We will release the source code and
pretrained models on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Natural Motion: Exploring Discontinuity for Video Frame Interpolation. (arXiv:2202.07291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07291">
<div class="article-summary-box-inner">
<span><p>Video interpolation is the task that synthesizes the intermediate frame given
two consecutive frames. Most of the previous studies have focused on
appropriate frame warping operations and refinement modules for the warped
frames. These studies have been conducted on natural videos having only
continuous motions. However, many practical videos contain a lot of
discontinuous motions, such as chat windows, watermarks, GUI elements, or
subtitles. We propose three techniques to expand the concept of transition
between two consecutive frames to address these issues. First is a new
architecture that can separate continuous and discontinuous motion areas. We
also propose a novel data augmentation strategy called figure-text mixing (FTM)
to make our model learn more general scenarios. Finally, we propose loss
functions to give supervisions of the discontinuous motion areas with the data
augmentation. We collected a special dataset consisting of some mobile games
and chatting videos. We show that our method significantly improves the
interpolation qualities of the videos on the special dataset. Moreover, our
model outperforms the state-of-the-art methods for natural video datasets
containing only continuous motions, such as DAVIS and UCF101.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. (arXiv:2202.07305v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07305">
<div class="article-summary-box-inner">
<span><p>Image narrative generation describes the creation of stories regarding the
content of image data from a subjective viewpoint. Given the importance of the
subjective feelings of writers, characters, and readers in storytelling, image
narrative generation methods must consider human emotion, which is their major
difference from descriptive caption generation tasks. The development of
automated methods to generate story-like text associated with images may be
considered to be of considerable social significance, because stories serve
essential functions both as entertainment and also for many practical purposes
such as education and advertising. In this study, we propose a model called
ViNTER (Visual Narrative Transformer with Emotion arc Representation) to
generate image narratives that focus on time series representing varying
emotions as "emotion arcs," to take advantage of recent advances in multimodal
Transformer-based pre-trained models. We present experimental results of both
manual and automatic evaluations, which demonstrate the effectiveness of the
proposed emotion-aware approach to image narrative generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HAA4D: Few-Shot Human Atomic Action Recognition via 3D Spatio-Temporal Skeletal Alignment. (arXiv:2202.07308v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07308">
<div class="article-summary-box-inner">
<span><p>Human actions involve complex pose variations and their 2D projections can be
highly ambiguous. Thus 3D spatio-temporal or 4D (i.e., 3D+T) human skeletons,
which are photometric and viewpoint invariant, are an excellent alternative to
2D+T skeletons/pixels to improve action recognition accuracy. This paper
proposes a new 4D dataset HAA4D which consists of more than 3,300 RGB videos in
300 human atomic action classes. HAA4D is clean, diverse, class-balanced where
each class is viewpoint-balanced with the use of 4D skeletons, in which as few
as one 4D skeleton per class is sufficient for training a deep recognition
model. Further, the choice of atomic actions makes annotation even easier,
because each video clip lasts for only a few seconds. All training and testing
3D skeletons in HAA4D are globally aligned, using a deep alignment model to the
same global space, making each skeleton face the negative z-direction. Such
alignment makes matching skeletons more stable by reducing intraclass
variations and thus with fewer training samples per class needed for action
recognition. Given the high diversity and skeletal alignment in HAA4D, we
construct the first baseline few-shot 4D human atomic action recognition
network without bells and whistles, which produces comparable or higher
performance than relevant state-of-the-art techniques relying on embedded space
encoding without explicit skeletal alignment, using the same small number of
training samples of unseen classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Social Media Images for Building Function Classification. (arXiv:2202.07315v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07315">
<div class="article-summary-box-inner">
<span><p>Urban land use on a building instance level is crucial geo-information for
many applications, yet difficult to obtain. An intuitive approach to close this
gap is predicting building functions from ground level imagery. Social media
image platforms contain billions of images, with a large variety of motifs
including but not limited to street perspectives. To cope with this issue this
study proposes a filtering pipeline to yield high quality, ground level imagery
from large social media image datasets. The pipeline ensures that all resulting
images have full and valid geotags with a compass direction to relate image
content and spatial objects from maps.
</p>
<p>We analyze our method on a culturally diverse social media dataset from
Flickr with more than 28 million images from 42 cities around the world. The
obtained dataset is then evaluated in a context of 3-classes building function
classification task. The three building classes that are considered in this
study are: commercial, residential, and other. Fine-tuned state-of-the-art
architectures yield F1-scores of up to 0.51 on the filtered images. Our
analysis shows that the performance is highly limited by the quality of the
labels obtained from OpenStreetMap, as the metrics increase by 0.2 if only
human validated labels are considered. Therefore, we consider these labels to
be weak and publish the resulting images from our pipeline together with the
buildings they are showing as a weakly labeled dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Framework for Masked and Mask-Free Face Recognition via Feature Rectification. (arXiv:2202.07358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07358">
<div class="article-summary-box-inner">
<span><p>Face recognition under ideal conditions is now considered a well-solved
problem with advances in deep learning. Recognizing faces under occlusion,
however, still remains a challenge. Existing techniques often fail to recognize
faces with both the mouth and nose covered by a mask, which is now very common
under the COVID-19 pandemic. Common approaches to tackle this problem include
1) discarding information from the masked regions during recognition and 2)
restoring the masked regions before recognition. Very few works considered the
consistency between features extracted from masked faces and from their
mask-free counterparts. This resulted in models trained for recognizing masked
faces often showing degraded performance on mask-free faces. In this paper, we
propose a unified framework, named Face Feature Rectification Network
(FFR-Net), for recognizing both masked and mask-free faces alike. We introduce
rectification blocks to rectify features extracted by a state-of-the-art
recognition model, in both spatial and channel dimensions, to minimize the
distance between a masked face and its mask-free counterpart in the rectified
feature space. Experiments show that our unified framework can learn a
rectified feature space for recognizing both masked and mask-free faces
effectively, achieving state-of-the-art results. Project code:
https://github.com/haoosz/FFR-Net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Driver Referencing: A Comparison of Pointing to Objects Inside and Outside the Vehicle. (arXiv:2202.07360v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07360">
<div class="article-summary-box-inner">
<span><p>Advanced in-cabin sensing technologies, especially vision based approaches,
have tremendously progressed user interaction inside the vehicle, paving the
way for new applications of natural user interaction. Just as humans use
multiple modes to communicate with each other, we follow an approach which is
characterized by simultaneously using multiple modalities to achieve natural
human-machine interaction for a specific task: pointing to or glancing towards
objects inside as well as outside the vehicle for deictic references. By
tracking the movements of eye-gaze, head and finger, we design a multimodal
fusion architecture using a deep neural network to precisely identify the
driver's referencing intent. Additionally, we use a speech command as a trigger
to separate each referencing event. We observe differences in driver behavior
in the two pointing use cases (i.e. for inside and outside objects), especially
when analyzing the preciseness of the three modalities eye, head, and finger.
We conclude that there is no single modality that is solely optimal for all
cases as each modality reveals certain limitations. Fusion of multiple
modalities exploits the relevant characteristics of each modality, hence
overcoming the case dependent limitations of each individual modality.
Ultimately, we propose a method to identity whether the driver's referenced
object lies inside or outside the vehicle, based on the predicted pointing
direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-based Anomaly Detection on X-ray Images of Fuel Cell Electrodes. (arXiv:2202.07361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07361">
<div class="article-summary-box-inner">
<span><p>Anomaly detection in X-ray images has been an active and lasting research
area in the last decades, especially in the domain of medical X-ray images. For
this work, we created a real-world labeled anomaly dataset, consisting of
16-bit X-ray image data of fuel cell electrodes coated with a platinum catalyst
solution and perform anomaly detection on the dataset using a deep learning
approach. The dataset contains a diverse set of anomalies with 11 identified
common anomalies where the electrodes contain e.g. scratches, bubbles, smudges
etc. We experiment with 16-bit image to 8-bit image conversion methods to
utilize pre-trained Convolutional Neural Networks as feature extractors
(transfer learning) and find that we achieve the best performance by maximizing
the contrasts globally across the dataset during the 16-bit to 8-bit
conversion, through histogram equalization. We group the fuel cell electrodes
with anomalies into a single class called abnormal and the normal fuel cell
electrodes into a class called normal, thereby abstracting the anomaly
detection problem into a binary classification problem. We achieve a balanced
accuracy of 85.18\%. The anomaly detection is used by the company, Serenergy,
for optimizing the time spend on the quality control of the fuel cell
electrodes
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SODAR: Segmenting Objects by DynamicallyAggregating Neighboring Mask Representations. (arXiv:2202.07402v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07402">
<div class="article-summary-box-inner">
<span><p>Recent state-of-the-art one-stage instance segmentation model SOLO divides
the input image into a grid and directly predicts per grid cell object masks
with fully-convolutional networks, yielding comparably good performance as
traditional two-stage Mask R-CNN yet enjoying much simpler architecture and
higher efficiency. We observe SOLO generates similar masks for an object at
nearby grid cells, and these neighboring predictions can complement each other
as some may better segment certain object part, most of which are however
directly discarded by non-maximum-suppression. Motivated by the observed gap,
we develop a novel learning-based aggregation method that improves upon SOLO by
leveraging the rich neighboring information while maintaining the architectural
efficiency. The resulting model is named SODAR. Unlike the original per grid
cell object masks, SODAR is implicitly supervised to learn mask representations
that encode geometric structure of nearby objects and complement adjacent
representations with context. The aggregation method further includes two novel
designs: 1) a mask interpolation mechanism that enables the model to generate
much fewer mask representations by sharing neighboring representations among
nearby grid cells, and thus saves computation and memory; 2) a deformable
neighbour sampling mechanism that allows the model to adaptively adjust
neighbor sampling locations thus gathering mask representations with more
relevant context and achieving higher performance. SODAR significantly improves
the instance segmentation performance, e.g., it outperforms a SOLO model with
ResNet-101 backbone by 2.2 AP on COCO \texttt{test} set, with only about 3\%
additional computation. We further show consistent performance gain with the
SOLOv2 model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable COVID-19 Infections Identification and Delineation Using Calibrated Pseudo Labels. (arXiv:2202.07422v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07422">
<div class="article-summary-box-inner">
<span><p>The upheaval brought by the arrival of the COVID-19 pandemic has continued to
bring fresh challenges over the past two years. During this COVID-19 pandemic,
there has been a need for rapid identification of infected patients and
specific delineation of infection areas in computed tomography (CT) images.
Although deep supervised learning methods have been established quickly, the
scarcity of both image-level and pixellevel labels as well as the lack of
explainable transparency still hinder the applicability of AI. Can we identify
infected patients and delineate the infections with extreme minimal
supervision? Semi-supervised learning (SSL) has demonstrated promising
performance under limited labelled data and sufficient unlabelled data.
Inspired by SSL, we propose a model-agnostic calibrated pseudo-labelling
strategy and apply it under a consistency regularization framework to generate
explainable identification and delineation results. We demonstrate the
effectiveness of our model with the combination of limited labelled data and
sufficient unlabelled data or weakly-labelled data. Extensive experiments have
shown that our model can efficiently utilize limited labelled data and provide
explainable classification and segmentation results for decision-making in
clinical routine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07427">
<div class="article-summary-box-inner">
<span><p>Authors of posts in social media communicate their emotions and what causes
them with text and images. While there is work on emotion and stimulus
detection for each modality separately, it is yet unknown if the modalities
contain complementary emotion information in social media. We aim at filling
this research gap and contribute a novel, annotated corpus of English
multimodal Reddit posts. On this resource, we develop models to automatically
detect the relation between image and text, an emotion stimulus category and
the emotion class. We evaluate if these tasks require both modalities and find
for the image-text relations, that text alone is sufficient for most categories
(complementary, illustrative, opposing): the information in the text allows to
predict if an image is required for emotion understanding. The emotions of
anger and sadness are best predicted with a multimodal model, while text alone
is sufficient for disgust, joy, and surprise. Stimuli depicted by objects,
animals, food, or a person are best predicted by image-only models, while
multimodal models are most effective on art, events, memes, places, or
screenshots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A precortical module for robust CNNs to light variations. (arXiv:2202.07432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07432">
<div class="article-summary-box-inner">
<span><p>We present a simple mathematical model for the mammalian low visual pathway,
taking into account its key elements: retina, lateral geniculate nucleus (LGN),
primary visual cortex (V1). The analogies between the cortical level of the
visual system and the structure of popular CNNs, used in image classification
tasks, suggests the introduction of an additional preliminary convolutional
module inspired to precortical neuronal circuits to improve robustness with
respect to global light intensity and contrast variations in the input images.
We validate our hypothesis on the popular databases MNIST, FashionMNIST and
SVHN, obtaining significantly more robust CNNs with respect to these
variations, once such extra module is added.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mathematical Cookbook for Snapshot Compressive Imaging. (arXiv:2202.07437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07437">
<div class="article-summary-box-inner">
<span><p>The author intends to provide you with a beautiful, elegant, user-friendly
cookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the
cookbook is composed of introduction and conventional optimization, using
regularization-based optimization algorithms for SCI. The latest releases are
strongly recommended! For any other questions, suggestions, or comments, feel
free to email the author.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Automated Analysis Framework for Trajectory Datasets. (arXiv:2202.07438v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07438">
<div class="article-summary-box-inner">
<span><p>Trajectory datasets of road users have become more important in the last
years for safety validation of highly automated vehicles. Several naturalistic
trajectory datasets with each more than 10.000 tracks were released and others
will follow. Considering this amount of data, it is necessary to be able to
compare these datasets in-depth with ease to get an overview. By now, the
datasets' own provided information is mainly limited to meta-data and
qualitative descriptions which are mostly not consistent with other datasets.
This is insufficient for users to differentiate the emerging datasets for
application-specific selection. Therefore, an automated analysis framework is
proposed in this work. Starting with analyzing individual tracks, fourteen
elementary characteristics, so-called detection types, are derived and used as
the base of this framework. To describe each traffic scenario precisely, the
detections are subdivided into common metrics, clustering methods and anomaly
detection. Those are combined using a modular approach. The detections are
composed into new scores to describe three defined attributes of each track
data quantitatively: interaction, anomaly and relevance. These three scores are
calculated hierarchically for different abstract layers to provide an overview
not just between datasets but also for tracks, spatial regions and individual
situations. So, an objective comparison between datasets can be realized.
Furthermore, it can help to get a deeper understanding of the recorded
infrastructure and its effect on road user behavior. To test the validity of
the framework, a study is conducted to compare the scores with human
perception. Additionally, several datasets are compared.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Walks for Adversarial Meshes. (arXiv:2202.07453v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07453">
<div class="article-summary-box-inner">
<span><p>A polygonal mesh is the most-commonly used representation of surfaces in
computer graphics; thus, a variety of classification networks have been
recently proposed. However, while adversarial attacks are wildly researched in
2D, almost no works on adversarial meshes exist. This paper proposes a novel,
unified, and general adversarial attack, which leads to misclassification of
numerous state-of-the-art mesh classification neural networks. Our attack
approach is black-box, i.e. it has access only to the network's predictions,
but not to the network's full architecture or gradients. The key idea is to
train a network to imitate a given classification network. This is done by
utilizing random walks along the mesh surface, which gather geometric
information. These walks provide insight onto the regions of the mesh that are
important for the correct prediction of the given classification network. These
mesh regions are then modified more than other regions in order to attack the
network in a manner that is barely visible to the naked eye.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Image Deblurring. (arXiv:2202.07456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07456">
<div class="article-summary-box-inner">
<span><p>With the improvement of social life quality and the real needs of daily work,
images are more and more all around us. Image blurring due to camera shake,
human movement, etc. has become the key to affecting image quality. How to
remove image blur and restore clear image has gradually become an important
research direction in the field of computer vision. After more than half a
century of unremitting efforts, the majority of scientific and technological
workers have made fruitful progress in image deblurring. This article reviews
the work of image deblurring and specifically introduces more classic image
deblurring methods, which is helpful to understand current research and look
forward to future trends. This article reviews the traditional image deblurring
methods and depth-represented image deblurring methods, and comprehensively
classifies and introduces the corresponding technical methods. This review can
provide some guidance for researchers in the field of image deblurring, and at
the same time facilitate their subsequent study and research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSensor: Deep Learning Testing Framework Based on Neuron Sensitivity. (arXiv:2202.07464v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07464">
<div class="article-summary-box-inner">
<span><p>Despite impressive capabilities and outstanding performance, deep neural
network(DNN) has captured increasing public concern for its security problem,
due to frequent occurrence of erroneous behaviors. Therefore, it is necessary
to conduct systematically testing before its deployment to real-world
applications. Existing testing methods have provided fine-grained criteria
based on neuron coverage and reached high exploratory degree of testing. But
there is still a gap between the neuron coverage and model's robustness
evaluation. To bridge the gap, we observed that neurons which change the
activation value dramatically due to minor perturbation are prone to trigger
incorrect corner cases. Motivated by it, we propose neuron sensitivity and
develop a novel white-box testing framework for DNN, donated as DeepSensor. The
number of sensitive neurons is maximized by particle swarm optimization, thus
diverse corner cases could be triggered and neuron coverage be further improved
when compared with baselines. Besides, considerable robustness enhancement can
be reached when adopting testing examples based on neuron sensitivity for
retraining. Extensive experiments implemented on scalable datasets and models
can well demonstrate the testing effectiveness and robustness improvement of
DeepSensor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Contrastive Learning for Dermatological Disease Diagnosis via On-device Learning. (arXiv:2202.07470v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07470">
<div class="article-summary-box-inner">
<span><p>Deep learning models have been deployed in an increasing number of edge and
mobile devices to provide healthcare. These models rely on training with a
tremendous amount of labeled data to achieve high accuracy. However, for
medical applications such as dermatological disease diagnosis, the private data
collected by mobile dermatology assistants exist on distributed mobile devices
of patients, and each device only has a limited amount of data. Directly
learning from limited data greatly deteriorates the performance of learned
models. Federated learning (FL) can train models by using data distributed on
devices while keeping the data local for privacy. Existing works on FL assume
all the data have ground-truth labels. However, medical data often comes
without any accompanying labels since labeling requires expertise and results
in prohibitively high labor costs. The recently developed self-supervised
learning approach, contrastive learning (CL), can leverage the unlabeled data
to pre-train a model, after which the model is fine-tuned on limited labeled
data for dermatological disease diagnosis. However, simply combining CL with FL
as federated contrastive learning (FCL) will result in ineffective learning
since CL requires diverse data for learning but each device only has limited
data. In this work, we propose an on-device FCL framework for dermatological
disease diagnosis with limited labels. Features are shared in the FCL
pre-training process to provide diverse and accurate contrastive information.
After that, the pre-trained model is fine-tuned with local labeled data
independently on each device or collaboratively with supervised federated
learning on all devices. Experiments on dermatological disease datasets show
that the proposed framework effectively improves the recall and precision of
dermatological disease diagnosis compared with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Lessons from Metric Learning Generalize to Image-Caption Retrieval?. (arXiv:2202.07474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07474">
<div class="article-summary-box-inner">
<span><p>The triplet loss with semi-hard negatives has become the de facto choice for
image-caption retrieval (ICR) methods that are optimized from scratch. Recent
progress in metric learning has given rise to new loss functions that
outperform the triplet loss on tasks such as image retrieval and representation
learning. We ask whether these findings generalize to the setting of ICR by
comparing three loss functions on two ICR methods. We answer this question
negatively: the triplet loss with semi-hard negative mining still outperforms
newly introduced loss functions from metric learning on the ICR task. To gain a
better understanding of these outcomes, we introduce an analysis method to
compare loss functions by counting how many samples contribute to the gradient
w.r.t. the query representation during optimization. We find that loss
functions that result in lower evaluation scores on the ICR task, in general,
take too many (non-informative) samples into account when computing a gradient
w.r.t. the query representation, which results in sub-optimal performance. The
triplet loss with semi-hard negatives is shown to outperform the other loss
functions, as it only takes one (hard) negative into account when computing the
gradient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real-time System for Detecting Landslide Reports on Social Media using Artificial Intelligence. (arXiv:2202.07475v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07475">
<div class="article-summary-box-inner">
<span><p>This paper presents an online system that leverages social media data in real
time to identify landslide-related information automatically using
state-of-the-art artificial intelligence techniques. The designed system can
(i) reduce the information overload by eliminating duplicate and irrelevant
content, (ii) identify landslide images, (iii) infer geolocation of the images,
and (iv) categorize the user type (organization or person) of the account
sharing the information. The system was deployed in February 2020 online at
https://landslide-aidr.qcri.org/landslide_system.php to monitor live Twitter
data stream and has been running continuously since then to provide
time-critical information to partners such as British Geological Survey and
European Mediterranean Seismological Centre. We trust this system can both
contribute to harvesting of global landslide data for further research and
support global landslide maps to facilitate emergency response and decision
making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DualConv: Dual Convolutional Kernels for Lightweight Deep Neural Networks. (arXiv:2202.07481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07481">
<div class="article-summary-box-inner">
<span><p>CNN architectures are generally heavy on memory and computational
requirements which makes them infeasible for embedded systems with limited
hardware resources. We propose dual convolutional kernels (DualConv) for
constructing lightweight deep neural networks. DualConv combines 3$\times$3 and
1$\times$1 convolutional kernels to process the same input feature map channels
simultaneously and exploits the group convolution technique to efficiently
arrange convolutional filters. DualConv can be employed in any CNN model such
as VGG-16 and ResNet-50 for image classification, YOLO and R-CNN for object
detection, or FCN for semantic segmentation. In this paper, we extensively test
DualConv for classification since these network architectures form the
backbones for many other tasks. We also test DualConv for image detection on
YOLO-V3. Experimental results show that, combined with our structural
innovations, DualConv significantly reduces the computational cost and number
of parameters of deep neural networks while surprisingly achieving slightly
higher accuracy than the original models in some cases. We use DualConv to
further reduce the number of parameters of the lightweight MobileNetV2 by 54%
with only 0.68% drop in accuracy on CIFAR-100 dataset. When the number of
parameters is not an issue, DualConv increases the accuracy of MobileNetV1 by
4.11% on the same dataset. Furthermore, DualConv significantly improves the
YOLO-V3 object detection speed and improves its accuracy by 4.4% on PASCAL VOC
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Texture Aware Autoencoder Pre-training And Pairwise Learning Refinement For Improved Iris Recognition. (arXiv:2202.07499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07499">
<div class="article-summary-box-inner">
<span><p>This paper presents a texture aware end-to-end trainable iris recognition
system, specifically designed for datasets like iris having limited training
data. We build upon our previous stagewise learning framework with certain key
optimization and architectural innovations. First, we pretrain a Stage-1
encoder network with an unsupervised autoencoder learning optimized with an
additional data relation loss on top of usual reconstruction loss. The data
relation loss enables learning better texture representation which is pivotal
for a texture rich dataset such as iris. Robustness of Stage-1 feature
representation is further enhanced with an auxiliary denoising task. Such
pre-training proves beneficial for effectively training deep networks on data
constrained iris datasets. Next, in Stage-2 supervised refinement, we design a
pairwise learning architecture for an end-to-end trainable iris recognition
system. The pairwise learning includes the task of iris matching inside the
training pipeline itself and results in significant improvement in recognition
performance compared to usual offline matching. We validate our model across
three publicly available iris datasets and the proposed model consistently
outperforms both traditional and deep learning baselines for both
Within-Dataset and Cross-Dataset configurations
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BED: A Real-Time Object Detection System for Edge Devices. (arXiv:2202.07503v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07503">
<div class="article-summary-box-inner">
<span><p>Deploying machine learning models to edge devices has many real-world
applications, especially for the scenarios that demand low latency, low power,
or data privacy. However, it requires substantial research and engineering
efforts due to the limited computational resources and memory of edge devices.
In this demo, we present BED, an object detection system for edge devices
practiced on the MAX78000 DNN accelerator. BED integrates on-device DNN
inference with a camera and a screen for image acquisition and output
exhibition, respectively. Experiment results indicate BED can provide accurate
detection with an only 300KB tiny DNN model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07508">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the problem of blind image super-resolution(SR) with
a reformulated degradation model and two novel modules. Following the common
practices of blind SR, our method proposes to improve both the kernel
estimation as well as the kernel based high resolution image restoration. To be
more specific, we first reformulate the degradation model such that the
deblurring kernel estimation can be transferred into the low resolution space.
On top of this, we introduce a dynamic deep linear filter module. Instead of
learning a fixed kernel for all images, it can adaptively generate deblurring
kernel weights conditional on the input and yields more robust kernel
estimation. Subsequently, a deep constrained least square filtering module is
applied to generate clean features based on the reformulation and estimated
kernel. The deblurred feature and the low input image feature are then fed into
a dual-path structured SR network and restore the final high resolution result.
To evaluate our method, we further conduct evaluations on several benchmarks,
including Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed
method achieves better accuracy and visual improvements against
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-Training Quantization for Cross-Platform Learned Image Compression. (arXiv:2202.07513v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07513">
<div class="article-summary-box-inner">
<span><p>It has been witnessed that learned image compression has outperformed
conventional image coding techniques and tends to be practical in industrial
applications. One of the most critical issues that need to be considered is the
non-deterministic calculation, which makes the probability prediction
cross-platform inconsistent and frustrates successful decoding. We propose to
solve this problem by introducing well-developed post-training quantization and
making the model inference integer-arithmetic-only, which is much simpler than
presently existing training and fine-tuning based approaches yet still keeps
the superior rate-distortion performance of learned image compression. Based on
that, we further improve the discretization of the entropy parameters and
extend the deterministic inference to fit Gaussian mixture models. With our
proposed methods, the current state-of-the-art image compression models can
infer in a cross-platform consistent manner, which makes the further
development and practice of learned image compression more promising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label fusion and training methods for reliable representation of inter-rater uncertainty. (arXiv:2202.07550v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07550">
<div class="article-summary-box-inner">
<span><p>Medical tasks are prone to inter-rater variability due to multiple factors
such as image quality, professional experience and training, or guideline
clarity. Training deep learning networks with annotations from multiple raters
is a common practice that mitigates the model's bias towards a single expert.
Reliable models generating calibrated outputs and reflecting the inter-rater
disagreement are key to the integration of artificial intelligence in clinical
practice. Various methods exist to take into account different expert labels.
We focus on comparing three label fusion methods: STAPLE, average of the
rater's segmentation, and random sampling each rater's segmentation during
training. Each label fusion method is studied using the conventional training
framework or the recently published SoftSeg framework that limits information
loss by treating the segmentation task as a regression. Our results, across 10
data splittings on two public datasets, indicate that SoftSeg models,
regardless of the ground truth fusion method, had better calibration and
preservation of the inter-rater rater variability compared with their
conventional counterparts without impacting the segmentation performance.
Conventional models, i.e., trained with a Dice loss, with binary inputs, and
sigmoid/softmax final activate, were overconfident and underestimated the
uncertainty associated with inter-rater variability. Conversely, fusing labels
by averaging with the SoftSeg framework led to underconfident outputs and
overestimation of the rater disagreement. In terms of segmentation performance,
the best label fusion method was different for the two datasets studied,
indicating this parameter might be task-dependent. However, SoftSeg had
segmentation performance systematically superior or equal to the conventionally
trained models and had the best calibration and preservation of the inter-rater
variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the repeatability of deep learning models with Monte Carlo dropout. (arXiv:2202.07562v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07562">
<div class="article-summary-box-inner">
<span><p>The integration of artificial intelligence into clinical workflows requires
reliable and robust models. Repeatability is a key attribute of model
robustness. Repeatable models output predictions with low variation during
independent tests carried out under similar conditions. During model
development and evaluation, much attention is given to classification
performance while model repeatability is rarely assessed, leading to the
development of models that are unusable in clinical practice. In this work, we
evaluate the repeatability of four model types (binary classification,
multi-class classification, ordinal classification, and regression) on images
that were acquired from the same patient during the same visit. We study the
performance of binary, multi-class, ordinal, and regression models on four
medical image classification tasks from public and private datasets: knee
osteoarthritis, cervical cancer screening, breast density estimation, and
retinopathy of prematurity. Repeatability is measured and compared on ResNet
and DenseNet architectures. Moreover, we assess the impact of sampling Monte
Carlo dropout predictions at test time on classification performance and
repeatability. Leveraging Monte Carlo predictions significantly increased
repeatability for all tasks on the binary, multi-class, and ordinal models
leading to an average reduction of the 95\% limits of agreement by 16% points
and of the disagreement rate by 7% points. The classification accuracy improved
in most settings along with the repeatability. Our results suggest that beyond
about 20 Monte Carlo iterations, there is no further gain in repeatability. In
addition to the higher test-retest agreement, Monte Carlo predictions were
better calibrated which leads to output probabilities reflecting more
accurately the true likelihood of being correctly classified.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification. (arXiv:2202.07570v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07570">
<div class="article-summary-box-inner">
<span><p>Progress in digital pathology is hindered by high-resolution images and the
prohibitive cost of exhaustive localized annotations. The commonly used
paradigm to categorize pathology images is patch-based processing, which often
incorporates multiple instance learning (MIL) to aggregate local patch-level
representations yielding image-level prediction. Nonetheless, diagnostically
relevant regions may only take a small fraction of the whole tissue, and
MIL-based aggregation operation assumes that all patch representations are
independent and thus mislays the contextual information from adjacent cell and
tissue microenvironments. Consequently, the computational resources dedicated
to a specific region are independent of its information contribution. This
paper proposes a transformer-based architecture specifically tailored for
histopathological image classification, which combines fine-grained local
attention with a coarse global attention mechanism to learn meaningful
representations of high-resolution images at an efficient computational cost.
More importantly, based on the observation above, we propose a novel
mixing-based data-augmentation strategy, namely ScoreMix, by leveraging the
distribution of the semantic regions of images during the training and
carefully guiding the data mixing via sampling the locations of discriminative
image content. Thorough experiments and ablation studies on three challenging
representative cohorts of Haematoxylin &amp; Eosin (H&amp;E) tumour regions-of-interest
(TRoIs) datasets have validated the superiority of our approach over existing
state-of-the-art methods and effectiveness of our proposed components, e.g.,
data augmentation in improving classification performance. We also demonstrate
our method's interpretability, robustness, and cross-domain generalization
capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Representation Learning with Feedback. (arXiv:2202.07572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07572">
<div class="article-summary-box-inner">
<span><p>This note complements the author's recent paper "Robust representation
learning with feedback for single image deraining" by providing heuristically
theoretical explanations on the mechanism of representation learning with
feedback, namely an essential merit of the works presented in this recent
article. This note facilitates understanding of key points in the mechanism of
representation learning with feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness Indicators for Systematic Assessments of Visual Feature Extractors. (arXiv:2202.07603v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07603">
<div class="article-summary-box-inner">
<span><p>Does everyone equally benefit from computer vision systems? Answers to this
question become more and more important as computer vision systems are deployed
at large scale, and can spark major concerns when they exhibit vast performance
discrepancies between people from various demographic and social backgrounds.
Systematic diagnosis of fairness, harms, and biases of computer vision systems
is an important step towards building socially responsible systems. To initiate
an effort towards standardized fairness audits, we propose three fairness
indicators, which aim at quantifying harms and biases of visual systems. Our
indicators use existing publicly available datasets collected for fairness
evaluations, and focus on three main types of harms and bias identified in the
literature, namely harmful label associations, disparity in learned
representations of social and demographic traits, and biased performance on
geographically diverse images from across the world.We define precise
experimental protocols applicable to a wide range of computer vision models.
These indicators are part of an ever-evolving suite of fairness probes and are
not intended to be a substitute for a thorough analysis of the broader impact
of the new computer vision technologies. Yet, we believe it is a necessary
first step towards (1) facilitating the widespread adoption and mandate of the
fairness assessments in computer vision research, and (2) tracking progress
towards building socially responsible models. To study the practical
effectiveness and broad applicability of our proposed indicators to any visual
system, we apply them to off-the-shelf models built using widely adopted model
training paradigms which vary in their ability to whether they can predict
labels on a given image or only produce the embeddings. We also systematically
study the effect of data domain and model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lie Point Symmetry Data Augmentation for Neural PDE Solvers. (arXiv:2202.07643v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07643">
<div class="article-summary-box-inner">
<span><p>Neural networks are increasingly being used to solve partial differential
equations (PDEs), replacing slower numerical solvers. However, a critical issue
is that neural PDE solvers require high-quality ground truth data, which
usually must come from the very solvers they are designed to replace. Thus, we
are presented with a proverbial chicken-and-egg problem. In this paper, we
present a method, which can partially alleviate this problem, by improving
neural PDE solver sample complexity -- Lie point symmetry data augmentation
(LPSDA). In the context of PDEs, it turns out that we are able to
quantitatively derive an exhaustive list of data transformations, based on the
Lie point symmetry group of the PDEs in question, something not possible in
other application areas. We present this framework and demonstrate how it can
easily be deployed to improve neural PDE solver sample complexity by an order
of magnitude.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyclic Differentiable Architecture Search. (arXiv:2006.10724v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10724">
<div class="article-summary-box-inner">
<span><p>Differentiable ARchiTecture Search, i.e., DARTS, has drawn great attention in
neural architecture search. It tries to find the optimal architecture in a
shallow search network and then measures its performance in a deep evaluation
network. The independent optimization of the search and evaluation networks,
however, leaves room for potential improvement by allowing interaction between
the two networks. To address the problematic optimization issue, we propose new
joint optimization objectives and a novel Cyclic Differentiable ARchiTecture
Search framework, dubbed CDARTS. Considering the structure difference, CDARTS
builds a cyclic feedback mechanism between the search and evaluation networks
with introspective distillation. First, the search network generates an initial
architecture for evaluation, and the weights of the evaluation network are
optimized. Second, the architecture weights in the search network are further
optimized by the label supervision in classification, as well as the
regularization from the evaluation network through feature distillation.
Repeating the above cycle results in joint optimization of the search and
evaluation networks and thus enables the evolution of the architecture to fit
the final evaluation network. The experiments and analysis on CIFAR, ImageNet
and NAS-Bench-201 demonstrate the effectiveness of the proposed approach over
the state-of-the-art ones. Specifically, in the DARTS search space, we achieve
97.52% top-1 accuracy on CIFAR10 and 76.3% top-1 accuracy on ImageNet. In the
chain-structured search space, we achieve 78.2% top-1 accuracy on ImageNet,
which is 1.1% higher than EfficientNet-B0. Our code and models are publicly
available at https://github.com/microsoft/Cream.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FILTRA: Rethinking Steerable CNN by Filter Transform. (arXiv:2105.11636v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11636">
<div class="article-summary-box-inner">
<span><p>Steerable CNN imposes the prior knowledge of transformation invariance or
equivariance in the network architecture to enhance the the network robustness
on geometry transformation of data and reduce overfitting. It has been an
intuitive and widely used technique to construct a steerable filter by
augmenting a filter with its transformed copies in the past decades, which is
named as filter transform in this paper. Recently, the problem of steerable CNN
has been studied from aspect of group representation theory, which reveals the
function space structure of a steerable kernel function. However, it is not yet
clear on how this theory is related to the filter transform technique. In this
paper, we show that kernel constructed by filter transform can also be
interpreted in the group representation theory. This interpretation help
complete the puzzle of steerable CNN theory and provides a novel and simple
approach to implement steerable convolution operators. Experiments are executed
on multiple datasets to verify the feasibility of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning distinct features helps, provably. (arXiv:2106.06012v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06012">
<div class="article-summary-box-inner">
<span><p>We study the diversity of the features learned by a two-layer neural network
trained with the least squares loss. We measure the diversity by the average
$L_2$-distance between the hidden-layer features and theoretically investigate
how learning non-redundant distinct features affects the performance of the
network. To do so, we derive novel generalization bounds depending on feature
diversity based on Rademacher complexity for such networks. Our analysis proves
that more distinct features at the network's hidden units within the
intermediate layer lead to better generalization. We also show how to extend
our results to deeper networks and different losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring convolutional neural networks with transfer learning for diagnosing Lyme disease from skin lesion images. (arXiv:2106.14465v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14465">
<div class="article-summary-box-inner">
<span><p>Lyme disease which is one of the most common infectious vector-borne diseases
manifests itself in most cases with erythema migrans (EM) skin lesions. Recent
studies show that convolutional neural networks (CNNs) perform well to identify
skin lesions from images. Lightweight CNN based pre-scanner applications for
resource-constrained mobile devices can help users with early diagnosis of Lyme
disease and prevent the transition to a severe late form thanks to appropriate
antibiotic therapy. Also, resource-intensive CNN based robust computer
applications can assist non-expert practitioners with an accurate diagnosis.
The main objective of this study is to extensively analyze the effectiveness of
CNNs for diagnosing Lyme disease from images and to find out the best CNN
architectures considering resource constraints. First, we created an EM dataset
with the help of expert dermatologists from Clermont-Ferrand University
Hospital Center of France. Second, we benchmarked this dataset for twenty-three
CNN architectures customized from VGG, ResNet, DenseNet, MobileNet, Xception,
NASNet, and EfficientNet architectures in terms of predictive performance,
computational complexity, and statistical significance. Third, to improve the
performance of the CNNs, we used custom transfer learning from ImageNet
pre-trained models as well as pre-trained the CNNs with the skin lesion dataset
HAM10000. Fourth, for model explainability, we utilized Gradient-weighted Class
Activation Mapping to visualize the regions of input that are significant to
the CNNs for making predictions. Fifth, we provided guidelines for model
selection based on predictive performance and computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving. (arXiv:2109.11615v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11615">
<div class="article-summary-box-inner">
<span><p>Sharing collective perception messages (CPM) between vehicles is investigated
to decrease occlusions so as to improve the perception accuracy and safety of
autonomous driving. However, highly accurate data sharing and low communication
overhead is a big challenge for collective perception, especially when
real-time communication is required among connected and automated vehicles. In
this paper, we propose an efficient and effective keypoints-based deep feature
fusion framework built on the 3D object detector PV-RCNN, called Fusion PV-RCNN
(FPV-RCNN for short), for collective perception. We introduce a
high-performance bounding box proposal matching module and a keypoints
selection strategy to compress the CPM size and solve the multi-vehicle data
fusion problem. Besides, we also propose an effective localization error
correction module based on the maximum consensus principle to increase the
robustness of the data fusion. Compared to a bird's-eye view (BEV) keypoints
feature fusion, FPV-RCNN achieves improved detection accuracy by about 9% at a
high evaluation criterion (IoU 0.7) on the synthetic dataset COMAP dedicated to
collective perception. In addition, its performance is comparable to two raw
data fusion baselines that have no data loss in sharing. Moreover, our method
also significantly decreases the CPM size to less than 0.3 KB, and is thus
about 50 times smaller than the BEV feature map sharing used in previous works.
Even with further decreased CPM feature channels, i.e., from 128 to 32, the
detection performance does not show apparent drops. The code of our method is
available at https://github.com/YuanYunshuang/FPV_RCNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Representation Learning for Spatial Image Steganalysis. (arXiv:2110.00957v3 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00957">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a graph representation learning architecture for
spatial image steganalysis, which is motivated by the assumption that
steganographic modifications unavoidably distort the statistical
characteristics of the hidden graph features derived from cover images. In the
detailed architecture, we translate each image to a graph, where nodes
represent the patches of the image and edges indicate the local relationships
between the patches. Each node is associated with a feature vector determined
from the corresponding patch by a shallow convolutional neural network (CNN)
structure. By feeding the graph to an attention network, the discriminative
features can be learned for efficient steganalysis. Experiments indicate that
the reported architecture achieves a competitive performance compared to the
benchmark CNN model, which has shown the potential of graph learning for
steganalysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v8 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09113">
<div class="article-summary-box-inner">
<span><p>Salt and pepper noise removal is a common inverse problem in image
processing. Traditional denoising methods have two limitations. First, noise
characteristics are often not described accurately. For example, the noise
location information is often ignored and the sparsity of the salt and pepper
noise is often described by L1 norm, which cannot illustrate the sparse
variables clearly. Second, conventional methods separate the contaminated image
into a recovered image and a noise part, thus resulting in recovering an image
with unsatisfied smooth parts and detail parts. In this study, we introduce a
noise detection strategy to determine the position of the noise, and a
non-convex sparsity regularization depicted by Lp quasi-norm is employed to
describe the sparsity of the noise, thereby addressing the first limitation.
The morphological component analysis framework with stationary Framelet
transform is adopted to decompose the processed image into cartoon, texture,
and noise parts to resolve the second limitation. Then, the alternating
direction method of multipliers (ADMM) is employed to solve the proposed model.
Finally, experiments are conducted to verify the proposed method and compare it
with some current state-of-the-art denoising methods. The experimental results
show that the proposed method can remove salt and pepper noise while preserving
the details of the processed image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Primal-Dual Deep Unrolling. (arXiv:2110.10093v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10093">
<div class="article-summary-box-inner">
<span><p>We propose a new type of efficient deep-unrolling networks for solving
imaging inverse problems. Conventional deep-unrolling methods require full
forward operator and its adjoint across each layer, and hence can be
significantly more expensive computationally as compared with other end-to-end
methods that are based on post-processing of model-based reconstructions,
especially for 3D image reconstruction tasks. We develop a stochastic
(ordered-subsets) variant of the classical learned primal-dual (LPD), which is
a state-of-the-art unrolling network for tomographic image reconstruction. The
proposed learned stochastic primal-dual (LSPD) network only uses subsets of the
forward and adjoint operators and offers considerable computational efficiency.
We provide theoretical analysis of a special case of our LSPD framework,
suggesting that it has the potential to achieve image reconstruction quality
competitive with the full-batch LPD while requiring only a fraction of the
computation. The numerical results for two different X-ray computed tomography
(CT) imaging tasks (namely, low-dose and sparse-view CT) corroborate this
theoretical finding, demonstrating the promise of LSPD networks for large-scale
imaging problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Track Boosting and Synthetic Data Aided Drone Detection. (arXiv:2111.12389v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12389">
<div class="article-summary-box-inner">
<span><p>This is the paper for the first place winning solution of the Drone vs. Bird
Challenge, organized by AVSS 2021. As the usage of drones increases with
lowered costs and improved drone technology, drone detection emerges as a vital
object detection task. However, detecting distant drones under unfavorable
conditions, namely weak contrast, long-range, low visibility, requires
effective algorithms. Our method approaches the drone detection problem by
fine-tuning a YOLOv5 model with real and synthetically generated data using a
Kalman-based object tracker to boost detection confidence. Our results indicate
that augmenting the real data with an optimal subset of synthetic data can
increase the performance. Moreover, temporal information gathered by object
tracking methods can increase performance further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unity is strength: Improving the Detection of Adversarial Examples with Ensemble Approaches. (arXiv:2111.12631v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12631">
<div class="article-summary-box-inner">
<span><p>A key challenge in computer vision and deep learning is the definition of
robust strategies for the detection of adversarial examples. Here, we propose
the adoption of ensemble approaches to leverage the effectiveness of multiple
detectors in exploiting distinct properties of the input data. To this end, the
ENsemble Adversarial Detector (ENAD) framework integrates scoring functions
from state-of-the-art detectors based on Mahalanobis distance, Local Intrinsic
Dimensionality, and One-Class Support Vector Machines, which process the hidden
features of deep neural networks. ENAD is designed to ensure high
standardization and reproducibility to the computational workflow. Importantly,
extensive tests on benchmark datasets, models and adversarial attacks show that
ENAD outperforms all competing methods in the large majority of settings. The
improvement over the state-of-the-art and the intrinsic generality of the
framework, which allows one to easily extend ENAD to include any set of
detectors, set the foundations for the new area of ensemble adversarial
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adverse Weather Image Translation with Asymmetric and Uncertainty-aware GAN. (arXiv:2112.04283v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04283">
<div class="article-summary-box-inner">
<span><p>Adverse weather image translation belongs to the unsupervised image-to-image
(I2I) translation task which aims to transfer adverse condition domain (eg,
rainy night) to standard domain (eg, day). It is a challenging task because
images from adverse domains have some artifacts and insufficient information.
Recently, many studies employing Generative Adversarial Networks (GANs) have
achieved notable success in I2I translation but there are still limitations in
applying them to adverse weather enhancement. Symmetric architecture based on
bidirectional cycle-consistency loss is adopted as a standard framework for
unsupervised domain transfer methods. However, it can lead to inferior
translation result if the two domains have imbalanced information. To address
this issue, we propose a novel GAN model, i.e., AU-GAN, which has an asymmetric
architecture for adverse domain translation. We insert a proposed feature
transfer network (${T}$-net) in only a normal domain generator (i.e., rainy
night-&gt; day) to enhance encoded features of the adverse domain image. In
addition, we introduce asymmetric feature matching for disentanglement of
encoded features. Finally, we propose uncertainty-aware cycle-consistency loss
to address the regional uncertainty of a cyclic reconstructed image. We
demonstrate the effectiveness of our method by qualitative and quantitative
comparisons with state-of-the-art models. Codes are available at
https://github.com/jgkwak95/AU-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real-Time Rendering Method for Light Field Display. (arXiv:2201.08266v3 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08266">
<div class="article-summary-box-inner">
<span><p>A real-time elemental image array (EIA) generation method which does not
sacrifice accuracy nor rely on high-performance hardware is developed, through
raytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both
offline and online working flow, experiments will verified the effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (arXiv:2201.12086v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12086">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pre-training (VLP) has advanced the performance for many
vision-language tasks. However, most existing pre-trained models only excel in
either understanding-based tasks or generation-based tasks. Furthermore,
performance improvement has been largely achieved by scaling up the dataset
with noisy image-text pairs collected from the web, which is a suboptimal
source of supervision. In this paper, we propose BLIP, a new VLP framework
which transfers flexibly to both vision-language understanding and generation
tasks. BLIP effectively utilizes the noisy web data by bootstrapping the
captions, where a captioner generates synthetic captions and a filter removes
the noisy ones. We achieve state-of-the-art results on a wide range of
vision-language tasks, such as image-text retrieval (+2.7% in average
recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).
BLIP also demonstrates strong generalization ability when directly transferred
to video-language tasks in a zero-shot manner. Code, models, and datasets are
released at https://github.com/salesforce/BLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning on 3D Point Clouds by Clustering and Contrasting. (arXiv:2202.02543v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02543">
<div class="article-summary-box-inner">
<span><p>Learning from unlabeled or partially labeled data to alleviate human labeling
remains a challenging research topic in 3D modeling. Along this line,
unsupervised representation learning is a promising direction to auto-extract
features without human intervention. This paper proposes a general unsupervised
approach, named \textbf{ConClu}, to perform the learning of point-wise and
global features by jointly leveraging point-level clustering and instance-level
contrasting. Specifically, for one thing, we design an Expectation-Maximization
(EM) like soft clustering algorithm that provides local supervision to extract
discriminating local features based on optimal transport. We show that this
criterion extends standard cross-entropy minimization to an optimal transport
problem, which we solve efficiently using a fast variant of the Sinkhorn-Knopp
algorithm. For another, we provide an instance-level contrasting method to
learn the global geometry, which is formulated by maximizing the similarity
between two augmentations of one point cloud. Experimental evaluations on
downstream applications such as 3D object classification and semantic
segmentation demonstrate the effectiveness of our framework and show that it
can outperform state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework. (arXiv:2202.02626v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02626">
<div class="article-summary-box-inner">
<span><p>Deep neural network models are used today in various applications of
artificial intelligence, the strengthening of which, in the face of adversarial
attacks is of particular importance. An appropriate solution to adversarial
attacks is adversarial training, which reaches a trade-off between robustness
and generalization. This paper introduces a novel framework (Layer
Sustainability Analysis (LSA)) for the analysis of layer vulnerability in an
arbitrary neural network in the scenario of adversarial attacks. LSA can be a
helpful toolkit to assess deep neural networks and to extend the adversarial
training approaches towards improving the sustainability of model layers via
layer monitoring and analysis. The LSA framework identifies a list of Most
Vulnerable Layers (MVL list) of the given network. The relative error, as a
comparison measure, is used to evaluate representation sustainability of each
layer against adversarial inputs. The proposed approach for obtaining robust
neural networks to fend off adversarial attacks is based on a layer-wise
regularization (LR) over LSA proposal(s) for adversarial training (AT); i.e.
the AT-LR procedure. AT-LR could be used with any benchmark adversarial attack
to reduce the vulnerability of network layers and to improve conventional
adversarial training approaches. The proposed idea performs well theoretically
and experimentally for state-of-the-art multilayer perceptron and convolutional
neural network architectures. Compared with the AT-LR and its corresponding
base adversarial training, the classification accuracy of more significant
perturbations increased by 16.35%, 21.79%, and 10.730% on Moon, MNIST, and
CIFAR-10 benchmark datasets, respectively. The LSA framework is available and
published at https://github.com/khalooei/LSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Deterministic Independent Component Analysis for Hyperspectral Unmixing. (arXiv:2202.02951v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02951">
<div class="article-summary-box-inner">
<span><p>We develop a new neural network based independent component analysis (ICA)
method by directly minimizing the dependence amongst all extracted components.
Using the matrix-based R{\'e}nyi's $\alpha$-order entropy functional, our
network can be directly optimized by stochastic gradient descent (SGD), without
any variational approximation or adversarial training. As a solid application,
we evaluate our ICA in the problem of hyperspectral unmixing (HU) and refute a
statement that "\emph{ICA does not play a role in unmixing hyperspectral
data}", which was initially suggested by \cite{nascimento2005does}. Code and
additional remarks of our DDICA is available at
https://github.com/hongmingli1995/DDICA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-level augmentation to improve robustness of deep neural networks to affine transformations. (arXiv:2202.05152v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05152">
<div class="article-summary-box-inner">
<span><p>Recent studies revealed that convolutional neural networks do not generalize
well to small image transformations, e.g. rotations by a few degrees or
translations of a few pixels. To improve the robustness to such
transformations, we propose to introduce data augmentation at intermediate
layers of the neural architecture, in addition to the common data augmentation
applied on the input images. By introducing small perturbations to activation
maps (features) at various levels, we develop the capacity of the neural
network to cope with such transformations. We conduct experiments on three
image classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101),
considering two different convolutional architectures (ResNet-18 and
DenseNet-121). When compared with two state-of-the-art stabilization methods,
the empirical results show that our approach consistently attains the best
trade-off between accuracy and mean flip rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Transport for Super Resolution Applied to Astronomy Imaging. (arXiv:2202.05354v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05354">
<div class="article-summary-box-inner">
<span><p>Super resolution is an essential tool in optics, especially on interstellar
scales, due to physical laws restricting possible imaging resolution. We
propose using optimal transport and entropy for super resolution applications.
We prove that the reconstruction is accurate when sparsity is known and noise
or distortion is small enough. We prove that the optimizer is stable and robust
to noise and perturbations. We compare this method to a state of the art
convolutional neural network and get similar results for much less
computational cost and greater methodological flexibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-light Image Enhancement by Retinex Based Algorithm Unrolling and Adjustment. (arXiv:2202.05972v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05972">
<div class="article-summary-box-inner">
<span><p>Motivated by their recent advances, deep learning techniques have been widely
applied to low-light image enhancement (LIE) problem. Among which, Retinex
theory based ones, mostly following a decomposition-adjustment pipeline, have
taken an important place due to its physical interpretation and promising
performance. However, current investigations on Retinex based deep learning are
still not sufficient, ignoring many useful experiences from traditional
methods. Besides, the adjustment step is either performed with simple image
processing techniques, or by complicated networks, both of which are
unsatisfactory in practice. To address these issues, we propose a new deep
learning framework for the LIE problem. The proposed framework contains a
decomposition network inspired by algorithm unrolling, and adjustment networks
considering both global brightness and local brightness sensitivity. By virtue
of algorithm unrolling, both implicit priors learned from data and explicit
priors borrowed from traditional methods can be embedded in the network,
facilitate to better decomposition. Meanwhile, the consideration of global and
local brightness can guide designing simple yet effective network modules for
adjustment. Besides, to avoid manually parameter tuning, we also propose a
self-supervised fine-tuning strategy, which can always guarantee a promising
performance. Experiments on a series of typical LIE datasets demonstrated the
effectiveness of the proposed method, both quantitatively and visually, as
compared with existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse facial inpainting guided by exemplars. (arXiv:2202.06358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06358">
<div class="article-summary-box-inner">
<span><p>Facial image inpainting is a task of filling visually realistic and
semantically meaningful contents for missing or masked pixels in a face image.
Although existing methods have made significant progress in achieving high
visual quality, the controllable diversity of facial image inpainting remains
an open problem in this field. This paper introduces EXE-GAN, a novel diverse
and interactive facial inpainting framework, which can not only preserve the
high-quality visual effect of the whole image but also complete the face image
with exemplar-like facial attributes. The proposed facial inpainting is
achieved based on generative adversarial networks by leveraging the global
style of input image, the stochastic style, and the exemplar style of exemplar
image. A novel attribute similarity metric is introduced to encourage networks
to learn the style of facial attributes from the exemplar in a self-supervised
way. To guarantee the natural transition across the boundary of inpainted
regions, a novel spatial variant gradient backpropagation technique is designed
to adjust the loss gradients based on the spatial location. A variety of
experimental results and comparisons on public CelebA-HQ and FFHQ datasets are
presented to demonstrate the superiority of the proposed method in terms of
both the quality and diversity in facial inpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADeADA: Adaptive Density-aware Active Domain Adaptation for Semantic Segmentation. (arXiv:2202.06484v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06484">
<div class="article-summary-box-inner">
<span><p>In the field of domain adaptation, a trade-off exists between the model
performance and the number of target domain annotations. Active learning,
maximizing model performance with few informative labeled data, comes in handy
for such a scenario. In this work, we present ADeADA, a general active domain
adaptation framework for semantic segmentation. To adapt the model to the
target domain with minimum queried labels, we propose acquiring labels of the
samples with high probability density in the target domain yet with low
probability density in the source domain, complementary to the existing source
domain labeled data. To further facilitate the label efficiency, we design an
adaptive budget allocation policy, which dynamically balances the labeling
budgets among different categories as well as between density-aware and
uncertainty-based methods. Extensive experiments show that our method
outperforms existing active learning and domain adaptation baselines on two
benchmarks, GTA5 -&gt; Cityscapes and SYNTHIA -&gt; Cityscapes. With less than 5%
target domain annotations, our method reaches comparable results with that of
full supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection. (arXiv:2202.06934v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06934">
<div class="article-summary-box-inner">
<span><p>Detection of small objects and objects far away in the scene is a major
challenge in surveillance applications. Such objects are represented by small
number of pixels in the image and lack sufficient details, making them
difficult to detect using conventional detectors. In this work, an open-source
framework called Slicing Aided Hyper Inference (SAHI) is proposed that provides
a generic slicing aided inference and fine-tuning pipeline for small object
detection. The proposed technique is generic in the sense that it can be
applied on top of any available object detector without any fine-tuning.
Experimental evaluations, using object detection baselines on the Visdrone and
xView aerial object detection datasets show that the proposed inference method
can increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and
TOOD detectors, respectively. Moreover, the detection accuracy can be further
increased with a slicing aided fine-tuning, resulting in a cumulative increase
of 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been
integrated with Detectron2, MMDetection and YOLOv5 models and it is publicly
available at https://github.com/obss/sahi.git .
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-16 23:07:13.184256021 UTC">2022-02-16 23:07:13 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>