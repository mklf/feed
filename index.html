<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-20T01:30:00Z">06-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08406">
<div class="article-summary-box-inner">
<span><p>Tweets are the most concise form of communication in online social media,
wherein a single tweet has the potential to make or break the discourse of the
conversation. Online hate speech is more accessible than ever, and stifling its
propagation is of utmost importance for social media companies and users for
congenial communication. Most of the research barring a recent few has focused
on classifying an individual tweet regardless of the tweet thread/context
leading up to that point. One of the classical approaches to curb hate speech
is to adopt a reactive strategy after the hate speech postage. The ex-post
facto strategy results in neglecting subtle posts that do not show the
potential to instigate hate speech on their own but may portend in the
subsequent discussion ensuing in the post's replies. In this paper, we propose
DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring
in through its reply chain in the future. It uses the semantic and propagating
structure of the tweet threads to maximize the contextual information leading
up to and the fall of hate intensity at each subsequent tweet. We explore three
publicly available Twitter datasets -- Anti-Racism contains the reply tweets of
a collection of social media discourse on racist remarks during US political
and Covid-19 background; Anti-Social presents a dataset of 40 million tweets
amidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents
Twitter datasets collated based on anti-Asian behaviours during COVID-19
pandemic. All the curated datasets consist of structural graph information of
the Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art
baselines significantly. It beats the best baseline by an 11\% margin on the
Person correlation coefficient and a decrease of 25\% on RMSE for the
Anti-Racism dataset with a similar performance on the other two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Multi-Task Models for Misogyny Identification and Categorization on Arabic Social Media. (arXiv:2206.08407v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08407">
<div class="article-summary-box-inner">
<span><p>The prevalence of toxic content on social media platforms, such as hate
speech, offensive language, and misogyny, presents serious challenges to our
interconnected society. These challenging issues have attracted widespread
attention in Natural Language Processing (NLP) community. In this paper, we
present the submitted systems to the first Arabic Misogyny Identification
shared task. We investigate three multi-task learning models as well as their
single-task counterparts. In order to encode the input text, our models rely on
the pre-trained MARBERT language model. The overall obtained results show that
all our submitted models have achieved the best performances (top three ranked
submissions) in both misogyny identification and categorization tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CS-UM6P at SemEval-2022 Task 6: Transformer-based Models for Intended Sarcasm Detection in English and Arabic. (arXiv:2206.08415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08415">
<div class="article-summary-box-inner">
<span><p>Sarcasm is a form of figurative language where the intended meaning of a
sentence differs from its literal meaning. This poses a serious challenge to
several Natural Language Processing (NLP) applications such as Sentiment
Analysis, Opinion Mining, and Author Profiling. In this paper, we present our
participating system to the intended sarcasm detection task in English and
Arabic languages. Our system\footnote{The source code of our system is
available at \url{https://github.com/AbdelkaderMH/iSarcasmEval}} consists of
three deep learning-based models leveraging two existing pre-trained language
models for Arabic and English. We have participated in all sub-tasks. Our
official submissions achieve the best performance on sub-task A for Arabic
language and rank second in sub-task B. For sub-task C, our system is ranked
7th and 11th on Arabic and English datasets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogueScript: Using Dialogue Agents to Produce a Script. (arXiv:2206.08425v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08425">
<div class="article-summary-box-inner">
<span><p>We present a novel approach to generating scripts by using agents with
different personality types. To manage character interaction in the script, we
employ simulated dramatic networks. Automatic and human evaluation on multiple
criteria shows that our approach outperforms a vanilla-GPT2-based baseline. We
further introduce a new metric to evaluate dialogue consistency based on
natural language inference and demonstrate its validity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAAMA 2.0: An Integrated System that Answers Boolean and Extractive Question. (arXiv:2206.08441v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08441">
<div class="article-summary-box-inner">
<span><p>Recent machine reading comprehension datasets include extractive and boolean
questions but current approaches do not offer integrated support for answering
both question types. We present a multilingual machine reading comprehension
system and front-end demo that handles boolean questions by providing both a
YES/NO answer and highlighting supporting evidence, and handles extractive
questions by highlighting the answer in the passage. Our system, GAAMA 2.0, is
ranked first on the Tydi QA leaderboard at the time of this writing. We
contrast two different implementations of our approach. The first includes
several independent stacks of transformers allowing easy deployment of each
component. The second is a single stack of transformers utilizing adapters to
reduce GPU memory footprint in a resource-constrained environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriching Abusive Language Detection with Community Context. (arXiv:2206.08445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08445">
<div class="article-summary-box-inner">
<span><p>Uses of pejorative expressions can be benign or actively empowering. When
models for abuse detection misclassify these expressions as derogatory, they
inadvertently censor productive conversations held by marginalized groups. One
way to engage with non-dominant perspectives is to add context around
conversations. Previous research has leveraged user- and thread-level features,
but it often neglects the spaces within which productive conversations take
place. Our paper highlights how community context can improve classification
outcomes in abusive language detection. We make two main contributions to this
end. First, we demonstrate that online communities cluster by the nature of
their support towards victims of abuse. Second, we establish how community
context improves accuracy and reduces the false positive rates of
state-of-the-art abusive language classifiers. These findings suggest a
promising direction for context-aware models in abusive language research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Methods for Estimating and Improving Robustness of Language Models. (arXiv:2206.08446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08446">
<div class="article-summary-box-inner">
<span><p>Despite their outstanding performance, large language models (LLMs) suffer
notorious flaws related to their preference for simple, surface-level textual
relations over full semantic complexity of the problem. This proposal
investigates a common denominator of this problem in their weak ability to
generalise outside of the training domain. We survey diverse research
directions providing estimations of model generalisation ability and find that
incorporating some of these measures in the training objectives leads to
enhanced distributional robustness of neural models. Based on these findings,
we present future research directions towards enhancing the robustness of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering. (arXiv:2206.08486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08486">
<div class="article-summary-box-inner">
<span><p>Effective multi-hop question answering (QA) requires reasoning over multiple
scattered paragraphs and providing explanations for answers. Most existing
approaches cannot provide an interpretable reasoning process to illustrate how
these models arrive at an answer. In this paper, we propose a Question
Decomposition method based on Abstract Meaning Representation (QDAMR) for
multi-hop QA, which achieves interpretable reasoning by decomposing a multi-hop
question into simpler sub-questions and answering them in order. Since
annotating the decomposition is expensive, we first delegate the complexity of
understanding the multi-hop question to an AMR parser. We then achieve the
decomposition of a multi-hop question via segmentation of the corresponding AMR
graph based on the required reasoning type. Finally, we generate sub-questions
using an AMR-to-Text generation model and answer them with an off-the-shelf QA
model. Experimental results on HotpotQA demonstrate that our approach is
competitive for interpretable reasoning and that the sub-questions generated by
QDAMR are well-formed, outperforming existing question-decomposition-based
multi-hop QA approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Numerical Reasoning Question Answering System with Fine-grained Retriever and the Ensemble of Multiple Generators for FinQA. (arXiv:2206.08506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08506">
<div class="article-summary-box-inner">
<span><p>The numerical reasoning in the financial domain -- performing quantitative
analysis and summarizing the information from financial reports -- can greatly
increase business efficiency and reduce costs of billions of dollars. Here, we
propose a numerical reasoning question answering system to answer numerical
reasoning questions among financial text and table data sources, consisting of
a retriever module, a generator module, and an ensemble module. Specifically,
in the retriever module, in addition to retrieving the whole row data, we
innovatively design a cell retriever that retrieves the gold cells to avoid
bringing unrelated and similar cells in the same row to the inputs of the
generator module. In the generator module, we utilize multiple generators to
produce programs, which are operation steps to answer the question. Finally, in
the ensemble module, we integrate multiple programs to choose the best program
as the output of our system. In the final private test set in FinQA
Competition, our system obtains 69.79 execution accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. (arXiv:2206.08514v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08514">
<div class="article-summary-box-inner">
<span><p>Textual backdoor attacks are a kind of practical threat to NLP systems. By
injecting a backdoor in the training phase, the adversary could control model
predictions via predefined triggers. As various attack and defense models have
been proposed, it is of great significance to perform rigorous evaluations.
However, we highlight two issues in previous backdoor learning evaluations: (1)
The differences between real-world scenarios (e.g. releasing poisoned datasets
or models) are neglected, and we argue that each scenario has its own
constraints and concerns, thus requires specific evaluation protocols; (2) The
evaluation metrics only consider whether the attacks could flip the models'
predictions on poisoned samples and retain performances on benign samples, but
ignore that poisoned samples should also be stealthy and semantic-preserving.
To address these issues, we categorize existing works into three practical
scenarios in which attackers release datasets, pre-trained models, and
fine-tuned models respectively, then discuss their unique evaluation
methodologies. On metrics, to completely evaluate poisoned samples, we use
grammar error increase and perplexity difference for stealthiness, along with
text similarity for validity. After formalizing the frameworks, we develop an
open-source toolkit OpenBackdoor to foster the implementations and evaluations
of textual backdoor learning. With this toolkit, we perform extensive
experiments to benchmark attack and defense models under the suggested
paradigm. To facilitate the underexplored defenses against poisoned datasets,
we further propose CUBE, a simple yet strong clustering-based defense baseline.
We hope that our frameworks and benchmarks could serve as the cornerstones for
future model development and evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation. (arXiv:2206.08522v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08522">
<div class="article-summary-box-inner">
<span><p>Benefiting from language flexibility and compositionality, humans naturally
intend to use language to command an embodied agent for complex tasks such as
navigation and object manipulation. In this work, we aim to fill the blank of
the last mile of embodied agents -- object manipulation by following human
guidance, e.g., "move the red mug next to the box while keeping it upright." To
this end, we introduce an Automatic Manipulation Solver (AMSolver) simulator
and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,
containing various language instructions on categorized robotic manipulation
tasks. Specifically, modular rule-based task templates are created to
automatically generate robot demonstrations with language instructions,
consisting of diverse object shapes and appearances, action types, and motion
constraints. We also develop a keypoint-based model 6D-CLIPort to deal with
multi-view observations and language input and output a sequence of 6 degrees
of freedom (DoF) actions. We hope the new simulator and benchmark will
facilitate future research on language-guided robotic manipulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Correction of Human Translations. (arXiv:2206.08593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08593">
<div class="article-summary-box-inner">
<span><p>We introduce translation error correction (TEC), the task of automatically
correcting human-generated translations. Imperfections in machine translations
(MT) have long motivated systems for improving translations post-hoc with
automatic post-editing. In contrast, little attention has been devoted to the
problem of automatically correcting human translations, despite the intuition
that humans make distinct errors that machines would be well-suited to assist
with, from typos to inconsistencies in translation conventions. To investigate
this, we build and release the Aced corpus with three TEC datasets. We show
that human errors in TEC exhibit a more diverse range of errors and far fewer
translation fluency errors than the MT errors in automatic post-editing
datasets, suggesting the need for dedicated TEC models that are specialized to
correct human errors. We show that pre-training instead on synthetic errors
based on human errors improves TEC F-score by as much as 5.1 points. We
conducted a human-in-the-loop user study with nine professional translation
editors and found that the assistance of our TEC system led them to produce
significantly higher quality revised translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment. (arXiv:2206.08614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08614">
<div class="article-summary-box-inner">
<span><p>Computational inference of aesthetics is an ill-defined task due to its
subjective nature. Many datasets have been proposed to tackle the problem by
providing pairs of images and aesthetic scores based on human ratings. However,
humans are better at expressing their opinion, taste, and emotions by means of
language rather than summarizing them in a single number. In fact, photo
critiques provide much richer information as they reveal how and why users rate
the aesthetics of visual stimuli. In this regard, we propose the Reddit Photo
Critique Dataset (RPCD), which contains tuples of image and photo critiques.
RPCD consists of 74K images and 220K comments and is collected from a Reddit
community used by hobbyists and professional photographers to improve their
photography skills by leveraging constructive community feedback. The proposed
dataset differs from previous aesthetics datasets mainly in three aspects,
namely (i) the large scale of the dataset and the extension of the comments
criticizing different aspects of the image, (ii) it contains mostly UltraHD
images, and (iii) it can easily be extended to new data as it is collected
through an automatic pipeline. To the best of our knowledge, in this work, we
propose the first attempt to estimate the aesthetic quality of visual stimuli
from the critiques. To this end, we exploit the polarity of the sentiment of
criticism as an indicator of aesthetic judgment. We demonstrate how sentiment
polarity correlates positively with the aesthetic judgment available for two
aesthetic assessment benchmarks. Finally, we experiment with several models by
using the sentiment scores as a target for ranking images. Dataset and
baselines are available (https://github.com/mediatechnologycenter/aestheval).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08657">
<div class="article-summary-box-inner">
<span><p>Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a cross-modal encoder, or feed the last-layer
uni-modal features directly into the top cross-modal encoder, ignoring the
semantic information at the different levels in the deep uni-modal encoders.
Both approaches possibly restrict vision-language representation learning and
limit model performance. In this paper, we introduce multiple bridge layers
that build a connection between the top layers of uni-modal encoders and each
layer of the cross-modal encoder. This enables comprehensive bottom-up
interactions between visual and textual representations at different semantic
levels, resulting in more effective cross-modal alignment and fusion. Our
proposed Bridge-Tower, pre-trained with only $4$M images, achieves
state-of-the-art performance on various downstream vision-language tasks. On
the VQAv2 test-std set, Bridge-Tower achieves an accuracy of $78.73\%$,
outperforming the previous state-of-the-art METER model by $1.09\%$ with the
same pre-training data and almost no additional parameters and computational
cost. Notably, when further scaling the model, Bridge-Tower achieves an
accuracy of $81.15\%$, surpassing models that are pre-trained on
orders-of-magnitude larger datasets. Code is available at
https://github.com/microsoft/BridgeTower.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Quantitative and Qualitative Analysis of Suicide Ideation Detection using Deep Learning. (arXiv:2206.08673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08673">
<div class="article-summary-box-inner">
<span><p>For preventing youth suicide, social media platforms have received much
attention from researchers. A few researches apply machine learning, or deep
learning-based text classification approaches to classify social media posts
containing suicidality risk. This paper replicated competitive social
media-based suicidality detection/prediction models. We evaluated the
feasibility of detecting suicidal ideation using multiple datasets and
different state-of-the-art deep learning models, RNN-, CNN-, and
Attention-based models. Using two suicidality evaluation datasets, we evaluated
28 combinations of 7 input embeddings with 4 commonly used deep learning models
and 5 pretrained language models in quantitative and qualitative ways. Our
replication study confirms that deep learning works well for social media-based
suicidality detection in general, but it highly depends on the dataset's
quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BITS Pilani at HinglishEval: Quality Evaluation for Code-Mixed Hinglish Text Using Transformers. (arXiv:2206.08680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08680">
<div class="article-summary-box-inner">
<span><p>Code-Mixed text data consists of sentences having words or phrases from more
than one language. Most multi-lingual communities worldwide communicate using
multiple languages, with English usually one of them. Hinglish is a Code-Mixed
text composed of Hindi and English but written in Roman script. This paper aims
to determine the factors influencing the quality of Code-Mixed text data
generated by the system. For the HinglishEval task, the proposed model uses
multi-lingual BERT to find the similarity between synthetically generated and
human-generated sentences to predict the quality of synthetically generated
Hinglish sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical and Neural Methods for Cross-lingual Entity Label Mapping in Knowledge Graphs. (arXiv:2206.08709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08709">
<div class="article-summary-box-inner">
<span><p>Knowledge bases such as Wikidata amass vast amounts of named entity
information, such as multilingual labels, which can be extremely useful for
various multilingual and cross-lingual applications. However, such labels are
not guaranteed to match across languages from an information consistency
standpoint, greatly compromising their usefulness for fields such as machine
translation. In this work, we investigate the application of word and sentence
alignment techniques coupled with a matching algorithm to align cross-lingual
entity labels extracted from Wikidata in 10 languages. Our results indicate
that mapping between Wikidata's main labels stands to be considerably improved
(up to $20$ points in F1-score) by any of the employed methods. We show how
methods relying on sentence embeddings outperform all others, even across
different scripts. We believe the application of such techniques to measure the
similarity of label pairs, coupled with a knowledge base rich in high-quality
entity labels, to be an excellent asset to machine translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CookDial: A dataset for task-oriented dialogs grounded in procedural documents. (arXiv:2206.08723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08723">
<div class="article-summary-box-inner">
<span><p>This work presents a new dialog dataset, CookDial, that facilitates research
on task-oriented dialog systems with procedural knowledge understanding. The
corpus contains 260 human-to-human task-oriented dialogs in which an agent,
given a recipe document, guides the user to cook a dish. Dialogs in CookDial
exhibit two unique features: (i) procedural alignment between the dialog flow
and supporting document; (ii) complex agent decision-making that involves
segmenting long sentences, paraphrasing hard instructions and resolving
coreference in the dialog context. In addition, we identify three challenging
(sub)tasks in the assumed task-oriented dialog system: (1) User Question
Understanding, (2) Agent Action Frame Prediction, and (3) Agent Response
Generation. For each of these tasks, we develop a neural baseline model, which
we evaluate on the CookDial dataset. We publicly release the CookDial dataset,
comprising rich annotations of both dialogs and recipe documents, to stimulate
further research on domain-specific document-grounded dialog systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crowdsourcing Relative Rankings of Multi-Word Expressions: Experts versus Non-Experts. (arXiv:2206.08724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08724">
<div class="article-summary-box-inner">
<span><p>In this study we investigate to which degree experts and non-experts agree on
questions of difficulty in a crowdsourcing experiment. We ask non-experts
(second language learners of Swedish) and two groups of experts (teachers of
Swedish as a second/foreign language and CEFR experts) to rank multi-word
expressions in a crowdsourcing experiment. We find that the resulting rankings
by all the three tested groups correlate to a very high degree, which suggests
that judgments produced in a comparative setting are not influenced by
professional insights into Swedish as a second language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The ITU Faroese Pairs Dataset. (arXiv:2206.08727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08727">
<div class="article-summary-box-inner">
<span><p>This article documents a dataset of sentence pairs between Faroese and
Danish, produced at ITU Copenhagen. The data covers tranlsation from both
source languages, and is intended for use as training data for machine
translation systems in this language pair.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE. (arXiv:2206.08790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08790">
<div class="article-summary-box-inner">
<span><p>The human perception system is often assumed to recruit motor knowledge when
processing auditory speech inputs. Using articulatory modeling and deep
learning, this study examines how this articulatory information can be used for
discovering speech units in a self-supervised setting. We used vector-quantized
variational autoencoders (VQ-VAE) to learn discrete representations from
articulatory and acoustic speech data. In line with the zero-resource paradigm,
an ABX test was then used to investigate how the extracted representations
encode phonetically relevant properties. Experiments were conducted on three
different corpora in English and French. We found that articulatory information
rather organises the latent representations in terms of place of articulation
whereas the speech acoustics mainly structure the latent space in terms of
manner of articulation. We show that an optimal fusion of the two modalities
can lead to a joint representation of these phonetic dimensions more accurate
than each modality considered individually. Since articulatory information is
usually not available in a practical situation, we finally investigate the
benefit it provides when inferred from the speech acoustics in a
self-supervised manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language with Vision: a Study on Grounded Word and Sentence Embeddings. (arXiv:2206.08823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08823">
<div class="article-summary-box-inner">
<span><p>Language grounding to vision is an active field of research aiming to enrich
text-based representations of word meanings by leveraging perceptual knowledge
from vision. Despite many attempts at language grounding, it is still unclear
how to effectively inject visual knowledge into the word embeddings of a
language in such a way that a proper balance of textual and visual knowledge is
maintained. Some common concerns are the following. Is visual grounding
beneficial for abstract words or is its contribution only limited to concrete
words? What is the optimal way of bridging the gap between text and vision? How
much do we gain by visually grounding textual embeddings? The present study
addresses these questions by proposing a simple yet very effective grounding
approach for pre-trained word embeddings. Our model aligns textual embeddings
with vision while largely preserving the distributional statistics that
characterize word use in text corpora. By applying a learned alignment, we are
able to generate visually grounded embeddings for unseen words, including
abstract words. A series of evaluations on word similarity benchmarks shows
that visual grounding is beneficial not only for concrete words, but also for
abstract words. We also show that our method for visual grounding offers
advantages for contextualized embeddings, but only when these are trained on
corpora of relatively modest size. Code and grounded embeddings for English are
available at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What can Speech and Language Tell us About the Working Alliance in Psychotherapy. (arXiv:2206.08835v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08835">
<div class="article-summary-box-inner">
<span><p>We are interested in the problem of conversational analysis and its
application to the health domain. Cognitive Behavioral Therapy is a structured
approach in psychotherapy, allowing the therapist to help the patient to
identify and modify the malicious thoughts, behavior, or actions. This
cooperative effort can be evaluated using the Working Alliance Inventory
Observer-rated Shortened - a 12 items inventory covering task, goal, and
relationship - which has a relevant influence on therapeutic outcomes. In this
work, we investigate the relation between this alliance inventory and the
spoken conversations (sessions) between the patient and the psychotherapist. We
have delivered eight weeks of e-therapy, collected their audio and video call
sessions, and manually transcribed them. The spoken conversations have been
annotated and evaluated with WAI ratings by professional therapists. We have
investigated speech and language features and their association with WAI items.
The feature types include turn dynamics, lexical entrainment, and
conversational descriptors extracted from the speech and language signals. Our
findings provide strong evidence that a subset of these features are strong
indicators of working alliance. To the best of our knowledge, this is the first
and a novel study to exploit speech and language for characterising working
alliance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. (arXiv:2206.08853v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08853">
<div class="article-summary-box-inner">
<span><p>Autonomous agents have made great strides in specialist domains like Atari
games and Go. However, they typically learn tabula rasa in isolated
environments with limited and manually conceived objectives, thus failing to
generalize across a wide spectrum of tasks and capabilities. Inspired by how
humans continually learn and adapt in the open world, we advocate a trinity of
ingredients for building generalist agents: 1) an environment that supports a
multitude of tasks and goals, 2) a large-scale database of multimodal
knowledge, and 3) a flexible and scalable agent architecture. We introduce
MineDojo, a new framework built on the popular Minecraft game that features a
simulation suite with thousands of diverse open-ended tasks and an
internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and
forum discussions. Using MineDojo's data, we propose a novel agent learning
algorithm that leverages large pre-trained video-language models as a learned
reward function. Our agent is able to solve a variety of open-ended tasks
specified in free-form language without any manually designed dense shaping
reward. We open-source the simulation suite and knowledge bases
(https://minedojo.org) to promote research towards the goal of generally
capable embodied agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">niksss at HinglishEval: Language-agnostic BERT-based Contextual Embeddings with Catboost for Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed Hinglish Text. (arXiv:2206.08910v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08910">
<div class="article-summary-box-inner">
<span><p>This paper describes the system description for the HinglishEval challenge at
INLG 2022. The goal of this task was to investigate the factors influencing the
quality of the code-mixed text generation system. The task was divided into two
subtasks, quality rating prediction and annotators disagreement prediction of
the synthetic Hinglish dataset. We attempted to solve these tasks using
sentence-level embeddings, which are obtained from mean pooling the
contextualized word embeddings for all input tokens in our text. We
experimented with various classifiers on top of the embeddings produced for
respective tasks. Our best-performing system ranked 1st on subtask B and 3rd on
subtask A.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Model to Measure the Spread Power of Rumors. (arXiv:2002.07563v5 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.07563">
<div class="article-summary-box-inner">
<span><p>With technologies that have democratized the production and reproduction of
information, a significant portion of daily interacted posts in social media
has been infected by rumors. Despite the extensive research on rumor detection
and verification, so far, the problem of calculating the spread power of rumors
has not been considered. To address this research gap, the present study seeks
a model to calculate the Spread Power of Rumor (SPR) as the function of
content-based features in two categories: False Rumor (FR) and True Rumor (TR).
For this purpose, the theory of Allport and Postman will be adopted, which it
claims that importance and ambiguity are the key variables in rumor-mongering
and the power of rumor. Totally 42 content features in two categories
"importance" (28 features) and "ambiguity" (14 features) are introduced to
compute SPR. The proposed model is evaluated on two datasets, Twitter and
Telegram. The results showed that (i) the spread power of False Rumor documents
is rarely more than True Rumors. (ii) there is a significant difference between
the SPR means of two groups False Rumor and True Rumor. (iii) SPR as a
criterion can have a positive impact on distinguishing False Rumors and True
Rumors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiSubs: A Large-scale Multimodal and Multilingual Dataset. (arXiv:2103.01910v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01910">
<div class="article-summary-box-inner">
<span><p>This paper introduces a large-scale multimodal and multilingual dataset that
aims to facilitate research on grounding words to images in their contextual
usage in language. The dataset consists of images selected to unambiguously
illustrate concepts expressed in sentences from movie subtitles. The dataset is
a valuable resource as (i) the images are aligned to text fragments rather than
whole sentences; (ii) multiple images are possible for a text fragment and a
sentence; (iii) the sentences are free-form and real-world like; (iv) the
parallel texts are multilingual. We set up a fill-in-the-blank game for humans
to evaluate the quality of the automatic image selection process of our
dataset. We show the utility of the dataset on two automatic tasks: (i)
fill-in-the-blank; (ii) lexical translation. Results of the human evaluation
and automatic models demonstrate that images can be a useful complement to the
textual context. The dataset will benefit research on visual grounding of words
especially in the context of free-form sentences, and can be obtained from
https://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GrASP: A Library for Extracting and Exploring Human-Interpretable Textual Patterns. (arXiv:2104.03958v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03958">
<div class="article-summary-box-inner">
<span><p>Data exploration is an important step of every data science and machine
learning project, including those involving textual data. We provide a novel
language tool, in the form of a publicly available Python library for
extracting patterns from textual data. The library integrates a first public
implementation of the existing GrASP algorithm. It allows users to extract
patterns using a number of general-purpose built-in linguistic attributes (such
as hypernyms, part-of-speech tags, and syntactic dependency tags), as envisaged
for the original algorithm, as well as domain-specific custom attributes which
can be incorporated into the library by implementing two functions. The library
is equipped with a web-based interface empowering human users to conveniently
explore data via the extracted patterns, using complementary pattern-centric
and example-centric views: the former includes a reading in natural language
and statistics of each extracted pattern; the latter shows applications of each
extracted pattern to training examples. We demonstrate the usefulness of the
library in classification (spam detection and argument mining), model analysis
(machine translation), and artifact discovery in datasets (SNLI and
20Newsgroups).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsolved Problems in ML Safety. (arXiv:2109.13916v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13916">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards ("Robustness"),
identifying hazards ("Monitoring"), reducing inherent model hazards
("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout,
we clarify each problem's motivation and provide concrete research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR. (arXiv:2110.04484v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04484">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training could effectively improve the performance of
low-resource automatic speech recognition (ASR). However, existing
self-supervised pre-training are task-agnostic, i.e., could be applied to
various downstream tasks. Although it enlarges the scope of its application,
the capacity of the pre-trained model is not fully utilized for the ASR task,
and the learned representations may not be optimal for ASR. In this work, in
order to build a better pre-trained model for low-resource ASR, we propose a
pre-training approach called wav2vec-S, where we use task-specific
semi-supervised pre-training to refine the self-supervised pre-trained model
for the ASR task thus more effectively utilize the capacity of the pre-trained
model to generate task-specific representations for ASR. Experiments show that
compared to wav2vec 2.0, wav2vec-S only requires a marginal increment of
pre-training time but could significantly improve ASR performance on in-domain,
cross-domain and cross-lingual datasets. Average relative WER reductions are
24.5% and 6.6% for 1h and 10h fine-tuning, respectively. Furthermore, we show
that semi-supervised pre-training could close the representation gap between
the self-supervised pre-trained model and the corresponding fine-tuned model
through canonical correlation analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Descriptive Patterns and Their Application to Characterizing Classification Errors. (arXiv:2110.09599v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09599">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep learning methods achieve human-like performance on many
tasks, but make errors nevertheless. Characterizing these errors in easily
interpretable terms gives insight into whether a classifier is prone to making
systematic errors, but also gives a way to act and improve the classifier. We
propose to discover those feature-value combinations (i.e., patterns) that
strongly correlate with correct resp. erroneous predictions to obtain a global
and interpretable description for arbitrary classifiers. We show this is an
instance of the more general label description problem, which we formulate in
terms of the Minimum Description Length principle. To discover a good pattern
set, we develop the efficient Premise algorithm. Through an extensive set of
experiments we show it performs very well in practice on both synthetic and
real-world data. Unlike existing solutions, it ably recovers ground truth
patterns, even on highly imbalanced data over many features. Through two case
studies on Visual Question Answering and Named Entity Recognition, we confirm
that Premise gives clear and actionable insight into the systematic errors made
by modern NLP classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. (arXiv:2110.13900v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13900">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) achieves great success in speech recognition,
while limited exploration has been attempted for other speech processing tasks.
As speech signal contains multi-faceted information including speaker identity,
paralinguistics, spoken content, etc., learning universal representations for
all speech tasks is challenging. To tackle the problem, we propose a new
pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM
jointly learns masked speech prediction and denoising in pre-training. By this
means, WavLM does not only keep the speech content modeling capability by the
masked speech prediction, but also improves the potential to non-ASR tasks by
the speech denoising. In addition, WavLM employs gated relative position bias
for the Transformer structure to better capture the sequence ordering of input
speech. We also scale up the training dataset from 60k hours to 94k hours.
WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and
brings significant improvements for various speech processing tasks on their
representative benchmarks. The code and pre-trained models are available at
https://aka.ms/wavlm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decision Attentive Regularization to Improve Simultaneous Speech Translation Systems. (arXiv:2110.15729v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15729">
<div class="article-summary-box-inner">
<span><p>Simultaneous translation systems start producing the output while processing
the partial source sentence in the incoming input stream. These systems need to
decide when to read more input and when to write the output. These decisions
depend on the structure of source/target language and the information contained
in the partial input sequence. Hence, read/write decision policy remains the
same across different input modalities, i.e., speech and text. This motivates
us to leverage the text transcripts corresponding to the speech input for
improving simultaneous speech-to-text translation (SimulST). We propose
Decision Attentive Regularization (DAR) to improve the decision policy of
SimulST systems by using the simultaneous text-to-text translation (SimulMT)
task. We also extend several techniques from the offline speech translation
domain to explore the role of SimulMT task in improving SimulST performance.
Overall, we achieve 34.66% / 4.5 BLEU improvement over the baseline model
across different latency regimes for the MuST-C English-German (EnDe) SimulST
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Normalized Importance Sampling for Neural Language Modeling. (arXiv:2111.06310v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06310">
<div class="article-summary-box-inner">
<span><p>To mitigate the problem of having to traverse over the full vocabulary in the
softmax normalization of a neural language model, sampling-based training
criteria are proposed and investigated in the context of large vocabulary
word-based neural language models. These training criteria typically enjoy the
benefit of faster training and testing, at a cost of slightly degraded
performance in terms of perplexity and almost no visible drop in word error
rate. While noise contrastive estimation is one of the most popular choices,
recently we show that other sampling-based criteria can also perform well, as
long as an extra correction step is done, where the intended class posterior
probability is recovered from the raw model outputs. In this work, we propose
self-normalized importance sampling. Compared to our previous work, the
criteria considered in this work are self-normalized and there is no need to
further conduct a correction step. Through self-normalized language model
training as well as lattice rescoring experiments, we show that our proposed
self-normalized importance sampling is competitive in both research-oriented
and production-oriented automatic speech recognition tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Neural Machine Translation with Dependency-Scaled Self-Attention Network. (arXiv:2111.11707v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11707">
<div class="article-summary-box-inner">
<span><p>Syntax knowledge contributes its powerful strength in Neural machine
translation (NMT) tasks. The early NMT model supposed that syntax details can
be automatically learned from numerous texts via attention networks. However,
succeeding researches pointed out that limited by the uncontrolled nature of
attention computation, the model requires an external syntax to capture the
deep syntactic awareness. Although recent syntax-aware NMT methods have bored
great fruits in combining syntax, the additional workloads they introduced
render the model heavy and slow. Particularly, these efforts scarcely involve
the Transformer-based NMT and modify its core self-attention network (SAN). To
this end, we propose a parameter-free, dependency-scaled self-attention network
(Deps-SAN) for syntax-aware Transformer-based NMT. It integrates a quantified
matrix of syntactic dependencies to impose explicit syntactic constraints into
the SAN to learn syntactic details and dispel the dispersion of attention
distributions. Two knowledge sparsing techniques are further proposed to avoid
the model overfitting the dependency noises. Extensive experiments and analyses
on the two benchmark NMT tasks verify the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03857">
<div class="article-summary-box-inner">
<span><p>This paper presents a grounded language-image pre-training (GLIP) model for
learning object-level, language-aware, and semantic-rich visual
representations. GLIP unifies object detection and phrase grounding for
pre-training. The unification brings two benefits: 1) it allows GLIP to learn
from both detection and grounding data to improve both tasks and bootstrap a
good grounding model; 2) GLIP can leverage massive image-text pairs by
generating grounding boxes in a self-training fashion, making the learned
representation semantic-rich. In our experiments, we pre-train GLIP on 27M
grounding data, including 3M human-annotated and 24M web-crawled image-text
pairs. The learned representations demonstrate strong zero-shot and few-shot
transferability to various object-level recognition tasks. 1) When directly
evaluated on COCO and LVIS (without seeing any images in COCO during
pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many
supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val
and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13
downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised
Dynamic Head. Code is released at https://github.com/microsoft/GLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Interpretation of Saliency-based Explanation Over Text. (arXiv:2201.11569v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11569">
<div class="article-summary-box-inner">
<span><p>While a lot of research in explainable AI focuses on producing effective
explanations, less work is devoted to the question of how people understand and
interpret the explanation. In this work, we focus on this question through a
study of saliency-based explanations over textual data. Feature-attribution
explanations of text models aim to communicate which parts of the input text
were more influential than others towards the model decision. Many current
explanation methods, such as gradient-based or Shapley value-based methods,
provide measures of importance which are well-understood mathematically. But
how does a person receiving the explanation (the explainee) comprehend it? And
does their understanding match what the explanation attempted to communicate?
We empirically investigate the effect of various factors of the input, the
feature-attribution explanation, and visualization procedure, on laypeople's
interpretation of the explanation. We query crowdworkers for their
interpretation on tasks in English and German, and fit a GAMM model to their
responses considering the factors of interest. We find that people often
mis-interpret the explanations: superficial and unrelated factors, such as word
length, influence the explainees' importance assignment despite the explanation
communicating importance directly. We then show that some of this distortion
can be attenuated: we propose a method to adjust saliencies based on model
estimates of over- and under-perception, and explore bar charts as an
alternative to heatmap saliency visualization. We find that both approaches can
attenuate the distorting effect of specific factors, leading to
better-calibrated understanding of the explanation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROCK: Causal Inference Principles for Reasoning about Commonsense Causality. (arXiv:2202.00436v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00436">
<div class="article-summary-box-inner">
<span><p>Commonsense causality reasoning (CCR) aims at identifying plausible causes
and effects in natural language descriptions that are deemed reasonable by an
average person. Although being of great academic and practical interest, this
problem is still shadowed by the lack of a well-posed theoretical framework;
existing work usually relies on deep language models wholeheartedly, and is
potentially susceptible to confounding co-occurrences. Motivated by classical
causal principles, we articulate the central question of CCR and draw parallels
between human subjects in observational studies and natural languages to adopt
CCR to the potential-outcomes framework, which is the first such attempt for
commonsense tasks. We propose a novel framework, ROCK, to Reason O(A)bout
Commonsense K(C)ausality, which utilizes temporal signals as incidental
supervision, and balances confounding effects using temporal propensities that
are analogous to propensity scores. The ROCK implementation is modular and
zero-shot, and demonstrates good CCR capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Structure with Undirected Neural Networks. (arXiv:2202.03760v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03760">
<div class="article-summary-box-inner">
<span><p>Neural networks are powerful function estimators, leading to their status as
a paradigm of choice for modeling structured data. However, unlike other
structured representations that emphasize the modularity of the problem --
e.g., factor graphs -- neural networks are usually monolithic mappings from
inputs to outputs, with a fixed computation order. This limitation prevents
them from capturing different directions of computation and interaction between
the modeled variables.
</p>
<p>In this paper, we combine the representational strengths of factor graphs and
of neural networks, proposing undirected neural networks (UNNs): a flexible
framework for specifying computations that can be performed in any order. For
particular choices, our proposed models subsume and extend many existing
architectures: feed-forward, recurrent, self-attention networks, auto-encoders,
and networks with implicit layers. We demonstrate the effectiveness of
undirected neural architectures, both unstructured and structured, on a range
of tasks: tree-constrained dependency parsing, convolutional image
classification, and sequence completion with attention. By varying the
computation order, we show how a single UNN can be used both as a classifier
and a prototype generator, and how it can fill in missing parts of an input
sequence, making them a promising field for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages. (arXiv:2204.08582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08582">
<div class="article-summary-box-inner">
<span><p>We present the MASSIVE dataset--Multilingual Amazon Slu resource package
(SLURP) for Slot-filling, Intent classification, and Virtual assistant
Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant
utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE
was created by tasking professional translators to localize the English-only
SLURP dataset into 50 typologically diverse languages from 29 genera. We also
present modeling results on XLM-R and mT5, including exact match accuracy,
intent classification accuracy, and slot-filling F1 score. We have released our
dataset, modeling code, and models publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoSe-Co: Text Conditioned Generative CommonSense Contextualizer. (arXiv:2206.05706v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05706">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (PTLMs) have been shown to perform well on
natural language tasks. Many prior works have leveraged structured commonsense
present in the form of entities linked through labeled relations in Knowledge
Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static
module which limits coverage since KGs contain finite knowledge. Generative
methods train PTLMs on KG triples to improve the scale at which knowledge can
be obtained. However, training on symbolic KG entities limits their
applicability in tasks involving natural language text where they ignore
overall context. To mitigate this, we propose a CommonSense Contextualizer
(CoSe-Co) conditioned on sentences as input to make it generically usable in
tasks for generating knowledge relevant to the overall context of input text.
To train CoSe-Co, we propose a novel dataset comprising of sentence and
commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and
contain novel entities not present in the underlying KG. We augment generated
knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading
to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets.
We also demonstrate its applicability in improving performance of a baseline
model for paraphrase generation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonwords Pronunciation Classification in Language Development Tests for Preschool Children. (arXiv:2206.08058v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08058">
<div class="article-summary-box-inner">
<span><p>This work aims to automatically evaluate whether the language development of
children is age-appropriate. Validated speech and language tests are used for
this purpose to test the auditory memory. In this work, the task is to
determine whether spoken nonwords have been uttered correctly. We compare
different approaches that are motivated to model specific language structures:
Low-level features (FFT), speaker embeddings (ECAPA-TDNN), grapheme-motivated
embeddings (wav2vec 2.0), and phonetic embeddings in form of senones (ASR
acoustic model). Each of the approaches provides input for VGG-like 5-layer CNN
classifiers. We also examine the adaptation per nonword. The evaluation of the
proposed systems was performed using recordings from different kindergartens of
spoken nonwords. ECAPA-TDNN and low-level FFT features do not explicitly model
phonetic information; wav2vec2.0 is trained on grapheme labels, our ASR
acoustic model features contain (sub-)phonetic information. We found that the
more granular the phonetic modeling is, the higher are the achieved recognition
rates. The best system trained on ASR acoustic model features with VTLN
achieved an accuracy of 89.4% and an area under the ROC (Receiver Operating
Characteristic) curve (AUC) of 0.923. This corresponds to an improvement in
accuracy of 20.2% and AUC of 0.309 relative compared to the FFT-baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All the World's a (Hyper)Graph: A Data Drama. (arXiv:2206.08225v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08225">
<div class="article-summary-box-inner">
<span><p>We introduce Hyperbard, a dataset of diverse relational data representations
derived from Shakespeare's plays. Our representations range from simple graphs
capturing character co-occurrence in single scenes to hypergraphs encoding
complex communication settings and character contributions as hyperedges with
edge-specific node weights. By making multiple intuitive representations
readily available for experimentation, we facilitate rigorous representation
robustness checks in graph learning, graph mining, and network analysis,
highlighting the advantages and drawbacks of specific representations.
Leveraging the data released in Hyperbard, we demonstrate that many solutions
to popular graph mining problems are highly dependent on the representation
choice, thus calling current graph curation practices into question. As an
homage to our data source, and asserting that science can also be art, we
present all our points in the form of a play.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature Extraction from Downstream Tasks. (arXiv:2206.08398v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08398">
<div class="article-summary-box-inner">
<span><p>Contemporary artificial neural networks (ANN) are trained end-to-end, jointly
learning both features and classifiers for the task of interest. Though
enormously effective, this paradigm imposes significant costs in assembling
annotated task-specific datasets and training large-scale networks. We propose
to decouple feature learning from downstream lung ultrasound tasks by
introducing an auxiliary pre-task of visual biomarker classification. We
demonstrate that one can learn an informative, concise, and interpretable
feature space from ultrasound videos by training models for predicting
biomarker labels. Notably, biomarker feature extractors can be trained from
data annotated with weak video-scale supervision. These features can be used by
a variety of downstream Expert models targeted for diverse clinical tasks
(Diagnosis, lung severity, S/F ratio). Crucially, task-specific expert models
are comparable in accuracy to end-to-end models directly trained for such
target tasks, while being significantly lower cost to train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Going Deeper than Tracking: a Survey of Computer-Vision Based Recognition of Animal Pain and Affective States. (arXiv:2206.08405v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08405">
<div class="article-summary-box-inner">
<span><p>Advances in animal motion tracking and pose recognition have been a game
changer in the study of animal behavior. Recently, an increasing number of
works go 'deeper' than tracking, and address automated recognition of animals'
internal states such as emotions and pain with the aim of improving animal
welfare, making this a timely moment for a systematization of the field. This
paper provides a comprehensive survey of computer vision-based research on
recognition of affective states and pain in animals, addressing both facial and
bodily behavior analysis. We summarize the efforts that have been presented so
far within this topic -- classifying them across different dimensions,
highlight challenges and research gaps, and provide best practice
recommendations for advancing the field, and some future directions for
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time motion amplification on mobile devices. (arXiv:2206.08422v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08422">
<div class="article-summary-box-inner">
<span><p>A simple motion amplification algorithm suitable for real-time applications
on mobile devices is presented. It is based on motion enhancement by moving
average differencing (MEMAD), a temporal high-pass filter for video streams.
MEMAD can amplify small moving objects or subtle motion in larger objects. It
is computationally sufficiently simple to be implemented in real time on
smartphones. In the specific implementation as an Android phone app, MEMAD is
demonstrated on examples chosen such as to motivate applications in the
engineering, biological, and medical sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes. (arXiv:2206.08423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08423">
<div class="article-summary-box-inner">
<span><p>Indoor scenes exhibit significant appearance variations due to myriad
interactions between arbitrarily diverse object shapes, spatially-changing
materials, and complex lighting. Shadows, highlights, and inter-reflections
caused by visible and invisible light sources require reasoning about
long-range interactions for inverse rendering, which seeks to recover the
components of image formation, namely, shape, material, and lighting. In this
work, our intuition is that the long-range attention learned by transformer
architectures is ideally suited to solve longstanding challenges in
single-image inverse rendering. We demonstrate with a specific instantiation of
a dense vision transformer, IRISformer, that excels at both single-task and
multi-task reasoning required for inverse rendering. Specifically, we propose a
transformer architecture to simultaneously estimate depths, normals,
spatially-varying albedo, roughness and lighting from a single image of an
indoor scene. Our extensive evaluations on benchmark datasets demonstrate
state-of-the-art results on each of the above tasks, enabling applications like
object insertion and material editing in a single unconstrained real image,
with greater photorealism than prior works. Code and data are publicly released
at https://github.com/ViLab-UCSD/IRISformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SATBench: Benchmarking the speed-accuracy tradeoff in object recognition by humans and dynamic neural networks. (arXiv:2206.08427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08427">
<div class="article-summary-box-inner">
<span><p>The core of everyday tasks like reading and driving is active object
recognition. Attempts to model such tasks are currently stymied by the
inability to incorporate time. People show a flexible tradeoff between speed
and accuracy and this tradeoff is a crucial human skill. Deep neural networks
have emerged as promising candidates for predicting peak human object
recognition performance and neural activity. However, modeling the temporal
dimension i.e., the speed-accuracy tradeoff (SAT), is essential for them to
serve as useful computational models for how humans recognize objects. To this
end, we here present the first large-scale (148 observers, 4 neural networks, 8
tasks) dataset of the speed-accuracy tradeoff (SAT) in recognizing ImageNet
images. In each human trial, a beep, indicating the desired reaction time,
sounds at a fixed delay after the image is presented, and observer's response
counts only if it occurs near the time of the beep. In a series of blocks, we
test many beep latencies, i.e., reaction times. We observe that human accuracy
increases with reaction time and proceed to compare its characteristics with
the behavior of several dynamic neural networks that are capable of
inference-time adaptive computation. Using FLOPs as an analog for reaction
time, we compare networks with humans on curve-fit error, category-wise
correlation, and curve steepness, and conclude that cascaded dynamic neural
networks are a promising model of human reaction time in object recognition
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation and Relighting of Human Eyes. (arXiv:2206.08428v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08428">
<div class="article-summary-box-inner">
<span><p>A unique challenge in creating high-quality animatable and relightable 3D
avatars of people is modeling human eyes. The challenge of synthesizing eyes is
multifold as it requires 1) appropriate representations for the various
components of the eye and the periocular region for coherent viewpoint
synthesis, capable of representing diffuse, refractive and highly reflective
surfaces, 2) disentangling skin and eye appearance from environmental
illumination such that it may be rendered under novel lighting conditions, and
3) capturing eyeball motion and the deformation of the surrounding skin to
enable re-gazing. These challenges have traditionally necessitated the use of
expensive and cumbersome capture setups to obtain high-quality results, and
even then, modeling of the eye region holistically has remained elusive. We
present a novel geometry and appearance representation that enables
high-fidelity capture and photorealistic animation, view synthesis and
relighting of the eye region using only a sparse set of lights and cameras. Our
hybrid representation combines an explicit parametric surface model for the
eyeball with implicit deformable volumetric representations for the periocular
region and the interior of the eye. This novel hybrid model has been designed
to address the various parts of that challenging facial area - the explicit
eyeball surface allows modeling refraction and high-frequency specular
reflection at the cornea, whereas the implicit representation is well suited to
model lower-frequency skin reflection via spherical harmonics and can represent
non-surface structures such as hair or diffuse volumetric bodies, both of which
are a challenge for explicit surface models. We show that for high-resolution
close-ups of the eye, our model can synthesize high-fidelity animated gaze from
novel views under unseen illumination conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Temporal Localization of Sensitive Activities in Movies and TV Episodes. (arXiv:2206.08429v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08429">
<div class="article-summary-box-inner">
<span><p>To help customers make better-informed viewing choices, video-streaming
services try to moderate their content and provide more visibility into which
portions of their movies and TV episodes contain age-appropriate material
(e.g., nudity, sex, violence, or drug-use). Supervised models to localize these
sensitive activities require large amounts of clip-level labeled data which is
hard to obtain, while weakly-supervised models to this end usually do not offer
competitive accuracy. To address this challenge, we propose a novel Coarse2Fine
network designed to make use of readily obtainable video-level weak labels in
conjunction with sparse clip-level labels of age-appropriate activities. Our
model aggregates frame-level predictions to make video-level classifications
and is therefore able to leverage sparse clip-level labels along with
video-level labels. Furthermore, by performing frame-level predictions in a
hierarchical manner, our approach is able to overcome the label-imbalance
problem caused due to the rare-occurrence nature of age-appropriate content. We
present comparative results of our approach using 41,234 movies and TV episodes
(~3 years of video-content) from 521 sub-genres and 250 countries making it by
far the largest-scale empirical analysis of age-appropriate activity
localization in long-form videos ever published. Our approach offers 107.2%
relative mAP improvement (from 5.5% to 11.4%) over existing state-of-the-art
activity-localization approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenSRH: optimizing brain tumor surgery using intraoperative stimulated Raman histology. (arXiv:2206.08439v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08439">
<div class="article-summary-box-inner">
<span><p>Accurate intraoperative diagnosis is essential for providing safe and
effective care during brain tumor surgery. Our standard-of-care diagnostic
methods are time, resource, and labor intensive, which restricts access to
optimal surgical treatments. To address these limitations, we propose an
alternative workflow that combines stimulated Raman histology (SRH), a rapid
optical imaging method, with deep learning-based automated interpretation of
SRH images for intraoperative brain tumor diagnosis and real-time surgical
decision support. Here, we present OpenSRH, the first public dataset of
clinical SRH images from 300+ brain tumors patients and 1300+ unique whole
slide optical images. OpenSRH contains data from the most common brain tumors
diagnoses, full pathologic annotations, whole slide tumor segmentations, raw
and processed optical imaging data for end-to-end model development and
validation. We provide a framework for patch-based whole slide SRH
classification and inference using weak (i.e. patient-level) diagnostic labels.
Finally, we benchmark two computer vision tasks: multiclass histologic brain
tumor classification and patch-based contrastive representation learning. We
hope OpenSRH will facilitate the clinical translation of rapid optical imaging
and real-time ML-based surgical decision support in order to improve the
access, safety, and efficacy of cancer surgery in the era of precision
medicine. Dataset access, code, and benchmarks are available at
opensrh.mlins.org.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TUSK: Task-Agnostic Unsupervised Keypoints. (arXiv:2206.08460v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08460">
<div class="article-summary-box-inner">
<span><p>Existing unsupervised methods for keypoint learning rely heavily on the
assumption that a specific keypoint type (e.g. elbow, digit, abstract geometric
shape) appears only once in an image. This greatly limits their applicability,
as each instance must be isolated before applying the method-an issue that is
never discussed or evaluated. We thus propose a novel method to learn
Task-agnostic, UnSupervised Keypoints (TUSK) which can deal with multiple
instances. To achieve this, instead of the commonly-used strategy of detecting
multiple heatmaps, each dedicated to a specific keypoint type, we use a single
heatmap for detection, and enable unsupervised learning of keypoint types
through clustering. Specifically, we encode semantics into the keypoints by
teaching them to reconstruct images from a sparse set of keypoints and their
descriptors, where the descriptors are forced to form distinct clusters in
feature space around learned prototypes. This makes our approach amenable to a
wider range of tasks than any previous unsupervised keypoint method: we show
experiments on multiple-instance detection and classification, object
discovery, and landmark detection-all unsupervised-with performance on par with
the state of the art, while also being able to deal with multiple instances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursive Neural Programs: Variational Learning of Image Grammars and Part-Whole Hierarchies. (arXiv:2206.08462v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08462">
<div class="article-summary-box-inner">
<span><p>Human vision involves parsing and representing objects and scenes using
structured representations based on part-whole hierarchies. Computer vision and
machine learning researchers have recently sought to emulate this capability
using capsule networks, reference frames and active predictive coding, but a
generative model formulation has been lacking. We introduce Recursive Neural
Programs (RNPs), which, to our knowledge, is the first neural generative model
to address the part-whole hierarchy learning problem. RNPs model images as
hierarchical trees of probabilistic sensory-motor programs that recursively
reuse learned sensory-motor primitives to model an image within different
reference frames, forming recursive image grammars. We express RNPs as
structured variational autoencoders (sVAEs) for inference and sampling, and
demonstrate parts-based parsing, sampling and one-shot transfer learning for
MNIST, Omniglot and Fashion-MNIST datasets, demonstrating the model's
expressive power. Our results show that RNPs provide an intuitive and
explainable way of composing objects and scenes, allowing rich compositionality
and intuitive interpretations of objects in terms of part-whole hierarchies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot AutoML with Pretrained Models. (arXiv:2206.08476v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08476">
<div class="article-summary-box-inner">
<span><p>Given a new dataset D and a low compute budget, how should we choose a
pre-trained model to fine-tune to D, and set the fine-tuning hyperparameters
without risking overfitting, particularly if D is small? Here, we extend
automated machine learning (AutoML) to best make these choices. Our
domain-independent meta-learning approach learns a zero-shot surrogate model
which, at test time, allows to select the right deep learning (DL) pipeline
(including the pre-trained model and fine-tuning hyperparameters) for a new
dataset D given only trivial meta-features describing D such as image
resolution or the number of classes. To train this zero-shot model, we collect
performance data for many DL pipelines on a large collection of datasets and
meta-train on this data to minimize a pairwise ranking objective. We evaluate
our approach under the strict time limit of the vision track of the ChaLearn
AutoDL challenge benchmark, clearly outperforming all challenge contenders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks on Vision Transformers. (arXiv:2206.08477v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08477">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have recently demonstrated exemplary performance on
a variety of vision tasks and are being used as an alternative to CNNs. Their
design is based on a self-attention mechanism that processes images as a
sequence of patches, which is quite different compared to CNNs. Hence it is
interesting to study if ViTs are vulnerable to backdoor attacks. Backdoor
attacks happen when an attacker poisons a small part of the training data for
malicious purposes. The model performance is good on clean test images, but the
attacker can manipulate the decision of the model by showing the trigger at
test time. To the best of our knowledge, we are the first to show that ViTs are
vulnerable to backdoor attacks. We also find an intriguing difference between
ViTs and CNNs - interpretation algorithms effectively highlight the trigger on
test images for ViTs but not for CNNs. Based on this observation, we propose a
test-time image blocking defense for ViTs which reduces the attack success rate
by a large margin. Code is available here:
https://github.com/UCDvision/backdoor_transformer.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orientation-guided Graph Convolutional Network for Bone Surface Segmentation. (arXiv:2206.08481v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08481">
<div class="article-summary-box-inner">
<span><p>Due to imaging artifacts and low signal-to-noise ratio in ultrasound images,
automatic bone surface segmentation networks often produce fragmented
predictions that can hinder the success of ultrasound-guided computer-assisted
surgical procedures. Existing pixel-wise predictions often fail to capture the
accurate topology of bone tissues due to a lack of supervision to enforce
connectivity. In this work, we propose an orientation-guided graph
convolutional network to improve connectivity while segmenting the bone
surface. We also propose an additional supervision on the orientation of the
bone surface to further impose connectivity. We validated our approach on 1042
vivo US scans of femur, knee, spine, and distal radius. Our approach improves
over the state-of-the-art methods by 5.01% in connectivity metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Image Enhancement. (arXiv:2206.08488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08488">
<div class="article-summary-box-inner">
<span><p>Editing flat-looking images into stunning photographs requires skill and
time. Automated image enhancement algorithms have attracted increased interest
by generating high-quality images without user interaction. However, the
quality assessment of a photograph is subjective. Even in tone and color
adjustments, a single photograph of auto-enhancement is challenging to fit user
preferences which are subtle and even changeable. To address this problem, we
present a semiautomatic image enhancement algorithm that can generate
high-quality images with multiple styles by controlling a few parameters. We
first disentangle photo retouching skills from high-quality images and build an
efficient enhancement system for each skill. Specifically, an encoder-decoder
framework encodes the retouching skills into latent codes and decodes them into
the parameters of image signal processing (ISP) functions. The ISP functions
are computationally efficient and consist of only 19 parameters. Despite our
approach requiring multiple inferences to obtain the desired result,
experimental results present that the proposed method achieves state-of-the-art
performances on the benchmark dataset for image quality and model efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Kinematic Motion Detection for Part-segmented 3D Shape Collections. (arXiv:2206.08497v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08497">
<div class="article-summary-box-inner">
<span><p>3D models of manufactured objects are important for populating virtual worlds
and for synthetic data generation for vision and robotics. To be most useful,
such objects should be articulated: their parts should move when interacted
with. While articulated object datasets exist, creating them is
labor-intensive. Learning-based prediction of part motions can help, but all
existing methods require annotated training data. In this paper, we present an
unsupervised approach for discovering articulated motions in a part-segmented
3D shape collection. Our approach is based on a concept we call category
closure: any valid articulation of an object's parts should keep the object in
the same semantic category (e.g. a chair stays a chair). We operationalize this
concept with an algorithm that optimizes a shape's part motion parameters such
that it can transform into other shapes in the collection. We evaluate our
approach by using it to re-discover part motions from the PartNet-Mobility
dataset. For almost all shape categories, our method's predicted motion
parameters have low error with respect to ground truth annotations,
outperforming two supervised motion prediction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do navigation agents learn about their environment?. (arXiv:2206.08500v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08500">
<div class="article-summary-box-inner">
<span><p>Today's state of the art visual navigation agents typically consist of large
deep learning models trained end to end. Such models offer little to no
interpretability about the learned skills or the actions of the agent taken in
response to its environment. While past works have explored interpreting deep
learning models, little attention has been devoted to interpreting embodied AI
systems, which often involve reasoning about the structure of the environment,
target characteristics and the outcome of one's actions. In this paper, we
introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal
and Object Goal navigation agents. We use iSEE to probe the dynamic
representations produced by these agents for the presence of information about
the agent as well as the environment. We demonstrate interesting insights about
navigation agents using iSEE, including the ability to encode reachable
locations (to avoid obstacles), visibility of the target, progress from the
initial spawn location as well as the dramatic effect on the behaviors of
agents when we mask out critical individual neurons. The code is available at:
https://github.com/allenai/iSEE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architecture Adaptation for Object Detection by Searching Channel Dimensions and Mapping Pre-trained Parameters. (arXiv:2206.08509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08509">
<div class="article-summary-box-inner">
<span><p>Most object detection frameworks use backbone architectures originally
designed for image classification, conventionally with pre-trained parameters
on ImageNet. However, image classification and object detection are essentially
different tasks and there is no guarantee that the optimal backbone for
classification is also optimal for object detection. Recent neural architecture
search (NAS) research has demonstrated that automatically designing a backbone
specifically for object detection helps improve the overall accuracy. In this
paper, we introduce a neural architecture adaptation method that can optimize
the given backbone for detection purposes, while still allowing the use of
pre-trained parameters. We propose to adapt both the micro- and
macro-architecture by searching for specific operations and the number of
layers, in addition to the output channel dimensions of each block. It is
important to find the optimal channel depth, as it greatly affects the feature
representation capability and computation cost. We conduct experiments with our
searched backbone for object detection and demonstrate that our backbone
outperforms both manually designed and searched state-of-the-art backbones on
the COCO dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Solid State LiDAR Odometry Using Continuous-time Filter Registration. (arXiv:2206.08517v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08517">
<div class="article-summary-box-inner">
<span><p>Solid-state LiDARs are more compact and cheaper than the conventional
mechanical multi-line spinning LiDARs, which have become increasingly popular
in autonomous driving recently. However, there are several challenges for these
new LiDAR sensors, including severe motion distortions, small field of view and
sparse point cloud, which hinder them from being widely used in LiDAR odometry.
To tackle these problems, we present an effective continuous-time LiDAR
odometry (ECTLO) method for the Risley prism-based LiDARs with non-repetitive
scanning patterns. To account for the noisy data, a filter-based point-to-plane
Gaussian Mixture Model is used for robust registration. Moreover, a LiDAR-only
continuous-time motion model is employed to relieve the inevitable distortions.
To facilitate the implicit data association in parallel, we maintain all map
points within a single range image. Extensive experiments have been conducted
on various testbeds using the solid-state LiDARs with different scanning
patterns, whose promising results demonstrate the efficacy of our proposed
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation. (arXiv:2206.08522v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08522">
<div class="article-summary-box-inner">
<span><p>Benefiting from language flexibility and compositionality, humans naturally
intend to use language to command an embodied agent for complex tasks such as
navigation and object manipulation. In this work, we aim to fill the blank of
the last mile of embodied agents -- object manipulation by following human
guidance, e.g., "move the red mug next to the box while keeping it upright." To
this end, we introduce an Automatic Manipulation Solver (AMSolver) simulator
and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,
containing various language instructions on categorized robotic manipulation
tasks. Specifically, modular rule-based task templates are created to
automatically generate robot demonstrations with language instructions,
consisting of diverse object shapes and appearances, action types, and motion
constraints. We also develop a keypoint-based model 6D-CLIPort to deal with
multi-view observations and language input and output a sequence of 6 degrees
of freedom (DoF) actions. We hope the new simulator and benchmark will
facilitate future research on language-guided robotic manipulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDNet: Contrastive Disentangled Network for Fine-Grained Image Categorization of Ocular B-Scan Ultrasound. (arXiv:2206.08524v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08524">
<div class="article-summary-box-inner">
<span><p>Precise and rapid categorization of images in the B-scan ultrasound modality
is vital for diagnosing ocular diseases. Nevertheless, distinguishing various
diseases in ultrasound still challenges experienced ophthalmologists. Thus a
novel contrastive disentangled network (CDNet) is developed in this work,
aiming to tackle the fine-grained image categorization (FGIC) challenges of
ocular abnormalities in ultrasound images, including intraocular tumor (IOT),
retinal detachment (RD), posterior scleral staphyloma (PSS), and vitreous
hemorrhage (VH). Three essential components of CDNet are the weakly-supervised
lesion localization module (WSLL), contrastive multi-zoom (CMZ) strategy, and
hyperspherical contrastive disentangled loss (HCD-Loss), respectively. These
components facilitate feature disentanglement for fine-grained recognition in
both the input and output aspects. The proposed CDNet is validated on our ZJU
Ocular Ultrasound Dataset (ZJUOUSD), consisting of 5213 samples. Furthermore,
the generalization ability of CDNet is validated on two public and widely-used
chest X-ray FGIC benchmarks. Quantitative and qualitative results demonstrate
the efficacy of our proposed CDNet, which achieves state-of-the-art performance
in the FGIC task. Code is available at:
https://github.com/ZeroOneGame/CDNet-for-OUS-FGIC .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Margin Representation Learning for Texture Classification. (arXiv:2206.08537v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08537">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel approach combining convolutional layers (CLs) and
large-margin metric learning for training supervised models on small datasets
for texture classification. The core of such an approach is a loss function
that computes the distances between instances of interest and support vectors.
The objective is to update the weights of CLs iteratively to learn a
representation with a large margin between classes. Each iteration results in a
large-margin discriminant model represented by support vectors based on such a
representation. The advantage of the proposed approach w.r.t. convolutional
neural networks (CNNs) is two-fold. First, it allows representation learning
with a small amount of data due to the reduced number of parameters compared to
an equivalent CNN. Second, it has a low training cost since the backpropagation
considers only support vectors. The experimental results on texture and
histopathologic image datasets have shown that the proposed approach achieves
competitive accuracy with lower computational cost and faster convergence when
compared to equivalent CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Classification of Brain Tumor Images Using Transfer Learning Based Deep Neural Network. (arXiv:2206.08543v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08543">
<div class="article-summary-box-inner">
<span><p>In recent advancement towards computer based diagnostics system, the
classification of brain tumor images is a challenging task. This paper mainly
focuses on elevating the classification accuracy of brain tumor images with
transfer learning based deep neural network. The classification approach is
started with the image augmentation operation including rotation, zoom,
hori-zontal flip, width shift, height shift, and shear to increase the
diversity in image datasets. Then the general features of the input brain tumor
images are extracted based on a pre-trained transfer learning method comprised
of Inception-v3. Fi-nally, the deep neural network with 4 customized layers is
employed for classi-fying the brain tumors in most frequent brain tumor types
as meningioma, glioma, and pituitary. The proposed model acquires an effective
performance with an overall accuracy of 96.25% which is much improved than some
existing multi-classification methods. Whereas, the fine-tuning of
hyper-parameters and inclusion of customized DNN with the Inception-v3 model
results in an im-provement of the classification accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Texture Generation Using Graph Generative Adversarial Network And Differentiable Rendering. (arXiv:2206.08547v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08547">
<div class="article-summary-box-inner">
<span><p>Novel texture synthesis for existing 3D mesh models is an important step
towards photo realistic asset generation for existing simulators. But existing
methods inherently work in the 2D image space which is the projection of the 3D
space from a given camera perspective. These methods take camera angle, 3D
model information, lighting information and generate photorealistic 2D image.
To generate a photorealistic image from another perspective or lighting, we
need to make a computationally expensive forward pass each time we change the
parameters. Also, it is hard to generate such images for a simulator that can
satisfy the temporal constraints the sequences of images should be similar but
only need to change the viewpoint of lighting as desired. The solution can not
be directly integrated with existing tools like Blender and Unreal Engine.
Manual solution is expensive and time consuming. We thus present a new system
called a graph generative adversarial network (GGAN) that can generate textures
which can be directly integrated into a given 3D mesh models with tools like
Blender and Unreal Engine and can be simulated from any perspective and
lighting condition easily.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images. (arXiv:2206.08549v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08549">
<div class="article-summary-box-inner">
<span><p>Evaluation metrics in image synthesis play a key role to measure performances
of generative models. However, most metrics mainly focus on image fidelity.
Existing diversity metrics are derived by comparing distributions, and thus
they cannot quantify the diversity or rarity degree of each generated image. In
this work, we propose a new evaluation metric, called `rarity score', to
measure the individual rarity of each image synthesized by generative models.
We first show empirical observation that common samples are close to each other
and rare samples are far from each other in nearest-neighbor distances of
feature space. We then use our metric to demonstrate that the extent to which
different generative models produce rare images can be effectively compared. We
also propose a method to compare rarities between datasets that share the same
concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in
different designs of feature spaces to better understand the relationship
between feature spaces and resulting sparse images. Code will be publicly
available online for the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Detection using Transfer Learning with Convolutional Neural Network. (arXiv:2206.08557v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08557">
<div class="article-summary-box-inner">
<span><p>The Novel Coronavirus disease 2019 (COVID-19) is a fatal infectious disease,
first recognized in December 2019 in Wuhan, Hubei, China, and has gone on an
epidemic situation. Under these circumstances, it became more important to
detect COVID-19 in infected people. Nowadays, the testing kits are gradually
lessening in number compared to the number of infected population. Under recent
prevailing conditions, the diagnosis of lung disease by analyzing chest CT
(Computed Tomography) images has become an important tool for both diagnosis
and prophecy of COVID-19 patients. In this study, a Transfer learning strategy
(CNN) for detecting COVID-19 infection from CT images has been proposed. In the
proposed model, a multilayer Convolutional neural network (CNN) with Transfer
learning model Inception V3 has been designed. Similar to CNN, it uses
convolution and pooling to extract features, but this transfer learning model
contains weights of dataset Imagenet. Thus it can detect features very
effectively which gives it an upper hand for achieving better accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Data Discovery: Mining Unknown Data using Submodular Information Measures. (arXiv:2206.08566v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08566">
<div class="article-summary-box-inner">
<span><p>Active Learning is a very common yet powerful framework for iteratively and
adaptively sampling subsets of the unlabeled sets with a human in the loop with
the goal of achieving labeling efficiency. Most real world datasets have
imbalance either in classes and slices, and correspondingly, parts of the
dataset are rare. As a result, there has been a lot of work in designing active
learning approaches for mining these rare data instances. Most approaches
assume access to a seed set of instances which contain these rare data
instances. However, in the event of more extreme rareness, it is reasonable to
assume that these rare data instances (either classes or slices) may not even
be present in the seed labeled set, and a critical need for the active learning
paradigm is to efficiently discover these rare data instances. In this work, we
provide an active data discovery framework which can mine unknown data slices
and classes efficiently using the submodular conditional gain and submodular
conditional mutual information functions. We provide a general algorithmic
framework which works in a number of scenarios including image classification
and object detection and works with both rare classes and rare slices present
in the unlabeled set. We show significant accuracy and labeling efficiency
gains with our approach compared to existing state-of-the-art active learning
approaches for actively discovering these rare classes and slices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rectify ViT Shortcut Learning by Visual Saliency. (arXiv:2206.08567v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08567">
<div class="article-summary-box-inner">
<span><p>Shortcut learning is common but harmful to deep learning models, leading to
degenerated feature representations and consequently jeopardizing the model's
generalizability and interpretability. However, shortcut learning in the widely
used Vision Transformer framework is largely unknown. Meanwhile, introducing
domain-specific knowledge is a major approach to rectifying the shortcuts,
which are predominated by background related factors. For example, in the
medical imaging field, eye-gaze data from radiologists is an effective human
visual prior knowledge that has the great potential to guide the deep learning
models to focus on meaningful foreground regions of interest. However,
obtaining eye-gaze data is time-consuming, labor-intensive and sometimes even
not practical. In this work, we propose a novel and effective saliency-guided
vision transformer (SGT) model to rectify shortcut learning in ViT with the
absence of eye-gaze data. Specifically, a computational visual saliency model
is adopted to predict saliency maps for input image samples. Then, the saliency
maps are used to distil the most informative image patches. In the proposed
SGT, the self-attention among image patches focus only on the distilled
informative ones. Considering this distill operation may lead to global
information lost, we further introduce, in the last encoder layer, a residual
connection that captures the self-attention across all the image patches. The
experiment results on four independent public datasets show that our SGT
framework can effectively learn and leverage human prior knowledge without eye
gaze data and achieves much better performance than baselines. Meanwhile, it
successfully rectifies the harmful shortcut learning and significantly improves
the interpretability of the ViT model, demonstrating the promise of
transferring human prior knowledge derived visual saliency in rectifying
shortcut learning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Contextual Predictions with Vision Transformer for Video Anomaly Detection. (arXiv:2206.08568v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08568">
<div class="article-summary-box-inner">
<span><p>Video Anomaly Detection(VAD) has been traditionally tackled in two main
methodologies: the reconstruction-based approach and the prediction-based one.
As the reconstruction-based methods learn to generalize the input image, the
model merely learns an identity function and strongly causes the problem called
generalizing issue. On the other hand, since the prediction-based ones learn to
predict a future frame given several previous frames, they are less sensitive
to the generalizing issue. However, it is still uncertain if the model can
learn the spatio-temporal context of a video. Our intuition is that the
understanding of the spatio-temporal context of a video plays a vital role in
VAD as it provides precise information on how the appearance of an event in a
video clip changes. Hence, to fully exploit the context information for anomaly
detection in video circumstances, we designed the transformer model with three
different contextual prediction streams: masked, whole and partial. By learning
to predict the missing frames of consecutive normal frames, our model can
effectively learn various normality patterns in the video, which leads to a
high reconstruction error at the abnormal cases that are unsuitable to the
learned context. To verify the effectiveness of our approach, we assess our
model on the public benchmark datasets: USCD Pedestrian 2, CUHK Avenue and
ShanghaiTech and evaluate the performance with the anomaly score metric of
reconstruction error. The results demonstrate that our proposed approach
achieves a competitive performance compared to the existing video anomaly
detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Bi-directional Motion Estimation for Video Frame Interpolation. (arXiv:2206.08572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08572">
<div class="article-summary-box-inner">
<span><p>We present a novel simple yet effective algorithm for motion-based video
frame interpolation. Existing motion-based interpolation methods typically rely
on a pre-trained optical flow model or a U-Net based pyramid network for motion
estimation, which either suffer from large model size or limited capacity in
handling complex and large motion cases. In this work, by carefully integrating
intermediateoriented forward-warping, lightweight feature encoder, and
correlation volume into a pyramid recurrent framework, we derive a compact
model to simultaneously estimate the bidirectional motion between input frames.
It is 15 times smaller in size than PWC-Net, yet enables more reliable and
flexible handling of challenging motion cases. Based on estimated
bi-directional motion, we forward-warp input frames and their context features
to intermediate frame, and employ a synthesis network to estimate the
intermediate frame from warped representations. Our method achieves excellent
performance on a broad range of video frame interpolation benchmarks. Code will
be available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HairFIT: Pose-Invariant Hairstyle Transfer via Flow-based Hair Alignment and Semantic-Region-Aware Inpainting. (arXiv:2206.08585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08585">
<div class="article-summary-box-inner">
<span><p>Hairstyle transfer is the task of modifying a source hairstyle to a target
one. Although recent hairstyle transfer models can reflect the delicate
features of hairstyles, they still have two major limitations. First, the
existing methods fail to transfer hairstyles when a source and a target image
have different poses (e.g., viewing direction or face size), which is prevalent
in the real world. Also, the previous models generate unrealistic images when
there is a non-trivial amount of regions in the source image occluded by its
original hair. When modifying long hair to short hair, shoulders or backgrounds
occluded by the long hair need to be inpainted. To address these issues, we
propose a novel framework for pose-invariant hairstyle transfer, HairFIT. Our
model consists of two stages: 1) flow-based hair alignment and 2) hair
synthesis. In the hair alignment stage, we leverage a keypoint-based optical
flow estimator to align a target hairstyle with a source pose. Then, we
generate a final hairstyle-transferred image in the hair synthesis stage based
on Semantic-region-aware Inpainting Mask (SIM) estimator. Our SIM estimator
divides the occluded regions in the source image into different semantic
regions to reflect their distinct features during the inpainting. To
demonstrate the effectiveness of our model, we conduct quantitative and
qualitative evaluations using multi-view datasets, K-hairstyle and VoxCeleb.
The results indicate that HairFIT achieves a state-of-the-art performance by
successfully transferring hairstyles between images of different poses, which
has never been achieved before.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Efficient Real-Time Semantic Segmentation: A Survey. (arXiv:2206.08605v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08605">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is the problem of assigning a class label to every
pixel in an image, and is an important component of an autonomous vehicle
vision stack for facilitating scene understanding and object detection.
However, many of the top performing semantic segmentation models are extremely
complex and cumbersome, and as such are not suited to deployment onboard
autonomous vehicle platforms where computational resources are limited and
low-latency operation is a vital requirement. In this survey, we take a
thorough look at the works that aim to address this misalignment with more
compact and efficient models capable of deployment on low-memory embedded
systems while meeting the constraint of real-time inference. We discuss several
of the most prominent works in the field, placing them within a taxonomy based
on their major contributions, and finally we evaluate the inference speed of
the discussed models under consistent hardware and software setups that
represent a typical research environment with high-end GPU and a realistic
deployed scenario using low-memory embedded GPU hardware. Our experimental
results demonstrate that many works are capable of real-time performance on
resource-constrained hardware, while illustrating the consistent trade-off
between latency and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Autoencoders for Generic Event Boundary Detection CVPR'2022 Kinetics-GEBD Challenge. (arXiv:2206.08610v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08610">
<div class="article-summary-box-inner">
<span><p>Generic Event Boundary Detection (GEBD) tasks aim at detecting generic,
taxonomy-free event boundaries that segment a whole video into chunks. In this
paper, we apply Masked Autoencoders to improve algorithm performance on the
GEBD tasks. Our approach mainly adopted the ensemble of Masked Autoencoders
fine-tuned on the GEBD task as a self-supervised learner with other base
models. Moreover, we also use a semi-supervised pseudo-label method to take
full advantage of the abundant unlabeled Kinetics-400 data while training. In
addition, we propose a soft-label method to partially balance the positive and
negative samples and alleviate the problem of ambiguous labeling in this task.
Lastly, a tricky segmentation alignment policy is implemented to refine
boundaries predicted by our models to more accurate locations. With our
approach, we achieved 85.94% on the F1-score on the Kinetics-GEBD test set,
which improved the F1-score by 2.31% compared to the winner of the 2021
Kinetics-GEBD Challenge. Our code is available at
https://github.com/ContentAndMaterialPortrait/MAE-GEBD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing. (arXiv:2206.08612v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08612">
<div class="article-summary-box-inner">
<span><p>Optoacoustic (OA) imaging is based on excitation of biological tissues with
nanosecond-duration laser pulses followed by subsequent detection of ultrasound
waves generated via light-absorption-mediated thermoelastic expansion. OA
imaging features a powerful combination between rich optical contrast and high
resolution in deep tissues. This enabled the exploration of a number of
attractive new applications both in clinical and laboratory settings. However,
no standardized datasets generated with different types of experimental set-up
and associated processing methods are available to facilitate advances in
broader applications of OA in clinical settings. This complicates an objective
comparison between new and established data processing methods, often leading
to qualitative results and arbitrary interpretations of the data. In this
paper, we provide both experimental and synthetic OA raw signals and
reconstructed image domain datasets rendered with different experimental
parameters and tomographic acquisition geometries. We further provide trained
neural networks to tackle three important challenges related to OA image
processing, namely accurate reconstruction under limited view tomographic
conditions, removal of spatial undersampling artifacts and anatomical
segmentation for improved image reconstruction. Specifically, we define 18
experiments corresponding to the aforementioned challenges as benchmarks to be
used as a reference for the development of more advanced processing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment. (arXiv:2206.08614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08614">
<div class="article-summary-box-inner">
<span><p>Computational inference of aesthetics is an ill-defined task due to its
subjective nature. Many datasets have been proposed to tackle the problem by
providing pairs of images and aesthetic scores based on human ratings. However,
humans are better at expressing their opinion, taste, and emotions by means of
language rather than summarizing them in a single number. In fact, photo
critiques provide much richer information as they reveal how and why users rate
the aesthetics of visual stimuli. In this regard, we propose the Reddit Photo
Critique Dataset (RPCD), which contains tuples of image and photo critiques.
RPCD consists of 74K images and 220K comments and is collected from a Reddit
community used by hobbyists and professional photographers to improve their
photography skills by leveraging constructive community feedback. The proposed
dataset differs from previous aesthetics datasets mainly in three aspects,
namely (i) the large scale of the dataset and the extension of the comments
criticizing different aspects of the image, (ii) it contains mostly UltraHD
images, and (iii) it can easily be extended to new data as it is collected
through an automatic pipeline. To the best of our knowledge, in this work, we
propose the first attempt to estimate the aesthetic quality of visual stimuli
from the critiques. To this end, we exploit the polarity of the sentiment of
criticism as an indicator of aesthetic judgment. We demonstrate how sentiment
polarity correlates positively with the aesthetic judgment available for two
aesthetic assessment benchmarks. Finally, we experiment with several models by
using the sentiment scores as a target for ranking images. Dataset and
baselines are available (https://github.com/mediatechnologycenter/aestheval).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Using Privileged Information for Zero-Shot Action Recognition. (arXiv:2206.08632v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08632">
<div class="article-summary-box-inner">
<span><p>Zero-Shot Action Recognition (ZSAR) aims to recognize video actions that have
never been seen during training. Most existing methods assume a shared semantic
space between seen and unseen actions and intend to directly learn a mapping
from a visual space to the semantic space. This approach has been challenged by
the semantic gap between the visual space and semantic space. This paper
presents a novel method that uses object semantics as privileged information to
narrow the semantic gap and, hence, effectively, assist the learning. In
particular, a simple hallucination network is proposed to implicitly extract
object semantics during testing without explicitly extracting objects and a
cross-attention module is developed to augment visual feature with the object
semantics. Experiments on the Olympic Sports, HMDB51 and UCF101 datasets have
shown that the proposed method outperforms the state-of-the-art methods by a
large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum Noticeable Difference based Adversarial Privacy Preserving Image Generation. (arXiv:2206.08638v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08638">
<div class="article-summary-box-inner">
<span><p>Deep learning models are found to be vulnerable to adversarial examples, as
wrong predictions can be caused by small perturbation in input for deep
learning models. Most of the existing works of adversarial image generation try
to achieve attacks for most models, while few of them make efforts on
guaranteeing the perceptual quality of the adversarial examples. High quality
adversarial examples matter for many applications, especially for the privacy
preserving. In this work, we develop a framework based on the Minimum
Noticeable Difference (MND) concept to generate adversarial privacy preserving
images that have minimum perceptual difference from the clean ones but are able
to attack deep learning models. To achieve this, an adversarial loss is firstly
proposed to make the deep learning models attacked by the adversarial images
successfully. Then, a perceptual quality-preserving loss is developed by taking
the magnitude of perturbation and perturbation-caused structural and gradient
changes into account, which aims to preserve high perceptual quality for
adversarial image generation. To the best of our knowledge, this is the first
work on exploring quality-preserving adversarial image generation based on the
MND concept for privacy preserving. To evaluate its performance in terms of
perceptual quality, the deep models on image classification and face
recognition are tested with the proposed method and several anchor methods in
this work. Extensive experimental results demonstrate that the proposed MND
framework is capable of generating adversarial images with remarkably improved
performance metrics (e.g., PSNR, SSIM, and MOS) than that generated with the
anchor methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-aware Evaluation of Time-Series Classification for Online Handwriting Recognition with Domain Shift. (arXiv:2206.08640v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08640">
<div class="article-summary-box-inner">
<span><p>For many applications, analyzing the uncertainty of a machine learning model
is indispensable. While research of uncertainty quantification (UQ) techniques
is very advanced for computer vision applications, UQ methods for
spatio-temporal data are less studied. In this paper, we focus on models for
online handwriting recognition, one particular type of spatio-temporal data.
The data is observed from a sensor-enhanced pen with the goal to classify
written characters. We conduct a broad evaluation of aleatoric (data) and
epistemic (model) UQ based on two prominent techniques for Bayesian inference,
Stochastic Weight Averaging-Gaussian (SWAG) and Deep Ensembles. Next to a
better understanding of the model, UQ techniques can detect out-of-distribution
data and domain shifts when combining right-handed and left-handed writers (an
underrepresented group).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Diversity of Multiple Trajectory Prediction based on Map-adaptive Lane Loss. (arXiv:2206.08641v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08641">
<div class="article-summary-box-inner">
<span><p>Prior arts in the field of motion predictions for autonomous driving tend to
focus on finding a trajectory that is close to the ground truth trajectory.
Such problem formulations and approaches, however, frequently lead to loss of
diversity and biased trajectory predictions. Therefore, they are unsuitable for
real-world autonomous driving where diverse and road-dependent multimodal
trajectory predictions are critical for safety. To this end, this study
proposes a novel loss function, \textit{Lane Loss}, that ensures map-adaptive
diversity and accommodates geometric constraints. A two-stage trajectory
prediction architecture with a novel trajectory candidate proposal module,
\textit{Trajectory Prediction Attention (TPA)}, is trained with Lane Loss
encourages multiple trajectories to be diversely distributed, covering feasible
maneuvers in a map-aware manner. Furthermore, considering that the existing
trajectory performance metrics are focusing on evaluating the accuracy based on
the ground truth future trajectory, a quantitative evaluation metric is also
suggested to evaluate the diversity of predicted multiple trajectories. The
experiments performed on the Argoverse dataset show that the proposed method
significantly improves the diversity of the predicted trajectories without
sacrificing the prediction accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Slot Attention for Vision-and-Language Navigation. (arXiv:2206.08645v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08645">
<div class="article-summary-box-inner">
<span><p>Vision-and-language navigation (VLN), a frontier study aiming to pave the way
for general-purpose robots, has been a hot topic in the computer vision and
natural language processing community. The VLN task requires an agent to
navigate to a goal location following natural language instructions in
unfamiliar environments.
</p>
<p>Recently, transformer-based models have gained significant improvements on
the VLN task. Since the attention mechanism in the transformer architecture can
better integrate inter- and intra-modal information of vision and language.
</p>
<p>However, there exist two problems in current transformer-based models.
</p>
<p>1) The models process each view independently without taking the integrity of
the objects into account.
</p>
<p>2) During the self-attention operation in the visual modality, the views that
are spatially distant can be inter-weaved with each other without explicit
restriction. This kind of mixing may introduce extra noise instead of useful
information.
</p>
<p>To address these issues, we propose 1) A slot-attention based module to
incorporate information from segmentation of the same object. 2) A local
attention mask mechanism to limit the visual attention span. The proposed
modules can be easily plugged into any VLN architecture and we use the
Recurrent VLN-Bert as our base model. Experiments on the R2R dataset show that
our model has achieved the state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label Predictions (CHAMP). (arXiv:2206.08653v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08653">
<div class="article-summary-box-inner">
<span><p>This paper considers the problem of Hierarchical Multi-Label Classification
(HMC), where (i) several labels can be present for each example, and (ii)
labels are related via a domain-specific hierarchy tree. Guided by the
intuition that all mistakes are not equal, we present Comprehensive Hierarchy
Aware Multi-label Predictions (CHAMP), a framework that penalizes a
misprediction depending on its severity as per the hierarchy tree. While there
have been works that apply such an idea to single-label classification, to the
best of our knowledge, there are limited such works for multilabel
classification focusing on the severity of mistakes. The key reason is that
there is no clear way of quantifying the severity of a misprediction a priori
in the multilabel setting. In this work, we propose a simple but effective
metric to quantify the severity of a mistake in HMC, naturally leading to
CHAMP. Extensive experiments on six public HMC datasets across modalities
(image, audio, and text) demonstrate that incorporating hierarchical
information leads to substantial gains as CHAMP improves both AUPRC (2.6%
median percentage improvement) and hierarchical metrics (2.85% median
percentage improvement), over stand-alone hierarchical or multilabel
classification methods. Compared to standard multilabel baselines, CHAMP
provides improved AUPRC in both robustness (8.87% mean percentage improvement )
and less data regimes. Further, our method provides a framework to enhance
existing multilabel classification algorithms with better mistakes (18.1% mean
percentage increment).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Implicit Feature Alignment Function for Semantic Segmentation. (arXiv:2206.08655v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08655">
<div class="article-summary-box-inner">
<span><p>Integrating high-level context information with low-level details is of
central importance in semantic segmentation. Towards this end, most existing
segmentation models apply bilinear up-sampling and convolutions to feature maps
of different scales, and then align them at the same resolution. However,
bilinear up-sampling blurs the precise information learned in these feature
maps and convolutions incur extra computation costs. To address these issues,
we propose the Implicit Feature Alignment function (IFA). Our method is
inspired by the rapidly expanding topic of implicit neural representations,
where coordinate-based neural networks are used to designate fields of signals.
In IFA, feature vectors are viewed as representing a 2D field of information.
Given a query coordinate, nearby feature vectors with their relative
coordinates are taken from the multi-level feature maps and then fed into an
MLP to generate the corresponding output. As such, IFA implicitly aligns the
feature maps at different levels and is capable of producing segmentation maps
in arbitrary resolutions. We demonstrate the efficacy of IFA on multiple
datasets, including Cityscapes, PASCAL Context, and ADE20K. Our method can be
combined with improvement on various architectures, and it achieves
state-of-the-art computation-accuracy trade-off on common benchmarks. Code will
be made available at https://github.com/hzhupku/IFA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08657">
<div class="article-summary-box-inner">
<span><p>Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a cross-modal encoder, or feed the last-layer
uni-modal features directly into the top cross-modal encoder, ignoring the
semantic information at the different levels in the deep uni-modal encoders.
Both approaches possibly restrict vision-language representation learning and
limit model performance. In this paper, we introduce multiple bridge layers
that build a connection between the top layers of uni-modal encoders and each
layer of the cross-modal encoder. This enables comprehensive bottom-up
interactions between visual and textual representations at different semantic
levels, resulting in more effective cross-modal alignment and fusion. Our
proposed Bridge-Tower, pre-trained with only $4$M images, achieves
state-of-the-art performance on various downstream vision-language tasks. On
the VQAv2 test-std set, Bridge-Tower achieves an accuracy of $78.73\%$,
outperforming the previous state-of-the-art METER model by $1.09\%$ with the
same pre-training data and almost no additional parameters and computational
cost. Notably, when further scaling the model, Bridge-Tower achieves an
accuracy of $81.15\%$, surpassing models that are pre-trained on
orders-of-magnitude larger datasets. Code is available at
https://github.com/microsoft/BridgeTower.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification. (arXiv:2206.08671v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08671">
<div class="article-summary-box-inner">
<span><p>Modern deep learning systems are increasingly deployed in situations such as
personalization and federated learning where it is necessary to support i)
learning on small amounts of data, and ii) communication efficient distributed
training protocols. In this work we develop FiLM Transfer (FiT) which fulfills
these requirements in the image classification setting. FiT uses an
automatically configured Naive Bayes classifier on top of a fixed backbone that
has been pretrained on large image datasets. Parameter efficient FiLM layers
are used to modulate the backbone, shaping the representation for the
downstream task. The network is trained via an episodic fine-tuning protocol.
The approach is parameter efficient which is key for enabling few-shot
learning, inexpensive model updates for personalization, and communication
efficient federated learning. We experiment with FiT on a wide range of
downstream datasets and show that it achieves better classification accuracy
than the state-of-the-art Big Transfer (BiT) algorithm at low-shot and on the
challenging VTAB-1k benchmark, with fewer than 1% of the updateable parameters.
Finally, we demonstrate the parameter efficiency of FiT in distributed low-shot
applications including model personalization and federated learning where model
update size is an important performance metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AggNet: Learning to Aggregate Faces for Group Membership Verification. (arXiv:2206.08683v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08683">
<div class="article-summary-box-inner">
<span><p>In some face recognition applications, we are interested to verify whether an
individual is a member of a group, without revealing their identity. Some
existing methods, propose a mechanism for quantizing precomputed face
descriptors into discrete embeddings and aggregating them into one group
representation. However, this mechanism is only optimized for a given closed
set of individuals and needs to learn the group representations from scratch
every time the groups are changed. In this paper, we propose a deep
architecture that jointly learns face descriptors and the aggregation mechanism
for better end-to-end performances. The system can be applied to new groups
with individuals never seen before and the scheme easily manages new
memberships or membership endings. We show through experiments on multiple
large-scale wild-face datasets, that the proposed method leads to higher
verification performance compared to other baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Double Descent: Where Network Pruning Aggravates Overfitting. (arXiv:2206.08684v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08684">
<div class="article-summary-box-inner">
<span><p>People usually believe that network pruning not only reduces the
computational cost of deep networks, but also prevents overfitting by
decreasing model capacity. However, our work surprisingly discovers that
network pruning sometimes even aggravates overfitting. We report an unexpected
sparse double descent phenomenon that, as we increase model sparsity via
network pruning, test performance first gets worse (due to overfitting), then
gets better (due to relieved overfitting), and gets worse at last (due to
forgetting useful information). While recent studies focused on the deep double
descent with respect to model overparameterization, they failed to recognize
that sparsity may also cause double descent. In this paper, we have three main
contributions. First, we report the novel sparse double descent phenomenon
through extensive experiments. Second, for this phenomenon, we propose a novel
learning distance interpretation that the curve of $\ell_{2}$ learning distance
of sparse models (from initialized parameters to final parameters) may
correlate with the sparse double descent curve well and reflect generalization
better than minima flatness. Third, in the context of sparse double descent, a
winning ticket in the lottery ticket hypothesis surprisingly may not always
win.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Real-Time Visual Tracking with Graded Color-names Features. (arXiv:2206.08701v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08701">
<div class="article-summary-box-inner">
<span><p>MeanShift algorithm has been widely used in tracking tasks because of its
simplicity and efficiency. However, the traditional MeanShift algorithm needs
to label the initial region of the target, which reduces the applicability of
the algorithm. Furthermore, it is only applicable to the scene with a large
overlap rate between the target area and the candidate area. Therefore, when
the target speed is fast, the target scale change, shape deformation or the
target occlusion occurs, the tracking performance will be deteriorated. In this
paper, we address the challenges above-mentioned by developing a tracking
method that combines the background models and the graded features of
color-names under the MeanShift framework. This method significantly improve
performance in the above scenarios. In addition, it facilitates the balance
between detection accuracy and detection speed. Experimental results
demonstrate the validation of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maximum Class Separation as Inductive Bias in One Matrix. (arXiv:2206.08704v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08704">
<div class="article-summary-box-inner">
<span><p>Maximizing the separation between classes constitutes a well-known inductive
bias in machine learning and a pillar of many traditional algorithms. By
default, deep networks are not equipped with this inductive bias and therefore
many alternative solutions have been proposed through differential
optimization. Current approaches tend to optimize classification and separation
jointly: aligning inputs with class vectors and separating class vectors
angularly. This paper proposes a simple alternative: encoding maximum
separation as an inductive bias in the network by adding one fixed matrix
multiplication before computing the softmax activations. The main observation
behind our approach is that separation does not require optimization but can be
solved in closed-form prior to training and plugged into a network. We outline
a recursive approach to obtain the matrix consisting of maximally separable
vectors for any number of classes, which can be added with negligible
engineering effort and computational overhead. Despite its simple nature, this
one matrix multiplication provides real impact. We show that our proposal
directly boosts classification, long-tailed recognition, out-of-distribution
detection, and open-set recognition, from CIFAR to ImageNet. We find
empirically that maximum separation works best as a fixed bias; making the
matrix learnable adds nothing to the performance. The closed-form
implementation and code to reproduce the experiments are on github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Algorithm for the SE(3)-Transformation on Neural Implicit Maps for Remapping Functions. (arXiv:2206.08712v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08712">
<div class="article-summary-box-inner">
<span><p>Implicit representations are widely used for object reconstruction due to
their efficiency and flexibility. In 2021, a novel structure named neural
implicit map has been invented for incremental reconstruction. A neural
implicit map alleviates the problem of inefficient memory cost of previous
online 3D dense reconstruction while producing better quality. % However, the
neural implicit map suffers the limitation that it does not support remapping
as the frames of scans are encoded into a deep prior after generating the
neural implicit map. This means, that neither this generation process is
invertible, nor a deep prior is transformable. The non-remappable property
makes it not possible to apply loop-closure techniques. % We present a neural
implicit map based transformation algorithm to fill this gap. As our neural
implicit map is transformable, our model supports remapping for this special
map of latent features. % Experiments show that our remapping module is capable
to well-transform neural implicit maps to new poses. Embedded into a SLAM
framework, our mapping model is able to tackle the remapping of loop closures
and demonstrates high-quality surface reconstruction. % Our implementation is
available at github\footnote{\url{https://github.com/Jarrome/IMT_Mapping}} for
the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReViSe: Remote Vital Signs Measurement Using Smartphone Camera. (arXiv:2206.08748v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08748">
<div class="article-summary-box-inner">
<span><p>Remote Photoplethysmography (rPPG) is a fast, effective, inexpensive and
convenient method for collecting biometric data as it enables vital signs
estimation using face videos. Remote contactless medical service provisioning
has proven to be a dire necessity during the COVID-19 pandemic. We propose an
end-to-end framework to measure people's vital signs including Heart Rate (HR),
Heart Rate Variability (HRV), Oxygen Saturation (SpO2) and Blood Pressure (BP)
based on the rPPG methodology from the video of a user's face captured with a
smartphone camera. We extract face landmarks with a deep learning-based neural
network model in real-time. Multiple face patches also called
Region-of-Interests (RoIs) are extracted by using the predicted face landmarks.
Several filters are applied to reduce the noise from the RoIs in the extracted
cardiac signals called Blood Volume Pulse (BVP) signal. We trained and
validated machine learning models using two public rPPG datasets namely the
TokyoTech rPPG and the Pulse Rate Detection (PURE) datasets, on which our
models achieved the following Mean Absolute Errors (MAE): a) for HR, 1.73 and
3.95 Beats-Per-Minute (bpm) respectively, b) for HRV, 18.55 and 25.03 ms
respectively, and c) for SpO2, a MAE of 1.64 on the PURE dataset. We validated
our end-to-end rPPG framework, ReViSe, in real life environment, and thereby
created the Video-HR dataset. Our HR estimation model achieved a MAE of 2.49
bpm on this dataset. Since no publicly available rPPG datasets existed for BP
measurement with face videos, we used a dataset with signals from fingertip
sensor to train our model and also created our own video dataset, Video-BP. On
our Video-BP dataset, our BP estimation model achieved a MAE of 6.7 mmHg for
Systolic Blood Pressure (SBP), and a MAE of 9.6 mmHg for Diastolic Blood
Pressure (DBP).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From a few Accurate 2D Correspondences to 3D Point Clouds. (arXiv:2206.08749v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08749">
<div class="article-summary-box-inner">
<span><p>Key points, correspondences, projection matrices, point clouds and dense
clouds are the skeletons in image-based 3D reconstruction, of which point
clouds have the important role in generating a realistic and natural model for
a 3D reconstructed object. To achieve a good 3D reconstruction, the point
clouds must be almost everywhere in the surface of the object. In this article,
with a main purpose to build the point clouds covering the entire surface of
the object, we propose a new feature named a geodesic feature or geo-feature.
Based on the new geo-feature, if there are several (given) initial world points
on the object's surface along with all accurately estimated projection
matrices, some new world points on the geodesics connecting any two of these
given world points will be reconstructed. Then the regions on the surface
bordering by these initial world points will be covered by the point clouds.
Thus, if the initial world points are around the surface, the point clouds will
cover the entire surface.
</p>
<p>This article proposes a new method to estimate the world points and
projection matrices from their correspondences. This method derives the
closed-form and iterative solutions for the world points and projection
matrices and proves that when the number of world points is less than seven and
the number of images is at least five, the proposed solutions are global
optimal. We propose an algorithm named World points from their Correspondences
(WPfC) to estimate the world points and projection matrices from their
correspondences, and another algorithm named Creating Point Clouds (CrPC) to
create the point clouds from the world points and projection matrices given by
the first algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Database for Perceived Quality Assessment of User-Generated VR Videos. (arXiv:2206.08751v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08751">
<div class="article-summary-box-inner">
<span><p>Virtual reality (VR) videos (typically in the form of 360$^\circ$ videos)
have gained increasing attention due to the fast development of VR technologies
and the remarkable popularization of consumer-grade 360$^\circ$ cameras and
displays. Thus it is pivotal to understand how people perceive user-generated
VR videos, which may suffer from commingled authentic distortions, often
localized in space and time. In this paper, we establish one of the largest
360$^\circ$ video databases, containing 502 user-generated videos with rich
content and distortion diversities. We capture viewing behaviors (i.e.,
scanpaths) of 139 users, and collect their opinion scores of perceived quality
under four different viewing conditions (two starting points $\times$ two
exploration times). We provide a thorough statistical analysis of recorded
data, resulting in several interesting observations, such as the significant
impact of viewing conditions on viewing behaviors and perceived quality.
Besides, we explore other usage of our data and analysis, including evaluation
of computational models for quality assessment and saliency detection of
360$^\circ$ videos. We have made the dataset and code available at
https://github.com/Yao-Yiru/VR-Video-Database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume Segmentation on Cone Beam Computed Tomography Images. (arXiv:2206.08778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08778">
<div class="article-summary-box-inner">
<span><p>3D tooth segmentation is a prerequisite for computer-aided dental diagnosis
and treatment. However, segmenting all tooth regions manually is subjective and
time-consuming. Recently, deep learning-based segmentation methods produce
convincing results and reduce manual annotation efforts, but it requires a
large quantity of ground truth for training. To our knowledge, there are few
tooth data available for the 3D segmentation study. In this paper, we establish
a fully annotated cone beam computed tomography dataset CTooth with tooth gold
standard. This dataset contains 22 volumes (7363 slices) with fine tooth labels
annotated by experienced radiographic interpreters. To ensure a relative even
data sampling distribution, data variance is included in the CTooth including
missing teeth and dental restoration. Several state-of-the-art segmentation
methods are evaluated on this dataset. Afterwards, we further summarise and
apply a series of 3D attention-based Unet variants for segmenting tooth
volumes. This work provides a new benchmark for the tooth volume segmentation
task. Experimental evidence proves that attention modules of the 3D UNet
structure boost responses in tooth areas and inhibit the influence of
background and noise. The best performance is achieved by 3D Unet with SKNet
attention module, of 88.04 \% Dice and 78.71 \% IOU, respectively. The
attention-based Unet framework outperforms other state-of-the-art methods on
the CTooth dataset. The codebase and dataset are released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma Grading. (arXiv:2206.08787v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08787">
<div class="article-summary-box-inner">
<span><p>Pancreatic cancers have one of the worst prognoses compared to other cancers,
as they are diagnosed when cancer has progressed towards its latter stages. The
current manual histological grading for diagnosing pancreatic adenocarcinomas
is time-consuming and often results in misdiagnosis. In digital pathology,
AI-based cancer grading must be extremely accurate in prediction and
uncertainty quantification to improve reliability and explainability and are
essential for gaining clinicians trust in the technology. We present Bayesian
Convolutional Neural Networks for automated pancreatic cancer grading from MGG
and HE stained images to estimate uncertainty in model prediction. We show that
the estimated uncertainty correlates with prediction error. Specifically, it is
useful in setting the acceptance threshold using a metric that weighs
classification accuracy-reject trade-off and misclassification cost controlled
by hyperparameters and can be employed in clinical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstructing vehicles from orthographic drawings using deep neural networks. (arXiv:2206.08789v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08789">
<div class="article-summary-box-inner">
<span><p>This paper explores the current state-of-the-art of object reconstruction
from multiple orthographic drawings using deep neural networks. It proposes two
algorithms to extract multiple views from a single image. The paper proposes a
system based on pixel-aligned implicit functions (PIFu) and develops an
advanced sampling strategy to generate signed distance samples. It also
compares this approach to depth map regression from multiple views.
Additionally, the paper uses a novel dataset for vehicle reconstruction from
the racing game Assetto Corsa, which features higher quality models than the
commonly used ShapeNET dataset. The trained neural network generalizes well to
real-world inputs and creates plausible and detailed reconstructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DU-Net based Unsupervised Contrastive Learning for Cancer Segmentation in Histology Images. (arXiv:2206.08791v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08791">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce an unsupervised cancer segmentation framework for
histology images. The framework involves an effective contrastive learning
scheme for extracting distinctive visual representations for segmentation. The
encoder is a Deep U-Net (DU-Net) structure that contains an extra fully
convolution layer compared to the normal U-Net. A contrastive learning scheme
is developed to solve the problem of lacking training sets with high-quality
annotations on tumour boundaries. A specific set of data augmentation
techniques are employed to improve the discriminability of the learned colour
features from contrastive learning. Smoothing and noise elimination are
conducted using convolutional Conditional Random Fields. The experiments
demonstrate competitive performance in segmentation even better than some
popular supervised networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FD-CAM: Improving Faithfulness and Discriminability of Visual Explanation for CNNs. (arXiv:2206.08792v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08792">
<div class="article-summary-box-inner">
<span><p>Class activation map (CAM) has been widely studied for visual explanation of
the internal working mechanism of convolutional neural networks. The key of
existing CAM-based methods is to compute effective weights to combine
activation maps in the target convolution layer. Existing gradient and score
based weighting schemes have shown superiority in ensuring either the
discriminability or faithfulness of the CAM, but they normally cannot excel in
both properties. In this paper, we propose a novel CAM weighting scheme, named
FD-CAM, to improve both the faithfulness and discriminability of the CAM-based
CNN visual explanation. First, we improve the faithfulness and discriminability
of the score-based weights by performing a grouped channel switching operation.
Specifically, for each channel, we compute its similarity group and switch the
group of channels on or off simultaneously to compute changes in the class
prediction score as the weights. Then, we combine the improved score-based
weights with the conventional gradient-based weights so that the
discriminability of the final CAM can be further improved. We perform extensive
comparisons with the state-of-the-art CAM algorithms. The quantitative and
qualitative results show our FD-CAM can produce more faithful and more
discriminative visual explanations of the CNNs. We also conduct experiments to
verify the effectiveness of the proposed grouped channel switching and weight
combination scheme on improving the results. Our code is available at
https://github.com/crishhh1998/FD-CAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Importance of Background Information for Out of Distribution Generalization. (arXiv:2206.08794v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08794">
<div class="article-summary-box-inner">
<span><p>Domain generalization in medical image classification is an important problem
for trustworthy machine learning to be deployed in healthcare. We find that
existing approaches for domain generalization which utilize ground-truth
abnormality segmentations to control feature attributions have poor
out-of-distribution (OOD) performance relative to the standard baseline of
empirical risk minimization (ERM). We investigate what regions of an image are
important for medical image classification and show that parts of the
background, that which is not contained in the abnormality segmentation,
provides helpful signal. We then develop a new task-specific mask which covers
all relevant regions. Utilizing this new segmentation mask significantly
improves the performance of the existing methods on the OOD test sets. To
obtain better generalization results than ERM, we find it necessary to scale up
the training data size in addition to the usage of these task-specific masks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Shadow Detection via Spatio-Temporal Interpolation Consistency Training. (arXiv:2206.08801v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08801">
<div class="article-summary-box-inner">
<span><p>It is challenging to annotate large-scale datasets for supervised video
shadow detection methods. Using a model trained on labeled images to the video
frames directly may lead to high generalization error and temporal inconsistent
results. In this paper, we address these challenges by proposing a
Spatio-Temporal Interpolation Consistency Training (STICT) framework to
rationally feed the unlabeled video frames together with the labeled images
into an image shadow detection network training. Specifically, we propose the
Spatial and Temporal ICT, in which we define two new interpolation schemes,
\textit{i.e.}, the spatial interpolation and the temporal interpolation. We
then derive the spatial and temporal interpolation consistency constraints
accordingly for enhancing generalization in the pixel-wise classification task
and for encouraging temporal consistent predictions, respectively. In addition,
we design a Scale-Aware Network for multi-scale shadow knowledge learning in
images, and propose a scale-consistency constraint to minimize the discrepancy
among the predictions at different scales. Our proposed approach is extensively
validated on the ViSha dataset and a self-annotated dataset. Experimental
results show that, even without video labels, our approach is better than most
state of the art supervised, semi-supervised or unsupervised image/video shadow
detection methods and other methods in related tasks. Code and dataset are
available at \url{https://github.com/yihong-97/STICT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Sampling: Exploring Out-of-Distribution data for Re-balancing Long-tailed datasets. (arXiv:2206.08802v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08802">
<div class="article-summary-box-inner">
<span><p>Deep neural networks usually perform poorly when the training dataset suffers
from extreme class imbalance. Recent studies found that directly training with
out-of-distribution data (i.e., open-set samples) in a semi-supervised manner
would harm the generalization performance. In this work, we theoretically show
that out-of-distribution data can still be leveraged to augment the minority
classes from a Bayesian perspective. Based on this motivation, we propose a
novel method called Open-sampling, which utilizes open-set noisy labels to
re-balance the class priors of the training dataset. For each open-set
instance, the label is sampled from our pre-defined distribution that is
complementary to the distribution of original class priors. We empirically show
that Open-sampling not only re-balances the class priors but also encourages
the neural network to learn separable representations. Extensive experiments
demonstrate that our proposed method significantly outperforms existing data
re-balancing methods and can boost the performance of existing state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis. (arXiv:2206.08826v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08826">
<div class="article-summary-box-inner">
<span><p>Alzheimer's Disease (AD) is the most common neurodegenerative disorder with
one of the most complex pathogeneses, making effective and clinically
actionable decision support difficult. The objective of this study was to
develop a novel multimodal deep learning framework to aid medical professionals
in AD diagnosis. We present a Multimodal Alzheimer's Disease Diagnosis
framework (MADDi) to accurately detect the presence of AD and mild cognitive
impairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in
that we use cross-modal attention, which captures interactions between
modalities - a method not previously explored in this domain. We perform
multi-class classification, a challenging task considering the strong
similarities between MCI and AD. We compare with previous state-of-the-art
models, evaluate the importance of attention, and examine the contribution of
each modality to the model's performance. MADDi classifies MCI, AD, and
controls with 96.88% accuracy on a held-out test set. When examining the
contribution of different attention schemes, we found that the combination of
cross-modal attention with self-attention performed the best, and no attention
layers in the model performed the worst, with a 7.9% difference in F1-Scores.
Our experiments underlined the importance of structured clinical data to help
machine learning models contextualize and interpret the remaining modalities.
Extensive ablation studies showed that any multimodal mixture of input features
without access to structured clinical information suffered marked performance
losses. This study demonstrates the merit of combining multiple input
modalities via cross-modal attention to deliver highly accurate AD diagnostic
decision support.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Confidence Calibration in Deep Learning: From Computer Vision to Medical Imaging. (arXiv:2206.08833v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08833">
<div class="article-summary-box-inner">
<span><p>Although deep learning prediction models have been successful in the
discrimination of different classes, they can often suffer from poor
calibration across challenging domains including healthcare. Moreover, the
long-tail distribution poses great challenges in deep learning classification
problems including clinical disease prediction. There are approaches proposed
recently to calibrate deep prediction in computer vision, but there are no
studies found to demonstrate how the representative models work in different
challenging contexts. In this paper, we bridge the confidence calibration from
computer vision to medical imaging with a comparative study of four high-impact
calibration models. Our studies are conducted in different contexts (natural
image classification and lung cancer risk estimation) including in balanced vs.
imbalanced training sets and in computer vision vs. medical imaging. Our
results support key findings: (1) We achieve new conclusions which are not
studied under different learning contexts, e.g., combining two calibration
models that both mitigate the overconfident prediction can lead to
under-confident prediction, and simpler calibration models from the computer
vision domain tend to be more generalizable to medical imaging. (2) We
highlight the gap between general computer vision tasks and medical imaging
prediction, e.g., calibration methods ideal for general computer vision tasks
may in fact damage the calibration of medical imaging prediction. (3) We also
reinforce previous conclusions in natural image classification settings. We
believe that this study has merits to guide readers to choose calibration
models and understand gaps between general computer vision and medical imaging
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-Graph Enhanced Cross-Modal Pretraining for Instance-level Product Retrieval. (arXiv:2206.08842v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08842">
<div class="article-summary-box-inner">
<span><p>Our goal in this research is to study a more realistic environment in which
we can conduct weakly-supervised multi-modal instance-level product retrieval
for fine-grained product categories. We first contribute the Product1M
datasets, and define two real practical instance-level retrieval tasks to
enable the evaluations on the price comparison and personalized
recommendations. For both instance-level tasks, how to accurately pinpoint the
product target mentioned in the visual-linguistic data and effectively decrease
the influence of irrelevant contents is quite challenging. To address this, we
exploit to train a more effective cross-modal pertaining model which is
adaptively capable of incorporating key concept information from the
multi-modal data, by using an entity graph whose node and edge respectively
denote the entity and the similarity relation between entities. Specifically, a
novel Entity-Graph Enhanced Cross-Modal Pretraining (EGE-CMP) model is proposed
for instance-level commodity retrieval, that explicitly injects entity
knowledge in both node-based and subgraph-based ways into the multi-modal
networks via a self-supervised hybrid-stream transformer, which could reduce
the confusion between different object contents, thereby effectively guiding
the network to focus on entities with real semantic. Experimental results well
verify the efficacy and generalizability of our EGE-CMP, outperforming several
SOTA cross-modal baselines like CLIP, UNITER and CAPTURE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. (arXiv:2206.08853v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08853">
<div class="article-summary-box-inner">
<span><p>Autonomous agents have made great strides in specialist domains like Atari
games and Go. However, they typically learn tabula rasa in isolated
environments with limited and manually conceived objectives, thus failing to
generalize across a wide spectrum of tasks and capabilities. Inspired by how
humans continually learn and adapt in the open world, we advocate a trinity of
ingredients for building generalist agents: 1) an environment that supports a
multitude of tasks and goals, 2) a large-scale database of multimodal
knowledge, and 3) a flexible and scalable agent architecture. We introduce
MineDojo, a new framework built on the popular Minecraft game that features a
simulation suite with thousands of diverse open-ended tasks and an
internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and
forum discussions. Using MineDojo's data, we propose a novel agent learning
algorithm that leverages large pre-trained video-language models as a learned
reward function. Our agent is able to solve a variety of open-ended tasks
specified in free-form language without any manually designed dense shaping
reward. We open-source the simulation suite and knowledge bases
(https://minedojo.org) to promote research towards the goal of generally
capable embodied agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2206.08861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08861">
<div class="article-summary-box-inner">
<span><p>Multiple Instance Learning (MIL) is widely used in analyzing
histopathological Whole Slide Images (WSIs). However, existing MIL methods do
not explicitly model the data distribution, and instead they only learn a
bag-level or instance-level decision boundary discriminatively by training a
classifier. In this paper, we propose DGMIL: a feature distribution guided deep
MIL framework for WSI classification and positive patch localization. Instead
of designing complex discriminative network architectures, we reveal that the
inherent feature distribution of histopathological image data can serve as a
very effective guide for instance classification. We propose a
cluster-conditioned feature distribution modeling method and a pseudo
label-based iterative feature space refinement strategy so that in the final
feature space the positive and negative instances can be easily separated.
Experiments on the CAMELYON16 dataset and the TCGA Lung Cancer dataset show
that our method achieves new SOTA for both global classification and positive
patch localization tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Lossless Neural Compression with Integer-Only Discrete Flows. (arXiv:2206.08869v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08869">
<div class="article-summary-box-inner">
<span><p>By applying entropy codecs with learned data distributions, neural
compressors have significantly outperformed traditional codecs in terms of
compression ratio. However, the high inference latency of neural networks
hinders the deployment of neural compressors in practical applications. In this
work, we propose Integer-only Discrete Flows (IODF), an efficient neural
compressor with integer-only arithmetic. Our work is built upon integer
discrete flows, which consists of invertible transformations between discrete
random variables. We propose efficient invertible transformations with
integer-only arithmetic based on 8-bit quantization. Our invertible
transformation is equipped with learnable binary gates to remove redundant
filters during inference. We deploy IODF with TensorRT on GPUs, achieving 10x
inference speedup compared to the fastest existing neural compressors, while
retaining the high compression rates on ImageNet32 and ImageNet64.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalization of Metric Learning via Listwise Self-distillation. (arXiv:2206.08880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08880">
<div class="article-summary-box-inner">
<span><p>Most deep metric learning (DML) methods employ a strategy that forces all
positive samples to be close in the embedding space while keeping them away
from negative ones. However, such a strategy ignores the internal relationships
of positive (negative) samples and often leads to overfitting, especially in
the presence of hard samples and mislabeled samples. In this work, we propose a
simple yet effective regularization, namely Listwise Self-Distillation (LSD),
which progressively distills a model's own knowledge to adaptively assign a
more appropriate distance target to each sample pair in a batch. LSD encourages
smoother embeddings and information mining within positive (negative) samples
as a way to mitigate overfitting and thus improve generalization. Our LSD can
be directly integrated into general DML frameworks. Extensive experiments show
that LSD consistently boosts the performance of various metric learning methods
on multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge-Aided Sensor Data Sharing in Vehicular Communication Networks. (arXiv:2206.08882v1 [cs.MA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08882">
<div class="article-summary-box-inner">
<span><p>Sensor data sharing in vehicular networks can significantly improve the range
and accuracy of environmental perception for connected automated vehicles.
Different concepts and schemes for dissemination and fusion of sensor data have
been developed. It is common to these schemes that measurement errors of the
sensors impair the perception quality and can result in road traffic accidents.
Specifically, when the measurement error from the sensors (also referred as
measurement noise) is unknown and time varying, the performance of the data
fusion process is restricted, which represents a major challenge in the
calibration of sensors. In this paper, we consider sensor data sharing and
fusion in a vehicular network with both, vehicle-to-infrastructure and
vehicle-to-vehicle communication. We propose a method, named Bidirectional
Feedback Noise Estimation (BiFNoE), in which an edge server collects and caches
sensor measurement data from vehicles. The edge estimates the noise and the
targets alternately in double dynamic sliding time windows and enhances the
distributed cooperative environment sensing at each vehicle with low
communication costs. We evaluate the proposed algorithm and data dissemination
strategy in an application scenario by simulation and show that the perception
accuracy is on average improved by around 80 % with only 12 kbps uplink and 28
kbps downlink bandwidth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer. (arXiv:2206.08883v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08883">
<div class="article-summary-box-inner">
<span><p>Transformer has achieved great successes in learning vision and language
representation, which is general across various downstream tasks. In visual
control, learning transferable state representation that can transfer between
different control tasks is important to reduce the training sample size.
However, porting Transformer to sample-efficient visual control remains a
challenging and unsolved problem. To this end, we propose a novel Control
Transformer (CtrlFormer), possessing many appealing benefits that prior arts do
not have. Firstly, CtrlFormer jointly learns self-attention mechanisms between
visual tokens and policy tokens among different control tasks, where multitask
representation can be learned and transferred without catastrophic forgetting.
Secondly, we carefully design a contrastive reinforcement learning paradigm to
train CtrlFormer, enabling it to achieve high sample efficiency, which is
important in control problems. For example, in the DMControl benchmark, unlike
recent advanced methods that failed by producing a zero score in the "Cartpole"
task after transfer learning with 100k samples, CtrlFormer can achieve a
state-of-the-art score with only 100k samples while maintaining the performance
of previous tasks. The code and models are released in our project homepage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling. (arXiv:2206.08885v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08885">
<div class="article-summary-box-inner">
<span><p>Supervised learning tasks such as cancer survival prediction from gigapixel
whole slide images (WSIs) are a critical challenge in computational pathology
that requires modeling complex features of the tumor microenvironment. These
learning tasks are often solved with deep multi-instance learning (MIL) models
that do not explicitly capture intratumoral heterogeneity. We develop a novel
variance pooling architecture that enables a MIL model to incorporate
intratumoral heterogeneity into its predictions. Two interpretability tools
based on representative patches are illustrated to probe the biological signals
captured by these models. An empirical study with 4,479 gigapixel WSIs from the
Cancer Genome Atlas shows that adding variance pooling onto MIL frameworks
improves survival prediction performance for five cancer types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representational Multiplicity Should Be Exposed, Not Eliminated. (arXiv:2206.08890v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08890">
<div class="article-summary-box-inner">
<span><p>It is prevalent and well-observed, but poorly understood, that two machine
learning models with similar performance during training can have very
different real-world performance characteristics. This implies elusive
differences in the internals of the models, manifesting as representational
multiplicity (RM). We introduce a conceptual and experimental setup for
analyzing RM and show that certain training methods systematically result in
greater RM than others, measured by activation similarity via singular vector
canonical correlation analysis (SVCCA). We further correlate it with predictive
multiplicity measured by the variance in i.i.d. and out-of-distribution test
set predictions, in four common image data sets. We call for systematic
measurement and maximal exposure, not elimination, of RM in models. Qualitative
tools such as our confabulator analysis can facilitate understanding and
communication of RM effects to stakeholders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimA: Simple Softmax-free Attention for Vision Transformers. (arXiv:2206.08898v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08898">
<div class="article-summary-box-inner">
<span><p>Recently, vision transformers have become very popular. However, deploying
them in many applications is computationally expensive partly due to the
Softmax layer in the attention block. We introduce a simple but effective,
Softmax-free attention block, SimA, which normalizes query and key matrices
with simple $\ell_1$-norm instead of using Softmax layer. Then, the attention
block in SimA is a simple multiplication of three matrices, so SimA can
dynamically change the ordering of the computation at the test time to achieve
linear computation on the number of tokens or the number of channels. We
empirically show that SimA applied to three SOTA variations of transformers,
DeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models,
without any need for Softmax layer. Interestingly, changing SimA from
multi-head to single-head has only a small effect on the accuracy, which
simplifies the attention block further. The code is available here:
$\href{https://github.com/UCDvision/sima}{\text{This https URL}}$
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colonoscopy 3D Video Dataset with Paired Depth from 2D-3D Registration. (arXiv:2206.08903v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08903">
<div class="article-summary-box-inner">
<span><p>Screening colonoscopy is an important clinical application for several 3D
computer vision techniques, including depth estimation, surface reconstruction,
and missing region detection. However, the development, evaluation, and
comparison of these techniques in real colonoscopy videos remain largely
qualitative due to the difficulty of acquiring ground truth data. In this work,
we present a Colonoscopy 3D Video Dataset (C3VD) acquired with a high
definition clinical colonoscope and high-fidelity colon models for benchmarking
computer vision methods in colonoscopy. We introduce a novel multimodal 2D-3D
registration technique to register optical video sequences with ground truth
rendered views of a known 3D model. The different modalities are registered by
transforming optical images to depth maps with a Generative Adversarial Network
and aligning edge features with an evolutionary optimizer. This registration
method achieves an average translation error of 0.321 millimeters and an
average rotation error of 0.159 degrees in simulation experiments where
error-free ground truth is available. The method also leverages video
information, improving registration accuracy by 55.6% for translation and 60.4%
for rotation compared to single frame registration. 22 short video sequences
were registered to generate 10,015 total frames with paired ground truth depth,
surface normals, optical flow, occlusion, six degree-of-freedom pose, coverage
maps, and 3D models. The dataset also includes screening videos acquired by a
gastroenterologist with paired ground truth pose and 3D surface models. The
dataset and registration source code are available at durr.jhu.edu/C3VD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks. (arXiv:2206.08916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08916">
<div class="article-summary-box-inner">
<span><p>We propose Unified-IO, a model that performs a large variety of AI tasks
spanning classical computer vision tasks, including pose estimation, object
detection, depth estimation and image generation, vision-and-language tasks
such as region captioning and referring expression comprehension, to natural
language processing tasks such as question answering and paraphrasing.
Developing a single unified model for such a large variety of tasks poses
unique challenges due to the heterogeneous inputs and outputs pertaining to
each task, including RGB images, per-pixel maps, binary masks, bounding boxes,
and language. We achieve this unification by homogenizing every supported input
and output into a sequence of discrete vocabulary tokens. This common
representation across all tasks allows us to train a single transformer-based
architecture, jointly on over 80 diverse datasets in the vision and language
fields. Unified-IO is the first model capable of performing all 7 tasks on the
GRIT benchmark and produces strong results across 16 diverse benchmarks like
NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail,
with no task or benchmark specific fine-tuning. Demos for Unified-IO are
available at https://unified-io.allenai.org.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix. (arXiv:2206.08919v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08919">
<div class="article-summary-box-inner">
<span><p>Existing vision-language pre-training (VLP) methods primarily rely on paired
image-text datasets, which are either annotated by enormous human labors, or
crawled from the internet followed by elaborate data cleaning techniques. To
reduce the dependency on well-aligned image-text pairs, it is promising to
directly leverage the large-scale text-only and image-only corpora. This paper
proposes a data augmentation method, namely cross-modal CutMix (CMC), for
implicit cross-modal alignment learning in unpaired VLP. Specifically, CMC
transforms natural sentences from the textual view into a multi-modal view,
where visually-grounded words in a sentence are randomly replaced by diverse
image patches with similar semantics. There are several appealing proprieties
of the proposed CMC. First, it enhances the data diversity while keeping the
semantic meaning intact for tackling problems where the aligned data are
scarce; Second, by attaching cross-modal noise on uni-modal data, it guides
models to learn token-level interactions across modalities for better
denoising. Furthermore, we present a new unpaired VLP method, dubbed as
VLMixer, that integrates CMC with contrastive learning to pull together the
uni-modal and multi-modal views for better instance-level alignments among
different modalities. Extensive experiments on five downstream tasks show that
VLMixer could surpass previous state-of-the-art unpaired VLP methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VectorMapNet: End-to-end Vectorized HD Map Learning. (arXiv:2206.08920v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08920">
<div class="article-summary-box-inner">
<span><p>Autonomous driving systems require a good understanding of surrounding
environments, including moving obstacles and static High-Definition (HD)
semantic maps. Existing methods approach the semantic map problem by offline
manual annotations, which suffer from serious scalability issues. More recent
learning-based methods produce dense rasterized segmentation predictions which
do not include instance information of individual map elements and require
heuristic post-processing that involves many hand-designed components, to
obtain vectorized maps. To that end, we introduce an end-to-end vectorized HD
map learning pipeline, termed VectorMapNet. VectorMapNet takes onboard sensor
observations and predicts a sparse set of polylines primitives in the
bird's-eye view to model the geometry of HD maps. Based on this pipeline, our
method can explicitly model the spatial relation between map elements and
generate vectorized maps that are friendly for downstream autonomous driving
tasks without the need for post-processing. In our experiments, VectorMapNet
achieves strong HD map learning performance on nuScenes dataset, surpassing
previous state-of-the-art methods by 14.2 mAP. Qualitatively, we also show that
VectorMapNet is capable of generating comprehensive maps and capturing more
fine-grained details of road geometry. To the best of our knowledge,
VectorMapNet is the first work designed toward end-to-end vectorized HD map
learning problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-task Attention Mechanism for Dense Multi-task Learning. (arXiv:2206.08927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08927">
<div class="article-summary-box-inner">
<span><p>Multi-task learning has recently become a promising solution for a
comprehensive understanding of complex scenes. Not only being memory-efficient,
multi-task models with an appropriate design can favor exchange of
complementary signals across tasks. In this work, we jointly address 2D
semantic segmentation, and two geometry-related tasks, namely dense depth,
surface normal estimation as well as edge estimation showing their benefit on
indoor and outdoor datasets. We propose a novel multi-task learning
architecture that exploits pair-wise cross-task exchange through
correlation-guided attention and self-attention to enhance the average
representation learning for all tasks. We conduct extensive experiments
considering three multi-task setups, showing the benefit of our proposal in
comparison to competitive baselines in both synthetic and real benchmarks. We
also extend our method to the novel multi-task unsupervised domain adaptation
setting. Our code is available at https://github.com/cv-rits/DenseMTL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAVA: Template-free Animatable Volumetric Actors. (arXiv:2206.08929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08929">
<div class="article-summary-box-inner">
<span><p>Coordinate-based volumetric representations have the potential to generate
photo-realistic virtual avatars from images. However, virtual avatars also need
to be controllable even to a novel pose that may not have been observed.
Traditional techniques, such as LBS, provide such a function; yet it usually
requires a hand-designed body template, 3D scan data, and limited appearance
models. On the other hand, neural representation has been shown to be powerful
in representing visual details, but are under explored on deforming dynamic
articulated actors. In this paper, we propose TAVA, a method to create T
emplate-free Animatable Volumetric Actors, based on neural representations. We
rely solely on multi-view data and a tracked skeleton to create a volumetric
model of an actor, which can be animated at the test time given novel pose.
Since TAVA does not require a body template, it is applicable to humans as well
as other creatures such as animals. Furthermore, TAVA is designed such that it
can recover accurate dense correspondences, making it amenable to
content-creation and editing tasks. Through extensive experiments, we
demonstrate that the proposed method generalizes well to novel poses as well as
unseen views and showcase basic editing capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Nested Dropout. (arXiv:2101.11353v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11353">
<div class="article-summary-box-inner">
<span><p>Nested dropout is a variant of dropout operation that is able to order
network parameters or features based on the pre-defined importance during
training. It has been explored for: I. Constructing nested nets: the nested
nets are neural networks whose architectures can be adjusted instantly during
testing time, e.g., based on computational constraints. The nested dropout
implicitly ranks the network parameters, generating a set of sub-networks such
that any smaller sub-network forms the basis of a larger one. II. Learning
ordered representation: the nested dropout applied to the latent representation
of a generative model (e.g., auto-encoder) ranks the features, enforcing
explicit order of the dense representation over dimensions.
</p>
<p>However, the dropout rate is fixed as a hyper-parameter during the whole
training process. For nested nets, when network parameters are removed, the
performance decays in a human-specified trajectory rather than in a trajectory
learned from data. For generative models, the importance of features is
specified as a constant vector, restraining the flexibility of representation
learning. To address the problem, we focus on the probabilistic counterpart of
the nested dropout. We propose a variational nested dropout (VND) operation
that draws samples of multi-dimensional ordered masks at a low cost, providing
useful gradients to the parameters of nested dropout. Based on this approach,
we design a Bayesian nested neural network that learns the order knowledge of
the parameter distributions. We further exploit the VND under different
generative models for learning ordered latent distributions. In experiments, we
show that the proposed approach outperforms the nested network in terms of
accuracy, calibration, and out-of-domain detection in classification tasks. It
also outperforms the related generative models on data generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification. (arXiv:2103.04053v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04053">
<div class="article-summary-box-inner">
<span><p>Real-world large-scale medical image analysis (MIA) datasets have three
challenges: 1) they contain noisy-labelled samples that affect training
convergence and generalisation, 2) they usually have an imbalanced distribution
of samples per class, and 3) they normally comprise a multi-label problem,
where samples can have multiple diagnoses. Current approaches are commonly
trained to solve a subset of those problems, but we are unaware of methods that
address the three problems simultaneously. In this paper, we propose a new
training module called Non-Volatile Unbiased Memory (NVUM), which
non-volatility stores running average of model logits for a new regularization
loss on noisy multi-label problem. We further unbias the classification
prediction in NVUM update for imbalanced learning problem. We run extensive
experiments to evaluate NVUM on new benchmarks proposed by this paper, where
training is performed on noisy multi-label imbalanced chest X-ray (CXR)
training sets, formed by Chest-Xray14 and CheXpert, and the testing is
performed on the clean multi-label CXR datasets OpenI and PadChest. Our method
outperforms previous state-of-the-art CXR classifiers and previous methods that
can deal with noisy labels on all evaluations. Our code is available at
https://github.com/FBLADL/NVUM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional GANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10060">
<div class="article-summary-box-inner">
<span><p>Conditional generative models aim to learn the underlying joint distribution
of data and labels to achieve conditional data generation. Among them, the
auxiliary classifier generative adversarial network (AC-GAN) has been widely
used, but suffers from the problem of low intra-class diversity of the
generated samples. The fundamental reason pointed out in this paper is that the
classifier of AC-GAN is generator-agnostic, which therefore cannot provide
informative guidance for the generator to approach the joint distribution,
resulting in a minimization of the conditional entropy that decreases the
intra-class diversity. Motivated by this understanding, we propose a novel
conditional GAN with an auxiliary discriminative classifier (ADC-GAN) to
resolve the above problem. Specifically, the proposed auxiliary discriminative
classifier becomes generator-aware by recognizing the class-labels of the real
data and the generated data discriminatively. Our theoretical analysis reveals
that the generator can faithfully learn the joint distribution even without the
original discriminator, making the proposed ADC-GAN robust to the value of the
coefficient hyperparameter and the selection of the GAN loss, and stable during
training. Extensive experimental results on synthetic and real-world datasets
demonstrate the superiority of ADC-GAN in conditional generative modeling
compared to state-of-the-art classifier-based and projection-based conditional
GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Foot Ulcer Segmentation Using an Ensemble of Convolutional Neural Networks. (arXiv:2109.01408v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01408">
<div class="article-summary-box-inner">
<span><p>Foot ulcer is a common complication of diabetes mellitus and, associated with
substantial morbidity and mortality, remains a major risk factor for lower leg
amputations. Extracting accurate morphological features from foot wounds is
crucial for appropriate treatment. Although visual inspection by a medical
professional is the common approach for diagnosis, this is subjective and
error-prone, and computer-aided approaches thus provide an interesting
alternative. Deep learning-based methods, and in particular convolutional
neural networks (CNNs), have shown excellent performance for various tasks in
medical image analysis including medical image segmentation.
</p>
<p>In this paper, we propose an ensemble approach based on two
encoder-decoder-based CNN models, namely LinkNet and U-Net, to perform foot
ulcer segmentation. To deal with a limited number of available training
samples, we use pre-trained weights (EfficientNetB1 for the LinkNet model and
EfficientNetB2 for the U-Net model) and perform further pre-training using the
Medetec dataset while also applying a number of morphological-based and
colour-based augmentation techniques. To boost the segmentation performance, we
incorporate five-fold cross-validation, test time augmentation and result
fusion.
</p>
<p>Applied on the publicly available chronic wound dataset and the MICCAI 2021
Foot Ulcer Segmentation (FUSeg) Challenge, our method achieves state-of-the-art
performance with data-based Dice scores of 92.07% and 88.80%, respectively, and
is the top ranked method in the FUSeg challenge leaderboard. The Dockerised
guidelines, inference codes and saved trained models are publicly available at
https://github.com/masih4/Foot_Ulcer_Segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09818">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks have demonstrated dermatologist-level
performance in the classification of melanoma from skin lesion images, but
prediction irregularities due to biases seen within the training data are an
issue that should be addressed before widespread deployment is possible. In
this work, we robustly remove bias and spurious variation from an automated
melanoma classification pipeline using two leading bias unlearning techniques.
We show that the biases introduced by surgical markings and rulers presented in
previous studies can be reasonably mitigated using these bias removal methods.
We also demonstrate the generalisation benefits of unlearning spurious
variation relating to the imaging instrument used to capture lesion images. Our
experimental results provide evidence that the effects of each of the
aforementioned biases are notably reduced, with different debiasing techniques
excelling at different tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsolved Problems in ML Safety. (arXiv:2109.13916v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13916">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards ("Robustness"),
identifying hazards ("Monitoring"), reducing inherent model hazards
("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout,
we clarify each problem's motivation and provide concrete research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Fast Adversarial Training with Learnable Adversarial Initialization. (arXiv:2110.05007v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05007">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) has been demonstrated to be effective in improving
model robustness by leveraging adversarial examples for training. However, most
AT methods are in face of expensive time and computational cost for calculating
gradients at multiple steps in generating adversarial examples. To boost
training efficiency, fast gradient sign method (FGSM) is adopted in fast AT
methods by calculating gradient only once. Unfortunately, the robustness is far
from satisfactory. One reason may arise from the initialization fashion.
Existing fast AT generally uses a random sample-agnostic initialization, which
facilitates the efficiency yet hinders a further robustness improvement. Up to
now, the initialization in fast AT is still not extensively explored. In this
paper, we boost fast AT with a sample-dependent adversarial initialization,
i.e., an output from a generative network conditioned on a benign image and its
gradient information from the target network. As the generative network and the
target network are optimized jointly in the training phase, the former can
adaptively generate an effective initialization with respect to the latter,
which motivates gradually improved robustness. Experimental evaluations on four
benchmark databases demonstrate the superiority of our proposed method over
state-of-the-art fast AT methods, as well as comparable robustness to advanced
multi-step AT methods. The code is released at
https://github.com//jiaxiaojunQAQ//FGSM-SDI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">whu-nercms at trecvid2021:instance search task. (arXiv:2111.00228v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00228">
<div class="article-summary-box-inner">
<span><p>We will make a brief introduction of the experimental methods and results of
the WHU-NERCMS in the TRECVID2021 in the paper. This year we participate in the
automatic and interactive tasks of Instance Search (INS). For the automatic
task, the retrieval target is divided into two parts, person retrieval, and
action retrieval. We adopt a two-stage method including face detection and face
recognition for person retrieval and two kinds of action detection methods
consisting of three frame-based human-object interaction detection methods and
two video-based general action detection methods for action retrieval. After
that, the person retrieval results and action retrieval results are fused to
initialize the result ranking lists. In addition, we make attempts to use
complementary methods to further improve search performance. For interactive
tasks, we test two different interaction strategies on the fusion results. We
submit 4 runs for automatic and interactive tasks respectively. The
introduction of each run is shown in Table 1. The official evaluations show
that the proposed strategies rank 1st in both automatic and interactive tracks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Triangular 3D Models, Materials, and Lighting From Images. (arXiv:2111.12503v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12503">
<div class="article-summary-box-inner">
<span><p>We present an efficient method for joint optimization of topology, materials
and lighting from multi-view image observations. Unlike recent multi-view
reconstruction approaches, which typically produce entangled 3D representations
encoded in neural networks, we output triangle meshes with spatially-varying
materials and environment lighting that can be deployed in any traditional
graphics engine unmodified. We leverage recent work in differentiable
rendering, coordinate-based networks to compactly represent volumetric
texturing, alongside differentiable marching tetrahedrons to enable
gradient-based optimization directly on the surface mesh. Finally, we introduce
a differentiable formulation of the split sum approximation of environment
lighting to efficiently recover all-frequency lighting. Experiments show our
extracted models used in advanced scene editing, material decomposition, and
high quality view interpolation, all running at interactive rates in
triangle-based renderers (rasterizers and path tracers). Project website:
https://nvlabs.github.io/nvdiffrec/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions. (arXiv:2111.14813v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14813">
<div class="article-summary-box-inner">
<span><p>Removing adverse weather conditions like rain, fog, and snow from images is
an important problem in many applications. Most methods proposed in the
literature have been designed to deal with just removing one type of
degradation. Recently, a CNN-based method using neural architecture search
(All-in-One) was proposed to remove all the weather conditions at once.
However, it has a large number of parameters as it uses multiple encoders to
cater to each weather removal task and still has scope for improvement in its
performance. In this work, we focus on developing an efficient solution for the
all adverse weather removal problem. To this end, we propose TransWeather, a
transformer-based end-to-end model with just a single encoder and a decoder
that can restore an image degraded by any weather condition. Specifically, we
utilize a novel transformer encoder using intra-patch transformer blocks to
enhance attention inside the patches to effectively remove smaller weather
degradations. We also introduce a transformer decoder with learnable weather
type embeddings to adjust to the weather degradation at hand. TransWeather
achieves improvements across multiple test datasets over both All-in-One
network as well as methods fine-tuned for specific tasks. TransWeather is also
validated on real world test images and found to be more effective than
previous methods. Implementation code can be accessed at
https://github.com/jeya-maria-jose/TransWeather .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03857">
<div class="article-summary-box-inner">
<span><p>This paper presents a grounded language-image pre-training (GLIP) model for
learning object-level, language-aware, and semantic-rich visual
representations. GLIP unifies object detection and phrase grounding for
pre-training. The unification brings two benefits: 1) it allows GLIP to learn
from both detection and grounding data to improve both tasks and bootstrap a
good grounding model; 2) GLIP can leverage massive image-text pairs by
generating grounding boxes in a self-training fashion, making the learned
representation semantic-rich. In our experiments, we pre-train GLIP on 27M
grounding data, including 3M human-annotated and 24M web-crawled image-text
pairs. The learned representations demonstrate strong zero-shot and few-shot
transferability to various object-level recognition tasks. 1) When directly
evaluated on COCO and LVIS (without seeing any images in COCO during
pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many
supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val
and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13
downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised
Dynamic Head. Code is released at https://github.com/microsoft/GLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning. (arXiv:2201.10029v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10029">
<div class="article-summary-box-inner">
<span><p>State-of-the-art approaches to ObjectGoal navigation rely on reinforcement
learning and typically require significant computational resources and time for
learning. We propose Potential functions for ObjectGoal Navigation with
Interaction-free learning (PONI), a modular approach that disentangles the
skills of `where to look?' for an object and `how to navigate to (x, y)?'. Our
key insight is that `where to look?' can be treated purely as a perception
problem, and learned without environment interactions. To address this, we
propose a network that predicts two complementary potential functions
conditioned on a semantic map and uses them to decide where to look for an
unseen object. We train the potential function network using supervised
learning on a passive dataset of top-down semantic maps, and integrate it into
a modular framework to perform ObjectGoal navigation. Experiments on Gibson and
Matterport3D demonstrate that our method achieves the state-of-the-art for
ObjectGoal navigation while incurring up to 1,600x less computational cost for
training. Code and pre-trained models are available:
https://vision.cs.utexas.edu/projects/poni/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning To Recognize Procedural Activities with Distant Supervision. (arXiv:2201.10990v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10990">
<div class="article-summary-box-inner">
<span><p>In this paper we consider the problem of classifying fine-grained, multi-step
activities (e.g., cooking different recipes, making disparate home
improvements, creating various forms of arts and crafts) from long videos
spanning up to several minutes. Accurately categorizing these activities
requires not only recognizing the individual steps that compose the task but
also capturing their temporal dependencies. This problem is dramatically
different from traditional action classification, where models are typically
optimized on videos that span only a few seconds and that are manually trimmed
to contain simple atomic actions. While step annotations could enable the
training of models to recognize the individual steps of procedural activities,
existing large-scale datasets in this area do not include such segment labels
due to the prohibitive cost of manually annotating temporal boundaries in long
videos. To address this issue, we propose to automatically identify steps in
instructional videos by leveraging the distant supervision of a textual
knowledge base (wikiHow) that includes detailed descriptions of the steps
needed for the execution of a wide variety of complex activities. Our method
uses a language model to match noisy, automatically-transcribed speech from the
video to step descriptions in the knowledge base. We demonstrate that video
models trained to recognize these automatically-labeled steps (without manual
supervision) yield a representation that achieves superior generalization
performance on four downstream tasks: recognition of procedural activities,
step classification, step forecasting and egocentric video classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PINs: Progressive Implicit Networks for Multi-Scale Neural Representations. (arXiv:2202.04713v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04713">
<div class="article-summary-box-inner">
<span><p>Multi-layer perceptrons (MLP) have proven to be effective scene encoders when
combined with higher-dimensional projections of the input, commonly referred to
as \textit{positional encoding}. However, scenes with a wide frequency spectrum
remain a challenge: choosing high frequencies for positional encoding
introduces noise in low structure areas, while low frequencies result in poor
fitting of detailed regions. To address this, we propose a progressive
positional encoding, exposing a hierarchical MLP structure to incremental sets
of frequency encodings. Our model accurately reconstructs scenes with wide
frequency bands and learns a scene representation at progressive level of
detail \textit{without explicit per-level supervision}. The architecture is
modular: each level encodes a continuous implicit representation that can be
leveraged separately for its respective resolution, meaning a smaller network
for coarser reconstructions. Experiments on several 2D and 3D datasets show
improvements in reconstruction accuracy, representational capacity and training
speed compared to baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artemis: Articulated Neural Pets with Appearance and Motion synthesis. (arXiv:2202.05628v3 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05628">
<div class="article-summary-box-inner">
<span><p>We, humans, are entering into a virtual era and indeed want to bring animals
to the virtual world as well for companion. Yet, computer-generated (CGI) furry
animals are limited by tedious off-line rendering, let alone interactive motion
control. In this paper, we present ARTEMIS, a novel neural modeling and
rendering pipeline for generating ARTiculated neural pets with appEarance and
Motion synthesIS. Our ARTEMIS enables interactive motion control, real-time
animation, and photo-realistic rendering of furry animals. The core of our
ARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient
octree-based representation for animal animation and fur rendering. The
animation then becomes equivalent to voxel-level deformation based on explicit
skeletal warping. We further use a fast octree indexing and efficient
volumetric rendering scheme to generate appearance and density features maps.
Finally, we propose a novel shading network to generate high-fidelity details
of appearance and opacity under novel poses from appearance and density feature
maps. For the motion control module in ARTEMIS, we combine state-of-the-art
animal motion capture approach with recent neural character control scheme. We
introduce an effective optimization scheme to reconstruct the skeletal motion
of real animals captured by a multi-view RGB and Vicon camera array. We feed
all the captured motion into a neural character control scheme to generate
abstract control signals with motion styles. We further integrate ARTEMIS into
existing engines that support VR headsets, providing an unprecedented immersive
experience where a user can intimately interact with a variety of virtual
animals with vivid movements and photo-realistic appearance. We make available
our ARTEMIS model and dynamic furry animal dataset at
https://haiminluo.github.io/publication/artemis/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Borrowing from yourself: Faster future video segmentation with partial channel update. (arXiv:2202.05748v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05748">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is a well-addressed topic in the computer vision
literature, but the design of fast and accurate video processing networks
remains challenging. In addition, to run on embedded hardware, computer vision
models often have to make compromises on accuracy to run at the required speed,
so that a latency/accuracy trade-off is usually at the heart of these real-time
systems' design. For the specific case of videos, models have the additional
possibility to make use of computations made for previous frames to mitigate
the accuracy loss while being real-time.
</p>
<p>In this work, we propose to tackle the task of fast future video segmentation
prediction through the use of convolutional layers with time-dependent channel
masking. This technique only updates a chosen subset of the feature maps at
each time-step, bringing simultaneously less computation and latency, and
allowing the network to leverage previously computed features. We apply this
technique to several fast architectures and experimentally confirm its benefits
for the future prediction subtask.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06767">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pre-training (VLP) models have shown remarkable performance
on various downstream tasks. Their success heavily relies on the scale of
pre-trained cross-modal datasets. However, the lack of large-scale datasets and
benchmarks in Chinese hinders the development of Chinese VLP models and broader
multilingual applications. In this work, we release a large-scale Chinese
cross-modal dataset named Wukong, which contains 100 million Chinese image-text
pairs collected from the web. Wukong aims to benchmark different multi-modal
pre-training methods to facilitate the VLP research and community development.
Furthermore, we release a group of models pre-trained with various image
encoders (ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques
into VLP such as locked-image text tuning, token-wise similarity in contrastive
learning, and reduced-token interaction. Extensive experiments and a
benchmarking of different downstream tasks including a new largest
human-verified image-text test dataset are also provided. Experiments show that
Wukong can serve as a promising Chinese pre-training dataset and benchmark for
different cross-modal learning methods. For the zero-shot image classification
task on 10 datasets, $Wukong_{ViT-L}$ achieves an average accuracy of 73.03%.
For the image-text retrieval task, it achieves a mean recall of 71.6% on
AIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are
benchmarked on downstream tasks with other variants on multiple datasets, e.g.,
Flickr8K-CN, Flickr-30K-CN, COCO-CN, et~al. More information can be referred
to: https://wukong-dataset.github.io/wukong-dataset/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BED: A Real-Time Object Detection System for Edge Devices. (arXiv:2202.07503v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07503">
<div class="article-summary-box-inner">
<span><p>Deploying deep neural networks~(DNNs) on edge devices provides efficient and
effective solutions for the real-world tasks. Edge devices have been used for
collecting a large volume of data efficiently in different domains. DNNs have
been an effective tool for data processing and analysis. However, designing
DNNs on edge devices is challenging due to the limited computational resources
and memory. To tackle this challenge, we demonstrate Object Detection System
for Edge Devices~(BED) on the MAX78000 DNN accelerator. It integrates on-device
DNN inference with a camera and an LCD display for image acquisition and
detection exhibition, respectively. BED is a concise, effective and detailed
solution, including model training, quantization, synthesis and deployment.
Experiment results indicate that BED can produce accurate detection with a
300-KB tiny DNN model, which takes only 91.9 ms of inference time and 1.845 mJ
of energy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Vision Transformer. (arXiv:2203.03821v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03821">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have made many breakthroughs in computer vision
tasks. However, considerable redundancy arises in the spatial dimension of an
input image, leading to massive computational costs. Therefore, We propose a
coarse-to-fine vision transformer (CF-ViT) to relieve computational burden
while retaining performance in this paper. Our proposed CF-ViT is motivated by
two important observations in modern ViT models: (1) The coarse-grained patch
splitting can locate informative regions of an input image. (2) Most images can
be well recognized by a ViT model in a small-length token sequence. Therefore,
our CF-ViT implements network inference in a two-stage manner. At coarse
inference stage, an input image is split into a small-length patch sequence for
a computationally economical classification. If not well recognized, the
informative patches are identified and further re-split in a fine-grained
granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For
example, without any compromise on performance, CF-ViT reduces 53% FLOPs of
LV-ViT, and also achieves 2.01x throughput.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetAug: Contrastive Learning via Meta Feature Augmentation. (arXiv:2203.05119v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05119">
<div class="article-summary-box-inner">
<span><p>What matters for contrastive learning? We argue that contrastive learning
heavily relies on informative features, or "hard" (positive or negative)
features. Early works include more informative features by applying complex
data augmentations and large batch size or memory bank, and recent works design
elaborate sampling approaches to explore informative features. The key
challenge toward exploring such features is that the source multi-view data is
generated by applying random data augmentations, making it infeasible to always
add useful information in the augmented data. Consequently, the informativeness
of features learned from such augmented data is limited. In response, we
propose to directly augment the features in latent space, thereby learning
discriminative representations without a large amount of input data. We perform
a meta learning technique to build the augmentation generator that updates its
network parameters by considering the performance of the encoder. However,
insufficient input data may lead the encoder to learn collapsed features and
therefore malfunction the augmentation generator. A new margin-injected
regularization is further added in the objective function to avoid the encoder
learning a degenerate mapping. To contrast all features in one gradient
back-propagation step, we adopt the proposed optimization-driven unified
contrastive loss instead of the conventional contrastive loss. Empirically, our
method achieves state-of-the-art results on several benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers. (arXiv:2203.10726v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10726">
<div class="article-summary-box-inner">
<span><p>Combining information from multi-view images is crucial to improve the
performance and robustness of automated methods for disease diagnosis. However,
due to the non-alignment characteristics of multi-view images, building
correlation and data fusion across views largely remain an open problem. In
this study, we present TransFusion, a Transformer-based architecture to merge
divergent multi-view imaging information using convolutional layers and
powerful attention mechanisms. In particular, the Divergent Fusion Attention
(DiFA) module is proposed for rich cross-view context modeling and semantic
dependency mining, addressing the critical issue of capturing long-range
correlations between unaligned data from different image views. We further
propose the Multi-Scale Attention (MSA) to collect global correspondence of
multi-scale feature representations. We evaluate TransFusion on the
Multi-Disease, Multi-View \&amp; Multi-Center Right Ventricular Segmentation in
Cardiac MRI (M\&amp;Ms-2) challenge cohort. TransFusion demonstrates leading
performance against the state-of-the-art methods and opens up new perspectives
for multi-view imaging integration towards robust medical image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning with Action-Free Pre-Training from Videos. (arXiv:2203.13880v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13880">
<div class="article-summary-box-inner">
<span><p>Recent unsupervised pre-training methods have shown to be effective on
language and vision domains by learning useful representations for multiple
downstream tasks. In this paper, we investigate if such unsupervised
pre-training methods can also be effective for vision-based reinforcement
learning (RL). To this end, we introduce a framework that learns
representations useful for understanding the dynamics via generative
pre-training on videos. Our framework consists of two phases: we pre-train an
action-free latent video prediction model, and then utilize the pre-trained
representations for efficiently learning action-conditional world models on
unseen environments. To incorporate additional action inputs during
fine-tuning, we introduce a new architecture that stacks an action-conditional
latent prediction model on top of the pre-trained action-free prediction model.
Moreover, for better exploration, we propose a video-based intrinsic bonus that
leverages pre-trained representations. We demonstrate that our framework
significantly improves both final performances and sample-efficiency of
vision-based RL in a variety of manipulation and locomotion tasks. Code is
available at https://github.com/younggyoseo/apv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UV Volumes for Real-time Rendering of Editable Free-view Human Performance. (arXiv:2203.14402v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14402">
<div class="article-summary-box-inner">
<span><p>Neural volume rendering enables photo-realistic renderings of a human
performer in free-view, a critical task in immersive VR/AR applications. But
the practice is severely limited by high computational costs in the rendering
process. To solve this problem, we propose the UV Volumes, a new approach that
can render an editable free-view video of a human performer in realtime. It
separates the high-frequency (i.e., non-smooth) human appearance from the 3D
volume, and encodes them into 2D neural texture stacks (NTS). The smooth UV
volumes allow much smaller and shallower neural networks to obtain densities
and texture coordinates in 3D while capturing detailed appearance in 2D NTS.
For editability, the mapping between the parameterized human model and the
smooth texture coordinates allows us a better generalization on novel poses and
shapes. Furthermore, the use of NTS enables interesting applications, e.g.,
retexturing. Extensive experiments on CMU Panoptic, ZJU Mocap, and H36M
datasets show that our model can render 960 * 540 images in 30FPS on average
with comparable photo-realism to state-of-the-art methods. The project and
supplementary materials are available at https://github.com/fanegg/UV-Volumes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI. (arXiv:2203.15163v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15163">
<div class="article-summary-box-inner">
<span><p>Prostate cancer is the second leading cause of cancer death among men in the
United States. The diagnosis of prostate MRI often relies on the accurate
prostate zonal segmentation. However, state-of-the-art automatic segmentation
methods often fail to produce well-contained volumetric segmentation of the
prostate zones since certain slices of prostate MRI, such as base and apex
slices, are harder to segment than other slices. This difficulty can be
overcome by accounting for the cross-slice relationship of adjacent slices, but
current methods do not fully learn and exploit such relationships. In this
paper, we propose a novel cross-slice attention mechanism, which we use in a
Transformer module to systematically learn the cross-slice relationship at
different scales. The module can be utilized in any existing learning-based
segmentation framework with skip connections. Experiments show that our
cross-slice attention is able to capture the cross-slice information in
prostate zonal segmentation and improve the performance of current
state-of-the-art methods. Our method improves segmentation accuracy in the
peripheral zone, such that the segmentation results are consistent across all
the prostate slices (apex, mid-gland, and base).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Weakly Supervised Object Detection by Sampling Pseudo Ground-Truth Boxes. (arXiv:2204.00147v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00147">
<div class="article-summary-box-inner">
<span><p>Semi- and weakly-supervised learning have recently attracted considerable
attention in the object detection literature since they can alleviate the cost
of annotation needed to successfully train deep learning models. State-of-art
approaches for semi-supervised learning rely on student-teacher models trained
using a multi-stage process, and considerable data augmentation. Custom
networks have been developed for the weakly-supervised setting, making it
difficult to adapt to different detectors. In this paper, a weakly
semi-supervised training method is introduced that reduces these training
challenges, yet achieves state-of-the-art performance by leveraging only a
small fraction of fully-labeled images with information in weakly-labeled
images. In particular, our generic sampling-based learning strategy produces
pseudo-ground-truth (GT) bounding box annotations in an online fashion,
eliminating the need for multi-stage training, and student-teacher network
configurations. These pseudo GT boxes are sampled from weakly-labeled images
based on the categorical score of object proposals accumulated via a score
propagation process. Empirical results on the Pascal VOC dataset, indicate that
the proposed approach improves performance by 5.0% when using VOC 2007 as
fully-labeled, and VOC 2012 as weak-labeled data. Also, with 5-10% fully
annotated images, we observed an improvement of more than 10% in mAP, showing
that a modest investment in image-level annotation, can substantially improve
detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out-of-Distribution Detection with Deep Nearest Neighbors. (arXiv:2204.06507v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06507">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection is a critical task for deploying machine
learning models in the open world. Distance-based methods have demonstrated
promise, where testing samples are detected as OOD if they are relatively far
away from in-distribution (ID) data. However, prior methods impose a strong
distributional assumption of the underlying feature space, which may not always
hold. In this paper, we explore the efficacy of non-parametric nearest-neighbor
distance for OOD detection, which has been largely overlooked in the
literature. Unlike prior works, our method does not impose any distributional
assumption, hence providing stronger flexibility and generality. We demonstrate
the effectiveness of nearest-neighbor-based OOD detection on several benchmarks
and establish superior performance. Under the same model trained on
ImageNet-1k, our method substantially reduces the false positive rate
(FPR@TPR95) by 24.77% compared to a strong baseline SSD+, which uses a
parametric approach Mahalanobis distance in detection. Code is available:
https://github.com/deeplearning-wisc/knn-ood.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognising Known Configurations of Garments For Dual-Arm Robotic Flattening. (arXiv:2205.00225v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00225">
<div class="article-summary-box-inner">
<span><p>Robotic deformable-object manipulation is a challenge in the robotic industry
because deformable objects have complicated and various object states.
Predicting those object states and updating manipulation planning is
time-consuming and computationally expensive. In this paper, we propose
learning known configurations of garments to allow a robot to recognise garment
states and choose a pre-designed manipulation plan for garment flattening.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction. (arXiv:2205.06672v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06672">
<div class="article-summary-box-inner">
<span><p>Classical multiple instance learning (MIL) methods are often based on the
identical and independent distributed assumption between instances, hence
neglecting the potentially rich contextual information beyond individual
entities. On the other hand, Transformers with global self-attention modules
have been proposed to model the interdependencies among all instances. However,
in this paper we question: Is global relation modeling using self-attention
necessary, or can we appropriately restrict self-attention calculations to
local regimes in large-scale whole slide images (WSIs)? We propose a
general-purpose local attention graph-based Transformer for MIL (LA-MIL),
introducing an inductive bias by explicitly contextualizing instances in
adaptive local regimes of arbitrary size. Additionally, an efficiently adapted
loss function enables our approach to learn expressive WSI embeddings for the
joint analysis of multiple biomarkers. We demonstrate that LA-MIL achieves
state-of-the-art results in mutation prediction for gastrointestinal cancer,
outperforming existing models on important biomarkers such as microsatellite
instability for colorectal cancer. Our findings suggest that local
self-attention sufficiently models dependencies on par with global modules. Our
LA-MIL implementation is available at https://github.com/agentdr1/LA_MIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Split Computing for Efficient Deep Edge Intelligence. (arXiv:2205.11269v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11269">
<div class="article-summary-box-inner">
<span><p>Deploying deep neural networks (DNNs) on IoT and mobile devices is a
challenging task due to their limited computational resources. Thus, demanding
tasks are often entirely offloaded to edge servers which can accelerate
inference, however, it also causes communication cost and evokes privacy
concerns. In addition, this approach leaves the computational capacity of end
devices unused. Split computing is a paradigm where a DNN is split into two
sections; the first section is executed on the end device, and the output is
transmitted to the edge server where the final section is executed. Here, we
introduce dynamic split computing, where the optimal split location is
dynamically selected based on the state of the communication channel. By using
natural bottlenecks that already exist in modern DNN architectures, dynamic
split computing avoids retraining and hyperparameter optimization, and does not
have any negative impact on the final accuracy of DNNs. Through extensive
experiments, we show that dynamic split computing achieves faster inference in
edge computing environments where the data rate and server load vary over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Interpretability via Polynomials. (arXiv:2205.14108v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14108">
<div class="article-summary-box-inner">
<span><p>Generalized Additive Models (GAMs) have quickly become the leading choice for
fully-interpretable machine learning. However, unlike uninterpretable methods
such as DNNs, they lack expressive power and easy scalability, and are hence
not a feasible alternative for real-world tasks. We present a new class of GAMs
that use tensor rank decompositions of polynomials to learn powerful, {\em
fully-interpretable} models. Our approach, titled Scalable Polynomial Additive
Models (SPAM) is effortlessly scalable and models {\em all} higher-order
feature interactions without a combinatorial parameter explosion. SPAM
outperforms all current interpretable approaches, and matches DNN/XGBoost
performance on a series of real-world benchmarks with up to hundreds of
thousands of features. We demonstrate by human subject evaluations that SPAMs
are demonstrably more interpretable in practice, and are hence an effortless
replacement for DNNs for creating interpretable and high-performance systems
suitable for large-scale machine learning. Source code is available at
https://github.com/facebookresearch/nbm-spam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Basis Models for Interpretability. (arXiv:2205.14120v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14120">
<div class="article-summary-box-inner">
<span><p>Due to the widespread use of complex machine learning models in real-world
applications, it is becoming critical to explain model predictions. However,
these models are typically black-box deep neural networks, explained post-hoc
via methods with known faithfulness limitations. Generalized Additive Models
(GAMs) are an inherently interpretable class of models that address this
limitation by learning a non-linear shape function for each feature separately,
followed by a linear model on top. However, these models are typically
difficult to train, require numerous parameters, and are difficult to scale.
</p>
<p>We propose an entirely new subfamily of GAMs that utilizes basis
decomposition of shape functions. A small number of basis functions are shared
among all features, and are learned jointly for a given task, thus making our
model scale much better to large-scale data with high-dimensional features,
especially when features are sparse. We propose an architecture denoted as the
Neural Basis Model (NBM) which uses a single neural network to learn these
bases. On a variety of tabular and image datasets, we demonstrate that for
interpretable machine learning, NBMs are the state-of-the-art in accuracy,
model size, and, throughput and can easily model all higher-order feature
interactions.
</p>
<p>Source code is available at https://github.com/facebookresearch/nbm-spam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks. (arXiv:2206.00843v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00843">
<div class="article-summary-box-inner">
<span><p>Efficient deep neural network (DNN) models equipped with compact operators
(e.g., depthwise convolutions) have shown great potential in reducing DNNs'
theoretical complexity (e.g., the total number of weights/operations) while
maintaining a decent model accuracy. However, existing efficient DNNs are still
limited in fulfilling their promise in boosting real-hardware efficiency, due
to their commonly adopted compact operators' low hardware utilization. In this
work, we open up a new compression paradigm for developing real-hardware
efficient DNNs, leading to boosted hardware efficiency while maintaining model
accuracy. Interestingly, we observe that while some DNN layers' activation
functions help DNNs' training optimization and achievable accuracy, they can be
properly removed after training without compromising the model accuracy.
Inspired by this observation, we propose a framework dubbed DepthShrinker,
which develops hardware-friendly compact networks via shrinking the basic
building blocks of existing efficient DNNs that feature irregular computation
patterns into dense ones with much improved hardware utilization and thus
real-hardware efficiency. Excitingly, our DepthShrinker framework delivers
hardware-friendly compact networks that outperform both state-of-the-art
efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and
1.53$\times$ throughput on Tesla V100 over SOTA channel-wise pruning method
MetaPruning. Our codes are available at:
https://github.com/facebookresearch/DepthShrinker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supernet Training for Federated Image Classification under System Heterogeneity. (arXiv:2206.01366v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01366">
<div class="article-summary-box-inner">
<span><p>Efficient deployment of deep neural networks across many devices and resource
constraints, especially on edge devices, is one of the most challenging
problems in the presence of data-privacy preservation issues. Conventional
approaches have evolved to either improve a single global model while keeping
each local training data decentralized (i.e., data-heterogeneity) or to train a
once-for-all network that supports diverse architectural settings to address
heterogeneous systems equipped with different computational capabilities (i.e.,
model-heterogeneity). However, little research has considered both directions
simultaneously. In this work, we propose a novel framework to consider both
scenarios, namely Federation of Supernet Training (FedSup), where clients send
and receive a supernet whereby it contains all possible architectures sampled
from itself. It is inspired by how averaging parameters in the model
aggregation stage of Federated Learning (FL) is similar to weight-sharing in
supernet training. Specifically, in the FedSup framework, a weight-sharing
approach widely used in the training single shot model is combined with the
averaging of Federated Learning (FedAvg). Under our framework, we present an
efficient algorithm (E-FedSup) by sending the sub-model to clients in the
broadcast stage for reducing communication costs and training overhead. We
demonstrate several strategies to enhance supernet training in the FL
environment and conduct extensive empirical evaluations. The resulting
framework is shown to pave the way for the robustness of both data- and
model-heterogeneity on several standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Video Restoration Transformer with Guided Deformable Attention. (arXiv:2206.02146v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02146">
<div class="article-summary-box-inner">
<span><p>Video restoration aims at restoring multiple high-quality frames from
multiple low-quality frames. Existing video restoration methods generally fall
into two extreme cases, i.e., they either restore all frames in parallel or
restore the video frame by frame in a recurrent way, which would result in
different merits and drawbacks. Typically, the former has the advantage of
temporal information fusion. However, it suffers from large model size and
intensive memory consumption; the latter has a relatively small model size as
it shares parameters across frames; however, it lacks long-range dependency
modeling ability and parallelizability. In this paper, we attempt to integrate
the advantages of the two cases by proposing a recurrent video restoration
transformer, namely RVRT. RVRT processes local neighboring frames in parallel
within a globally recurrent framework which can achieve a good trade-off
between model size, effectiveness, and efficiency. Specifically, RVRT divides
the video into multiple clips and uses the previously inferred clip feature to
estimate the subsequent clip feature. Within each clip, different frame
features are jointly updated with implicit feature aggregation. Across
different clips, the guided deformable attention is designed for clip-to-clip
alignment, which predicts multiple relevant locations from the whole inferred
clip and aggregates their features by the attention mechanism. Extensive
experiments on video super-resolution, deblurring, and denoising show that the
proposed RVRT achieves state-of-the-art performance on benchmark datasets with
balanced model size, testing memory and runtime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDQ: Stochastic Differentiable Quantization with Mixed Precision. (arXiv:2206.04459v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04459">
<div class="article-summary-box-inner">
<span><p>In order to deploy deep models in a computationally efficient manner, model
quantization approaches have been frequently used. In addition, as new hardware
that supports mixed bitwidth arithmetic operations, recent research on mixed
precision quantization (MPQ) begins to fully leverage the capacity of
representation by searching optimized bitwidths for different layers and
modules in a network. However, previous studies mainly search the MPQ strategy
in a costly scheme using reinforcement learning, neural architecture search,
etc., or simply utilize partial prior knowledge for bitwidth assignment, which
might be biased and sub-optimal. In this work, we present a novel Stochastic
Differentiable Quantization (SDQ) method that can automatically learn the MPQ
strategy in a more flexible and globally-optimized space with smoother gradient
approximation. Particularly, Differentiable Bitwidth Parameters (DBPs) are
employed as the probability factors in stochastic quantization between adjacent
bitwidth choices. After the optimal MPQ strategy is acquired, we further train
our network with entropy-aware bin regularization and knowledge distillation.
We extensively evaluate our method for several networks on different hardware
(GPUs and FPGA) and datasets. SDQ outperforms all state-of-the-art mixed or
single precision quantization with a lower bitwidth and is even better than the
full-precision counterparts across various ResNet and MobileNet families,
demonstrating the effectiveness and superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometrically Guided Integrated Gradients. (arXiv:2206.05903v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05903">
<div class="article-summary-box-inner">
<span><p>Interpretability methods for deep neural networks mainly focus on the
sensitivity of the class score with respect to the original or perturbed input,
usually measured using actual or modified gradients. Some methods also use a
model-agnostic approach to understanding the rationale behind every prediction.
In this paper, we argue and demonstrate that local geometry of the model
parameter space relative to the input can also be beneficial for improved
post-hoc explanations. To achieve this goal, we introduce an interpretability
method called "geometrically-guided integrated gradients" that builds on top of
the gradient calculation along a linear path as traditionally used in
integrated gradient methods. However, instead of integrating gradient
information, our method explores the model's dynamic behavior from multiple
scaled versions of the input and captures the best possible attribution for
each input. We demonstrate through extensive experiments that the proposed
approach outperforms vanilla and integrated gradients in subjective and
quantitative assessment. We also propose a "model perturbation" sanity check to
complement the traditionally used "model randomization" test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Decoder-free Object Detection with Transformers. (arXiv:2206.06829v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06829">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) are changing the landscape of object detection
approaches. A natural usage of ViTs in detection is to replace the CNN-based
backbone with a transformer-based backbone, which is straightforward and
effective, with the price of bringing considerable computation burden for
inference. More subtle usage is the DETR family, which eliminates the need for
many hand-designed components in object detection but introduces a decoder
demanding an extra-long time to converge. As a result, transformer-based object
detection can not prevail in large-scale applications. To overcome these
issues, we propose a novel decoder-free fully transformer-based (DFFT) object
detector, achieving high efficiency in both training and inference stages, for
the first time. We simplify objection detection into an encoder-only
single-level anchor-based dense prediction problem by centering around two
entry points: 1) Eliminate the training-inefficient decoder and leverage two
strong encoders to preserve the accuracy of single-level feature map
prediction; 2) Explore low-level semantic features for the detection task with
limited computational resources. In particular, we design a novel lightweight
detection-oriented transformer backbone that efficiently captures low-level
features with rich semantics based on a well-conceived ablation study.
Extensive experiments on the MS COCO benchmark demonstrate that DFFT_SMALL
outperforms DETR by 2.5% AP with 28% computation cost reduction and more than
$10$x fewer training epochs. Compared with the cutting-edge anchor-based
detector RetinaNet, DFFT_SMALL obtains over 5.5% AP gain while cutting down 70%
computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELUDE: Generating interpretable explanations via a decomposition into labelled and unlabelled features. (arXiv:2206.07690v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07690">
<div class="article-summary-box-inner">
<span><p>Deep learning models have achieved remarkable success in different areas of
machine learning over the past decade; however, the size and complexity of
these models make them difficult to understand. In an effort to make them more
interpretable, several recent works focus on explaining parts of a deep neural
network through human-interpretable, semantic attributes. However, it may be
impossible to completely explain complex models using only semantic attributes.
In this work, we propose to augment these attributes with a small set of
uninterpretable features. Specifically, we develop a novel explanation
framework ELUDE (Explanation via Labelled and Unlabelled DEcomposition) that
decomposes a model's prediction into two parts: one that is explainable through
a linear combination of the semantic attributes, and another that is dependent
on the set of uninterpretable features. By identifying the latter, we are able
to analyze the "unexplained" portion of the model, obtaining insights into the
information used by the model. We show that the set of unlabelled features can
generalize to multiple models trained with the same feature space and compare
our work to two popular attribute-oriented methods, Interpretable Basis
Decomposition and Concept Bottleneck, and discuss the additional insights ELUDE
provides.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids. (arXiv:2206.07695v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07695">
<div class="article-summary-box-inner">
<span><p>State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to
parameterize 3D radiance fields. While demonstrating impressive results,
querying an MLP for every sample along each ray leads to slow rendering.
Therefore, existing approaches often render low-resolution feature maps and
process them with an upsampling network to obtain the final image. Albeit
efficient, neural rendering often entangles viewpoint and content such that
changing the camera pose results in unwanted changes of geometry or appearance.
Motivated by recent results in voxel-based novel view synthesis, we investigate
the utility of sparse voxel grid representations for fast and 3D-consistent
generative modeling in this paper. Our results demonstrate that monolithic MLPs
can indeed be replaced by 3D convolutions when combining sparse voxel grids
with progressive growing, free space pruning and appropriate regularization. To
obtain a compact representation of the scene and allow for scaling to higher
voxel resolutions, our model disentangles the foreground object (modeled in 3D)
from the background (modeled in 2D). In contrast to existing approaches, our
method requires only a single forward pass to generate a full 3D scene. It
hence allows for efficient rendering from arbitrary viewpoints while yielding
3D consistent results with high visual fidelity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment Analysis in Videos. (arXiv:2206.07981v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07981">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis in videos is a key task in many real-world
applications, which usually requires integrating multimodal streams including
visual, verbal and acoustic behaviors. To improve the robustness of multimodal
fusion, some of the existing methods let different modalities communicate with
each other and modal the crossmodal interaction via transformers. However,
these methods only use the single-scale representations during the interaction
but forget to exploit multi-scale representations that contain different levels
of semantic information. As a result, the representations learned by
transformers could be biased especially for unaligned multimodal data. In this
paper, we propose a multi-scale cooperative multimodal transformer (MCMulT)
architecture for multimodal sentiment analysis. On the whole, the "multi-scale"
mechanism is capable of exploiting the different levels of semantic information
of each modality which are used for fine-grained crossmodal interactions.
Meanwhile, each modality learns its feature hierarchies via integrating the
crossmodal interactions from multiple level features of its source modality. In
this way, each pair of modalities progressively builds feature hierarchies
respectively in a cooperative manner. The empirical results illustrate that our
MCMulT model not only outperforms existing approaches on unaligned multimodal
sequences but also has strong performance on aligned multimodal sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07990">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised learning (SSL) methods have shown impressive results
in learning visual representations from unlabeled images. This paper aims to
improve their performance further by utilizing the architectural advantages of
the underlying neural network, as the current state-of-the-art visual pretext
tasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.
In particular, we focus on Vision Transformers (ViTs), which have gained much
attention recently as a better architectural choice, often outperforming
convolutional networks for various visual tasks. The unique characteristic of
ViT is that it takes a sequence of disjoint patches from an image and processes
patch-level representations internally. Inspired by this, we design a simple
yet effective visual pretext task, coined SelfPatch, for learning better
patch-level representations. To be specific, we enforce invariance against each
patch and its neighbors, i.e., each patch treats similar neighboring patches as
positive samples. Consequently, training ViTs with SelfPatch learns more
semantically meaningful relations among patches (without using human-annotated
labels), which can be beneficial, in particular, to downstream tasks of a dense
prediction type. Despite its simplicity, we demonstrate that it can
significantly improve the performance of existing SSL methods for various
visual tasks, including object detection and semantic segmentation.
Specifically, SelfPatch significantly improves the recent self-supervised ViT,
DINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance
segmentation, and +2.9 mIoU on ADE20K semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-20 23:07:54.896444222 UTC">2022-06-20 23:07:54 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>